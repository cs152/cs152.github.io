{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwB_R4qRZqli"
   },
   "source": [
    "# **Homework 4:** Automatic Differentiation and Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6aR8aa4Zqlk"
   },
   "source": [
    "### Collaborators\n",
    "\n",
    "Please list anyone you discussed or collaborated on this assignment with below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqLODUEJZqlk"
   },
   "source": [
    "LIST COLLABORATORS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLmN2xalZqll"
   },
   "source": [
    "### Course feedback\n",
    "\n",
    "Please submit this week's course survey here: https://forms.gle/PJjGy8wrhhQnAhbb6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5keiVoKpZqll"
   },
   "source": [
    "## Overview\n",
    "\n",
    "In this homework we will build a tiny neural network libarary from scratch and start on an implementation of automatic differentiation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gTaI5Rh_Zqll"
   },
   "outputs": [],
   "source": [
    "# Uncomment and run if using Colab!\n",
    "\n",
    "#import urllib.request\n",
    "#remote_url = 'https://gist.githubusercontent.com/gabehope/cb9e69f642104f107f25826a0931629a/raw/163f9cf5325db28826f4103d0f168702c77dfca1/hw4_support.py'\n",
    "#with urllib.request.urlopen(remote_url) as remote, open('hw4_support.py', 'w') as local:\n",
    "#  [local.write(str(line, encoding='utf-8')) for line in remote]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ri5sAl_TZqll"
   },
   "outputs": [],
   "source": [
    "# Run me first!\n",
    "from hw4_support import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpCXkg6IZqll"
   },
   "source": [
    "#### Python features\n",
    "This homework makes use of a few fancy features in Python that are worth knowing about if you are unfamiliar.\n",
    "- [Variable length arguments](https://book.pythontips.com/en/latest/args_and_kwargs.html) (e.g. `*args`)\n",
    "- [List comprehensions](https://book.pythontips.com/en/latest/comprehensions.html#list-comprehensions) (e.g. `[a**2 for a in range(5)]`)\n",
    "- [Magic methods](https://rszalski.github.io/magicmethods/) (e.g. `__add__`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiNW54HeZqll"
   },
   "source": [
    "## Part 1: Autograd\n",
    "\n",
    "In this homework we will be using a special version of Numpy from a package called `Autograd`. Assuming it is installed (`pip install autograd`), we can import it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DddV7le0Zqlm"
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ge3bsb7Zqlm"
   },
   "source": [
    "This special version of Numpy behaives exactly like normal numpy. We can create and do calculations with arrays just like we would before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cy7duWqTZqlm"
   },
   "outputs": [],
   "source": [
    "x = np.array([3., 2., 1])\n",
    "print('x:\\t', x)\n",
    "print('x^2:\\t', x ** 2)\n",
    "print('sum(x):\\t', np.sum(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KajOrrQUZqlo"
   },
   "source": [
    "However, Autograd also has a very important trick up its sleeve: it can take derivatives (and gradients) for us! This functionality can be accessed through the `grad` function. Let's start by seeing it in action with a very simple example, where we know the correct answer. The square function and its derivative can be written as:\n",
    "\n",
    "$f(x) = x^2, \\quad f'(x) = 2x$\n",
    "\n",
    "The following code uses Autograd to compute this derivative automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21MHj_EdZqlo"
   },
   "outputs": [],
   "source": [
    "from autograd import grad\n",
    "\n",
    "# Define a function\n",
    "def f(x):\n",
    "    return x ** 2\n",
    "\n",
    "# Use 'grad' to compute the derivative function\n",
    "f_prime = grad(f)\n",
    "\n",
    "# Verify that we get the correct answer\n",
    "x = 5.\n",
    "print('x:\\t', x)\n",
    "print('f(x):\\t', f(x))\n",
    "print(\"f'(x):\\t\", f_prime(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LueG6qy5Zqlo"
   },
   "source": [
    "We can start to see how `grad` operates. `grad` takes as input a function (e.g. $f(x)$) and returns a new function that computes the derivative of $f$ at $x$. ($f'(x)$). So:\n",
    "\n",
    "$\\text{grad}(f) \\longrightarrow f'$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaI4vR3jZqlo"
   },
   "source": [
    "#### **Q1:** Trying out `autograd`\n",
    "\n",
    "Define the following function in python:\n",
    "\n",
    "$f(x) = \\log(\\sin(x^3) + 3 x)$\n",
    "\n",
    "Use `grad` to compute the derivative of $f$ at $1.5$ (i.e. compute $f'(1.5)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQZGadTSZqlo"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "\n",
    "answer =\n",
    "print(\"f'(1.5)=\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LE9EEfT0Zqlp"
   },
   "source": [
    "As the name would imply, `grad` can more generally be used to compute the *gradient* of a function of the form $f(\\mathbf{x}): \\mathbb{R}^d\\rightarrow \\mathbb{R}$. Remember that for a function that takes in a vector and outputs a scalar, the gradient is vector of all partial derivatives of the output with respect to each input. For example, consider a function that gives the square of the 2-norm of a vector:\n",
    "\n",
    "$f(\\mathbf{x}) = ||\\mathbf{x}||^2_2 = \\mathbf{x}^T\\mathbf{x} = \\sum_{i=1}^d x_i^2$\n",
    "\n",
    "*Think about why these expressions are equivalent!*\n",
    "\n",
    "As we've seen, the gradient of this function can be written as:\n",
    "\n",
    "$\\nabla f(\\mathbf{x}) = 2\\mathbf{x} = \\begin{bmatrix}2x_1 \\\\ 2x_2 \\\\ \\vdots \\\\ 2x_d \\end{bmatrix}$\n",
    "\n",
    "Let's see what Autograd gives us in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4UNikxsZqlp"
   },
   "outputs": [],
   "source": [
    "# Define a function\n",
    "def f(x):\n",
    "    return np.sum(x ** 2)\n",
    "\n",
    "# Use 'grad' to compute the derivative function\n",
    "grad_f = grad(f)\n",
    "\n",
    "# Verify that we get the correct answer\n",
    "x = np.array([1., 2., 3])\n",
    "print('x:\\t', x)\n",
    "print('f(x):\\t', f(x))\n",
    "print(\"f'(x):\\t\", grad_f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZPvsCMKZqlp"
   },
   "source": [
    "We see that the gradient has the same shape as the input. So the gradient function is of the form: $\\mathbb{R}^d \\rightarrow \\mathbb{R}^d$\n",
    "\n",
    "This makes sense as the gradient should have exactly one partial derivative for each entry in the input to the function. As discussed, this even extends beyond vectors! We could have a function that takes in any datastructure and computes the set of partial derivatives with respect to each entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmPQ4NSHZqlp"
   },
   "source": [
    "#### **Q2:** More complex `autograd`\n",
    "\n",
    "Write a function that takes a `list` of vectors and computes the sum of the squared 2-norm for each vector. That is:\n",
    "\n",
    "$f([\\mathbf{a}, \\mathbf{b}, \\mathbf{c}...]) = ||\\mathbf{a}||^2 + ||\\mathbf{b}||^2 + ||\\mathbf{c}||^2+...$\n",
    "\n",
    "*Recall from above how we can compute each term in this sum!*\n",
    "\n",
    "Then use `grad` to compute the gradient of this function with respect to the given input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uO-1tcC6Zqlp"
   },
   "outputs": [],
   "source": [
    "# Define a function\n",
    "def f(x):\n",
    "    '''\n",
    "    Compute the sum of squared 2-norms for a list of vectors\n",
    "\n",
    "    Args:\n",
    "        x (list of arrays): A list of 1-dimensional arrays\n",
    "    Returns:\n",
    "        output (float): The result\n",
    "    '''\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "# Use 'grad' to compute the derivative function\n",
    "grad_f =\n",
    "\n",
    "# Verify that we get the correct answer\n",
    "x = [np.array([1., 2., 3]), np.array([7., 2.]), np.array([6.])]\n",
    "print('x:\\t', x)\n",
    "print('f(x):\\t', f(x))\n",
    "print(\"f'(x):\\t\", grad_f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyjHqmBVZqlp"
   },
   "source": [
    "A useful argument that we can give to `grad` is `argnum`. If our function takes more than one argument `argnum` lets us specify which one to take the gradient with respect to. For example, if we have the function:\n",
    "\n",
    "$f(x, y) = x^2y$\n",
    "\n",
    "Then:\n",
    "\n",
    "$f'_x(x,y)=2xy, \\quad f'_y(x, y)=x^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yvm6s_I5Zqlq"
   },
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    return x ** 2 * y\n",
    "\n",
    "print('f(3, 5) = ', f(3., 5.))\n",
    "\n",
    "df_dx = grad(f, argnum=0)(3., 5.)\n",
    "df_dy = grad(f, argnum=1)(3., 5.)\n",
    "\n",
    "print('df_dx = ', df_dx)\n",
    "print('df_dy = ', df_dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_Dxb1qeZqlq"
   },
   "source": [
    "Now that we have everything we need to apply automatic differentiation to train a neural network!\n",
    "\n",
    "Before we do that though, let's try out our automatic differentiation for logistic regression. Below is a slight modification of LogisticRegression implementation we saw in the last homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Gb7D_eoZqlq"
   },
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x):\n",
    "    # Computes the sigmoid function\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, dims):\n",
    "        '''\n",
    "        Args:\n",
    "            dims (int): d, the dimension of each input\n",
    "        '''\n",
    "        self.weights = np.zeros((dims + 1, 1))\n",
    "\n",
    "    def prediction_function(self, X, w):\n",
    "        '''\n",
    "        Get the result of our base function for prediction (i.e. x^t w)\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "            w (array): A (d+1) x 1 vector of weights.\n",
    "        Returns:\n",
    "            pred (array): A length N vector of f(X).\n",
    "        '''\n",
    "        X = np.pad(X, ((0,0), (0,1)), constant_values=1.)\n",
    "        return np.dot(X, w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict labels given a set of inputs.\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "        Returns:\n",
    "            pred (array): An N x 1 column vector of predictions in {0, 1}\n",
    "        '''\n",
    "        return (self.prediction_function(X, self.weights) > 0)\n",
    "\n",
    "    def predict_probability(self, X):\n",
    "        '''\n",
    "        Predict the probability of each class given a set of inputs\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "        Returns:\n",
    "            probs (array): An N x 1 column vector of predicted class probabilities\n",
    "        '''\n",
    "        return sigmoid(self.prediction_function(X, self.weights))\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        '''\n",
    "        Compute the accuracy of the model's predictions on a dataset\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "            y (array): A length N vector of labels.\n",
    "        Returns:\n",
    "            acc (float): The accuracy of the classifier\n",
    "        '''\n",
    "        y = y.reshape((-1, 1))\n",
    "        return (self.predict(X) == y).mean()\n",
    "\n",
    "    def nll(self, X, y, w=None):\n",
    "        '''\n",
    "        Compute the negative log-likelihood loss.\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "            y (array): A length N vector of labels.\n",
    "            w (array, optional): A (d+1) x 1 matrix of weights.\n",
    "        Returns:\n",
    "            nll (float): The NLL loss\n",
    "        '''\n",
    "        if w is None:\n",
    "            w = self.weights\n",
    "\n",
    "        y = y.reshape((-1, 1))\n",
    "        xw = self.prediction_function(X, w)\n",
    "        py = sigmoid((2 * y - 1) * xw)\n",
    "        return -(np.log(py)).sum()\n",
    "\n",
    "    def nll_gradient(self, X, y):\n",
    "        '''\n",
    "        Compute the gradient of the negative log-likelihood loss.\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "            y (array): A length N vector of labels.\n",
    "        Returns:\n",
    "            grad (array): A length (d + 1) vector with the gradient\n",
    "        '''\n",
    "        y = y.reshape((-1, 1))\n",
    "        xw = self.prediction_function(X, self.weights)\n",
    "        py = sigmoid((2 * y - 1) * xw)\n",
    "        grad = ((1 - py) * (2 * y - 1)).reshape((-1, 1)) * np.pad(X, [(0,0), (0,1)], constant_values=1.)\n",
    "        return -np.sum(grad, axis=0)\n",
    "\n",
    "    def nll_and_grad_no_autodiff(self, X, y):\n",
    "        # Compute nll_and_grad without automatic diferentiation\n",
    "        return self.nll(X, y), self.nll_gradient(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Dlko2mgZqlq"
   },
   "source": [
    "#### **Q3:** Logistic regression using `autograd`\n",
    "Write the method `nll_and_grad` for the LogisticRegression class using the `grad` function from Autograd. Verify that it gives a similar answer to `nll_and_grad_no_autodiff`.\n",
    "\n",
    "***Hint:** Note that the `nll` function can optionally take in the parameters. You can use this functionality and the `argnum` argument of `grad` in your answer. You can assume that `self` refers to the model object, so you can access the weights via `self.weights`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29888aXVZqlq"
   },
   "outputs": [],
   "source": [
    "def nll_and_grad(self, X, y):\n",
    "    ## YOUR CODE HERE\n",
    "    loss =\n",
    "    grads =\n",
    "    return loss, grads\n",
    "\n",
    "LogisticRegression.nll_and_grad = nll_and_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Feja9i8hZqlq"
   },
   "source": [
    "This implementation quite inefficient (we'll fix this in the future!), so we'll test our model on a very small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOZn-Y66Zqlq"
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(100, noise=0.1)\n",
    "model = LogisticRegression(2)\n",
    "gradient_descent(model, X, y, lr=3e-2, steps=250)\n",
    "\n",
    "print('Model accuracy: %.3f' % model.accuracy(X, y))\n",
    "plot_boundary(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Feature transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in class we dicussed *feature transforms* an easy way to get more expressive models, using our linear model tools. Here we'll try applying some basic feature transforms to this problem and see if we can improve the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q4:** Quadratic feature transforms\n",
    "\n",
    "Create a *transformed* versions of the training and test datasets by adding quadratic features. Only add the unary quadratic terms ($x_i^2$) **not** the cross terms ($x_i x_j$). For a single dimension the transform would look like:\n",
    "$$\\phi(x_i) = \\begin{bmatrix} x_i \\\\ x_i^2 \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "In general, the transform should look like:\n",
    "\n",
    "$$\\textbf{Single observation: }\\phi(\\mathbf{x}) = \\begin{bmatrix}x_1 \\\\ \\vdots \\\\ x_d \\\\ x_1^2 \\\\ \\vdots \\\\ x_d^2 \\end{bmatrix}, \\quad \\textbf{Dataset: } \\phi(\\mathbf{X}) = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1d} & x_{11}^2 & \\dots & x_{1d}^2 \\\\ x_{21} & x_{22} & \\dots & x_{2d} & x_{21}^2 & \\dots & x_{2d}^2 \\\\  \\vdots & \\vdots & & \\vdots & \\vdots & & \\vdots \\\\ x_{N1} & x_{N2} & \\dots & x_{Nd} & x_{N1}^2 & \\dots & x_{Nd}^2 \\\\  \\end{bmatrix} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "Xtrain_quad =\n",
    "Xtest_quad =\n",
    "\n",
    "assert Xtrain_quad.shape == (Xtrain.shape[0], 2 * Xtrain.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q5:** Evaluating quadratic transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a **new** logistic regression model and evaluate the training and test accuracy and loss as you did in question 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "print('Training accuracy: %.3f, loss: %.3f' % (train_acc, train_loss))\n",
    "print('Test accuracy: %.3f, loss: %.3f' % (test_acc, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q6:** Evaluating sin transforms\n",
    "\n",
    "Repeat questions 3 & 4, but using a different transform, defined as:\n",
    "\n",
    "$$\\phi(x_i) = \\begin{bmatrix} x_i \\\\ \\sin(10 x_i) \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "Xtrain_sin =\n",
    "Xtest_sin =\n",
    "\n",
    "assert Xtrain_sin.shape == (Xtrain.shape[0], 2 * Xtrain.shape[1])\n",
    "\n",
    "\n",
    "print('Training accuracy: %.3f, loss: %.3f' % (train_acc, train_loss))\n",
    "print('Test accuracy: %.3f, loss: %.3f' % (test_acc, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q7:** Comparing feature transforms\n",
    "\n",
    "Based on the results, would you use any feature transform for this problem? If so, which one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Implementing a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dT1spRbUZqlr"
   },
   "source": [
    "Now let's extend our model to be a neural network! We'll create a neural network class that extends our logistic regression class. First we'll setup the needed weight matrices.\n",
    "\n",
    "#### **Q8:** Initializing a neural network\n",
    "Fill in the Neural Network `__init__` method below. The method should take in the input data dimension and a list of integers specifying the size of each hidden layer (the number of neurons in each layer). The function should create a list of numpy arrays of the appropriate shapes for the weight matrices.\n",
    "\n",
    "For example if `dims` is `2` and `hidden_sizes` is `[4, 4]`, then `self.weights` should have 3 entries of shapes `[(4x2), (4x4), (1x4)]`. This network is shown below (may not show in colab)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCUqxxf2Zqlr"
   },
   "source": [
    "<div scale=0.5><svg xmlns=\"http://www.w3.org/2000/svg\" style=\"cursor: move;\" viewbox=\"100 100 1660 899\" width=\"600\" height=\"400\"><g transform=\"translate(-1450.305465592915,-694.5417988897334) scale(2.441893025338307)\"><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,469.5, 846.6666666666666,429.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,469.5, 846.6666666666666,469.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,469.5, 846.6666666666666,509.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,469.5, 846.6666666666666,549.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,509.5, 846.6666666666666,429.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,509.5, 846.6666666666666,469.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,509.5, 846.6666666666666,509.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,509.5, 846.6666666666666,549.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,429.5, 1026.6666666666667,429.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,429.5, 1026.6666666666667,469.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,429.5, 1026.6666666666667,509.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,469.5, 1026.6666666666667,429.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,469.5, 1026.6666666666667,469.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,469.5, 1026.6666666666667,509.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,509.5, 1026.6666666666667,429.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,509.5, 1026.6666666666667,469.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,509.5, 1026.6666666666667,509.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,549.5, 1026.6666666666667,429.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,549.5, 1026.6666666666667,469.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,549.5, 1026.6666666666667,509.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M1026.6666666666667,429.5, 1206.6666666666667,489.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M1026.6666666666667,469.5, 1206.6666666666667,489.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M1026.6666666666667,509.5, 1206.6666666666667,489.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,429.5, 1026.6666666666667,549.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,469.5, 1026.6666666666667,549.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,509.5, 1026.6666666666667,549.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,549.5, 1026.6666666666667,549.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M1026.6666666666667,549.5, 1206.6666666666667,489.5\"></path><circle r=\"10\" class=\"node\" id=\"0_0\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"666.6666666666666\" cy=\"469.5\"></circle><circle r=\"10\" class=\"node\" id=\"0_1\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"666.6666666666666\" cy=\"509.5\"></circle><circle r=\"10\" class=\"node\" id=\"1_0\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"846.6666666666666\" cy=\"429.5\"></circle><circle r=\"10\" class=\"node\" id=\"1_1\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"846.6666666666666\" cy=\"469.5\"></circle><circle r=\"10\" class=\"node\" id=\"1_2\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"846.6666666666666\" cy=\"509.5\"></circle><circle r=\"10\" class=\"node\" id=\"1_3\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"846.6666666666666\" cy=\"549.5\"></circle><circle r=\"10\" class=\"node\" id=\"2_0\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1026.6666666666667\" cy=\"429.5\"></circle><circle r=\"10\" class=\"node\" id=\"2_1\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1026.6666666666667\" cy=\"469.5\"></circle><circle r=\"10\" class=\"node\" id=\"2_2\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1026.6666666666667\" cy=\"509.5\"></circle><text class=\"text\" dy=\".35em\" style=\"font-size: 12px;\" x=\"631.6666666666666\" y=\"589.5\">Input Layer ∈ ℝ²</text><text class=\"text\" dy=\".35em\" style=\"font-size: 12px;\" x=\"811.6666666666666\" y=\"589.5\">Hidden Layer ∈ ℝ⁴</text><text class=\"text\" dy=\".35em\" style=\"font-size: 12px;\" x=\"991.6666666666667\" y=\"589.5\">Hidden Layer ∈ ℝ⁴</text><circle r=\"10\" class=\"node\" id=\"2_3\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1026.6666666666667\" cy=\"549.5\"></circle><circle r=\"10\" class=\"node\" id=\"3_0\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1206.6666666666667\" cy=\"489.5\"></circle><text class=\"text\" dy=\".35em\" style=\"font-size: 12px;\" x=\"1171.6666666666667\" y=\"589.5\">Output Layer ∈ ℝ¹</text></g><defs><marker id=\"arrow\" viewBox=\"0 -5 10 10\" markerWidth=\"7\" markerHeight=\"7\" orient=\"auto\" refX=\"40\"><path d=\"M0,-5L10,0L0,5\" style=\"stroke: rgb(80, 80, 80); fill: none;\"></path></marker></defs></svg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlsvHO1XZqlr"
   },
   "source": [
    "If you find it easier you could also define the weights in terms of $W^T$ instead, in which case the shapes would be: `[(2x4), (4x4), (4x1)]`. You could also consider how to add a bias term at each layer as in logistic regression (but this isn't nessecary for full credit).\n",
    "\n",
    "The values in each array should be drawn from a normal distribution with standard deviation 1. You can create such a matrix in numpy using:\n",
    "\n",
    "```\n",
    "np.random.normal(scale=1., size=shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASaMH4ODZqlr"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(LogisticRegression):\n",
    "    def __init__(self, dims, hidden_sizes=[]):\n",
    "        ## YOUR CODE HERE\n",
    "        self.weights =\n",
    "\n",
    "test_nn_constructor(NeuralNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArUPP-rwZqlr"
   },
   "source": [
    "Recall that for logistic regression the prediction function (before threholding or sigmoid) was $\\mathbf{X}\\mathbf{w}$. We now want to implement the prediction function for our neural network class. This function should perform the appropriate feature transforms and multiply by the regression weights. For a neural network with a single hidden layer this will look like:\n",
    "\n",
    "$f(\\mathbf{X}) = \\sigma(\\mathbf{X}\\mathbf{W}_1^T)\\mathbf{w}_0$\n",
    "\n",
    "Use the **sigmoid** activation function for this problem.\n",
    "\n",
    "For multiple layers we can also think of this a a **chain** of feature transforms:\n",
    "$$\\Phi_1 = \\sigma(\\mathbf{X}\\mathbf{W}_1^T)$$\n",
    "$$\\Phi_2 = \\sigma(\\Phi_1 \\mathbf{W}_2^T)$$\n",
    "$$...$$\n",
    "$$\\Phi_l = \\sigma(\\Phi_{l-1} \\mathbf{W}_l^T)$$\n",
    "$$f(\\mathbf{X}) = \\Phi_l\\mathbf{w}_0$$\n",
    "Where $\\Phi_i$ is just the variable that represents the neurons at layer $i$ (the result of the first $i$ transforms applied to $\\mathbf{X}$).\n",
    "\n",
    "\n",
    "#### **Q9:** Prediction function\n",
    "Implement the prediction function as described above. Note that the prediction function should use the weights passed into the `w` argument rather than `self.weights`, this will make it easier to implement the next question.\n",
    "\n",
    "***Hint:** Note that this function should **not** apply a final sigmoid or thresholding, instead it should be the equivalent of `linear_function` from the previous homework*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0i9BSdrMZqlr"
   },
   "outputs": [],
   "source": [
    "def prediction_function(self, X, w):\n",
    "    '''\n",
    "    Get the result of our base function for prediction (i.e. x^t w)\n",
    "\n",
    "    Args:\n",
    "        X (array): An N x d matrix of observations.\n",
    "        w (list of arrays): A list of weight matrices\n",
    "    Returns:\n",
    "        pred (array): An N x 1 matrix of f(X).\n",
    "    '''\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "    return pred.reshape((-1, 1))\n",
    "\n",
    "NeuralNetwork.prediction_function = prediction_function\n",
    "test_nn_prediction_function(NeuralNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2hPzrFdZqlr"
   },
   "source": [
    "#### **Q10:** Neural network loss\n",
    "Implement an `nll_and_grad` method for the `NeuralNetwork` class using Autograd to compute the gradient with respect to each weight matrix.\n",
    "\n",
    "***Hint:** If you use `np.pad` anywhere in your implementation, Autograd may complain if you don't include the keyword argument `mode='constant'`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UqIRXTRFZqlr"
   },
   "outputs": [],
   "source": [
    "def nll_and_grad(self, X, y):\n",
    "    '''\n",
    "    Get the negative log-likelihood loss and its gradient\n",
    "\n",
    "    Args:\n",
    "        X (array): An N x d matrix of observations.\n",
    "        y (array): A length N vector of labels\n",
    "    Returns:\n",
    "        nll (float): The negative log-likelihood\n",
    "        grads (list of arrays): A list of the gradient of the nll with respect\n",
    "                                to each value in self.weights.\n",
    "    '''\n",
    "    ## YOUR CODE HERE\n",
    "    loss =\n",
    "    grads =\n",
    "    return loss, grads\n",
    "\n",
    "NeuralNetwork.nll_and_grad = nll_and_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfqyawruZqlr"
   },
   "source": [
    "We now have everything in place to train a neural network from scratch! Let's try it on our tiny dataset. Feel free to change the inputs.\n",
    "\n",
    "***Hint**: If this give very poor results and/or runs very slowly, make sure to carefully check the shape of each operation in your code to make sure it matches your expectation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3pdQh1B6Zqls"
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(100, noise=0.1)\n",
    "model = NeuralNetwork(2, [5, 5])\n",
    "gradient_descent(model, X, y, lr=3e-2, steps=250)\n",
    "\n",
    "print('Model accuracy: %.3f' % model.accuracy(X, y))\n",
    "plot_boundary(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q11:** Comparison\n",
    "\n",
    "How does the neural network compare to explicit feature transforms in this case? How would you expect it to compare on other datasets?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
