{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 4:** Automatic Differentiation and Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborators\n",
    "\n",
    "Please list anyone you discussed or collaborated on this assignment with below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIST COLLABORATORS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course feedback\n",
    "\n",
    "Please submit this week's course survey here: https://forms.gle/PJjGy8wrhhQnAhbb6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this homework we will build a tiny automatic differentiation, matrix and neural network library from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run me first!\n",
    "from hw4_support import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python features\n",
    "This homework makes use of a few fancy features in Python that are worth knowing about if you are unfamiliar.\n",
    "- [Variable length arguments](https://book.pythontips.com/en/latest/args_and_kwargs.html) (e.g. `*args`)\n",
    "- [List comprehensions](https://book.pythontips.com/en/latest/comprehensions.html#list-comprehensions) (e.g. `[a**2 for a in range(5)]`)\n",
    "- [Magic methods](https://rszalski.github.io/magicmethods/) (e.g. `__add__`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Reverse-mode automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by developing an automatic differentiation class that uses *reverse-mode automatic differentiation*, as this is what will be most useful for neural networks. \n",
    "\n",
    "Recall that for reverse-mode AD to work, everytime we perform an operation on one or more numbers we need to store the result of that operation as well as the *parent values* (the inputs to the operation). We also need to be able to compute the derivative of that operation. Since for every operation we need to store several pieces of data and several functions, it makes sense to define a *class* to represent the result of an operation. \n",
    "\n",
    "For example, if we want to make a class that represents the operation `c=a+b` our class needs several properties:\n",
    "- `value`: The value of the operation (`c`)\n",
    "- `parents`: The parent operations (`a` and `b`)\n",
    "- `grad`: The derivative of the final loss with respect to `c` ($\\frac{dL}{dc}$)\n",
    "- `func`: A function that computes the operation (`a+b`)\n",
    "- `grads`: A function that computes the derivatives of the operation ($\\frac{dc}{da}$ and $\\frac{dc}{db}$)\n",
    "\n",
    "For this homework, we've provided the outline of such a class, called `AutogradValue`. This will be the base class for all of our possible operations and represents declaring a variable with a value (`a = 5`). This is useful because it lets us define values that we might want to find derivatives with respect to.\n",
    "\n",
    "Let's see how this will work in practice. If we want to take derivatives we will first define the inputs using `AutogradValue`.\n",
    "\n",
    "```{python}\n",
    "a = AutogradValue(5)\n",
    "b = AutogradValue(2)\n",
    "```\n",
    "Then we can perform whatever operations we want on these inputs:\n",
    "```{python}\n",
    "c = a + b\n",
    "L = log(c)\n",
    "```\n",
    "Each of these operations will produce a new `AutogradValue` object representing the result of that operation.\n",
    "\n",
    "Finally we can run the backward pass by running a method `backward()` (that we will write) on the outout `L`. This will compute the gradients of  `L` with respect to each input that we defined ($\\frac{dL}{da}$ and $\\frac{dL}{db}$). Rather than returning these derivatives, the `backward()` method will *update* the `grad` property of `a` and `b`, making it easy to access the correct derivative.\n",
    "\n",
    "```{python}\n",
    "L.backward()\n",
    "dL_da = a.grad\n",
    "```\n",
    "\n",
    "We'll also be able to compute operations with non-AutogradValue numbers, but obviously won't be able to compute derivaitives with respect to these values.\n",
    "```{python}\n",
    "s = 4\n",
    "L = s * a\n",
    "dL_da = a.grad # Will work because a is an AutogradValue\n",
    "dL_ds = s.grad # Will give an error because s is not an AutogradValue\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen what our final produce will look like, let's define our `AutogradValue` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutogradValue:\n",
    "    '''\n",
    "    Base class for automatic differentiation operations. Represents variable delcaration.\n",
    "    Subclasses will overwrite func and grads to define new operations.\n",
    "\n",
    "    Properties:\n",
    "        parents (list): A list of the inputs to the operation, may be AutogradValue or float\n",
    "        args    (list): A list of raw values of each input (as floats)\n",
    "        grad    (float): The derivative of the final loss with respect to this value (dL/da)\n",
    "        value   (float): The value of the result of this operation\n",
    "    '''\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        self.parents = list(args)\n",
    "        self.args = [arg.value if isinstance(arg, AutogradValue) else arg for arg in self.parents]\n",
    "        self.grad = 0.\n",
    "        self.value = self.forward_pass()\n",
    "\n",
    "    def func(self, input):\n",
    "        '''\n",
    "        Compute the value of the operation given the inputs.\n",
    "        For declaring a variable, this is just the identity function (return the input).\n",
    "\n",
    "        Args:\n",
    "            input (float): The input to the operation\n",
    "        Returns:\n",
    "            value (float): The result of the operation\n",
    "        '''\n",
    "        return input\n",
    "\n",
    "    def grads(self, *args):\n",
    "        '''\n",
    "        Compute the derivative of the operation with respect to each input.\n",
    "        In the base case the derivative of the identity function is just 1. (da/da = 1).\n",
    "\n",
    "        Args:\n",
    "            input (float): The input to the operation\n",
    "        Returns:\n",
    "            grads (tuple): The derivative of the operation with respect to each input\n",
    "                            Here there is only a single input, so we return a length-1 tuple.\n",
    "        '''\n",
    "        return (1,)\n",
    "    \n",
    "    def forward_pass(self):\n",
    "        # Calls func to compute the value of this operation \n",
    "        return self.func(*self.args)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        # Python magic function for string representation.\n",
    "        return str(self.value)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined the framework for an operation that can be used in automatic differentiation, we need to define some actual useful operations by subclassing `AutogradValue`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1\n",
    "Fill out the `func` and `grads` methods of each subclass below. Recall that `func` should always return the result of the operation and `grads` should always return a `tuple` of the derivative with respect to each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _add(AutogradValue):\n",
    "    # Addition operator (a + b)\n",
    "    def func(self, a, b):\n",
    "        return a + b\n",
    "    \n",
    "    def grads(self, a, b):\n",
    "        return 1., 1.\n",
    "    \n",
    "class _neg(AutogradValue):\n",
    "    # Negation operator (-a)\n",
    "    def func(self, a):\n",
    "        return -a\n",
    "    \n",
    "    def grads(self, a):\n",
    "        return (-1.,)\n",
    "\n",
    "class _sub(AutogradValue):\n",
    "    # Subtraction operator (a - b)\n",
    "    def func(self, a, b):\n",
    "        # Your code here\n",
    "    \n",
    "    def grads(self, a, b):\n",
    "        # Your code here\n",
    "\n",
    "class _mul(AutogradValue):\n",
    "    # Multiplication operator (a * b)\n",
    "    def func(self, a, b):\n",
    "        # Your code here\n",
    "    \n",
    "    def grads(self, a, b):\n",
    "        # Your code here\n",
    "\n",
    "class _div(AutogradValue):\n",
    "    # Division operator (a / b)\n",
    "    def func(self, a, b):\n",
    "        # Your code here\n",
    "    \n",
    "    def grads(self, a, b):\n",
    "        # Your code here\n",
    "    \n",
    "class _exp(AutogradValue):\n",
    "    # Exponent operator (e^a, or exp(a))\n",
    "    def func(self, a):\n",
    "        # Your code here\n",
    "    \n",
    "    def grads(self, a):\n",
    "        # Your code here\n",
    "\n",
    "class _log(AutogradValue):\n",
    "    # (Natural) log operator (log(a))\n",
    "    def func(self, a):\n",
    "        # Your code here\n",
    "    \n",
    "    def grads(self, a):\n",
    "        # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we'll define our basic functions and operators in terms of the operator classes we just wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp(a):\n",
    "    return _exp(a) if isinstance(a, AutogradValue) else math.exp(a)\n",
    "def log(a):\n",
    "    return _log(a) if isinstance(a, AutogradValue) else math.log(a)\n",
    "\n",
    "# Note: Remember that above we defined a class for each type of operation\n",
    "# so in this code we are overriding the basic operators for AutogradValue\n",
    "# such that they construct a new object of the class corresponding to the\n",
    "# given operation and return it. \n",
    "# (You don't need to everything that's happening here to do the HW)\n",
    "AutogradValue.exp = lambda a: _exp(a)\n",
    "AutogradValue.log = lambda a: _log(a)\n",
    "AutogradValue.__add__ = lambda a, b: _add(a, b)\n",
    "AutogradValue.__radd__ = lambda a, b: _add(b, a)\n",
    "AutogradValue.__sub__ = lambda a, b: _sub(a, b)\n",
    "AutogradValue.__rsub__ = lambda a, b: _sub(b, a)\n",
    "AutogradValue.__neg__ = lambda a: _neg(a)\n",
    "AutogradValue.__mul__ = lambda a, b: _mul(a, b)\n",
    "AutogradValue.__rmul__ = lambda a, b: _mul(b, a)\n",
    "AutogradValue.__truediv__ = lambda a, b: _div(a, b)\n",
    "AutogradValue.__rtruediv__ = lambda a, b: _div(b, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now be able to use our `AutogradValue` objects as if they are numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = AutogradValue(5)\n",
    "b = AutogradValue(2)\n",
    "\n",
    "print((a + 5) * b)\n",
    "print(log(b))\n",
    "\n",
    "test_operators(AutogradValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let confirm that we do keep the entire compuational graph for operations defined in this way. \n",
    "\n",
    "#### Q2\n",
    "Write a function `graph_print` that takes a single argument. If the argument is an `AutogradValue` (or one of its subclasses), print its `value` property and then call `graph_print` on each of its parents. If the argument is not an `AutogradValue`, just print it. The format of printing is not important.\n",
    "\n",
    "***Hint:** You can use the built-in Python function `isinstance` to determine if something is an `AutogradValue` or one of its subclasses. e.g. `isinstance(a, AutogradValue)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_print(a):\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "a = AutogradValue(5.)\n",
    "b = AutogradValue(2.)\n",
    "c = log((a + 5) * b)\n",
    "graph_print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function should print (it's ok if the numbers or order aren't exact):\n",
    "```\n",
    "2.995732273553991\n",
    "20.0\n",
    "10.0\n",
    "5.0\n",
    "5.0\n",
    "5\n",
    "2.0\n",
    "2.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in order to do automatic differentiation, we need to define how to do the backward pass. We'll start with the backward_step for a single operation.\n",
    "\n",
    "#### Q3\n",
    "Fill in the method `backward_pass` which computes a single step of the reverse pass through the computational graph (assume `self` is an `AutogradValue` instance). If `backward_pass` is called on a value `c`, the method should:\n",
    "- Assume that `self.grad` contains the derivaive of the final loss with respect to `c` ($\\frac{dL}{dc}$).\n",
    "- Check if each parent of `c`  is an `AutogradValue`. If it is, update that parent's `grad` property to account for `c` (e.g. for parent `a`, update the value of $\\frac{dL}{da}$)\n",
    "\n",
    "**For example:** if `c` represents the result of an addition so `c = a + b`,\n",
    "calling `backward_pass` on `c` will update the `grad` property of both `a` and `b`. (`a.grad` represents $\\frac{dL}{da}$ and is initialized to `0`).\n",
    "\n",
    "***Hint:** `grads` will be one of the methods we wrote in Q1. Recall that if `c` has parents `a` and `b` then `grads` method will give $\\frac{dc}{da}$ and $\\frac{dc}{db}$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(self):\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "AutogradValue.backward_pass = backward_pass\n",
    "\n",
    "# Test our implementation\n",
    "test_backward_pass(AutogradValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need to define the backward method itself. We will call this on the loss value to find the derivatives of the loss with respect to each input. This means working our way backward through the sequence of operations. Remember that if `c=a+b`, then if `c.grad` is $\\frac{dL}{dc}$, calling `backward_pass` on `c` will update $\\frac{dL}{da}$ (`a.grad`) and $\\frac{dL}{db}$ (`b.grad`).\n",
    " \n",
    "The complication is that `c` may be used in multiple operations, so we can't call `backward_pass` on `c` until we've called `backward_pass` on each child operation of `c` otherwise `c.grad` won't have the correct value of $\\frac{dL}{dc}$, as in this example:\n",
    "\n",
    "```{python}\n",
    "c = a + b\n",
    "g = c * 2\n",
    "h = c + 4\n",
    "L = g * h\n",
    "\n",
    "L.backward_pass() # Updates dL/dg and dL/dh\n",
    "h.backward_pass() # Updates dL/dc\n",
    "\n",
    "##WRONG ORDER\n",
    "c.backward_pass() # Incorrect because dL/dc hasn't accounted for dL/dg\n",
    "g.backward_pass()\n",
    "\n",
    "## CORRECT ORDER\n",
    "g.backward_pass() # Updates dL/dc\n",
    "c.backward_pass() # Updates dL/da and dL/db\n",
    "```\n",
    "\n",
    "#### Q4\n",
    "Fill in the `backward` method for `AutogradValue`. Your backward method should call `backward_pass` on each operation used to compute the loss (`self` is the loss value). Some important things to keep in mind:\n",
    "- `backward_pass` should only be called **once** on each operation\n",
    "- `backward_pass` must be called on **every child** of an operation before it can be called on the operation.\n",
    "- You should not try to call `backward_pass` on values that aren't instances of `AutogradValue`, even though this might be stored in `operation.parents`\n",
    "\n",
    "***Hint:** The efficiency of this method will have a large impact on the running time of later problems! We won't score efficiency in grading, but it still might be worth optimizing this function a bit.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self):\n",
    "    # We call backward on the loss, so dL/dL = 1\n",
    "    self.grad = 1.\n",
    "    \n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "AutogradValue.backward = backward\n",
    "# Test our implementation\n",
    "test_backward(AutogradValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our `AutogradValue` class to compute derivatives!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = AutogradValue(5)\n",
    "b = AutogradValue(2)\n",
    "\n",
    "L = -log(5 *b + a)\n",
    "L.backward()\n",
    "print(a.grad, b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to train a neural network using our automatic differentiation implementation, we're going to want to be able to use numpy to do matrix operations. Fortunately, the our `AutogradValue` class is (mostly) compatible with numpy!\n",
    "\n",
    "We can create arrays of `AutogradValue` and take derivatives as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([AutogradValue(5), AutogradValue(2)])\n",
    "L = np.dot(a, a)\n",
    "L.backward()\n",
    "print('Gradient for a', a[0].grad, a[1].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be a bit tedious to define every AutogradValue array in this way, so let's write some convinience functions to make doing automatic differentiation with numpy easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5\n",
    "Complete the following two functions `wrap_array` and `unwrap_gradient`. \n",
    "\n",
    "`wrap_array` should take a numpy array of floats and return a new array where every element has been made into an `AutogradValue`. \n",
    "\n",
    "`unwrap_gradient` should take a numpy array of `AutogradValue` and return a new array of floats, where every element is the extracted `grad` property of the corresponding element from the original array.\n",
    "\n",
    "***Hint:** You can create an array from nested lists as shown above.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_array(a):\n",
    "    '''\n",
    "    Wraps the elements of an array with AutogradValue\n",
    "\n",
    "    Args:\n",
    "        a (array of float): The array to wrap\n",
    "    Returns:\n",
    "        g (array of AutogradValue): An array g, such that g[i,j] = AutogradValue(a[i,j])\n",
    "    '''\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "def unwrap_gradient(a):\n",
    "    '''\n",
    "    Unwraps the gradient of an array with AutogradValues\n",
    "\n",
    "    Args:\n",
    "        a (array of AutogradValue): The array to unwrap\n",
    "    Returns:\n",
    "        g (array of float): An array g, such that g[i,j] = a[i,j].grad\n",
    "    '''\n",
    "    ## YOUR CODE HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementing a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have everything we need to apply our automatic differentiation to train a neural network!\n",
    "\n",
    "Before we do that though, let's try out our automatic differentiation for logistic regression. Below is a slight modification of LogisticRegression implementation we saw in the last homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x):\n",
    "    # Computes the sigmoid function\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, dims):\n",
    "        '''\n",
    "        Args:\n",
    "            dims (int): d, the dimension of each input\n",
    "        '''\n",
    "        self.weights = np.zeros((dims + 1, 1))\n",
    "\n",
    "    def prediction_function(self, X, w):\n",
    "        '''\n",
    "        Get the result of our base function for prediction (i.e. x^t w)\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "            w (array): A (d+1) vector of weights.\n",
    "        Returns:\n",
    "            pred (array): A length N vector of f(X).\n",
    "        '''\n",
    "        X = np.pad(X, ((0,0), (0,1)), constant_values=1.)\n",
    "        return np.dot(X, w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict labels given a set of inputs.\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "        Returns:\n",
    "            pred (array): A length N vector of predictions in {0, 1}\n",
    "        '''\n",
    "        return (self.prediction_function(X, self.weights) > 0)\n",
    "    \n",
    "    def predict_probability(self, X):\n",
    "        '''\n",
    "        Predict the probability of each class given a set of inputs\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "        Returns:\n",
    "            probs (array): A length N vector of predicted class probabilities\n",
    "        '''\n",
    "        return sigmoid(self.prediction_function(X, self.weights))\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        '''\n",
    "        Compute the accuracy of the model's predictions on a dataset\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "            y (array): A length N vector of labels.\n",
    "        Returns:\n",
    "            acc (float): The accuracy of the classifier\n",
    "        '''\n",
    "        return (self.predict(X) == y).mean()\n",
    "\n",
    "    def nll(self, X, y, w=None):\n",
    "        '''\n",
    "        Compute the negative log-likelihood loss.\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "            y (array): A length N vector of labels.\n",
    "            w (array, optional): A (d+1) x 1 matrix of weights.\n",
    "        Returns:\n",
    "            nll (float): The NLL loss\n",
    "        '''\n",
    "        if w is None:\n",
    "            w = self.weights\n",
    "\n",
    "        xw = self.prediction_function(X, w)\n",
    "        py = sigmoid((2 * y - 1) * xw)\n",
    "        return -(np.log(py)).sum()\n",
    "    \n",
    "    def nll_gradient(self, X, y):\n",
    "        '''\n",
    "        Compute the gradient of the negative log-likelihood loss.\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "            y (array): A length N vector of labels.\n",
    "        Returns:\n",
    "            grad (array): A length (d + 1) vector with the gradient\n",
    "        '''\n",
    "        xw = self.prediction_function(X, self.weights)\n",
    "        py = sigmoid((2 * y - 1) * xw)\n",
    "        grad = ((1 - py) * (2 * y - 1)).reshape((-1, 1)) * np.pad(X, [(0,0), (0,1)], constant_values=1.)\n",
    "        return -np.sum(grad, axis=0)\n",
    "    \n",
    "    def nll_and_grad_no_autodiff(self, X, y):\n",
    "        # Compute nll_and_grad without automatic diferentiation\n",
    "        return self.nll(X, y), self.nll_gradient(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6 \n",
    "Write the method `nll_and_grad` for the LogisticRegression class using the automatic differentiation tools we built above. Verify that it gives a similar answer to `nll_and_grad_no_autodiff`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_and_grad(self, X, y):\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "LogisticRegression.nll_and_grad = nll_and_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our automatic differentiation is very inefficient (we'll fix this in the next homework!), so we'll test our model on a very small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(100, noise=0.1)\n",
    "model = LogisticRegression(2)\n",
    "gradient_descent(model, X, y, lr=3e-2, steps=250)\n",
    "\n",
    "print('Model accuracy: %.3f' % model.accuracy(X, y))\n",
    "plot_boundary(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extend our model to be a neural network! We'll create a neural network class that extends our logistic regression class. First we'll setup the needed weight matrices.\n",
    "\n",
    "#### Q7\n",
    "Fill in the Neural Network `__init__` method below. The method should take in the input data dimension and a list of integers specifying the size of each hidden layer (the number of neurons in each layer). The function should create a list of numpy arrays of the appropriate shapes for the weight matrices. \n",
    "\n",
    "For example if `dims` is `2` and `hidden_sizes` is `[4, 4]`, then `self.weights` should have 3 entries of shapes `[(4x2), (4x4), (4,)]`. This network is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div scale=0.5><svg xmlns=\"http://www.w3.org/2000/svg\" style=\"cursor: move;\" viewbox=\"100 100 1660 899\" width=\"600\" height=\"400\"><g transform=\"translate(-1450.305465592915,-694.5417988897334) scale(2.441893025338307)\"><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,469.5, 846.6666666666666,429.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,469.5, 846.6666666666666,469.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,469.5, 846.6666666666666,509.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,469.5, 846.6666666666666,549.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,509.5, 846.6666666666666,429.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,509.5, 846.6666666666666,469.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,509.5, 846.6666666666666,509.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,509.5, 846.6666666666666,549.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,429.5, 1026.6666666666667,429.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,429.5, 1026.6666666666667,469.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,429.5, 1026.6666666666667,509.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,469.5, 1026.6666666666667,429.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,469.5, 1026.6666666666667,469.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,469.5, 1026.6666666666667,509.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,509.5, 1026.6666666666667,429.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,509.5, 1026.6666666666667,469.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,509.5, 1026.6666666666667,509.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,549.5, 1026.6666666666667,429.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,549.5, 1026.6666666666667,469.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,549.5, 1026.6666666666667,509.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M1026.6666666666667,429.5, 1206.6666666666667,489.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M1026.6666666666667,469.5, 1206.6666666666667,489.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M1026.6666666666667,509.5, 1206.6666666666667,489.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,429.5, 1026.6666666666667,549.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,469.5, 1026.6666666666667,549.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,509.5, 1026.6666666666667,549.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,549.5, 1026.6666666666667,549.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M1026.6666666666667,549.5, 1206.6666666666667,489.5\"></path><circle r=\"10\" class=\"node\" id=\"0_0\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"666.6666666666666\" cy=\"469.5\"></circle><circle r=\"10\" class=\"node\" id=\"0_1\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"666.6666666666666\" cy=\"509.5\"></circle><circle r=\"10\" class=\"node\" id=\"1_0\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"846.6666666666666\" cy=\"429.5\"></circle><circle r=\"10\" class=\"node\" id=\"1_1\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"846.6666666666666\" cy=\"469.5\"></circle><circle r=\"10\" class=\"node\" id=\"1_2\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"846.6666666666666\" cy=\"509.5\"></circle><circle r=\"10\" class=\"node\" id=\"1_3\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"846.6666666666666\" cy=\"549.5\"></circle><circle r=\"10\" class=\"node\" id=\"2_0\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1026.6666666666667\" cy=\"429.5\"></circle><circle r=\"10\" class=\"node\" id=\"2_1\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1026.6666666666667\" cy=\"469.5\"></circle><circle r=\"10\" class=\"node\" id=\"2_2\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1026.6666666666667\" cy=\"509.5\"></circle><text class=\"text\" dy=\".35em\" style=\"font-size: 12px;\" x=\"631.6666666666666\" y=\"589.5\">Input Layer ∈ ℝ²</text><text class=\"text\" dy=\".35em\" style=\"font-size: 12px;\" x=\"811.6666666666666\" y=\"589.5\">Hidden Layer ∈ ℝ⁴</text><text class=\"text\" dy=\".35em\" style=\"font-size: 12px;\" x=\"991.6666666666667\" y=\"589.5\">Hidden Layer ∈ ℝ⁴</text><circle r=\"10\" class=\"node\" id=\"2_3\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1026.6666666666667\" cy=\"549.5\"></circle><circle r=\"10\" class=\"node\" id=\"3_0\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1206.6666666666667\" cy=\"489.5\"></circle><text class=\"text\" dy=\".35em\" style=\"font-size: 12px;\" x=\"1171.6666666666667\" y=\"589.5\">Output Layer ∈ ℝ¹</text></g><defs><marker id=\"arrow\" viewBox=\"0 -5 10 10\" markerWidth=\"7\" markerHeight=\"7\" orient=\"auto\" refX=\"40\"><path d=\"M0,-5L10,0L0,5\" style=\"stroke: rgb(80, 80, 80); fill: none;\"></path></marker></defs></svg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you find it easier you could also define the weights in terms of $W^T$ instead, in which case the shapes would be: `[(2x4), (4x4), (4,)]`.\n",
    "\n",
    "The values in each array should be drawn from a normal distribution with standard deviation 0.01. You can create such a matrix in numpy using:\n",
    "\n",
    "```\n",
    "np.random.normal(scale=0.01, size=shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(LogisticRegression):\n",
    "    def __init__(self, dims, hidden_sizes=[]):\n",
    "        ## YOUR CODE HERE\n",
    "        self.weights = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that for logistic regression the prediction function (before threholding or sigmoid) was $\\mathbf{X}\\mathbf{w}$. We now want to implement the prediction function for our neural network class. This function should perform the appropriate feature transforms and multiply by the regression weights. For a neural network with a single hidden layer this will look like:\n",
    "\n",
    "$f(\\mathbf{X}) = \\sigma(\\mathbf{X}\\mathbf{W}_1^T)\\mathbf{w}_0$\n",
    "\n",
    "Use the **sigmoid** activation function for this problem.\n",
    "\n",
    "For multiple layers we can also think of this a a **chain** of feature transforms:\n",
    "$$\\Phi_1 = \\sigma(\\mathbf{X}\\mathbf{W}_1^T)$$\n",
    "$$\\Phi_2 = \\sigma(\\Phi_1 \\mathbf{W}_2^T)$$\n",
    "$$...$$\n",
    "$$\\Phi_l = \\sigma(\\Phi_{l-1} \\mathbf{W}_l^T)$$\n",
    "$$f(\\mathbf{X}) = \\Phi_l\\mathbf{w}_0$$\n",
    "Where $\\Phi_i$ is just the variable that represents the neurons at layer $i$ (the result of the first $i$ transforms applied to $\\mathbf{X}$).\n",
    "\n",
    "#### Q8 \n",
    "Implement the prediction function as described above. Note that the prediction function should use the weights passed into the `w` argument rather than `self.weights`, this will make it easier to implement the next question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_function(self, X, w):\n",
    "    '''\n",
    "    Get the result of our base function for prediction (i.e. x^t w)\n",
    "\n",
    "    Args:\n",
    "        X (array): An N x d matrix of observations.\n",
    "        w (list of arrays): A list of weight matrices\n",
    "    Returns:\n",
    "        pred (array): An N x 1 matrix of f(X).\n",
    "    '''\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "\n",
    "NeuralNetwork.prediction_function = prediction_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q9\n",
    "Implement an `nll_and_grad` method for the `NeuralNetwork` class using your automatic differentiation implmentation to compute the gradient with respect to each weight matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_and_grad(self, X, y):\n",
    "    '''\n",
    "    Get the negative log-likelihood loss and its gradient\n",
    "\n",
    "    Args:\n",
    "        X (array): An N x d matrix of observations.\n",
    "        y (array): A length N vector of labels\n",
    "    Returns:\n",
    "        nll (float): The negative log-likelihood\n",
    "        grads (list of arrays): A list of the gradient of the nll with respect\n",
    "                                to each value in self.weights.\n",
    "    '''\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "NeuralNetwork.nll_and_grad = nll_and_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything in place to train a neural network from scratch! Let's try it on our tiny dataset. Feel free to change the inputs.\n",
    "\n",
    "***Hint**: If this give very poor results and/or runs very slowly, make sure to carefully check the shape of each operation in your code to make sure it matches your expectation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(100, noise=0.1)\n",
    "model = NeuralNetwork(2, [5, 5])\n",
    "gradient_descent(model, X, y, lr=3e-2, steps=250)\n",
    "\n",
    "print('Model accuracy: %.3f' % model.accuracy(X, y))\n",
    "plot_boundary(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Forward-mode automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try out the other type of automatic differentiation that we learned about: *forward-mode*. To do this we'll create a subclass of our `AutogradValue` class called `ForwardValue`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that for this version of automatic differentiaion each operation needs to keep track of the derivative of it's value with respect *each* original input. For example, in the code below the `g` object needs to store both $\\frac{dg}{da}$ *and* $\\frac{dg}{db}$.\n",
    "\n",
    "```\n",
    "a = ForwardValue(5)\n",
    "b = ForwardValue(2)\n",
    "c = a + b\n",
    "g = c * 2\n",
    "```\n",
    "\n",
    "Our `ForwardValue` class will maintain a `dict` called `forward_grads` that maps each original input object to the derivative of the current value with respect to that input (so `g` will have a `dict` with keys that are the `a` and `b` **objects**). \n",
    "\n",
    "In the base case, when we declare a variable the derivative with respect to itself is just 1 ($\\frac{da}{da}=1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardValue(AutogradValue):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__(*args)\n",
    "        self.forward_grads = {self: 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see now that our `forward_pass` method needs to update `forward_grads` (e.g. to compute $\\frac{dg}{da}$ and $\\frac{dg}{db}$) using the `forward_grads` values of its parents (e.g. $\\frac{dc}{da}$ and $\\frac{dc}{db}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q10\n",
    "Implement the `forward_pass` method below for forward-mode automatic differentiation. This method should update the `foward_grads` property of the operation such that:\n",
    "- `foward_grads` has an entry for every input that appears in `foward_grads` of *any* parent operation.\n",
    "- If an input appears in more than 1 parent, make sure to *add* the gradients appropritately (if `g` has parents `b` and `c` then $\\frac{dg}{da} = \\frac{dg}{db}\\frac{db}{da} + \\frac{dg}{dc}\\frac{dc}{da}$  )\n",
    "- Parents that are not `AutogradValue` objects are ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(self):\n",
    "    # Clear forward_grads if it exists\n",
    "    self.forward_grads = {} \n",
    "    ## YOUR CODE HERE\n",
    "    \n",
    "\n",
    "    # Make sure to still return the operation's value\n",
    "    return self.func(*self.args)\n",
    "\n",
    "# Overwrite the AutogradValue method so that operators still work\n",
    "AutogradValue.forward_pass = forward_pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take derivates of functions much like with our reverse-mode implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ForwardValue(5)\n",
    "b = ForwardValue(2)\n",
    "\n",
    "L = -log(5 *b + a)\n",
    "print(L.forward_grads[a], L.forward_grads[b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q11\n",
    "\n",
    "Complete the `ForwardModeNeuralNetwork` class that inherits from `NeuralNetwork` by implementing `nll_and_grad` to use the forward-mode implementation you just wrote!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardModeNeuralNetwork(NeuralNetwork):\n",
    "    def nll_and_grad(self, X, y):\n",
    "        ## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again test it on our tiny dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(100, noise=0.1)\n",
    "model = ForwardModeNeuralNetwork(2, [5, 5])\n",
    "gradient_descent(model, X, y, lr=3e-2, steps=250)\n",
    "\n",
    "print('Model accuracy: %.3f' % model.accuracy(X, y))\n",
    "plot_boundary(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q12\n",
    "Based on what we've learned and your experience here, why would we choose forward or reverse-mode automatic differentiation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
