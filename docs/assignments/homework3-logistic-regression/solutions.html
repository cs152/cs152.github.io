<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.319">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>CS 152: Neural Networks - Homework 3: Logistic regression and feature transforms</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">CS 152: Neural Networks</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../calendar/calendar.html" rel="" target="">
 <span class="menu-text">Calendar</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/orgs/CS152-Neural-Networks-Fall-2023/repositories" rel="" target="">
 <span class="menu-text">Homeworks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://harveymuddcollege.instructure.com/courses/615/" rel="" target="">
 <span class="menu-text">Canvas</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/orgs/CS152-Neural-Networks-Fall-2023/" rel="" target="">
 <span class="menu-text">Github</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#background" id="toc-background" class="nav-link active" data-scroll-target="#background">Background</a></li>
  <li><a href="#part-1-logistic-regression-and-feature-transforms" id="toc-part-1-logistic-regression-and-feature-transforms" class="nav-link" data-scroll-target="#part-1-logistic-regression-and-feature-transforms">Part 1: Logistic regression and feature transforms</a></li>
  <li><a href="#part-2-multinomial-logistic-regression" id="toc-part-2-multinomial-logistic-regression" class="nav-link" data-scroll-target="#part-2-multinomial-logistic-regression">Part 2: Multinomial logistic regression</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><strong>Homework 3:</strong> Logistic regression and feature transforms</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>In this homework, we will use the following convention for dimentionality:</p>
<p><span class="math inline">\(N:\quad\text{Number of observations in a dataset, so } \mathcal{D} = \{ (\mathbf{x}_1, y_1),\ (\mathbf{x}_2, y_2),\ ... \,(\mathbf{x}_N, y_N) \}\)</span></p>
<p><span class="math inline">\(d:\quad\ \text{Dimension of input (number of features), so } \mathbf{x}_i \in \mathbb{R}^d\)</span></p>
<p><span class="math inline">\(C: \quad\ \text{Number of classes, so } y_i \in \{1,...,C\}\)</span></p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Run me first!</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> hw2_code_backup <span class="im">import</span> get_dataset, gradient_descent, test_nll, test_nll_grad, test_predict, test_predict_probability, test_softmax, test_split</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># UNCOMMENT IF NEEDED, SEE Q10</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> autograd.numpy <span class="im">as</span> np</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="background" class="level1">
<h1>Background</h1>
<p>In class we derived the logistic regression model for making predictions on binary data. Recall the the prediction function for logistic regression can be written as:</p>
<p><span class="math display">\[f(\mathbf{x}) = \mathbb{I}(\mathbf{x}^T\mathbf{w} \geq 0)\]</span></p>
<p>The estimated probability of <span class="math inline">\(y=1\)</span> as: <span class="math display">\[p(y=1\mid \mathbf{x}, \mathbf{w}) = \sigma(\mathbf{x}^T\mathbf{w})\]</span></p>
<p>Also recall that the negative log-likelihood loss for logistic regression can be written as:</p>
<p><span class="math display">\[\mathbf{NLL}(\mathbf{w}, \mathbf{X}, \mathbf{y})=-\sum_{i=1}^N \log \sigma\big((2 y_i - 1)\mathbf{x}_i^T\mathbf{w}\big)\]</span></p>
<p>and it’s gradient with respect to <span class="math inline">\(\mathbf{w}\)</span> is: <span class="math display">\[\nabla_{\mathbf{w}}\mathbf{NLL}(\mathbf{w}, \mathbf{X}, \mathbf{y})=-\sum_{i=1}^N \big(1 - \sigma((2 y_i - 1)\mathbf{x}_i^T\mathbf{w})\big) \big(2 y_i - 1\big)\mathbf{x}_i\]</span></p>
<p>Below is an implementation of logistic regression using the functions we derived in class. In this example, we’ve created a logistic regression class that encapsulates the weights along with all of the functions we need to train and make predictions with the model.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> linear_function(X, w):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Returns a linear function of X (and adds bias)</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.pad(X, ((<span class="dv">0</span>,<span class="dv">0</span>), (<span class="dv">0</span>,<span class="dv">1</span>)), constant_values<span class="op">=</span><span class="fl">1.</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.dot(X, w)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x):</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Computes the sigmoid function</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LogisticRegression:</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dims):</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="co">'''</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co">            dims (int): d, the dimension of each input</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="co">        '''</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> np.zeros((dims <span class="op">+</span> <span class="dv">1</span>,))</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>        <span class="co">'''</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="co">        Predict labels given a set of inputs.</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="co">            X (array): An N x d matrix of observations.</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="co">            pred (int array): A length N array of predictions in {0, 1}</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="co">        '''</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (linear_function(X, <span class="va">self</span>.weights) <span class="op">&gt;</span> <span class="dv">0</span>).astype(<span class="bu">int</span>)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_probability(<span class="va">self</span>, X):</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        <span class="co">'''</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="co">        Predict the probability of each class given a set of inputs</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a><span class="co">            X (array): An N x d matrix of observations.</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a><span class="co">            probs (array): A length N vector of predicted class probabilities</span></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a><span class="co">        '''</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> sigmoid(linear_function(X, <span class="va">self</span>.weights))</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> accuracy(<span class="va">self</span>, X, y):</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>        <span class="co">'''</span></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a><span class="co">        Compute the accuracy of the model's predictions on a dataset</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a><span class="co">            X (array): An N x d matrix of observations.</span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a><span class="co">            y (array): A length N vector of labels.</span></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a><span class="co">            acc (float): The accuracy of the classifier</span></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a><span class="co">        '''</span></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.mean(<span class="va">self</span>.predict(X) <span class="op">==</span> y)</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> nll(<span class="va">self</span>, X, y):</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>        <span class="co">'''</span></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a><span class="co">        Compute the negative log-likelihood loss.</span></span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a><span class="co">            X (array): An N x d matrix of observations.</span></span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a><span class="co">            y (int array): A length N vector of labels.</span></span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a><span class="co">            nll (float): The NLL loss</span></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a><span class="co">        '''</span></span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>        xw <span class="op">=</span> linear_function(X, <span class="va">self</span>.weights)</span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>        py <span class="op">=</span> sigmoid((<span class="dv">2</span> <span class="op">*</span> y <span class="op">-</span> <span class="dv">1</span>) <span class="op">*</span> xw)</span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="op">-</span>np.<span class="bu">sum</span>(np.log(py))</span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> nll_gradient(<span class="va">self</span>, X, y):</span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>        <span class="co">'''</span></span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a><span class="co">        Compute the gradient of the negative log-likelihood loss.</span></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a><span class="co">            X (array): An N x d matrix of observations.</span></span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a><span class="co">            y (array): A length N vector of labels.</span></span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a><span class="co">            grad (array): A length (d + 1) vector with the gradient</span></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a><span class="co">        '''</span></span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>        xw <span class="op">=</span> linear_function(X, <span class="va">self</span>.weights)</span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>        py <span class="op">=</span> sigmoid((<span class="dv">2</span> <span class="op">*</span> y <span class="op">-</span> <span class="dv">1</span>) <span class="op">*</span> xw)</span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>        grad <span class="op">=</span> ((<span class="dv">1</span> <span class="op">-</span> py) <span class="op">*</span> (<span class="dv">2</span> <span class="op">*</span> y <span class="op">-</span> <span class="dv">1</span>)).reshape((<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)) <span class="op">*</span> np.pad(X, [(<span class="dv">0</span>,<span class="dv">0</span>), (<span class="dv">0</span>,<span class="dv">1</span>)], constant_values<span class="op">=</span><span class="fl">1.</span>)</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="op">-</span>np.<span class="bu">sum</span>(grad, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> nll_and_grad(<span class="va">self</span>, X, y):</span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>        <span class="co">'''</span></span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a><span class="co">        Compute both the NLL and it's gradient</span></span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a><span class="co">            X (array): An N x d matrix of observations.</span></span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a><span class="co">            y (array): A length N vector of labels.</span></span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a><span class="co">            nll (float): The NLL loss</span></span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a><span class="co">            grad (array): A length (d + 1) vector with the gradient</span></span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a><span class="co">        '''</span></span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.nll(X, y), <span class="va">self</span>.nll_gradient(X, y)</span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s take a look at how to use this class. The provided code includes a function <code>get_dataset</code> that downloads and loads one of serval different datasets. For this first example, we will use the humans and horses dataset, a dataset of images of humans and horses. We can load the dataset as follows:</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>images, labels, label_names <span class="op">=</span> get_dataset(<span class="st">'horses_and_humans'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-4-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>As we saw in class, before we can use logistic regression on image data, we first need to reshape it from a 3-dimensional array into a 2-dimensional matrix:</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>image_shape <span class="op">=</span> images[<span class="dv">0</span>].shape                <span class="co"># Keep track of the original image shape</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> images.reshape((images.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>))    <span class="co"># Reshape into an N x d matrix X</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> labels</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Image shape: '</span>, images.shape, <span class="st">', X shape: '</span>, X.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Image shape:  (1027, 48, 48) , X shape:  (1027, 2304)</code></pre>
</div>
</div>
<p>We can create a model using the <code>LogisticRegression</code> class, specifying the number of features (<span class="math inline">\(d\)</span>):</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(X.shape[<span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can train the model using the <code>gradient_descent</code> function provided in the support code:</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> gradient_descent(model, X, y, lr<span class="op">=</span><span class="fl">1e-6</span>, steps<span class="op">=</span><span class="dv">2500</span>, image_shape<span class="op">=</span>image_shape, watch<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Uncomment to run with a live visualization</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># losses = gradient_descent(model, X, y, lr=1e-6, steps=500, image_shape=images[0].shape, watch=True)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loss 711.86, accuracy: 0.57:   0%|          | 0/2500 [00:00&lt;?, ?it/s]Loss 358.28, accuracy: 0.86: 100%|██████████| 2500/2500 [00:48&lt;00:00, 51.11it/s]</code></pre>
</div>
</div>
<p>We can make predictions using the built-in methods:</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>prediction <span class="op">=</span> model.predict(X)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>probabilities <span class="op">=</span> model.predict_probability(X)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the probability of the prediction [p(y=1) if prediction is 1 otherwise p(y=0)]</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>probability_of_prediction <span class="op">=</span> np.where(prediction, probabilities, <span class="dv">1</span> <span class="op">-</span> probabilities)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Show an image and the corresponding prediction</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>plt.imshow(X[<span class="dv">0</span>].reshape(image_shape), cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Prediction: </span><span class="sc">%s</span><span class="st">, probability: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> (label_names[prediction[<span class="dv">0</span>]], probability_of_prediction[<span class="dv">0</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Prediction: human, probability: 0.641</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-8-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="part-1-logistic-regression-and-feature-transforms" class="level1">
<h1>Part 1: Logistic regression and feature transforms</h1>
<p>We’ll first evaluate the performance of the logistic regression model above.</p>
<section id="q1-train-and-test-splits" class="level4">
<h4 class="anchored" data-anchor-id="q1-train-and-test-splits"><strong>Q1:</strong> Train and test splits</h4>
<p>Write a function to split the provided dataset into a <em>train set</em> and a <em>test set</em>. The train set should include 70% of the observations and the test set should include the remaining 30%. The data should be randomly shuffled to make sure there is no bias in the ordering.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> split_data(X, y):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    inds <span class="op">=</span> np.arange(X.shape[<span class="dv">0</span>])</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    np.random.shuffle(inds)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    Xtrain, Xtest <span class="op">=</span> X[inds[:<span class="dv">700</span>]], X[inds[<span class="dv">700</span>:]]</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    ytrain, ytest <span class="op">=</span> y[inds[:<span class="dv">700</span>]], y[inds[<span class="dv">700</span>:]]</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Xtrain, ytrain, Xtest, ytest</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Test the function</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>Xtrain, ytrain, Xtest, ytest <span class="op">=</span> split_data(X, y)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>test_split(X, y, Xtrain, ytrain, Xtest, ytest)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Passed!</code></pre>
</div>
</div>
</section>
<section id="q2-model-evaluation" class="level4">
<h4 class="anchored" data-anchor-id="q2-model-evaluation"><strong>Q2:</strong> Model evaluation</h4>
<p>Using the function you just wrote, train a new logistic regression model on just the training data. Evaluate the <strong>accuracy</strong> and <strong>loss</strong> of the trained model on <em>both</em> the <strong>training data</strong> and the <strong>test data</strong>.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co">## YOUR CODE HERE</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">10</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>Xtrain, ytrain, Xtest, ytest <span class="op">=</span> split_data(X, y)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(X.shape[<span class="dv">1</span>])</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> gradient_descent(model, Xtrain, ytrain, lr<span class="op">=</span><span class="fl">1e-6</span>, steps<span class="op">=</span><span class="dv">2500</span>, image_shape<span class="op">=</span>image_shape, watch<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>train_acc, train_loss <span class="op">=</span> model.accuracy(Xtrain, ytrain), model.nll(Xtrain, ytrain)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>test_acc, test_loss <span class="op">=</span> model.accuracy(Xtest, ytest), model.nll(Xtest, ytest)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Training accuracy: </span><span class="sc">%.3f</span><span class="st">, loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> (train_acc, train_loss))</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Test accuracy: </span><span class="sc">%.3f</span><span class="st">, loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> (test_acc, test_loss))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loss 254.93, accuracy: 0.86: 100%|██████████| 2500/2500 [00:36&lt;00:00, 67.96it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training accuracy: 0.863, loss: 254.908
Test accuracy: 0.813, loss: 141.683</code></pre>
</div>
</div>
<p>Recall that in class we dicussed <em>feature transforms</em> an easy way to get more expressive models, using our linear model tools. Here we’ll try applying some basic feature transforms to this problem and see if we can improve the performance.</p>
</section>
<section id="q3-quadratic-feature-transforms" class="level4">
<h4 class="anchored" data-anchor-id="q3-quadratic-feature-transforms"><strong>Q3:</strong> Quadratic feature transforms</h4>
<p>Create a <em>transformed</em> versions of the training and test datasets by adding quadratic features. Only add the unary quadratic terms (<span class="math inline">\(x_i^2\)</span>) <strong>not</strong> the cross terms (<span class="math inline">\(x_i x_j\)</span>). For a single dimension the transform would look like: <span class="math display">\[\phi(x_i) = \begin{bmatrix} x_i \\ x_i^2 \end{bmatrix}\]</span></p>
<p>In general, the transform should look like:</p>
<p><span class="math display">\[\textbf{Single observation: }\phi(\mathbf{x}) = \begin{bmatrix}x_1 \\ \vdots \\ x_d \\ x_1^2 \\ \vdots \\ x_d^2 \end{bmatrix}, \quad \textbf{Dataset: } \phi(\mathbf{X}) = \begin{bmatrix}x_{11} &amp; x_{12} &amp; \dots &amp; x_{1d} &amp; x_{11}^2 &amp; \dots &amp; x_{1d}^2 \\ x_{21} &amp; x_{22} &amp; \dots &amp; x_{2d} &amp; x_{21}^2 &amp; \dots &amp; x_{2d}^2 \\  \vdots &amp; \vdots &amp; &amp; \vdots &amp; \vdots &amp; &amp; \vdots \\ x_{N1} &amp; x_{N2} &amp; \dots &amp; x_{Nd} &amp; x_{N1}^2 &amp; \dots &amp; x_{Nd}^2 \\  \end{bmatrix} \]</span></p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co">## YOUR CODE HERE</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>Xtrain_quad <span class="op">=</span> np.concatenate([Xtrain, Xtrain <span class="op">**</span> <span class="dv">2</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>Xtest_quad <span class="op">=</span> np.concatenate([Xtest, Xtest <span class="op">**</span> <span class="dv">2</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> Xtrain_quad.shape <span class="op">==</span> (Xtrain.shape[<span class="dv">0</span>], <span class="dv">2</span> <span class="op">*</span> Xtrain.shape[<span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="q4-evaluating-quadratic-transforms" class="level4">
<h4 class="anchored" data-anchor-id="q4-evaluating-quadratic-transforms"><strong>Q4:</strong> Evaluating quadratic transforms</h4>
<p>Train a <strong>new</strong> logistic regression model and evaluate the training and test accuracy and loss as you did in question 2.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co">## YOUR CODE HERE</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(Xtrain_quad.shape[<span class="dv">1</span>])</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> gradient_descent(model, Xtrain_quad, ytrain, lr<span class="op">=</span><span class="fl">1e-6</span>, steps<span class="op">=</span><span class="dv">2500</span>, image_shape<span class="op">=</span>image_shape, watch<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>train_acc, train_loss <span class="op">=</span> model.accuracy(Xtrain_quad, ytrain), model.nll(Xtrain_quad, ytrain)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>test_acc, test_loss <span class="op">=</span> model.accuracy(Xtest_quad, ytest), model.nll(Xtest_quad, ytest)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Training accuracy: </span><span class="sc">%.3f</span><span class="st">, loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> (train_acc, train_loss))</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Test accuracy: </span><span class="sc">%.3f</span><span class="st">, loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> (test_acc, test_loss))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loss 190.14, accuracy: 0.92: 100%|██████████| 2500/2500 [01:00&lt;00:00, 41.39it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training accuracy: 0.920, loss: 190.113
Test accuracy: 0.853, loss: 109.164</code></pre>
</div>
</div>
</section>
<section id="q5-evaluating-sin-transforms" class="level4">
<h4 class="anchored" data-anchor-id="q5-evaluating-sin-transforms"><strong>Q5:</strong> Evaluating sin transforms</h4>
<p>Repeat questions 3 &amp; 4, but using a different transform, defined as:</p>
<p><span class="math display">\[\phi(x_i) = \begin{bmatrix} x_i \\ \sin(10 x_i) \end{bmatrix}\]</span></p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co">## YOUR CODE HERE</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>Xtrain_sin <span class="op">=</span> np.concatenate([Xtrain, np.sin(<span class="dv">10</span> <span class="op">*</span> Xtrain)], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>Xtest_sin <span class="op">=</span> np.concatenate([Xtest, np.sin(<span class="dv">10</span> <span class="op">*</span> Xtest)], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> Xtrain_sin.shape <span class="op">==</span> (Xtrain.shape[<span class="dv">0</span>], <span class="dv">2</span> <span class="op">*</span> Xtrain.shape[<span class="dv">1</span>])</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(Xtrain_sin.shape[<span class="dv">1</span>])</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> gradient_descent(model, Xtrain_sin, ytrain, lr<span class="op">=</span><span class="fl">1e-6</span>, steps<span class="op">=</span><span class="dv">2500</span>, image_shape<span class="op">=</span>image_shape, watch<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>train_acc, train_loss <span class="op">=</span> model.accuracy(Xtrain_sin, ytrain), model.nll(Xtrain_sin, ytrain)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>test_acc, test_loss <span class="op">=</span> model.accuracy(Xtest_sin, ytest), model.nll(Xtest_sin, ytest)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Training accuracy: </span><span class="sc">%.3f</span><span class="st">, loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> (train_acc, train_loss))</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Test accuracy: </span><span class="sc">%.3f</span><span class="st">, loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> (test_acc, test_loss))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>  0%|          | 0/2500 [00:00&lt;?, ?it/s]Loss 136.40, accuracy: 0.98: 100%|██████████| 2500/2500 [00:59&lt;00:00, 42.21it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training accuracy: 0.979, loss: 136.363
Test accuracy: 0.844, loss: 121.323</code></pre>
</div>
</div>
</section>
<section id="q6-comparing-feature-transforms" class="level4">
<h4 class="anchored" data-anchor-id="q6-comparing-feature-transforms"><strong>Q6:</strong> Comparing feature transforms</h4>
<p>Based on the results, would you use any feature transform for this problem? If so, which one?</p>
<p>YOUR ANSWER HERE</p>
</section>
</section>
<section id="part-2-multinomial-logistic-regression" class="level1">
<h1>Part 2: Multinomial logistic regression</h1>
<p>In this part, we will look at implementing <strong>multinomial logistic regression</strong>. Recall that this model extends logistic regression to the cases where there may be more than 2 possible labels, so <span class="math inline">\(y\in\{1,...,C\}\)</span>, where <span class="math inline">\(C\)</span> is the number of <em>classes</em> (possible outputs).</p>
<p>We saw that rather than having a single weight vector, this model has a weight vector for <em>each class</em>, <span class="math inline">\(\mathbf{w}_1,...,\mathbf{w}_C\)</span>. We can view these together as the <em>rows</em> of a weight matrix <span class="math inline">\(\mathbf{W}\)</span>: <span class="math display">\[\mathbf{W} = \begin{bmatrix} \mathbf{w}_1^T \\ \mathbf{w}_2^T \\ \vdots \\ \mathbf{w}_C^T \end{bmatrix}\]</span></p>
<p>We saw that the prediction function for this model was: <span class="math display">\[f(\mathbf{x}) = \underset{c\in \{1,...,C\}}{\text{argmax}}\ \mathbf{x}^T\mathbf{w}\]</span></p>
<p>The probabilistic model was defined as: <span class="math display">\[
p(y_i=c \mid \mathbf{x}, \mathbf{W}) = \text{softmax}(\mathbf{x}^T\mathbf{W})_c=\frac{e^{\mathbf{x}^T\mathbf{w}_c}}{\sum_{j=1}^Ce^{\mathbf{x}^T\mathbf{w}_j}}
\]</span></p>
<p>The negative log-likelihood loss was defined as: <span class="math display">\[
\textbf{NLL}(\mathbf{W}, \mathbf{X}, \mathbf{y})=-\sum_{i=1}^N \bigg(\mathbf{x}_i^T\mathbf{w}_{y_i}- \log\sum_{j=1}^Ce^{\mathbf{x}_i^T\mathbf{w}_{j}}\bigg)
\]</span></p>
<p>In the next few questions we will create a <em>modified</em> version of our logistic regression class that supports multinomal logistic regression. The class definition is below. A few things to note:</p>
<p><strong>1:</strong> We will still assume <code>y</code> is an array of <code>int</code> in numpy. We can convert an array <code>y</code> to an array of <code>int</code> with <code>y = y.astype(int)</code> and back with <code>y = y.astype(float)</code></p>
<p><strong>2:</strong> Remeber that numpy is <strong>0-indexed</strong>, so our classes will actually be <span class="math inline">\(0\)</span> to <span class="math inline">\(C-1\)</span>, (<span class="math inline">\(y\in \{0,...,C-1\}\)</span>)</p>
<p><strong>2:</strong> We will assume our weight matrix is an <span class="math inline">\(C \times d\)</span> matrix as shown below</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultinomialLogisticRegression(LogisticRegression):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, classes, dims):</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>        <span class="co">'''</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="co">            classes (int): C, the number of possible outputs</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="co">            dims (int): d, the dimension of each input</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="co">        '''</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classes <span class="op">=</span> classes</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> np.zeros((classes, dims <span class="op">+</span> <span class="dv">1</span>,))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="q7-prediction" class="level4">
<h4 class="anchored" data-anchor-id="q7-prediction"><strong>Q7:</strong> Prediction</h4>
<p>Write a function to make a prediction using the multinomial logistic regression prediction rule above. (assume <code>self</code> is the <code>MultinomialLogisticRegression</code> object)</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> multiclass_predict(<span class="va">self</span>, X):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Predict labels given a set of inputs.</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="co">        X (array): An N x d matrix of observations.</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="co">        pred (int array): A length N array of predictions in {0,...,(C-1)}</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>    W <span class="op">=</span> <span class="va">self</span>.weights</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">## YOUR CODE HERE</span></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>    pred <span class="op">=</span> linear_function(X, <span class="va">self</span>.weights.T).argmax(axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pred</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a><span class="co">## Test the function</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>test_predict(multiclass_predict)</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a><span class="co">## Add it to our class</span></span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>MultinomialLogisticRegression.predict <span class="op">=</span> multiclass_predict</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Passed!</code></pre>
</div>
</div>
</section>
<section id="q7-softmax" class="level4">
<h4 class="anchored" data-anchor-id="q7-softmax"><strong>Q7:</strong> Softmax</h4>
<p>Implement the softmax function. <span class="math display">\[
\text{softmax}(\mathbf{x})_c = \frac{e^{x_c}}{\sum_{j=1}^Ce^{x_j}}, \quad
\text{softmax}(\mathbf{x}) = \begin{bmatrix}\frac{e^{x_1}}{\sum_{j=1}^Ce^{x_j}} \\ \frac{e^{x_2}}{\sum_{j=1}^Ce^{x_j}} \\ \vdots \\ \frac{e^{x_C}}{\sum_{j=1}^Ce^{x_j}} \end{bmatrix}
\]</span></p>
<p>You function should accept inputs as either a length <span class="math inline">\(C\)</span> vector or as an <span class="math inline">\(N\times C\)</span> matrix. If the input is a matrix, the softmax function should be applied to each <strong>row</strong> of the matrix.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> softmax(x):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Apply the softmax function to a vector or matrix</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="co">        X (array): An N x C matrix of transformed inputs (or a length C vector)</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="co">        probs (array):  An N x C matrix with the softmax function applied to each row</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span> </span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>    ex <span class="op">=</span> np.exp(x)</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ex <span class="op">/</span> np.<span class="bu">sum</span>(ex, axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>test_softmax(softmax)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Passed!</code></pre>
</div>
</div>
</section>
<section id="q7-multinomial-logistic-regression-nll" class="level4">
<h4 class="anchored" data-anchor-id="q7-multinomial-logistic-regression-nll"><strong>Q7:</strong> Multinomial logistic regression NLL</h4>
<p>Implement a function to compute the multinomial logistic regression negative log-likelihood. <span class="math display">\[
\textbf{NLL}(\mathbf{W}, \mathbf{X}, \mathbf{y})=-\sum_{i=1}^N \bigg(\mathbf{x}_i^T\mathbf{w}_{y_i}- \log\sum_{j=1}^Ce^{\mathbf{x}_i^T\mathbf{w}_{j}}\bigg)
\]</span></p>
<p><em><strong>Hint:</strong> Recall that <span class="math inline">\(y_i\)</span> is an integer, so <span class="math inline">\(\mathbf{w}_{j}\)</span> refers to the row of the weight matrix at index <span class="math inline">\(y_i\)</span> (you could access this as <code>W[y[i]]</code>). It’s possible to answer this question without loops, but you may find it easier to loop over each possible class/observation.</em></p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> nll(<span class="va">self</span>, X, y):</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute the negative log-likelihood loss.</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="co">        X (array): An N x d matrix of observations.</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="co">        y (int array): A length N vector of labels.</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a><span class="co">        nll (float): The NLL loss</span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>    xw <span class="op">=</span> linear_function(X, <span class="va">self</span>.weights.T)</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> xw[np.arange(X.shape[<span class="dv">0</span>]), y].<span class="bu">sum</span>() <span class="op">-</span> np.log(np.<span class="bu">sum</span>(np.exp(xw), axis<span class="op">=</span><span class="dv">1</span>)).<span class="bu">sum</span>()</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>loss</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>test_nll(nll)</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>MultinomialLogisticRegression.nll <span class="op">=</span> nll</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Passed!</code></pre>
</div>
</div>
</section>
<section id="q8-gradient-of-nll" class="level4">
<h4 class="anchored" data-anchor-id="q8-gradient-of-nll"><strong>Q8:</strong> Gradient of NLL</h4>
<p>Derive the gradient of the negative log-likelihood with respect to <span class="math inline">\(\mathbf{w}_c\)</span>, the weight vector for a <strong>single class</strong>.</p>
<p><em><strong>Hint:</strong> Again note that <span class="math inline">\(\mathbf{w}_{y_i}\)</span> refers to the weight vector corresponding to the <strong>true</strong> class of observation <span class="math inline">\(i\)</span>, and so only depend on <span class="math inline">\(\mathbf{w}_c\)</span> if <span class="math inline">\(y_i=c\)</span>. This means that:</em> <span class="math display">\[\frac{d}{d\mathbf{w}_c} \mathbf{x_i}^T \mathbf{w}_{y_i} = \begin{cases}\mathbf{x}_i \quad \text{ if } y_i = c \\ 0 \quad \ \text{ otherwise}  \end{cases}\]</span> <em>We can write this more compactly using an indicator function:</em> <span class="math display">\[\frac{d}{d\mathbf{w}_c} \mathbf{x_i}^T \mathbf{w}_{y_i} = \mathbb{I}(y_i=c)\ \mathbf{x}_i \]</span></p>
<p><span class="math display">\[\nabla_{\mathbf{w}_c} \textbf{NLL}(\mathbf{W}, \mathbf{X}, \mathbf{y}) = \frac{d}{d\mathbf{w}_c}-\sum_{i=1}^N \bigg(\mathbf{x}_i^T\mathbf{w}_{y_i}- \log\sum_{j=1}^Ce^{\mathbf{x}_i^T\mathbf{w}_{j}}\bigg)\]</span></p>
<p>The first step is to apply the addition/subtraction rule:</p>
<p><span class="math display">\[= -\sum_{i=1}^N \bigg(\frac{d}{d\mathbf{w}_c}\big(\mathbf{x}_i^T\mathbf{w}_{y_i}\big)- \frac{d}{d\mathbf{w}_c}\log\sum_{j=1}^Ce^{\mathbf{x}_i^T\mathbf{w}_{j}}\bigg)\]</span></p>
<p>Then we can apply the rule we see above: <span class="math display">\[= -\sum_{i=1}^N \bigg(\mathbb{I}(y_i=c)\ \mathbf{x}_i- \frac{d}{d\mathbf{w}_c}\log\sum_{j=1}^Ce^{\mathbf{x}_i^T\mathbf{w}_{j}}\bigg)\]</span> Next we’ll apply the chain rule to the <span class="math inline">\(\log\)</span> function: <span class="math display">\[= -\sum_{i=1}^N \bigg(\mathbb{I}(y_i=c)\ \mathbf{x}_i- \bigg(\frac{1}{\sum_{j=1}^Ce^{\mathbf{x}_i^T\mathbf{w}_{j}}}\bigg)\ \frac{d}{d\mathbf{w}_c} \bigg(\sum_{j=1}^Ce^{\mathbf{x}_i^T\mathbf{w}_{j}}\bigg) \bigg)\]</span> We see that only one term in the inner summation depends on <span class="math inline">\(\mathbf{w}_c\)</span> <span class="math display">\[= -\sum_{i=1}^N \bigg(\mathbb{I}(y_i=c)\ \mathbf{x}_i- \bigg(\frac{1}{\sum_{j=1}^Ce^{\mathbf{x}_i^T\mathbf{w}_{j}}}\bigg)\  \bigg(\frac{d}{d\mathbf{w}_c}e^{\mathbf{x}_i^T\mathbf{w}_{c}}\bigg) \bigg)\]</span> Applying the chain rule to the exponential we get: <span class="math display">\[= -\sum_{i=1}^N \bigg(\mathbb{I}(y_i=c)\ \mathbf{x}_i- \bigg(\frac{1}{\sum_{j=1}^Ce^{\mathbf{x}_i^T\mathbf{w}_{j}}}\bigg)\  \bigg(e^{\mathbf{x}_i^T\mathbf{w}_{c}} \frac{d}{d\mathbf{w}_c} \mathbf{x}_i^T\mathbf{w}_{c}\bigg) \bigg)\]</span> <span class="math display">\[= -\sum_{i=1}^N \bigg(\mathbb{I}(y_i=c)\ \mathbf{x}_i- \bigg(\frac{1}{\sum_{j=1}^Ce^{\mathbf{x}_i^T\mathbf{w}_{j}}}\bigg)\  \bigg(e^{\mathbf{x}_i^T\mathbf{w}_{c}}  \mathbf{x}_i\bigg) \bigg)\]</span> Factoring out <span class="math inline">\(\mathbf{x}_i\)</span> we get: <span class="math display">\[= -\sum_{i=1}^N  \mathbf{x}_i\bigg(\mathbb{I}(y_i=c) - \frac{e^{\mathbf{x}_i^T\mathbf{w}_{c}}  }{\sum_{j=1}^Ce^{\mathbf{x}_i^T\mathbf{w}_{j}}} \bigg)\]</span></p>
</section>
<section id="q9-implementing-the-gradient" class="level4">
<h4 class="anchored" data-anchor-id="q9-implementing-the-gradient"><strong>Q9:</strong> Implementing the gradient</h4>
<p>Write a function that computes the gradient of the negative log-likelihood with repect to the weight vector for a given class, using the results of the derivation above.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> nll_gradient_c(W, X, y, c):</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute the negative log-likelihood loss.</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="co">        W (array): The C x d weight matrix.</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="co">        X (array): An N x d matrix of observations.</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="co">        y (int array): A length N vector of labels.</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="co">        c (int): The class to compute the gradient for</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="co">        grad (array): A length d vector representing the gradient with respect to w_c</span></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.pad(X, [(<span class="dv">0</span>, <span class="dv">0</span>), (<span class="dv">0</span>, <span class="dv">1</span>)], constant_values<span class="op">=</span><span class="fl">1.</span>)</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>    grad <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> np.exp(np.dot(X, W.T)).<span class="bu">sum</span>(axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>    gradc <span class="op">=</span> <span class="op">-</span> X[y <span class="op">==</span> c].<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>    wc <span class="op">=</span> W[:, c]</span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>    exwc <span class="op">=</span> np.exp(np.dot(X, wc))</span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>    gradc <span class="op">=</span> gradc <span class="op">+</span> (grad <span class="op">*</span> exwc).dot(X)</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> gradc</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="q10-implementing-the-full-gradient" class="level4">
<h4 class="anchored" data-anchor-id="q10-implementing-the-full-gradient"><strong>Q10:</strong> Implementing the full gradient</h4>
<p>Using the function you just wrote, write a function to compute the full gradient with respect to the <span class="math inline">\(C \times d\)</span> weight matrix. <em><strong>Hint:</strong> The output should be a matrix!</em></p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> nll_gradient(<span class="va">self</span>, X, y):</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute the negative log-likelihood loss.</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="co">        X (array): An N x d matrix of observations.</span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="co">        y (int array): A length N vector of labels.</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="co">        grad (array): A C x d matrix representing the gradient with respect to W</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.pad(X, [(<span class="dv">0</span>, <span class="dv">0</span>), (<span class="dv">0</span>, <span class="dv">1</span>)], constant_values<span class="op">=</span><span class="fl">1.</span>)</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>    grad <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> np.exp(np.dot(X, <span class="va">self</span>.weights.T)).<span class="bu">sum</span>(axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>    grads <span class="op">=</span> []</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> c <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.classes):</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>        gradc <span class="op">=</span> <span class="op">-</span> X[y <span class="op">==</span> c].<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>        wc <span class="op">=</span> <span class="va">self</span>.weights[c]</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>        exwc <span class="op">=</span> np.exp(np.dot(X, wc))</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>        gradc <span class="op">=</span> gradc <span class="op">+</span> (grad <span class="op">*</span> exwc).dot(X)</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>        grads.append(gradc)</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.stack(grads)</span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>test_nll_grad(nll_gradient)</span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>MultinomialLogisticRegression.nll_gradient <span class="op">=</span> nll_gradient</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Passed!</code></pre>
</div>
</div>
<p>Note if you are struggling with this problem, you can uncomment the following cell and the import statement at the top of this notebook to get a valid gradient function based on you nll function.</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co">'''</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="co">def autograd_nll_gradient(self, X, y):</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="co">    import autograd</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="co">    def autograd_nll(W):</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="co">        temp_model = MultinomialLogisticRegression(self.classes, X.shape[1])</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="co">        temp_model.weights = W</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a><span class="co">        return temp_model.nll(X, y)</span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a><span class="co">    return autograd.grad(autograd_nll)(self.weights)</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a><span class="co">#test_nll_grad(nll_gradient)</span></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a><span class="co">test_nll_grad(autograd_nll_gradient)</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a><span class="co">MultinomialLogisticRegression.nll_gradient = autograd_nll_gradient</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a><span class="co">'''</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>'\ndef autograd_nll_gradient(self, X, y):\n    import autograd\n    def autograd_nll(W):\n        temp_model = MultinomialLogisticRegression(self.classes, X.shape[1])\n        temp_model.weights = W\n        return temp_model.nll(X, y)\n    return autograd.grad(autograd_nll)(self.weights)\n#test_nll_grad(nll_gradient)\ntest_nll_grad(autograd_nll_gradient)\nMultinomialLogisticRegression.nll_gradient = autograd_nll_gradient\n'</code></pre>
</div>
</div>
<p>Finally, we will test out our multinomial logistic regression classifier on the MNIST dataset (https://en.wikipedia.org/wiki/MNIST_database), one of the most popular datasets in machine learning! We’ll start by loading it as before.</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>images, labels, label_names <span class="op">=</span> get_dataset(<span class="st">'mnist'</span>)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>image_shape <span class="op">=</span> images[<span class="dv">0</span>].shape                <span class="co"># Keep track of the original image shape</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> images.reshape((images.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>))    <span class="co"># Reshape into an N x d matrix X</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> labels</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Image shape: '</span>, images.shape, <span class="st">', X shape: '</span>, X.shape)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the initial model</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> MultinomialLogisticRegression(classes<span class="op">=</span><span class="bu">len</span>(label_names), dims<span class="op">=</span>X.shape[<span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-21-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Image shape:  (5000, 28, 28) , X shape:  (5000, 784)</code></pre>
</div>
</div>
</section>
<section id="q11-repeat-question-2-using-the-mnist-dataset" class="level4">
<h4 class="anchored" data-anchor-id="q11-repeat-question-2-using-the-mnist-dataset"><strong>Q11:</strong> Repeat question 2 using the MNIST dataset</h4>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>Xtrain, ytrain, Xtest, ytest <span class="op">=</span> split_data(X, y)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> MultinomialLogisticRegression(classes<span class="op">=</span><span class="bu">len</span>(label_names), dims<span class="op">=</span>X.shape[<span class="dv">1</span>])</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> gradient_descent(model, Xtrain, ytrain, lr<span class="op">=</span><span class="fl">1e-6</span>, steps<span class="op">=</span><span class="dv">2500</span>, image_shape<span class="op">=</span>image_shape, watch<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>train_acc, train_loss <span class="op">=</span> model.accuracy(Xtrain, ytrain), model.nll(Xtrain, ytrain)</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>test_acc, test_loss <span class="op">=</span> model.accuracy(Xtest, ytest), model.nll(Xtest, ytest)</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Training accuracy: </span><span class="sc">%.3f</span><span class="st">, loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> (train_acc, train_loss))</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Test accuracy: </span><span class="sc">%.3f</span><span class="st">, loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> (test_acc, test_loss))</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Training accuracy: </span><span class="sc">%.3f</span><span class="st">, loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> (train_acc, train_loss))</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Test accuracy: </span><span class="sc">%.3f</span><span class="st">, loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> (test_acc, test_loss))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loss 442.68, accuracy: 0.87: 100%|██████████| 2500/2500 [01:07&lt;00:00, 37.21it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training accuracy: 0.870, loss: 442.595
Test accuracy: 0.845, loss: 3025.243
Training accuracy: 0.870, loss: 442.595
Test accuracy: 0.845, loss: 3025.243</code></pre>
</div>
</div>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    plt.imshow(model.weights[i, :<span class="op">-</span><span class="dv">1</span>].reshape((<span class="dv">28</span>, <span class="dv">28</span>)))</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-23-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-23-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-23-output-3.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-23-output-4.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-23-output-5.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-23-output-6.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-23-output-7.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-23-output-8.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-23-output-9.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-23-output-10.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="q12-repeat-question-3-using-the-mnist-dataset" class="level4">
<h4 class="anchored" data-anchor-id="q12-repeat-question-3-using-the-mnist-dataset"><strong>Q12:</strong> Repeat question 3 using the MNIST dataset</h4>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co">## YOUR CODE HERE</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>Xtrain_quad <span class="op">=</span> np.concatenate([Xtrain, Xtrain <span class="op">**</span> <span class="dv">2</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>Xtest_quad <span class="op">=</span> np.concatenate([Xtest, Xtest <span class="op">**</span> <span class="dv">2</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> Xtrain_quad.shape <span class="op">==</span> (Xtrain.shape[<span class="dv">0</span>], <span class="dv">2</span> <span class="op">*</span> Xtrain.shape[<span class="dv">1</span>])</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> Xtrain_quad.shape <span class="op">==</span> (Xtrain.shape[<span class="dv">0</span>], <span class="dv">2</span> <span class="op">*</span> Xtrain.shape[<span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="q13-repeat-question-4-using-the-mnist-dataset" class="level4">
<h4 class="anchored" data-anchor-id="q13-repeat-question-4-using-the-mnist-dataset"><strong>Q13:</strong> Repeat question 4 using the MNIST dataset</h4>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co">## YOUR CODE HERE</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> MultinomialLogisticRegression(classes<span class="op">=</span><span class="bu">len</span>(label_names), dims<span class="op">=</span>Xtrain_quad.shape[<span class="dv">1</span>])</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> gradient_descent(model, Xtrain_quad, ytrain, lr<span class="op">=</span><span class="fl">1e-6</span>, steps<span class="op">=</span><span class="dv">2500</span>, image_shape<span class="op">=</span>image_shape, watch<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>train_acc, train_loss <span class="op">=</span> model.accuracy(Xtrain_quad, ytrain), model.nll(Xtrain_quad, ytrain)</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>test_acc, test_loss <span class="op">=</span> model.accuracy(Xtest_quad, ytest), model.nll(Xtest_quad, ytest)</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Training accuracy: </span><span class="sc">%.3f</span><span class="st">, loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> (train_acc, train_loss))</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Test accuracy: </span><span class="sc">%.3f</span><span class="st">, loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> (test_acc, test_loss))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loss 414.28, accuracy: 0.89: 100%|██████████| 2500/2500 [01:16&lt;00:00, 32.50it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training accuracy: 0.887, loss: 414.195
Test accuracy: 0.850, loss: 2944.114</code></pre>
</div>
</div>
</section>
<section id="q14-repeat-question-5-using-the-mnist-dataset" class="level4">
<h4 class="anchored" data-anchor-id="q14-repeat-question-5-using-the-mnist-dataset"><strong>Q14:</strong> Repeat question 5 using the MNIST dataset</h4>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co">## YOUR CODE HERE</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>Xtrain_sin <span class="op">=</span> np.concatenate([Xtrain, np.sin(<span class="dv">10</span> <span class="op">*</span> Xtrain)], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>Xtest_sin <span class="op">=</span> np.concatenate([Xtest, np.sin(<span class="dv">10</span> <span class="op">*</span> Xtest)], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> Xtrain_sin.shape <span class="op">==</span> (Xtrain.shape[<span class="dv">0</span>], <span class="dv">2</span> <span class="op">*</span> Xtrain.shape[<span class="dv">1</span>])</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> MultinomialLogisticRegression(<span class="bu">len</span>(label_names), Xtrain_sin.shape[<span class="dv">1</span>])</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> gradient_descent(model, Xtrain_sin, ytrain, lr<span class="op">=</span><span class="fl">1e-6</span>, steps<span class="op">=</span><span class="dv">2500</span>, image_shape<span class="op">=</span>image_shape, watch<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>train_acc, train_loss <span class="op">=</span> model.accuracy(Xtrain_sin, ytrain), model.nll(Xtrain_sin, ytrain)</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>test_acc, test_loss <span class="op">=</span> model.accuracy(Xtest_sin, ytest), model.nll(Xtest_sin, ytest)</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Training accuracy: </span><span class="sc">%.3f</span><span class="st">, loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> (train_acc, train_loss))</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Test accuracy: </span><span class="sc">%.3f</span><span class="st">, loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> (test_acc, test_loss))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loss 388.06, accuracy: 0.89: 100%|██████████| 2500/2500 [01:12&lt;00:00, 34.38it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training accuracy: 0.890, loss: 387.972
Test accuracy: 0.850, loss: 2865.387</code></pre>
</div>
</div>
</section>
<section id="q15-final-evaluation" class="level4">
<h4 class="anchored" data-anchor-id="q15-final-evaluation"><strong>Q15:</strong> Final evaluation</h4>
<p>Based on the results, would you use any feature transform for this problem? If so, which one?</p>
<p>In this case we see that the feature transform makes very little difference.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>