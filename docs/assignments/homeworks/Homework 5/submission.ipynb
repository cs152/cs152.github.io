{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckmBOk_nmdDl"
   },
   "source": [
    "# **Homework 5:** Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR NAME HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborators\n",
    "\n",
    "Please list anyone you discussed or collaborated on this assignment with below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIST COLLABORATORS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course feedback\n",
    "\n",
    "Please submit this week's course survey here: https://forms.gle/ELjvh2PK7iiAHbaC8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to PDF\n",
    "\n",
    "Please convert this notebook to PDF for submission to Gradescope using this tool: https://blank-app-ufu2uvdeosc.streamlit.app/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cE8XDumNmdDp"
   },
   "outputs": [],
   "source": [
    "# Uncomment and run if using Colab!\n",
    "\n",
    "#import urllib.request\n",
    "#remote_url = 'https://gist.githubusercontent.com/gabehope/d3e6b10338a1ba78f53204fc7502eda5/raw/52631870b1475b5ef8d9701f1c676fa97bf7b300/hw5_support.py'\n",
    "#with urllib.request.urlopen(remote_url) as remote, open('hw5_support.py', 'w') as local:\n",
    "#  [local.write(str(line, encoding='utf-8')) for line in remote]\n",
    "\n",
    "# Run me first!\n",
    "from hw5_support import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _add(AutogradValue):\n",
    "    # Addition operator (a + b)\n",
    "    def func(self, a, b):\n",
    "        return a + b\n",
    "\n",
    "    def grads(self, a, b):\n",
    "        return 1., 1.\n",
    "\n",
    "class _neg(AutogradValue):\n",
    "    # Negation operator (-a)\n",
    "    def func(self, a):\n",
    "        return -a\n",
    "\n",
    "    def grads(self, a):\n",
    "        return (-1.,)\n",
    "\n",
    "class _sub(AutogradValue):\n",
    "    # Subtraction operator (a - b)\n",
    "    def func(self, a, b):\n",
    "        # Your code here\n",
    "\n",
    "    def grads(self, a, b):\n",
    "        # Your code here\n",
    "\n",
    "class _mul(AutogradValue):\n",
    "    # Multiplication operator (a * b)\n",
    "    def func(self, a, b):\n",
    "        # Your code here\n",
    "\n",
    "    def grads(self, a, b):\n",
    "        # Your code here\n",
    "\n",
    "class _div(AutogradValue):\n",
    "    # Division operator (a / b)\n",
    "    def func(self, a, b):\n",
    "        # Your code here\n",
    "\n",
    "    def grads(self, a, b):\n",
    "        # Your code here\n",
    "\n",
    "class _exp(AutogradValue):\n",
    "    # Exponent operator (e^a, or exp(a))\n",
    "    def func(self, a):\n",
    "        # Your code here\n",
    "\n",
    "    def grads(self, a):\n",
    "        # Your code here\n",
    "\n",
    "class _log(AutogradValue):\n",
    "    # (Natural) log operator (log(a))\n",
    "    def func(self, a):\n",
    "        # Your code here\n",
    "\n",
    "    def grads(self, a):\n",
    "        # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we'll define our basic functions and operators in terms of the operator classes we just wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp(a):\n",
    "    return _exp(a) if isinstance(a, AutogradValue) else math.exp(a)\n",
    "def log(a):\n",
    "    return _log(a) if isinstance(a, AutogradValue) else math.log(a)\n",
    "\n",
    "# Note: Remember that above we defined a class for each type of operation\n",
    "# so in this code we are overriding the basic operators for AutogradValue\n",
    "# such that they construct a new object of the class corresponding to the\n",
    "# given operation and return it.\n",
    "# (You don't need to everything that's happening here to do the HW)\n",
    "AutogradValue.exp = lambda a: _exp(a)\n",
    "AutogradValue.log = lambda a: _log(a)\n",
    "AutogradValue.__add__ = lambda a, b: _add(a, b)\n",
    "AutogradValue.__radd__ = lambda a, b: _add(b, a)\n",
    "AutogradValue.__sub__ = lambda a, b: _sub(a, b)\n",
    "AutogradValue.__rsub__ = lambda a, b: _sub(b, a)\n",
    "AutogradValue.__neg__ = lambda a: _neg(a)\n",
    "AutogradValue.__mul__ = lambda a, b: _mul(a, b)\n",
    "AutogradValue.__rmul__ = lambda a, b: _mul(b, a)\n",
    "AutogradValue.__truediv__ = lambda a, b: _div(a, b)\n",
    "AutogradValue.__rtruediv__ = lambda a, b: _div(b, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now be able to use our `ForwardValue` objects as if they are numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ForwardValue(5)\n",
    "b = ForwardValue(2)\n",
    "\n",
    "print((a + 5) * b)\n",
    "print(log(b))\n",
    "\n",
    "test_operators(ForwardValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(self, args):\n",
    "    # Clear forward_grads if it exists\n",
    "    self.forward_grads = {}\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "\n",
    "    # Make sure to still return the operation's value\n",
    "    return self.func(*self.parent_values)\n",
    "\n",
    "# Overwrite the AutogradValue method so that operators still work\n",
    "AutogradValue.forward_pass = forward_pass\n",
    "test_forward_mode(ForwardValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take derivates of functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ForwardValue(5)\n",
    "b = ForwardValue(2)\n",
    "L = -log(5 *b + a)\n",
    "\n",
    "dL_da = L.forward_grads[a]\n",
    "dL_db = L.forward_grads[b]\n",
    "print('dL/da = %.3f, dL/db = %.3f' % (dL_da, dL_db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also implement our own very simple version of Autograd's `grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(f):\n",
    "    def ad_function(x, *args):\n",
    "        x = ForwardValue(x)\n",
    "        output = f(x, *args)\n",
    "        return output.forward_grads[x]\n",
    "    return ad_function\n",
    "\n",
    "# Define a function\n",
    "def f(x):\n",
    "    return x * x\n",
    "\n",
    "# Use 'grad' to compute the derivative function\n",
    "f_prime = grad(f)\n",
    "\n",
    "# Verify that we get the correct answer\n",
    "x = 5.\n",
    "print('x:\\t', x)\n",
    "print('f(x):\\t', f(x))\n",
    "print(\"f'(x):\\t\", f_prime(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdGlWf_ImdDs"
   },
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRvB3PmYmdDs"
   },
   "outputs": [],
   "source": [
    "def graph_print(a):\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "a = AutogradValue(5.)\n",
    "b = AutogradValue(2.)\n",
    "c = log((a + 5) * b)\n",
    "graph_print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4viVgxkPmdDs"
   },
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oub6-gpsmdDs"
   },
   "outputs": [],
   "source": [
    "def backward_pass(self):\n",
    "    ## YOUR CODE HERE\n",
    "    local_grads = self.grads(*self.parent_values)\n",
    "\n",
    "\n",
    "AutogradValue.backward_pass = backward_pass\n",
    "\n",
    "# Test our implementation\n",
    "test_backward_pass(AutogradValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2W8is1bUmdDx"
   },
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7ylfIvNmdDx"
   },
   "outputs": [],
   "source": [
    "def backward(self):\n",
    "    # We call backward on the loss, so dL/dL = 1\n",
    "    self.grad = 1.\n",
    "\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "AutogradValue.backward = backward\n",
    "# Test our implementation\n",
    "test_backward(AutogradValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yPfKa3vmdDx"
   },
   "source": [
    "Now we can use our `AutogradValue` class to compute derivatives!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WvEONxmrmdDx"
   },
   "outputs": [],
   "source": [
    "a = AutogradValue(5)\n",
    "b = AutogradValue(2)\n",
    "\n",
    "L = -log(5 *b + a)\n",
    "L.backward()\n",
    "print(a.grad, b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_hhoyzVmdDx"
   },
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0KG9iTTTmdDx"
   },
   "outputs": [],
   "source": [
    "def wrap_array(a):\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "def unwrap_gradient(a):\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "\n",
    "test_wrap_unwrap(wrap_array, unwrap_gradient, AutogradValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qS-UQZImdDx"
   },
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tjPd3UnXmdDx"
   },
   "outputs": [],
   "source": [
    "def nll_and_grad(self, X, y):\n",
    "    ## YOUR CODE HERE\n",
    "    loss =\n",
    "    grads =\n",
    "    return loss.value, grads\n",
    "\n",
    "NeuralNetwork.nll_and_grad = nll_and_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2yR_dWYmdDx"
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(100, noise=0.1)\n",
    "model = NeuralNetwork(2, [5, 5])\n",
    "gradient_descent(model, X, y, lr=3e-2, steps=250)\n",
    "\n",
    "print('Model accuracy: %.3f' % model.accuracy(X, y))\n",
    "plot_boundary(model, X, y)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
