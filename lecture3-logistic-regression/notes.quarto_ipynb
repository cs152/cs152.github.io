{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Lecture 3: Logistic regression\"\n",
        "format:\n",
        "    html:\n",
        "        toc: true\n",
        "        toc-depth: 3\n",
        "---"
      ],
      "id": "113579c6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import os\n",
        "import contextlib\n",
        "with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f):\n",
        "    from manim import *\n",
        "import autograd.numpy as np\n",
        "\n",
        "\n",
        "class LectureScene(Scene):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.camera.background_color = \"#ffffff\"\n",
        "        self.template = TexTemplate()\n",
        "        self.template.add_to_preamble(r\"\\usepackage{amsmath}\")\n",
        "\n",
        "class ThreeDLectureScene(ThreeDScene):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.camera.background_color = \"#ffffff\"\n",
        "        self.template = TexTemplate()\n",
        "        self.template.add_to_preamble(r\"\\usepackage{amsmath}\")\n",
        "    \n",
        "\n",
        "class VectorScene(LectureScene):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.ax = Axes(\n",
        "            x_range=[-7.5, 7.5, 1],\n",
        "            y_range=[-5, 5, 1],\n",
        "            x_length=12,\n",
        "            y_length=8,\n",
        "            axis_config={\"color\": GREY},\n",
        "        )\n",
        "        \n",
        "        #axes_labels.set_color(GREY)\n",
        "        self.add(self.ax)\n",
        "\n",
        "class PositiveVectorScene(LectureScene):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.ax = Axes(\n",
        "            x_range=[-2.5, 12.5, 1],\n",
        "            y_range=[-1, 9, 1],\n",
        "            x_length=12,\n",
        "            y_length=8,\n",
        "            axis_config={\"color\": GREY},\n",
        "        )\n",
        "                #axes_labels.set_color(GREY)\n",
        "        self.add(self.ax)\n",
        "\n",
        "class ComparisonVectorScene(LectureScene):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.ax1 = Axes(\n",
        "            x_range=[-5, 5, 1],\n",
        "            y_range=[-5, 5, 1],\n",
        "            x_length=6,\n",
        "            y_length=6,\n",
        "            axis_config={\"color\": GREY},\n",
        "        )\n",
        "        self.ax2 = Axes(\n",
        "            x_range=[-5, 5, 1],\n",
        "            y_range=[-5, 5, 1],\n",
        "            x_length=6,\n",
        "            y_length=6,\n",
        "            axis_config={\"color\": GREY},\n",
        "        )\n",
        "        axgroup = Group(self.ax1, self.ax2)\n",
        "        axgroup.arrange_in_grid(buf=2)\n",
        "        \n",
        "        #axes_labels.set_color(GREY)\n",
        "        self.add(axgroup)"
      ],
      "id": "a263f19b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classification\n",
        "\n",
        "**Interactive visualization for this lecture available [here](./viz-v2.qmd)**\n",
        "\n",
        "## Functions with categorical outputs\n",
        "\n",
        "In the last lecture we considered approximating functions of the form:\n",
        "\n",
        "$$\n",
        "y=f(\\mathbf{x}), \\quad \\text{Input: } \\mathbf{x} \\in\\mathbb{R}^n \\longrightarrow \\text{ Output: }y \\in\\mathbb{R}\n",
        "$$\n",
        "\n",
        "In that setup our function takes in a vector and produces a real number as an output (for example a miles per gallon rating).\n",
        "\n",
        "In many real-world problems, the output we want to model is not a continuous value, but a *categorical* value, meaning the function produces one choice from an unordered of possible outputs. A well-studied example of this kind of prediction is labeling; we might want to assign a label to an image based on the image's content.\n",
        "\n",
        "![](pictures/catdogmouse.png)\n",
        "\n",
        "We call the prediction of categorical outputs **classification**. The output is often also called the *class* of the observation.\n",
        "\n",
        "## Binary outputs\n",
        "\n",
        "In the simplest *binary* case our function produces one of two possible outputs.\n",
        "\n",
        "For example: consider the problem of labeling images as containing either cats or dogs. Conceptually we would like a function that maps images to either a cat label or a dog label:\n",
        "\n",
        "![](pictures/catdog.png){fig-align=\"center\"}\n",
        "\n",
        "For convenience and generality, we will typically use the set $\\{0, 1\\}$ to denote the possible outputs for a binary classification function. Therefore in general we are considering functions of the form:\n",
        "\n",
        "$$\n",
        "y=f(\\mathbf{x}), \\quad \\text{Input: } \\mathbf{x} \\in\\mathbb{R}^n \\longrightarrow \\text{ Output: }y \\in \\{0, 1\\}\n",
        "$$\n",
        "\n",
        "We can assign these outputs to correspond to our actual target labels. For instance we might say that $0 = \\textbf{\"cat\"}$ and $1=\\textbf{\"dog\"}$.\n",
        "\n",
        "## Visualizing categorical outputs\n",
        "\n",
        "As a simpler example, let's again consider the fuel efficiency example from the previous lecture. Perhaps our company has set a target fuel efficiency of 30 miles per gallon for our new model and we want to predict whether our design will meet that target. In this case our inputs will be the same as before, but our output will become a binary label:\n",
        "\n",
        "$$\n",
        "\\text{Input: } \\mathbf{x}_i= \\begin{bmatrix} \\text{Weight} \\\\ \\text{Horsepower} \\\\ \\text{Displacement} \\\\ \\text{0-60mph}  \\end{bmatrix}, \\quad \\text{Output: } y_i = \\begin{cases} 1: \\text{Meets target } (MPG \\geq 30) \\\\ 0:\n",
        "\\text{Fails to meet target } (MPG < 30) \\\\  \\end{cases}\n",
        "$$\n",
        "\n",
        "We can visualize which observations meet our target efficiency by again plotting weight against MPG and using colors to distinguish observations would have label $1$ vs. label $0$.\n"
      ],
      "id": "65f12c70"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "%%manim -sqh -v CRITICAL --progress_bar none HP_MPG\n",
        "import pandas as pd\n",
        "mpg_data = pd.read_csv('data/auto-mpg.csv')\n",
        "data = mpg_data.sort_values(by=['weight'])\n",
        "\n",
        "class HP_MPG(Scene):\n",
        "    def construct(self):\n",
        "        self.camera.background_color = \"#ffffff\"\n",
        "        self.ax = Axes(\n",
        "            x_range=[1000, 5500, 500],\n",
        "            y_range=[0, 50, 10],\n",
        "            x_length=12,\n",
        "            y_length=8,\n",
        "            axis_config={\"color\": GREY},\n",
        "        )        \n",
        "        \n",
        "        #axes_labels.set_color(GREY)\n",
        "        self.add(self.ax)\n",
        "        \n",
        "        \n",
        "        coords = [[c1, c2] for (c1, c2) in zip(data['weight'], data['mpg'])]\n",
        "        all_dots = [Dot(color=(BLACK if coord[1] > 25 else RED)).move_to(self.ax.c2p(coord[0],coord[1])) for coord in coords]\n",
        "        dots = VGroup(*all_dots)\n",
        "        labels = self.ax.get_axis_labels(x_label=\"Weight\\ (lbs)\", y_label=\"MPG\")\n",
        "        labels.set_color(GREY)\n",
        "        self.add(dots,  labels)\n"
      ],
      "id": "936ff11c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With this new output definition our dataset will look like:\n",
        "\n",
        "$$\n",
        "\\text{Honda Accord: } \\begin{bmatrix} \\text{Weight:} & \\text{2500 lbs} \\\\ \\text{Horsepower:} & \\text{ 123 HP} \\\\ \\text{Displacement:} & \\text{ 2.4 L} \\\\ \\text{0-60mph:} & \\text{ 7.8 Sec} \\end{bmatrix} \\longrightarrow \\text{1   (Meets target)}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{Dodge Aspen: } \\begin{bmatrix} \\text{Weight:} & \\text{3800 lbs} \\\\ \\text{Horsepower:} & \\text{ 155 HP} \\\\ \\text{Displacement:} & \\text{ 3.2 L} \\\\ \\text{0-60mph:} & \\text{ 6.8 Sec} \\end{bmatrix} \\longrightarrow  \\text{0   (Does not meet target)}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\vdots \\quad \\vdots\n",
        "$$\n",
        "\n",
        "In this case, we've gotten rid of the $MPG$ output variable and replaced it with a binary output $y_i \\in \\{0, 1\\}$. If we plot this version of the data, we can see more directly how this *classification* task differs from the *regression* task we saw in the last lecture.\n"
      ],
      "id": "98155763"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "%%manim -sqh -v CRITICAL --progress_bar none HP_MPG\n",
        "import pandas as pd\n",
        "mpg_data = pd.read_csv('data/auto-mpg.csv')\n",
        "data = mpg_data.sort_values(by=['weight'])\n",
        "\n",
        "class HP_MPG(Scene):\n",
        "    def construct(self):\n",
        "        self.camera.background_color = \"#ffffff\"\n",
        "        self.ax = Axes(\n",
        "            x_range=[1000, 5500, 500],\n",
        "            y_range=[-1, 2, 1],\n",
        "            x_length=12,\n",
        "            y_length=8,\n",
        "            y_axis_config={\n",
        "                \"numbers_to_include\": [0, 1],\n",
        "                \"numbers_with_elongated_ticks\": [0, 1],\n",
        "            },\n",
        "            axis_config={\"color\": GREY},\n",
        "        )        \n",
        "        axes_labels = self.ax.get_axis_labels()\n",
        "\n",
        "        axes_labels.set_color(GREY)\n",
        "        self.add(self.ax)\n",
        "        \n",
        "        \n",
        "        coords = [[c1, float(c2 > 25)] for (c1, c2) in zip(data['weight'], data['mpg'])]\n",
        "        all_dots = [Dot(color=(BLACK if coord[1] == 1 else RED)).move_to(self.ax.c2p(coord[0],coord[1])) for coord in coords]\n",
        "        dots = VGroup(*all_dots)\n",
        "        labels = self.ax.get_axis_labels(x_label=\"Weight\\ (lbs)\", y_label=\"Meets\\ target\")\n",
        "        labels.set_color(GREY)\n",
        "        self.add(dots,  labels)"
      ],
      "id": "6b469cf8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Making binary predictions\n",
        "\n",
        "We could fit a linear regression model to our binary data, by simply treating the labels $0$ and $1$ as real-valued outputs. For our fuel economy example, such a model would look like this:\n"
      ],
      "id": "46d44585"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "%%manim -sqh -v CRITICAL --progress_bar none HP_MPG_LR\n",
        "from sklearn.linear_model import LinearRegression\n",
        "x = np.array(mpg_data['weight'])[:, np.newaxis]\n",
        "y = (np.array(mpg_data['mpg']) > 25).astype(float)\n",
        "model = LinearRegression().fit(x, y)\n",
        "other_lines = [(model.coef_.item() * 1.2, model.intercept_.item() * 0.95), (model.coef_.item() * 0.7, model.intercept_.item() * 0.9), (model.coef_.item() * 1.8, model.intercept_.item() * 1.4)]\n",
        "\n",
        "class HP_MPG_LR(Scene):\n",
        "    def construct(self):\n",
        "        self.camera.background_color = \"#ffffff\"\n",
        "        self.ax = Axes(\n",
        "            x_range=[0000, 6500, 500],\n",
        "            y_range=[-1, 2, 1],\n",
        "            x_length=12,\n",
        "            y_length=8,\n",
        "            axis_config={\"color\": GREY},\n",
        "        )\n",
        "        labels = self.ax.get_axis_labels(x_label=\"Weight\\ (lbs)\", y_label=\"Meets\\ target\")\n",
        "        labels.set_color(GREY)\n",
        "        self.add(self.ax, labels)\n",
        "        \n",
        "\n",
        "        # Add data\n",
        "        coords = [[c1, float(c2 > 25)] for (c1, c2) in zip(data['weight'], data['mpg'])]\n",
        "        all_dots = [Dot(color=(BLACK if coord[1] == 1 else RED)).move_to(self.ax.c2p(coord[0],coord[1])) for coord in coords]\n",
        "        dots = VGroup(*all_dots)\n",
        "        self.add(dots)\n",
        "\n",
        "        # Add Regression line\n",
        "        plot = self.ax.plot(lambda x: model.predict(np.atleast_2d(x)).item(), x_range=[0000, 6500], color=BLUE)\n",
        "        self.add(plot)\n",
        "\n",
        "        eq = Tex(r'$f(x)=wx+b$', color=BLACK).to_corner(UR)\n",
        "        self.add(eq)"
      ],
      "id": "cda09bd7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, this doesn't really address our problem. How do we interpret a prediction of $-1$ or $10$ or $0.5$?\n",
        "\n",
        "A more suitable prediction function would *only* output one of our two possible labels $\\{0, 1\\}$. Fortunately, we can adapt our linear regression function in this way by defining a *cutoff* (typically 0), as follows:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x})=\\mathbf{x}^T\\mathbf{w} \\quad \\longrightarrow \\quad f(\\mathbf{x})=\\begin{cases} 1\\ \\text{   if   }\\ \\mathbf{x}^T\\mathbf{w} \\geq 0 \\\\\n",
        " 0\\ \\text{   if   }\\ \\mathbf{x}^T\\mathbf{w} < 0\\end{cases}\n",
        "$$\n",
        "\n",
        "We might also write this as:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = \\mathbb{I}(\\mathbf{x}^T\\mathbf{w} \\geq 0)\n",
        "$$\n",
        "\n",
        "Where $\\mathbb{I}$ is an *indicator function* that is $1$ if the boolean expression is true and $0$ otherwise.\n",
        "\n",
        "This gives us a prediction function that looks like step function in 1 dimension:\n"
      ],
      "id": "095558b8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "%%manim -sqh -v CRITICAL --progress_bar none HP_MPG_LR\n",
        "from sklearn.linear_model import LinearRegression\n",
        "x = np.array(mpg_data['weight'])[:, np.newaxis]\n",
        "y = (np.array(mpg_data['mpg']) > 25).astype(float)\n",
        "model = LinearRegression().fit(x, y)\n",
        "other_lines = [(model.coef_.item() * 1.2, model.intercept_.item() * 0.95), (model.coef_.item() * 0.7, model.intercept_.item() * 0.9), (model.coef_.item() * 1.8, model.intercept_.item() * 1.4)]\n",
        "\n",
        "class HP_MPG_LR(Scene):\n",
        "    def construct(self):\n",
        "        self.camera.background_color = \"#ffffff\"\n",
        "        self.ax = Axes(\n",
        "            x_range=[0000, 6500, 500],\n",
        "            y_range=[-1, 2, 1],\n",
        "            x_length=12,\n",
        "            y_length=8,\n",
        "            axis_config={\"color\": GREY},\n",
        "        )\n",
        "        labels = self.ax.get_axis_labels(x_label=\"Weight\\ (lbs)\", y_label=\"Meets\\ target\")\n",
        "        labels.set_color(GREY)\n",
        "        self.add(self.ax, labels)\n",
        "        \n",
        "\n",
        "        # Add data\n",
        "        coords = [[c1, float(c2 > 25)] for (c1, c2) in zip(data['weight'], data['mpg'])]\n",
        "        all_dots = [Dot(color=(BLACK if coord[1] == 1 else RED)).move_to(self.ax.c2p(coord[0],coord[1])) for coord in coords]\n",
        "        dots = VGroup(*all_dots)\n",
        "        self.add(dots)\n",
        "\n",
        "        # Add Regression line\n",
        "        plot = self.ax.plot(lambda x: float(model.predict(np.atleast_2d(x)).item() > 0.5), x_range=[0000, 6500, 1], color=BLUE)\n",
        "        self.add(plot)\n",
        "\n",
        "        eq = Tex(r'$f(x)=wx+b$', color=BLACK).to_corner(UR)\n",
        "        self.add(eq)"
      ],
      "id": "e6d258fd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpreting parameters\n",
        "\n",
        "For our efficiency example, the binary prediction function can be written as:\n",
        "\n",
        "$$\n",
        "\\text{Meets target} = f(\\mathbf{x})= \n",
        "$$\n",
        "\n",
        "$$\n",
        "\\big((\\text{weight})w_1 + (\\text{horsepower})w_2 + (\\text{displacement})w_3 + (\\text{0-60mph})w_4 + b\\big) \\geq 0\n",
        "$$\n",
        "\n",
        "Or in matrix notation:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x})= \\left( \\begin{bmatrix} \\text{Weight} \\\\ \\text{Horsepower} \\\\ \\text{Displacement} \\\\ \\text{0-60mph} \\\\ 1 \\end{bmatrix} \\cdot \\begin{bmatrix} w_1 \\\\ w_2\\\\ w_3 \\\\ w_4\\\\ b\\end{bmatrix} \\geq  0\\right) \n",
        "$$\n",
        "\n",
        "In this form we can see that the *sign* of each weight parameter determines whether the corresponding feature is more predictive of label $1$ or $0$ and to what extent. For instance, large positive weights indicate features that are very predictive of $1$.\n",
        "\n",
        "## Geometric interpretation of predictions\n",
        "\n",
        "Our binary prediction function also has a geometric interpretation if we think of $\\mathbf{w}$ and $\\mathbf{x}$ as vectors. Reall that the dot product between the vectors $\\mathbf{x}$ and $\\mathbf{w}$ can be written as:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}^T\\mathbf{w} = ||\\mathbf{x}||_2 ||\\mathbf{w}||_2 \\cos \\theta\n",
        "$$\n",
        "\n",
        "Where $\\theta$ is the angle between the two vectors. If the angle between $\\mathbf{w}$ and $\\mathbf{x}$ is in the range $[-\\frac{\\pi}{2}, \\frac{\\pi}{2}]$ (or $[-90^o, 90^o]$ in degrees), then the prediction will be $1$, otherwise it will be 0.\n"
      ],
      "id": "957b5b0f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "%%manim -sqh -v CRITICAL --progress_bar none VectorAddition\n",
        "\n",
        "class VectorAddition(Scene):\n",
        "    def construct(self):\n",
        "        self.camera.background_color = \"#ffffff\"\n",
        "        axes = Axes(\n",
        "            x_range=[-12.5, 12.5, 1],\n",
        "            y_range=[-8, 8, 1],\n",
        "            x_length=12,\n",
        "            y_length=8,\n",
        "            axis_config={\"color\": GREY},\n",
        "        )\n",
        "        \n",
        "        axes_labels = axes.get_axis_labels()\n",
        "        #axes_labels.set_color(GREY)\n",
        "        self.add(axes, axes_labels)\n",
        "\n",
        "        v1 = np.array([5, 5, 0])\n",
        "        arrow1 = Arrow(axes.c2p(*ORIGIN), axes.c2p(*v1), buff=0, color=BLUE)\n",
        "        dot1 = Dot(axes.c2p(*v1),  color=BLUE)\n",
        "        tt1 = Tex(r'$\\mathbf{w}$', color=BLUE).next_to(arrow1.get_end(), UP)\n",
        "        vec1 = Group(arrow1,  tt1)\n",
        "\n",
        "        v2 = np.array([6, 3, 0])\n",
        "        arrow2 = Arrow(axes.c2p(*ORIGIN), axes.c2p(*v2), buff=0, color=BLACK)\n",
        "        dot2 = Dot(axes.c2p(*v2),  color=BLACK)\n",
        "        tt2 = Tex(r'$\\mathbf{x}_1$', color=BLACK).next_to(arrow2.get_end(), UR)\n",
        "\n",
        "        v3 = np.array([-6, -1, 0])\n",
        "        arrow3 = Arrow(axes.c2p(*ORIGIN), axes.c2p(*v3), buff=0, color=RED)\n",
        "        dot3 = Dot(axes.c2p(*v3),  color=RED)\n",
        "        tt3 = Tex(r'$\\mathbf{x}_2$', color=RED).next_to(arrow3.get_end(), DL)\n",
        "        \n",
        "        vec2 = Group(arrow2,  tt2)\n",
        "        vec3 = Group(arrow3,  tt3)\n",
        "        plot = axes.plot(lambda x: -x, color=BLUE)\n",
        "        self.add(vec2, vec1, vec3, plot)\n",
        "        \n",
        "        eq = Tex(r'$f(x)=1$', color=BLACK).to_corner(UR)\n",
        "        eq2 = Tex(r'$f(x)=0$', color=RED).to_corner(DL)\n",
        "        self.add(eq, eq2)"
      ],
      "id": "e3681ee5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The blue line in the figure above is the set of points such that:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}^T \\mathbf{w} = 0\n",
        "$$\n",
        "\n",
        "thus it represents the boundary between the regions where $1$ and $0$ predictions are made. By definition, it is *perpendicular* to the direction of $\\mathbf{w}$.\n",
        "\n",
        "## Decision boundaries\n",
        "\n",
        "We can visualize a classification dataset as a function of two variables using color to distinguish between observations with each label. In this example we'll look at weight and engine displacement.\n"
      ],
      "id": "74819b34"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "%%manim -sqh -v CRITICAL --progress_bar none HP_MPG\n",
        "import pandas as pd\n",
        "mpg_data = pd.read_csv('data/auto-mpg.csv')\n",
        "data = mpg_data.sort_values(by=['weight'])\n",
        "\n",
        "class HP_MPG(Scene):\n",
        "    def construct(self):\n",
        "        self.camera.background_color = \"#ffffff\"\n",
        "        self.ax = Axes(\n",
        "            x_range=[1000, 5500, 500],\n",
        "            y_range=[0, 500, 100],\n",
        "            x_length=12,\n",
        "            y_length=8,\n",
        "            axis_config={\"color\": GREY},\n",
        "        )        \n",
        "        \n",
        "        #axes_labels.set_color(GREY)\n",
        "        self.add(self.ax)\n",
        "        \n",
        "        \n",
        "        coords = [[c1, c2, c3] for (c1, c2, c3) in zip(data['weight'], data['displacement'], data['mpg'])]\n",
        "        all_dots = [Dot(color=(BLACK if coord[2] > 25 else RED)).move_to(self.ax.c2p(coord[0],coord[1])) for coord in coords]\n",
        "        dots = VGroup(*all_dots)\n",
        "        labels = self.ax.get_axis_labels(x_label=\"Weight\\ (lbs)\", y_label=\"Displacement\")\n",
        "        labels.set_color(GREY)\n",
        "\n",
        "        self.add(dots,  labels)"
      ],
      "id": "0d970d47",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For a binary classification model the **decision boundary** is the border between regions of the input space corresponding to each prediction that we saw in the previous section. For a linear classification model the decision boundary is line or plane:\n",
        "\n",
        "$$\\mathbf{x}^T\\mathbf{w}=0$$\n",
        "\n",
        "Here we'll plot the decision boundary in the input space and color code observations by the *predicted* label.\n"
      ],
      "id": "4e98f5cb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "%%manim -sqh -v CRITICAL --progress_bar none HP_MPG\n",
        "import pandas as pd\n",
        "mpg_data = pd.read_csv('data/auto-mpg.csv')\n",
        "data = mpg_data.sort_values(by=['weight'])\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "x = np.array(mpg_data[['weight', 'displacement']])\n",
        "y = (np.array(mpg_data['mpg']) > 25).astype(float)\n",
        "model = LogisticRegression().fit(x, y)\n",
        "\n",
        "class HP_MPG(Scene):\n",
        "    def construct(self):\n",
        "        self.camera.background_color = \"#ffffff\"\n",
        "        self.ax = Axes(\n",
        "            x_range=[1000, 5500, 500],\n",
        "            y_range=[0, 500, 100],\n",
        "            x_length=12,\n",
        "            y_length=8,\n",
        "            axis_config={\"color\": GREY},\n",
        "        )        \n",
        "        \n",
        "        #axes_labels.set_color(GREY)\n",
        "        self.add(self.ax)\n",
        "        \n",
        "        \n",
        "        coords = [[c1, c2, c3] for (c1, c2, c3) in zip(data['weight'], data['displacement'], data['mpg'])]\n",
        "        all_dots = [Dot(color=(BLACK if \n",
        "                               model.predict(np.array([[coord[0],coord[1]]])).item() \n",
        "                               else RED)).move_to(self.ax.c2p(coord[0],coord[1])) for coord in coords]\n",
        "        dots = VGroup(*all_dots)\n",
        "        labels = self.ax.get_axis_labels(x_label=\"Weight\\ (lbs)\", y_label=\"Displacement\")\n",
        "        labels.set_color(GREY)\n",
        "        plot = self.ax.plot(lambda x: (-model.coef_[0, 0] * x - model.intercept_[0]) / model.coef_[0, 1], color=BLUE)\n",
        "        self.add(dots,  labels, plot)"
      ],
      "id": "6d20f100",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Measuring error\n",
        "\n",
        "A natural measure for error for binary classifiers is **accuracy**. The *accuracy* of a prediction function is the fraction of observations where the prediction matches the true output:\n",
        "\n",
        "$$\n",
        "\\textbf{Accuracy: }\\quad \\frac{\\text{\\# of correct predictions}}{\\text{Total predictions}}\n",
        "$$\n",
        "\n",
        "We can write this in terms of our prediction function as:\n",
        "\n",
        "$$\n",
        "\\textbf{Accuracy} = \\frac{1}{N} \\sum_{i=1}^N \\mathbb{I}\\big(f(\\mathbf{x}_i) = y_i\\big)\n",
        "$$\n",
        "\n",
        "Below we can plot the decision boundary compared to the *true* outputs and calculate the accuracy of our predictions.\n"
      ],
      "id": "efe99d33"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "%%manim -sqh -v CRITICAL --progress_bar none HP_MPG\n",
        "import pandas as pd\n",
        "mpg_data = pd.read_csv('data/auto-mpg.csv')\n",
        "data = mpg_data.sort_values(by=['weight'])\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "x = np.array(mpg_data[['weight', 'displacement']])\n",
        "y = (np.array(mpg_data['mpg']) > 25).astype(float)\n",
        "model = LogisticRegression().fit(x, y)\n",
        "print('Accuracy: %.4f' % model.score(x, y))\n",
        "\n",
        "class HP_MPG(Scene):\n",
        "    def construct(self):\n",
        "        self.camera.background_color = \"#ffffff\"\n",
        "        self.ax = Axes(\n",
        "            x_range=[1000, 5500, 500],\n",
        "            y_range=[0, 500, 100],\n",
        "            x_length=12,\n",
        "            y_length=8,\n",
        "            axis_config={\"color\": GREY},\n",
        "        )        \n",
        "        \n",
        "        #axes_labels.set_color(GREY)\n",
        "        self.add(self.ax)\n",
        "        \n",
        "        \n",
        "        coords = [[c1, c2, c3] for (c1, c2, c3) in zip(data['weight'], data['displacement'], data['mpg'])]\n",
        "        all_dots = [Dot(color=((BLACK if coord[2] > 25 else RED))).move_to(self.ax.c2p(coord[0],coord[1])) for coord in coords]\n",
        "        dots = VGroup(*all_dots)\n",
        "        labels = self.ax.get_axis_labels(x_label=\"Weight\\ (lbs)\", y_label=\"Displacement\")\n",
        "        labels.set_color(GREY)\n",
        "        plot = self.ax.plot(lambda x: (-model.coef_[0, 0] * x - model.intercept_[0]) / model.coef_[0, 1], color=BLUE)\n",
        "        self.add(dots,  labels, plot)\n"
      ],
      "id": "d9e4a303",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining a loss function\n",
        "\n",
        "In the last lecture we saw that we can find an optimal choice of parameters $\\mathbf{w}$ for a linear regression model by defining a measure of *error* or *loss* for our approximation on our dataset and minimizing that error as a function of $\\mathbf{w}$, either directly or with gradient descent.\n",
        "\n",
        "$$\n",
        "\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmin}} \\ \\mathbf{Loss}(\\mathbf{w})\n",
        "$$\n",
        "\n",
        "Gradient descent update:\n",
        "\n",
        "$$\n",
        "\\mathbf{w}^{(k+1)} \\quad \\longleftarrow \\quad \\mathbf{w}^{(k)} - \\alpha \\nabla_{\\mathbf{w}} \\mathbf{Loss}(\\mathbf{w})\n",
        "$$\n",
        "\n",
        "We might consider using (negative) accuracy as a loss function or the same mean squared error that we used for linear regression. However, if we tried to minimize one of these losses with gradient descent, we would run into a fundamental problem: the derivative of the indicator function is always $0$, meaning gradient descent will never update our model.\n",
        "\n",
        "To get around this problem, we need to turn back to our *maximum likelihood estimation* approach.\n",
        "\n",
        "# Logistic Regression\n",
        "\n",
        "## The Bernoulli distribution\n",
        "\n",
        "The **Bernoulli** distribution is a probability distribution over two possible outcomes. It is often thought of as the distribution of a coin flip, where the probability of heads is defined by a *parameter* $q$ in the range $[0,1]$.\n",
        "\n",
        "$$\n",
        "\\text{Probability of }\\textbf{heads: } \\ \\ q, \\quad \\text{Probability of }\\textbf{tails: } 1-q\n",
        "$$\n",
        "\n",
        "Again we typically use $0$ and $1$ to denote the two possible outcomes, so we can write the *probability mass function* (or *likelihood*) of the Bernoulli distribution as:\n",
        "\n",
        "$$\n",
        "p(y)=\\begin{cases} q\\quad\\ \\ \\ \\ \\ \\ \\  \\text{if }\\ y=1\\\\\n",
        "1-q\\quad \\text{if }\\ y=0\\\\\n",
        " \\end{cases}\\quad q\\in[0,1],\\ y\\in\\{0, 1\\}\n",
        "$$\n",
        "\n",
        "Using the fact that $y$ can only be $0$ or $1$, we can write this more compactly as:\n",
        "\n",
        "$$\n",
        "p(y) = q^y(1-q)^{1-y}\n",
        "$$\n",
        "\n",
        "Recall that the probability mass function tells us the probability of any outcome under our distribution. We can write the log probability mass function as:\n",
        "\n",
        "$$\n",
        "\\log p(y) = y\\log q + (1-y)\\log(1-q)\n",
        "$$\n",
        "\n",
        "## A probabilistic model for binary classification\n",
        "\n",
        "In the previous lecture we saw that we could define a *probabilistic model* for outcomes given inputs by making an strong assumption about how the observed outputs were generated. In particular, we assumed that each $y_i$ was sampled from a Normal distribution where the mean was a linear function of the input $\\mathbf{x}_i$.\n",
        "\n",
        "$$\n",
        "y_i \\sim \\mathcal{N}(\\mathbf{x}_i^T\\mathbf{w},\\ \\sigma^2)\n",
        "$$\n",
        "\n",
        "Given everything we've seen, we might want to do the same for binary outputs by defining a probabilistic model where each binary label \\$y\\$\\_i\\$ is drawn from a Bernoulli where $q$ is a linear function of $\\mathbf{x}_i$. Unfortunately $q$ needs to be restricted to the interval $[0,1]$ and a linear function can make no such guarantee about its output.\n",
        "\n",
        "$$\n",
        "\\mathbf{x}^T\\mathbf{w}\\notin [0, 1] \\quad \\longrightarrow \\quad y_i \\sim \\mathbf{Bernoulli}(\\mathbf{ q=? })\\quad \n",
        "$$\n",
        "\n",
        "However, if we had a way to map the outputs of our linear function into the range $[0,1]$, we could define such a model. This means we need a *function* of the form:\n",
        "\n",
        "$$\n",
        "\\textbf{Need }\\ g(x):\\ \\mathbb{R} \\longrightarrow [0,1]\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\textbf{Input: } x \\in \\mathbb{R} \\longrightarrow \\textbf{Output: } y \\in [0,1]\n",
        "$$\n",
        "\n",
        "## Sigmoid function\n",
        "\n",
        "The **sigmoid** (or **logistic**) function is exactly such a function.\n",
        "\n",
        "$$\n",
        "\\sigma(x) = \\frac{1}{1+e^{-x}}\n",
        "$$\n"
      ],
      "id": "d4822da9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "%%manim -sqh -v CRITICAL --progress_bar none VectorAddition\n",
        "\n",
        "class VectorAddition(Scene):\n",
        "    def construct(self):\n",
        "        self.camera.background_color = \"#ffffff\"\n",
        "        axes = Axes(\n",
        "            x_range=[-8, 8, 1],\n",
        "            y_range=[-2, 2, 1],\n",
        "            x_length=12,\n",
        "            y_length=8,\n",
        "            axis_config={\"color\": GREY},\n",
        "        )\n",
        "        \n",
        "        axes_labels = axes.get_axis_labels()\n",
        "        #axes_labels.set_color(GREY)\n",
        "        self.add(axes, axes_labels)\n",
        "\n",
        "        plot = axes.plot(lambda x: 1/ (1+ np.exp(-x)), color=BLUE)\n",
        "        self.add(plot)\n",
        "        \n",
        "        eq = Tex(r'$\\sigma(x)=\\frac{1}{1+e^{-x}}$', color=BLACK).to_corner(UR)\n",
        "        self.add(eq)"
      ],
      "id": "380e209d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This \"S\"-shaped function *squashes* any real number into the range $[0,1]$. The sigmoid function has a number of other nice properties. It is *smooth*, *monotonic* and *differentiable*. It's derivative has a convenient form that can be written in terms of the sigmoid function itself.\n",
        "\n",
        "$$\n",
        "\\frac{d}{dx}\\sigma(x) = \\sigma(x)\\big(1-\\sigma(x)\\big)\n",
        "$$\n"
      ],
      "id": "a64f5238"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "%%manim -sqh -v CRITICAL --progress_bar none VectorAddition\n",
        "\n",
        "class VectorAddition(Scene):\n",
        "    def construct(self):\n",
        "        self.camera.background_color = \"#ffffff\"\n",
        "        axes = Axes(\n",
        "            x_range=[-8, 8, 1],\n",
        "            y_range=[-2, 2, 1],\n",
        "            x_length=12,\n",
        "            y_length=8,\n",
        "            axis_config={\"color\": GREY},\n",
        "        )\n",
        "        \n",
        "        axes_labels = axes.get_axis_labels()\n",
        "        #axes_labels.set_color(GREY)\n",
        "        self.add(axes, axes_labels)\n",
        "\n",
        "        def sig(x):\n",
        "            return 1/ (1+ np.exp(-x))\n",
        "\n",
        "        plot = axes.plot(lambda x: sig(x), color=BLUE)\n",
        "        plot2 = axes.plot(lambda x:sig(x) * (1-sig(x)), color=RED)\n",
        "        self.add(plot, plot2)\n",
        "        \n",
        "        eq = Tex(r'$\\sigma(x)=\\frac{1}{1+e^{-x}}$', color=BLUE).to_corner(UR)\n",
        "        eq2 = Tex(r'$\\frac{d}{dx}\\sigma(x)$', color=RED).next_to(eq, DOWN)\n",
        "        self.add(eq, eq2)"
      ],
      "id": "5b43e18b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's particularly useful for modeling probabilities because:\n",
        "\n",
        "$$\n",
        "\\sigma(0) = 0.5\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "1-\\sigma(x) = \\sigma(-x)\n",
        "$$\n",
        "\n",
        "## A probabilistic model for binary classification\n",
        "\n",
        "With the sigmoid as our mapping function, we can now define our linear probabilistic model for binary classification as:\n",
        "\n",
        "$$\n",
        "y_i \\sim \\mathbf{Bernoulli}\\big(\\mathbf{ \\sigma(\\mathbf{x}_i^T\\mathbf{w} })\\big)\n",
        "$$\n",
        "\n",
        "Using this definition, we can easily write out the probability of each output given the input $(\\mathbf{x}_i)$ and model parameters $(\\mathbf{w})$.\n",
        "\n",
        "$$\n",
        "p(y_i = 1\\mid \\mathbf{x}_i, \\mathbf{w}) = \\sigma(\\mathbf{x}_i^T\\mathbf{w}), \\quad p(y_i=0\\mid \\mathbf{x}_i, \\mathbf{w})=1-\\sigma(\\mathbf{x}_i^T\\mathbf{w})=\\sigma(-\\mathbf{x}_i^T\\mathbf{w})\n",
        "$$\n",
        "\n",
        "For our fuel efficiency example, we can plot the predicted probability that our target is met, $p(y=1\\mid \\mathbf{x}, \\mathbf{w})$ under our model as a function of the input (in this case `weight`). We see that the result is again an s-curve.\n"
      ],
      "id": "a7b2bdd6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "%%manim -sqh -v CRITICAL --progress_bar none HP_MPG_LR\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "x = np.array(mpg_data['weight'])[:, np.newaxis]\n",
        "y = (np.array(mpg_data['mpg']) > 25).astype(float)\n",
        "model = LogisticRegression().fit(x, y)\n",
        "\n",
        "\n",
        "class HP_MPG_LR(Scene):\n",
        "    def construct(self):\n",
        "        self.camera.background_color = \"#ffffff\"\n",
        "        self.ax = Axes(\n",
        "            x_range=[0000, 6500, 500],\n",
        "            y_range=[-1, 2, 1],\n",
        "            x_length=12,\n",
        "            y_length=8,\n",
        "            axis_config={\"color\": GREY},\n",
        "        )\n",
        "        labels = self.ax.get_axis_labels(x_label=\"Weight\\ (lbs)\", y_label=\"Meets\\ target\")\n",
        "        labels.set_color(GREY)\n",
        "        self.add(self.ax, labels)\n",
        "        \n",
        "\n",
        "        # Add data\n",
        "        coords = [[c1, float(c2 > 25)] for (c1, c2) in zip(data['weight'], data['mpg'])]\n",
        "        all_dots = [Dot(color=(BLACK if coord[1] == 1 else RED)).move_to(self.ax.c2p(coord[0],coord[1])) for coord in coords]\n",
        "        dots = VGroup(*all_dots)\n",
        "        self.add(dots)\n",
        "\n",
        "        # Add Regression line\n",
        "        plot = self.ax.plot(lambda x: float(model.predict_proba(np.atleast_2d(x))[:,1].item()), x_range=[0000, 6500], color=BLUE)\n",
        "        self.add(plot)\n",
        "\n",
        "        eq = Tex(r'$p(y=1\\mid x, w, b) = \\sigma(wx+b)$', color=BLACK).to_corner(UR)\n",
        "        self.add(eq)"
      ],
      "id": "93222ac4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We call this probabilistic model for binary outputs: **logistic regression**.\n",
        "\n",
        "## Logistic regression decision boundary\n",
        "\n",
        "When we're making predictions we typically don't want to sample an output, we want to make a definite prediction. In this case either $0$ or $1$. A reasonable way to do this is to simply predict the output that is most likely under our model:\n",
        "\n",
        "$$\n",
        "\\textbf{Prediction function: } f(\\mathbf{x}) = \\begin{cases}1 \\ \\text{if } p(y=1\\mid\\mathbf{x}, \\mathbf{w}) \\geq p(y=0\\mid\\mathbf{x}, \\mathbf{w}) \\\\\n",
        "0 \\text{ otherwise} \\end{cases}\n",
        "$$\n",
        "\n",
        "Since there's only two possible outcomes, this is equivalent to checking if the probability of class $1$ is greater than 50%. $$p(y=1\\mid \\mathbf{x}, \\mathbf{w}) =\\sigma(\\mathbf{x}^T\\mathbf{w})\\geq 0.5$$\n",
        "\n",
        "Since $\\sigma(0) =0.5$, we see that this is equivalent to the decision rule for classification we defined earlier!\n",
        "\n",
        "$$\n",
        "p(y_i=1)\\geq 0.5 \\quad \\longrightarrow \\quad \\mathbf{x}^T\\mathbf{w}\\geq 0\n",
        "$$\n",
        "\n",
        "## Maximum likelihood estimation review\n",
        "\n",
        "Now that we've setup our model, we can look at how to find the optimal $\\mathbf{w}$ using the principle of *maximum likelihood estimation*.\n"
      ],
      "id": "ee3d067b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "%%manim -sqh -v CRITICAL --progress_bar none HP_MPG_LR\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "x = np.array(mpg_data['weight'])[:, np.newaxis]\n",
        "y = (np.array(mpg_data['mpg']) > 25).astype(float)\n",
        "model = LogisticRegression().fit(x, y)\n",
        "\n",
        "\n",
        "class HP_MPG_LR(Scene):\n",
        "    def construct(self):\n",
        "        self.camera.background_color = \"#ffffff\"\n",
        "        self.ax = Axes(\n",
        "            x_range=[0000, 6500, 500],\n",
        "            y_range=[-1, 2, 1],\n",
        "            x_length=12,\n",
        "            y_length=8,\n",
        "            axis_config={\"color\": GREY},\n",
        "        )\n",
        "        labels = self.ax.get_axis_labels(x_label=\"Weight\\ (lbs)\", y_label=\"Meets\\ target\")\n",
        "        labels.set_color(GREY)\n",
        "        self.add(self.ax, labels)\n",
        "        \n",
        "\n",
        "        # Add data\n",
        "        coords = [[c1, float(c2 > 25)] for (c1, c2) in zip(data['weight'], data['mpg'])]\n",
        "        all_dots = [Dot(color=(BLACK if coord[1] == 1 else RED)).move_to(self.ax.c2p(coord[0],coord[1])) for coord in coords]\n",
        "        dots = VGroup(*all_dots)\n",
        "        self.add(dots)\n",
        "\n",
        "        # Add Regression line\n",
        "        plot = self.ax.plot(lambda x: float(model.predict_proba(np.atleast_2d(x))[:,1].item()), x_range=[0000, 6500], color=BLUE)\n",
        "        self.add(plot)\n",
        "\n",
        "\n",
        "        model.coef_ *= 1.2\n",
        "        model.intercept_ -= model.intercept_ * 0.1\n",
        "        plot = self.ax.plot(lambda x: float(model.predict_proba(np.atleast_2d(x))[:,1].item()), x_range=[0000, 6500], color=ORANGE)\n",
        "        self.add(plot)\n",
        "\n",
        "        model.coef_ *= 2\n",
        "        model.intercept_ += model.intercept_ * 2.5\n",
        "        plot = self.ax.plot(lambda x: float(model.predict_proba(np.atleast_2d(x))[:,1].item()), x_range=[0000, 6500], color=GREEN)\n",
        "        self.add(plot)\n",
        "\n",
        "        model.coef_ /= 5\n",
        "        model.intercept_ -= model.intercept_ * .8\n",
        "        plot = self.ax.plot(lambda x: float(model.predict_proba(np.atleast_2d(x))[:,1].item()), x_range=[0000, 6500], color=PURPLE)\n",
        "        self.add(plot)\n",
        "\n",
        "        model.coef_ *= 8\n",
        "        model.intercept_ += model.intercept_ * 10\n",
        "        plot = self.ax.plot(lambda x: float(model.predict_proba(np.atleast_2d(x))[:,1].item()), x_range=[0000, 6500], color=LIGHT_PINK)\n",
        "        self.add(plot)\n",
        "        \n",
        "        \n",
        "\n",
        "        eq = Tex(r'$p(y=1\\mid x, w, b) = \\sigma(wx+b)$', color=BLACK).to_corner(UR)\n",
        "        self.add(eq)"
      ],
      "id": "673d6f43",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "Recall that the *maximum likelihood estimate* of our parameter $\\mathbf{w}$ is the choice of $\\mathbf{w}$ that maximizes the (conditional) probability of the data we observed under our model\n",
        "\n",
        "$$\n",
        "\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmax}} \\ p(\\mathbf{y} \\mid \\mathbf{X}, \\mathbf{w}) =\\underset{\\mathbf{w}}{\\text{argmax}} \\ p(y_1,...,y_N \\mid \\mathbf{x}_1, ...,\\mathbf{x}_N, \\mathbf{w}) $$\n",
        "\n",
        "Again, our model also assumes *conditional independence* across observations so:\n",
        "\n",
        "$$\n",
        "p(y_1,...,y_N \\mid \\mathbf{x}_1, ...,\\mathbf{x}_N, \\mathbf{w}) = \\prod_{i=1}^N p(y_i\\mid \\mathbf{x}_i, \\mathbf{w})\n",
        "$$\n",
        "\n",
        "For convenience, it is typical to frame the optimal value in terms of the *negative log-likelihood* rather than the likelihood, but the two are equivalent.\n",
        "\n",
        "$$\n",
        "\\underset{\\mathbf{w}}{\\text{argmax}} \\prod_{i=1}^N p(y_i\\mid \\mathbf{x}_i, \\mathbf{w}) = \\underset{\\mathbf{w}}{\\text{argmin}} - \\sum_{i=1}^N \\log p(y_i \\mid \\mathbf{x}_i, \\mathbf{w}) = \\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})\n",
        "$$\n",
        "\n",
        "Thus, the negative log-likelihood is a natural *loss function* to optimize to find $\\mathbf{w}^*$.\n",
        "\n",
        "$$\n",
        "\\textbf{Loss}(\\mathbf{w}) =\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})=- \\sum_{i=1}^N \\log p(y_i \\mid \\mathbf{x}_i, \\mathbf{w}) \n",
        "$$\n",
        "\n",
        "## Maximum likelihood for logistic regression\n",
        "\n",
        "We can now write out the negative log-likelihood for our logistic regression model using the Bernoulli PMF we defined above\n",
        "\n",
        "$$\n",
        "\\mathbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = -\\sum_{i=1}^N \\bigg[ y_i\\log \\sigma(\\mathbf{x}_i^T\\mathbf{w}) + (1-y_i)\\log(1-\\sigma(\\mathbf{x}_i^T\\mathbf{w})) \\bigg]\n",
        "$$\n",
        "\n",
        "Using our knowledge of the sigmoid function, we can write this even more compactly:\n",
        "\n",
        "$$\n",
        "\\mathbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) =-\\sum_{i=1}^N \\bigg[ y_i\\log \\sigma(\\mathbf{x}_i^T\\mathbf{w}) + (1-y_i)\\log \\sigma(-\\mathbf{x}_i^T\\mathbf{w}) \\bigg] \n",
        "$$\n",
        "\n",
        "$$\n",
        "= -\\sum_{i=1}^N \\log\\sigma\\big((2y_i-1)\\mathbf{x}_i^T\\mathbf{w}\\big)\n",
        "$$\n",
        "\n",
        "Note that $2y_i-1$ is $1$ if $y_i=1$ and is $-1$ if $y_i=0$.\n",
        "\n",
        "For our logistic regression model, maximum likelihood is intuitive. In the ideal case our model would always predict the correct class with probability 1.\n",
        "\n",
        "$$\n",
        "\\textbf{Best case scenerio: } p(y_i\\mid \\mathbf{x}_i, \\mathbf{w})=1, \\quad \\forall i \\in \\{1,...,N\\} \n",
        "$$\n",
        "\n",
        "This is generally not possible though due to the constraints of our linear function.\n",
        "\n",
        "We can also write the negative log-likelihood compactly using matrix-vector notation.\n",
        "\n",
        "$$\n",
        "\\mathbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = -\\mathbf{y}^T\\log \\sigma(\\mathbf{X}\\mathbf{w}) - (1-\\mathbf{y})^T\\log \\sigma(-\\mathbf{X}\\mathbf{w})\n",
        "$$\n",
        "\n",
        "It's worth noting that in neural network literature, this loss is often called the **binary cross-entropy loss**.\n",
        "\n",
        "## Optimizing logistic regression\n",
        "\n",
        "As we saw with linear regression, we can find the optimal paramters $\\mathbf{w}^*$ under this loss function using gradient descent:\\\n",
        "$$\n",
        "\\mathbf{w}^{(i+1)} \\leftarrow \\mathbf{w}^{(i)} - \\alpha \\nabla_{\\mathbf{w}} \\mathbf{NLL}(\\mathbf{w}^{(i)}, \\mathbf{X}, \\mathbf{y})\n",
        "$$\n",
        "\n",
        "To use this, we first need to derive the gradient of the negative log-likelihood with respect to $\\mathbf{w}$. We'll start by writing out the simplest version of the NLL that we saw above:\n",
        "\n",
        "$$\n",
        "\\mathbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = -\\sum_{i=1}^N \\log\\sigma\\big((2y_i-1)\\mathbf{x}_i^T\\mathbf{w}\\big)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\nabla_{\\mathbf{w}}\\mathbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = \\frac{d}{d\\mathbf{w}}-\\sum_{i=1}^N \\log\\sigma\\big((2y_i-1)\\mathbf{x}_i^T\\mathbf{w}\\big)\n",
        "$$\n",
        "\n",
        "As a first step, recall that the addition rule tells us that the derivative of a sum is a sum of derivatives:\n",
        "\n",
        "$$\n",
        " = -\\sum_{i=1}^N \\frac{d}{d\\mathbf{w}} \\log\\sigma\\big((2y_i-1)\\mathbf{x}_i^T\\mathbf{w}\\big)\n",
        "$$\n",
        "\n",
        "Next we'll apply the chain rule to the $\\log$ function, remembering that $\\frac{d}{dx} \\log x = \\frac{1}{x}$:\n",
        "\n",
        "$$\n",
        " = -\\sum_{i=1}^N \\bigg(\\frac{1}{ \\sigma\\big((2y_i-1)\\mathbf{x}_i^T\\mathbf{w}\\big) }\\bigg)\\frac{d}{d\\mathbf{w}} \\sigma\\big((2y_i-1)\\mathbf{x}_i^T\\mathbf{w}\\big)\n",
        "$$\n",
        "\n",
        "Then we can apply the chain rule to the sigmoid function, using the fact that $\\frac{d}{dx} \\sigma(x)=\\sigma(x)(1-\\sigma(x))$:\n",
        "\n",
        "$$\n",
        " = -\\sum_{i=1}^N \\bigg(\\frac{1}{ \\sigma\\big((2y_i-1)\\mathbf{x}_i^T\\mathbf{w}\\big) } \\bigg)\n",
        "\\bigg(\\sigma\\big((2y_i-1)\\mathbf{x}_i^T\\mathbf{w}\\big)\\bigg) \\bigg(1-\\sigma\\big((2y_i-1)\\mathbf{x}_i^T\\mathbf{w}\\big)  \\bigg)\n",
        "\\frac{d}{d\\mathbf{w}}\\big((2y_i-1)\\mathbf{x}_i^T\\mathbf{w}\\big)\n",
        "$$\n",
        "\n",
        "We now see that the first 2 terms cancel!\n",
        "\n",
        "$$\n",
        " = -\\sum_{i=1}^N  \\bigg(1-\\sigma\\big((2y_i-1)\\mathbf{x}_i^T\\mathbf{w}\\big)  \\bigg)\n",
        "\\frac{d}{d\\mathbf{w}}\\big((2y_i-1)\\mathbf{x}_i^T\\mathbf{w}\\big)\n",
        "$$\n",
        "\n",
        "Finally we're left with the gradient of a linear function, which is just:\n",
        "\n",
        "$$\\frac{d}{d\\mathbf{w}}\\big((2y_i-1)\\mathbf{x}_i^T\\mathbf{w}\\big)=(2y_i-1)\\mathbf{x}_i$$\n",
        "\n",
        "Note that the transpose is irrelevant as we're no longer signifying a dot-product and $\\mathbf{x}_i$ is just a vector. So finally we're left with\n",
        "\n",
        "$$\n",
        "\\nabla_{\\mathbf{w}}\\mathbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = -\\sum_{i=1}^N  \\bigg(1-\\sigma\\big((2y_i-1)\\mathbf{x}_i^T\\mathbf{w}\\big)  \\bigg)\n",
        "\\bigg((2y_i-1)\\mathbf{x}_i \\bigg)\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```{ojs}\n",
        "//| echo: false\n",
        "//| \n",
        "MathJax = {\n",
        "  const MathJax = await require('mathjax@2.7.5/MathJax.js?config=TeX-MML-AM_CHTML')\n",
        "    .catch(() => window.MathJax)\n",
        "  \n",
        "  // configure MathJax\n",
        "  MathJax.Hub.Config({\n",
        "    tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]},\n",
        "    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ],\n",
        "    processEscapes: true\n",
        "  })  \n",
        "  return MathJax\n",
        "}\n",
        "\n",
        "Plotly = require(\"https://cdn.plot.ly/plotly-latest.min.js\");\n",
        "tfbase = require('@tensorflow/tfjs@4.11.0')\n",
        "pyodide = {\n",
        "  const p =\n",
        "    await require(\"https://cdn.jsdelivr.net/pyodide/v0.22.1/full/pyodide.js\");\n",
        "  console.log(p);\n",
        "  return p.loadPyodide();\n",
        "}\n",
        "\n",
        "PyScope = function() {\n",
        "  let scope = pyodide.toPy({});\n",
        "  \n",
        "  let py = async (strings, ...expressions) => {\n",
        "    let globals = {};\n",
        "    const code = strings.reduce((result, string, index) => {\n",
        "      if (expressions[index]) {\n",
        "        const name = `x${index}`;\n",
        "        globals[name] = expressions[index];\n",
        "        return result + string + name;\n",
        "      }\n",
        "      return result + string;\n",
        "    }, '');\n",
        "    await pyodide.loadPackagesFromImports(code);\n",
        "    scope.update(pyodide.globals);\n",
        "    const result = await pyodide.runPythonAsync(\n",
        "      code,\n",
        "      {\n",
        "        globals: scope\n",
        "      }\n",
        "    );\n",
        "    if (result?.t2Js) return result.t2Js();\n",
        "    if (result?.toJs) return result.toJs();\n",
        "    return result;\n",
        "  };\n",
        "  \n",
        "  return py;\n",
        "}\n",
        "\n",
        "py = {\n",
        "  let testscope = PyScope();\n",
        "  let py = async (strings, ...expressions) => {\n",
        "    let globals = {};\n",
        "    const code = strings.reduce((result, string, index) => {\n",
        "      if (expressions[index]) {\n",
        "        const name = `x${index}`;\n",
        "        globals[name] = expressions[index];\n",
        "        return result + string + name;\n",
        "      }\n",
        "      return result + string;\n",
        "    }, '');\n",
        "    await pyodide.loadPackagesFromImports(code);\n",
        "    pyodide.globals.update(pyodide.toPy(globals))\n",
        "    const result = await pyodide.runPythonAsync(\n",
        "      code,\n",
        "      {globals: pyodide.globals}\n",
        "    );\n",
        "    if (result?.t2Js) return result.t2Js();\n",
        "    if (result?.toJs) return result.toJs();\n",
        "    return result;\n",
        "  };\n",
        "\n",
        "  const sigmoidGradConfig  = {\n",
        "    kernelName: tfbase.Sigmoid,\n",
        "    inputsToSave: ['x'],\n",
        "    gradFunc: (dy, saved) => {\n",
        "      const [x] = saved;\n",
        "      const y = tfbase.sigmoid(x);\n",
        "      return {x: () => tfbase.mul(dy, tfbase.mul(y, tfbase.sub(tfbase.scalar(1), y)))};\n",
        "    }\n",
        "  };\n",
        "  tfbase.registerGradient(sigmoidGradConfig);\n",
        "\n",
        "  const tanhGradConfig = {\n",
        "    kernelName: tfbase.Tanh,\n",
        "    inputsToSave: ['x'],\n",
        "    gradFunc: (dy, saved) => {\n",
        "      const [x] = saved;\n",
        "      const y = tfbase.tanh(x);\n",
        "      return {x: () => tfbase.mul(tfbase.sub(tfbase.scalar(1), tfbase.square(y)), dy)};\n",
        "    }\n",
        "  };\n",
        "  tfbase.registerGradient(tanhGradConfig);\n",
        "   const expGradConfig = {\n",
        "    kernelName: tfbase.Exp,\n",
        "    inputsToSave: ['x'],\n",
        "    gradFunc: (dy, saved) => {\n",
        "      const [x] = saved;\n",
        "      const y = tfbase.exp(x);\n",
        "      return {x: () => tfbase.mul(dy, y)};\n",
        "    }\n",
        "  }; \n",
        "  tfbase.registerGradient(expGradConfig);\n",
        "\n",
        "  function dispatchEvent(element){\n",
        "    element.dispatchEvent(new Event(\"input\", {bubbles: true}));\n",
        "  }\n",
        "  pyodide.globals.update(pyodide.toPy({Plotbase: Plot, tfbase: tfbase, Plotlybase: Plotly, dispatchEvent: dispatchEvent, d3base: d3}))\n",
        "  \n",
        "await py`\n",
        "from pyodide.ffi import create_once_callable\n",
        "from types import SimpleNamespace\n",
        "from pyodide.ffi import to_js\n",
        "from js import Object, document\n",
        "import pandas\n",
        "import numpy as np\n",
        "\n",
        "tfbase = SimpleNamespace(**tfbase)\n",
        "\n",
        "def convert_tensor(a, *args):\n",
        "  if isinstance(a, Parameter):\n",
        "    a = a.value.value\n",
        "  if isinstance(a, Tensor):\n",
        "    a = a.value\n",
        "  if isinstance(a, np.ndarray):\n",
        "    a = a.tolist()\n",
        "  return to_js(a)\n",
        "\n",
        "def convert(a):\n",
        "  if isinstance(a, Parameter):\n",
        "    a = a.value.value\n",
        "  if isinstance(a, Tensor):\n",
        "    a = a.value\n",
        "  if isinstance(a, np.ndarray):\n",
        "    a = a.tolist()\n",
        "  return to_js(a, dict_converter=Object.fromEntries, default_converter=convert_tensor)\n",
        "\n",
        "def convert_start(shape, start):\n",
        "  start = start or 0\n",
        "  if start < 0:\n",
        "    start = shape + start\n",
        "  start = min(start, shape - 1)\n",
        "  return start\n",
        "\n",
        "def convert_end(shape, start, end): \n",
        "  start = convert_start(shape, start)\n",
        "  if end is None:\n",
        "    end = shape\n",
        "  else:\n",
        "    end = convert_start(shape, end)\n",
        "  return end - start\n",
        "\n",
        "class Tensor:\n",
        "  keepall = False\n",
        "\n",
        "  class Keep:\n",
        "    def __enter__(self):\n",
        "        self.value = Tensor.keepall\n",
        "        Tensor.keepall = True\n",
        "    \n",
        "    def __exit__(self, *args):\n",
        "        Tensor.keepall = self.value\n",
        "\n",
        "  def __init__(self, *args, value=None, keep=None, **kwargs):\n",
        "    if keep is None:\n",
        "      self.keep = Tensor.keepall\n",
        "    else:\n",
        "      self.keep = keep\n",
        "\n",
        "    if not (value is None):\n",
        "      self.value = value\n",
        "    elif len(args) and isinstance(args[0], Tensor):\n",
        "      self.value = tfbase.add(args[0].value, 0)\n",
        "    elif len(args) and args[0] is None:\n",
        "      self.value = tfbase.tensor(0.)\n",
        "    else:\n",
        "      args = [convert(a) for a in args]\n",
        "      kwargs = {k: convert(a) for (k, a) in kwargs.items()}\n",
        "      self.value = tfbase.tensor(*args, **kwargs)\n",
        "\n",
        "  def __getattr__(self, name):\n",
        "    if name == 'T':\n",
        "      return self.transpose()\n",
        "    attr = getattr(self.value, name)\n",
        "    if callable(attr):\n",
        "      def run(*args, **kwargs):\n",
        "        args = [convert(a) for a in args]\n",
        "        kwargs = {k: convert(a) for (k, a) in kwargs.items()}\n",
        "        output = attr(*args, **kwargs)\n",
        "        return Tensor(value=output)\n",
        "      # Prevent premature garbage collection\n",
        "      run._ref = self\n",
        "      return run\n",
        "    return attr\n",
        "\n",
        "  def __add__(a, b):\n",
        "    return Tensor(value=tfbase.add(convert(a), convert(b)))\n",
        "  def __radd__(a, b):\n",
        "    return Tensor(value=tfbase.add(convert(b), convert(a)))\n",
        "  def __sub__(a, b):\n",
        "    return Tensor(value=tfbase.sub(convert(a), convert(b)))\n",
        "  def __rsub__(a, b):\n",
        "    return Tensor(value=tfbase.sub(convert(b), convert(a)))\n",
        "  def __mul__(a, b):\n",
        "    return Tensor(value=tfbase.mul(convert(a), convert(b)))\n",
        "  def __rmul__(a, b):\n",
        "    return Tensor(value=tfbase.mul(convert(b), convert(a)))\n",
        "  def __truediv__(a, b):\n",
        "    return Tensor(value=tfbase.div(convert(a), convert(b)))\n",
        "  def __rtruediv__(a, b):\n",
        "    return Tensor(value=tfbase.div(convert(b), convert(a)))\n",
        "  def __floordiv__(a, b):\n",
        "    return Tensor(value=tfbase.floorDiv(convert(a), convert(b)))\n",
        "  def __rfloordiv__(a, b):\n",
        "    return Tensor(value=tfbase.floorDiv(convert(b), convert(a)))\n",
        "  def __pow__(a, b):\n",
        "    return Tensor(value=tfbase.pow(convert(a), convert(b)))\n",
        "  def __rpow__(a, b):\n",
        "    return Tensor(value=tfbase.pow(convert(b), convert(a)))\n",
        "  def __neg__(a):\n",
        "    return Tensor(value=tfbase.neg(convert(a)))\n",
        "  def __eq__(a, b):\n",
        "    return Tensor(value=tfbase.equal(convert(a), convert(b)))\n",
        "  def __neq__(a, b):\n",
        "    return Tensor(value=tfbase.notEqual(convert(a), convert(b)))\n",
        "  def __lt__(a, b):\n",
        "    return Tensor(value=tfbase.less(convert(a), convert(b)))\n",
        "  def __gt__(a, b):\n",
        "    return Tensor(value=tfbase.greater(convert(a), convert(b)))\n",
        "  def __leq__(a, b):\n",
        "    return Tensor(value=tfbase.lessEqual(convert(a), convert(b)))\n",
        "  def __geq__(a, b):\n",
        "    return Tensor(value=tfbase.greaterEqual(convert(a), convert(b)))\n",
        "\n",
        "  def __del__(self):\n",
        "    if hasattr(self.value, 'dispose') and not self.keep:\n",
        "      self.value.dispose()\n",
        "\n",
        "  def __iter__(self):\n",
        "    for x in self.value.arraySync():\n",
        "        yield Tensor(x)\n",
        "\n",
        "  def __getitem__(self, args):\n",
        "    tosqueeze = []\n",
        "    starts, ends, steps = [], [], []\n",
        "    value = self\n",
        "    \n",
        "    if not (type(args) is tuple):\n",
        "      args = (args,)\n",
        "    \n",
        "    for ind in range(len(args)):\n",
        "      if args[ind] is Ellipsis:\n",
        "        start = args[:ind]\n",
        "        rest = args[(ind + 1):]\n",
        "        args = start + tuple([slice(None)] * (len(self.value.shape) - (len(start) + len(rest)))) + rest\n",
        "        break\n",
        "    \n",
        "    for i, (shape, dim) in enumerate(zip(self.value.shape, args)):\n",
        "      if isinstance(dim, slice):\n",
        "        starts.append(dim.start or 0)\n",
        "        ends.append(dim.stop or shape)\n",
        "        steps.append(dim.step or 1)\n",
        "      elif Tensor(dim).shape:\n",
        "        t = Tensor(dim)\n",
        "        if t.value.dtype == 'bool':\n",
        "          inds = [ind for (ind, e) in enumerate(t.value.arraySync()) if e]\n",
        "          inds = tf.cast(tf.reshape(Tensor(inds), [-1]), 'int32')\n",
        "          value = tf.gather(value, inds, i)\n",
        "        else:\n",
        "          inds = tf.cast(tf.reshape(t, [-1]), 'int32')\n",
        "          value = tf.gather(value, inds, i)\n",
        "      else:\n",
        "        starts.append(dim)\n",
        "        ends.append(dim + 1)\n",
        "        steps.append(1)\n",
        "        tosqueeze.append(i)\n",
        "    value = tf.stridedSlice(value, convert(starts), convert(ends), convert(steps))\n",
        "    if len(tosqueeze) > 0:\n",
        "      value = tf.squeeze(value, tosqueeze)\n",
        "    return value\n",
        "\n",
        "  def t2Js(self):\n",
        "    return to_js(self.value.arraySync())\n",
        "\n",
        "class wrapper:\n",
        "  def __init__(self, f):\n",
        "    self.f = f\n",
        "\n",
        "  def __call__(self, x, *args, **kwargs):\n",
        "    with Tensor.Keep():\n",
        "      return convert(self.f(Tensor(value=x), *args, **kwargs))\n",
        "\n",
        "class grad:\n",
        "  def __init__(self, f):\n",
        "    self.f = f\n",
        "    self.wrapper = wrapper(f)\n",
        "\n",
        "  def __call__(self, x, *args, **kwargs):\n",
        "    output = tfbase.grad(create_once_callable(self.wrapper))(x.value, *args, **kwargs)\n",
        "    return Tensor(value=output)\n",
        "\n",
        "class wrappers:\n",
        "  def __init__(self, f):\n",
        "    self.f = f\n",
        "\n",
        "  def __call__(self, *args):\n",
        "    with Tensor.Keep():\n",
        "      wrapped_args = [Tensor(value=x) for x in args]\n",
        "      return convert(self.f(*wrapped_args))\n",
        "\n",
        "class grads:\n",
        "  def __init__(self, f):\n",
        "    self.f = f\n",
        "    self.wrapper = wrappers(f)\n",
        "\n",
        "  def __call__(self, *args):\n",
        "    output = tfbase.grads(create_once_callable(self.wrapper))(to_js([arg.value for arg in args]))\n",
        "    return [Tensor(value=x) for x in output]\n",
        "\n",
        "tf = Tensor(value=tfbase)\n",
        "Plotbase = SimpleNamespace(**Plotbase)\n",
        "Plotlybase = SimpleNamespace(**Plotlybase)\n",
        "d3base = SimpleNamespace(**d3base)\n",
        "\n",
        "def meshgrid(*args):\n",
        "  return tuple([Tensor(value=a) for a in tfbase.meshgrid(*[convert(arg) for arg in args])])\n",
        "tf.meshgrid = meshgrid\n",
        "\n",
        "def default_convert(obj, default_f, other):\n",
        "  if isinstance(obj, Tensor):\n",
        "    obj = obj.t2Js()\n",
        "  if isinstance(obj, pandas.DataFrame):\n",
        "    obj = obj.to_dict('records') \n",
        "  return default_f(obj)\n",
        "\n",
        "def plotconvert(a):\n",
        "  return to_js(a, dict_converter=Object.fromEntries, default_converter=default_convert)\n",
        "\n",
        "class PlotWrapper:\n",
        "  def __init__(self, base=None):\n",
        "    self.base = base\n",
        "    \n",
        "  def __getattr__(self, name):\n",
        "    attr = getattr(self.base, name)\n",
        "    if callable(attr):\n",
        "      def run(*args, **kwargs):\n",
        "        args = [plotconvert(a) for a in args]\n",
        "        kwargs = {k: plotconvert(a) for (k, a) in kwargs.items()}\n",
        "        return attr(*args, **kwargs)\n",
        "      return run\n",
        "    return attr\n",
        "\n",
        "Plot = PlotWrapper(Plotbase)\n",
        "Plotly = PlotWrapper(Plotlybase)\n",
        "d3 = PlotWrapper(d3base)\n",
        "\n",
        "def PlotlyFigure(width=800, height=None, hide_toolbar=True, overlay=True):\n",
        "  if height is None:\n",
        "    height = 0.75 * width\n",
        "\n",
        "  width, height = int(width), int(height)\n",
        "  container = document.createElement('div')\n",
        "  container.style.width = str(width) + 'px'\n",
        "  container.style.height = str(height) + 'px'\n",
        "  \n",
        "  lineplot = document.createElement('div')\n",
        "  lineplot.classList.add(\"plotlydiv\")\n",
        "\n",
        "  if hide_toolbar:\n",
        "    container.classList.add(\"hidetoolbar\")\n",
        "\n",
        "  container.append(lineplot)\n",
        "  if overlay:\n",
        "    overlay = document.createElement('div')\n",
        "    overlay.classList.add(\"plotlyoverlay\")\n",
        "    \n",
        "    container.append(overlay)\n",
        "    \n",
        "    container.style.position = 'relative'\n",
        "    overlay.style.top = '0'\n",
        "    overlay.style.bottom = '0'\n",
        "    overlay.style.width = '100%'\n",
        "    overlay.style.position = 'absolute'\n",
        "  return container\n",
        "  \n",
        "def PlotlyInput(width=800, height=None, hide_toolbar=True, sync=None):\n",
        "  container = PlotlyFigure(width, height, hide_toolbar)\n",
        "  lineplot, overlay = container.childNodes[0], container.childNodes[1]\n",
        "  if sync is None:\n",
        "    sync = container\n",
        "\n",
        "  class mover:\n",
        "    def __init__(self):\n",
        "      self.mousedown = False\n",
        "    \n",
        "    def __call__(self, event):\n",
        "      if event.type == 'mousedown':\n",
        "        self.mousedown = True\n",
        "      if event.type == 'mouseleave':\n",
        "        self.mousedown = False\n",
        "      if event.type == 'mouseup':\n",
        "        self.mousedown = False\n",
        "  \n",
        "      if self.mousedown:\n",
        "        x = float(lineplot._fullLayout.xaxis.p2c(event.layerX - lineplot._fullLayout.margin.l))\n",
        "        y = float(lineplot._fullLayout.yaxis.p2c(event.layerY - lineplot._fullLayout.margin.t))\n",
        "        sync.value = to_js([x, y])\n",
        "        dispatchEvent(sync)\n",
        "        \n",
        "  \n",
        "  e = mover()\n",
        "  overlay.addEventListener('mousemove', to_js(e))\n",
        "  overlay.addEventListener('mousedown', to_js(e))\n",
        "  overlay.addEventListener('mouseup', to_js(e))\n",
        "  overlay.addEventListener('mouseleave', to_js(e))\n",
        "  container.value = to_js([0., 0.])\n",
        "  return container\n",
        "\n",
        "def PlotlyReactive(container, traces=[], layout={}, options={}):\n",
        "  full_layout = dict(width=int(container.style.width.replace('px', '')), height=int(container.style.height.replace('px', '')))\n",
        "  full_layout.update(layout)\n",
        "  full_options = {'displayModeBar' : not container.classList.contains('hidetoolbar')}\n",
        "  full_options.update(options)\n",
        "  plot = container.childNodes[0]\n",
        "  Plotly.react(plot, traces, full_layout, full_options)\n",
        "\n",
        "def colorMap(t, cmap='inferno', cmin=None, cmax=None, scale='linear', res=100):\n",
        "  import matplotlib.cm as cm\n",
        "  if cmin is None:\n",
        "    cmin = tf.min(t)\n",
        "  if cmax is None:\n",
        "    cmax = tf.max(t)\n",
        "  \n",
        "  t = (t - cmin) / (cmax - cmin)\n",
        "  if scale == 'log':\n",
        "    e = tf.exp(1)\n",
        "    t = t * (e - 1) + 1\n",
        "    t = tf.log(t)\n",
        "  cmap = Tensor(cm.get_cmap(cmap, res + 1)(range(res + 1)))\n",
        "  t = t * res\n",
        "  shape = t.shape\n",
        "  tflat = tf.reshape(t, [-1])\n",
        "  tfloor = tf.gather(cmap, tf.floor(tflat).cast('int32'))\n",
        "  tceil = tf.gather(cmap, tf.ceil(tflat).cast('int32'))\n",
        "  tfrac = tf.reshape(tflat - tf.floor(tflat), [-1, 1])\n",
        "  tflat = tfrac * tceil + (1. - tfrac) * tfloor\n",
        "  t = tf.reshape(tflat, list(shape) + [4])\n",
        "  return t\n",
        "\n",
        "def plotTensor(t, canvas, size=None, cmap=None, interpolation='nearest', **kwargs):\n",
        "  if not (cmap is None):\n",
        "    t = colorMap(t, cmap, **kwargs)\n",
        "  if size is None:\n",
        "    size = (canvas.height, canvas.width)\n",
        "  if interpolation == 'bilinear':\n",
        "    t = tfbase.image['resizeBilinear'](t.value, list(size))\n",
        "  else:\n",
        "    t = tfbase.image['resizeNearestNeighbor'](t.value, list(size))\n",
        "  tfbase.browser['toPixels'](t, canvas)\n",
        "\n",
        "from itertools import chain\n",
        "import math\n",
        "\n",
        "class Module:\n",
        "    def __init__(self):\n",
        "        self._submodules = dict()\n",
        "        self.eval = False\n",
        "        self._store = False\n",
        "\n",
        "    def parameters(self):\n",
        "        return chain.from_iterable(map(lambda x: x.parameters(), self._submodules.values()))\n",
        "    \n",
        "    def __setattr__(self, name, value):\n",
        "        if isinstance(value, Module):\n",
        "            self._submodules[name] = value\n",
        "        super().__setattr__(name, value)\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        value = self.forward(*args, **kwargs)\n",
        "        self._store = False\n",
        "        return value\n",
        "    \n",
        "    def forward(self):\n",
        "        raise NotImplementedError()\n",
        "    \n",
        "    def train(self):\n",
        "        self.eval = False\n",
        "        for sm in self._submodules.values():\n",
        "            sm.train()\n",
        "    \n",
        "    def eval(self):\n",
        "        self.eval = True\n",
        "        for sm in self._submodules.values():\n",
        "            sm.eval()\n",
        "\n",
        "    def store(self):\n",
        "        self.store = True\n",
        "        for sm in self._submodules.values():\n",
        "            sm.eval()\n",
        "\n",
        "class Parameter(Module):\n",
        "    def __init__(self, value):\n",
        "        super().__init__()\n",
        "        self.value = value\n",
        "        self.temp = None\n",
        "        self.grad = None\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self]\n",
        "    \n",
        "class Sequential(Module):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__()\n",
        "        self.sequence = []\n",
        "        for arg in args:\n",
        "            if isinstance(arg, Module):\n",
        "                self.sequence.append(arg)\n",
        "            else:\n",
        "                self.sequence.extend(arg)\n",
        "        \n",
        "        self._submodules = {k: v for (k,v) in enumerate(self.sequence)}\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.sequence[index]\n",
        "\n",
        "    def forward(self, X):\n",
        "        for m in self.sequence:\n",
        "            X = m(X)\n",
        "        return X\n",
        "    \n",
        "ModuleList = Sequential\n",
        "\n",
        "class Sigmoid(Module):\n",
        "    def forward(self, X):\n",
        "        return tf.sigmoid(X)\n",
        "    \n",
        "class ReLU(Module):\n",
        "    def forward(self, X):\n",
        "        return tf.relu(X)\n",
        "    \n",
        "class Tanh(Module):\n",
        "    def forward(self, X):\n",
        "        return tf.tanh(X)\n",
        "    \n",
        "class Linear(Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        # Kaiming He initialization\n",
        "        self.W = Parameter(tf.randomNormal([in_features, out_features]) * math.sqrt((2 / out_features) / 3))\n",
        "        self.b = Parameter(tf.randomNormal([out_features]) * math.sqrt((2 / out_features) / 3))\n",
        "        self.input = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Returns a new Matrix\n",
        "        self.input = None\n",
        "        return tf.dot(x, self.W) + self.b\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "class Optimizer:\n",
        "    def __init__(self, model, loss=None, store=False):\n",
        "        self.parameters = list(model.parameters())\n",
        "        self.model = model\n",
        "        self.loss = loss\n",
        "        self.store = store\n",
        "\n",
        "    def _grads(self, loss, *args, **kwargs):\n",
        "        def loss_internal(*params):\n",
        "            for val, param in zip(params, self.parameters):\n",
        "                param.temp = param.value\n",
        "                param.value = val\n",
        "            try:\n",
        "                l = loss(self.model, *args, **kwargs)\n",
        "            finally:\n",
        "                for param in self.parameters:\n",
        "                    param.value = param.temp\n",
        "                    param.temp = None\n",
        "            return l\n",
        "        \n",
        "        return grads(loss_internal)(*map(lambda p: p.value, self.parameters))\n",
        "    \n",
        "    def _step(self, grads):\n",
        "        raise NotImplementedError()\n",
        "    \n",
        "    def step(self, *args, **kwargs):\n",
        "        grads = self._grads(self.loss, *args, **kwargs)\n",
        "        if self.store:\n",
        "          for grad, param in zip(grads, self.parameters):\n",
        "            param.grad = grad\n",
        "        return self._step(grads)\n",
        "    \n",
        "    def stepWithLoss(self, loss, *args, **kwargs):\n",
        "        grads = self._grads(loss, *args, **kwargs)\n",
        "        return self._step(grads)\n",
        "    \n",
        "class SGD(Optimizer):\n",
        "    def __init__(self, model, loss, lr=0.001, store=False):\n",
        "        super().__init__(model, loss, store)\n",
        "        self.lr = lr\n",
        "\n",
        "    def _step(self, grads):\n",
        "        for grad, param in zip(grads, self.parameters):\n",
        "            param.value = param.value - self.lr * grad\n",
        "`\n",
        "  \n",
        "  return py;\n",
        "}\n",
        "```\n",
        "\n",
        "```{ojs}\n",
        "//| echo : false\n",
        "mpg = FileAttachment(\"auto-mpg.csv\").csv()\n",
        "```\n",
        "\n",
        "```{ojs}\n",
        "//| echo : false\n",
        "//| output: false\n",
        "data = py`\n",
        "# ${plots}\n",
        "# Setup the data and prediction functions\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(${mpg})[['weight', 'mpg']]\n",
        "df = df.astype(float).dropna().values\n",
        "\n",
        "x, y = df[:, :1], df[:, 1:]\n",
        "x = Tensor((x - x.mean()) / x.std())\n",
        "y = Tensor((y - y.mean()) / y.std()) > -0\n",
        "\n",
        "def get_batch(batchsize, x, y):\n",
        "  return x, y\n",
        "\n",
        "scale = Tensor([[1., 1.]])\n",
        "\n",
        "def predict(w, x, y=None):\n",
        "  w = w.reshape((-1, 2)) * scale\n",
        "  x = x.reshape((-1, 1))\n",
        "  x = tf.concat([x, tf.onesLike(x)], 1)\n",
        "  if y is None:\n",
        "    return tf.sigmoid(tf.dot(x, w.T))\n",
        "  else:\n",
        "    y = y.reshape((-1, 1))\n",
        "    z = tf.dot(x, w.T)\n",
        "    return y * tf.sigmoid(z) + (1 - y) * tf.sigmoid(-z) \n",
        "\n",
        "wrange = tf.linspace(-15, 5, 25)\n",
        "brange = tf.linspace(-10, 10, 25)\n",
        "ww, bb = tf.meshgrid(wrange, brange)\n",
        "paramgrid = tf.stack([ww.flatten(), bb.flatten()]).T\n",
        "eyetheta = 0\n",
        "\n",
        "(x, y)\n",
        "`\n",
        "\n",
        "surfaces = py`\n",
        "# ${batch} ${plots}\n",
        "# Plot the loss surface\n",
        "\n",
        "\n",
        "l1weight = 0.\n",
        "l2weight = 0.\n",
        "\n",
        "\n",
        "def loss(w, x, y):\n",
        "  w = w.reshape((-1, 2))\n",
        "  return (tf.mean(-tf.log(predict(w, x, y)), 0)) + l1weight * tf.abs(w).sum(1) + l2weight * (w ** 2).sum(1) \n",
        "\n",
        "lossgrid = loss(paramgrid, x, y).reshape(ww.shape)\n",
        "losscontour = plotconvert(dict(x=wrange, y=brange, z=lossgrid, type='contour', ncontours=25, showscale=False, ))\n",
        "losssurface = plotconvert(dict(x=wrange, y=brange, z=lossgrid, type='surface', showlegend=False, showscale=False, opacity=0.8,  contours=dict(x=dict(show=True), y=dict(show=True))))\n",
        "`\n",
        "\n",
        "py`\n",
        "# ${surfaces}\n",
        "\n",
        "cweights = ${weights}\n",
        "startpoint = dict(x=[cweights[0]], y=[cweights[1]], mode='markers', showlegend=False, marker=dict(color='firebrick', size=10, line= {'color': 'black', 'width': 3}))\n",
        "\n",
        "\n",
        "fullweightlist = [Tensor(cweights)]\n",
        "batchweightlist = [Tensor(cweights)]\n",
        "steps = int(${steps})\n",
        "lr = float(${learningrate}) if steps > 0 else 0.\n",
        "\n",
        "momentum = 0.\n",
        "nxbatch, nybatch = batches[0]\n",
        "batchgrad = grad(lambda t: loss(t, nxbatch, nybatch))(batchweightlist[-1])\n",
        "beta = 0.\n",
        "velocity = batchgrad\n",
        "magnitude = batchgrad ** 2\n",
        "if beta > 0:\n",
        "  batchgrad = batchgrad / tf.sqrt(magnitude + 1e-8)\n",
        "\n",
        "for i, (nxbatch, nybatch) in zip(range(max(1, steps)), batches):\n",
        "  fullgrad = lr * grad(lambda t: loss(t, x, y))(fullweightlist[-1])\n",
        "\n",
        "  bgrad = grad(lambda t: loss(t, nxbatch, nybatch))(batchweightlist[-1])\n",
        "  velocity = momentum * velocity + (1 - momentum) * bgrad\n",
        "  magnitude = beta * magnitude + (1. - beta) * (bgrad ** 2)\n",
        "  batchgrad = velocity\n",
        "  if beta > 0:\n",
        "    batchgrad = velocity / tf.sqrt(magnitude + 1e-8)\n",
        "  \n",
        "  fullweightlist.append((fullweightlist[-1] - fullgrad).flatten())\n",
        "  batchweightlist.append((batchweightlist[-1] - lr * batchgrad).flatten())\n",
        "  \n",
        "\n",
        "fullweights = tf.stack(fullweightlist)\n",
        "batchweights = tf.stack(batchweightlist)\n",
        "\n",
        "gradplot = dict(x=fullweights[:, 0], y=fullweights[:, 1], showlegend=False, line=dict(color='black'))\n",
        "batchgradplot = dict(x=batchweights[:, 0], y=batchweights[:, 1], showlegend=False, line=dict(color='orange'))\n",
        "\n",
        "\n",
        "zloss = loss(fullweights, x, y)\n",
        "batchzloss = loss(batchweights, x, y)\n",
        "threedgradplot = dict(x=fullweights[:, 0], y=fullweights[:, 1], z=zloss, showlegend=False, marker=dict(size=4), line=dict(color='black', width=4), type='scatter3d')\n",
        "threedbatchgradplot = dict(x=batchweights[:, 0], y=batchweights[:, 1], z=batchzloss, showlegend=False, marker=dict(size=4), line=dict(color='orange', width=4), type='scatter3d')\n",
        "\n",
        "finalloss = zloss[0].t2Js()\n",
        "\n",
        "PlotlyReactive(lossplot, [losscontour, startpoint, gradplot, batchgradplot], {'title': 'Loss = %.3f' % finalloss, 'showlegend': False, 'xaxis': {'range': [-15, 5], 'title': 'Slope (w)'}, 'yaxis': {'range': [-10, 10], 'title': {\n",
        "      'text': 'Bias (b)'\n",
        "    }}})\n",
        "`\n",
        "\n",
        "loss = py`\n",
        "# ${batch}\n",
        "# Plot the data scatterplot and prediction function\n",
        "\n",
        "inweights = ${weights}\n",
        "cweights = Tensor(inweights)\n",
        "errors = -tf.log(predict(cweights, xbatch.reshape((-1,)), y).flatten())\n",
        "losses = (errors)\n",
        "batchdata = dict(x=xbatch.reshape((-1,)), y=ybatch.reshape((-1,)), mode='markers', marker=dict(color=tf.sqrt(losses), colorscale='RdBu', cmin=0, cmax=1))\n",
        "\n",
        "xrange = tf.linspace(-2, 3, 50)\n",
        "pfunction = dict(x=xrange.flatten(), y=predict(cweights, xrange).flatten(), line=dict(color='black'))\n",
        "\n",
        "\n",
        "PlotlyReactive(scatterfig, [ batchdata, pfunction], {'title': 'p(y=1|x) = σ(%.2f x + %.2f)' % (inweights[0], inweights[1]), 'showlegend': False, 'xaxis': {'range': [-2, 3], 'title': {'text': 'x (weight)'}}, 'yaxis': {'range': [-1, 2], 'title': {'text': 'y (MPG)'} }})\n",
        "\n",
        "histdata = dict(x=errors, type='histogram', xbins= dict(start=-3, end=3, size=0.15))\n",
        "losses.mean()\n",
        "`\n",
        "\n",
        "batch = py`\n",
        "# ${data}\n",
        "batchsize = 0\n",
        "batches = [get_batch(batchsize, x, y) for i in range(max(1, int(${steps})))]\n",
        "xbatch, ybatch = batches[0]\n",
        "`\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "::: {.column-screen .columns}\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "\n",
        "```{ojs}\n",
        "//| echo : false\n",
        "scatter = py`\n",
        "# Scatterplot figure\n",
        "scatterfig = PlotlyFigure(width=500, height=500)\n",
        "scatterfig\n",
        "`\n",
        "```\n",
        "\n",
        "```{ojs}\n",
        "//| echo : false\n",
        "viewof learningrate = Inputs.range([0, 100], {value: 1, step: 0.01, label: \" Learning rate\"})\n",
        "//learningrate = 0\n",
        "```\n",
        "\n",
        "\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "\n",
        "```{ojs}\n",
        "//| echo : false\n",
        "\n",
        "plots = py`\n",
        "lossplot = PlotlyInput(width=500, height=500)\n",
        "`\n",
        "\n",
        "viewof weights = py`\n",
        "# ${plots}\n",
        "lossplot\n",
        "`\n",
        "```\n",
        "\n",
        "```{ojs}\n",
        "//| echo : false\n",
        "viewof steps = Inputs.range([0, 10], {value: 0, step: 1, label: \"  Steps\"})\n",
        "```\n",
        "\n",
        "\n",
        ":::\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Comparing loss functions\n",
        "\n",
        "Let's look at how this loss function compares to the mean squared error loss we derived for logistic regression. One way to do this is to visualize the loss for a single observation as a function of the output of $\\mathbf{x}^T\\mathbf{w}$. Here we'll look at the loss for different models trying to predict an output of $y=0$:\n",
        "\n",
        "$$\n",
        "\\textbf{Let: }\\ y=0, \\quad z=\\mathbf{x}^T\\mathbf{w}\n",
        "$$\n"
      ],
      "id": "71a614b3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "%%manim -sqh -v CRITICAL --progress_bar none VectorAddition\n",
        "\n",
        "class VectorAddition(Scene):\n",
        "    def construct(self):\n",
        "        self.camera.background_color = \"#ffffff\"\n",
        "        axes = Axes(\n",
        "            x_range=[-3, 3, 1],\n",
        "            y_range=[-0.1, 2, 1],\n",
        "            x_length=12,\n",
        "            y_length=8,\n",
        "            axis_config={\"color\": GREY},\n",
        "        )\n",
        "        \n",
        "        axes_labels = axes.get_axis_labels()\n",
        "        #axes_labels.set_color(GREY)\n",
        "        self.add(axes, axes_labels)\n",
        "\n",
        "        plot = axes.plot(lambda x: x ** 2, color=BLUE)\n",
        "        plot2 = axes.plot(lambda x: -np.log(1 / (1 + np.exp(x))), color=RED)\n",
        "        plot3 = axes.plot_line_graph([-10, -0.0001,0.0001, 10], (np.array([-10, -0.0001,0.0001, 10]) > 0).astype(float), line_color=BLACK, add_vertex_dots=False)\n",
        "        self.add(plot, plot2, plot3)\n",
        "\n",
        "        eq0 = Tex(r'$\\mathbb{I}(z \\geq 0) =0 $', color=BLACK).to_corner(UR)\n",
        "        eq = Tex(r'$-\\log \\sigma(z)$', color=RED).next_to(eq0, DOWN)\n",
        "        eq2 = Tex(r'$z^2$', color=BLUE).next_to(eq, DOWN)\n",
        "        self.add(eq, eq2, eq0)"
      ],
      "id": "5eed39a5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the squared error loss is best when the output is exactly 0, while the logistic regression NLL wants the output of $\\mathbf{x}^T\\mathbf{w}$ to be a negative as possible so that $p(y=0\\mid \\mathbf{x}, \\mathbf{w}) \\longrightarrow 1$. Meanwhile the \"accuracy\" loss has no slope, making it impossible to optimize with gradient descent.\n",
        "\n",
        "# Multinomial logistic regression\n",
        "\n",
        "## Multi-class classification\n",
        "\n",
        "We've now seen a useful model for binary classification, but in many cases we want to predict between many different classes.\n",
        "\n",
        "![](pictures/catdogmouse.png){fig-align=\"center\"}\n",
        "\n",
        "We will typically use a set of integers $\\{1, 2,...,C\\}$ to denote the possible outputs for a general categorical function. Therefore we are considering functions of the form:\n",
        "\n",
        "$$\n",
        "y=f(\\mathbf{x}), \\quad \\text{Input: } \\mathbf{x} \\in\\mathbb{R}^n \\longrightarrow \\text{ Output: }y \\in \\{1, 2, ...,C\\}\n",
        "$$\n",
        "\n",
        "It's important to note that we do *not* want to assume that the *ordering* of labels is meaningful. For instance if we're classifying images of animals we might set the labels such that:\n",
        "\n",
        "$$\n",
        "\\textbf{1:  Cat},\\quad\n",
        "\\textbf{2:  Dog},\\quad\n",
        "\\textbf{3:  Mouse}\n",
        "$$\n",
        "\n",
        "But this shouldn't lead to different results to the case where we assign the labels as:\n",
        "\n",
        "$$\n",
        "\\textbf{1:  Dog},\\quad\n",
        "\\textbf{2:  Mouse},\\quad\n",
        "\\textbf{3:  Cat}\n",
        "$$\n",
        "\n",
        "We call prediction of a categorical output with more than two possibilities **multi-class** **classification**.\n",
        "\n",
        "## Multi-class prediction functions\n",
        "\n",
        "A symmetric approach to defining a prediction function for multi-class classification is to define a *separate* linear function for each class and choose the class whose function gives the largest output.\n",
        "\n",
        "If $C$ is the number of possible classes, we will therefore have $C$ different parameter vectors $\\mathbf{w}_1,…,\\mathbf{w}_C$ and our prediction function will be defined as:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = \\underset{c\\in\\{1...C\\}}{\\text{argmax}}\\ \\mathbf{x}^T\\mathbf{w}_c\n",
        "$$\n",
        "\n",
        "For convenience, we can also define a matrix that contains all $C$ parameter vectors:\n",
        "\n",
        "$$\n",
        "\\mathbf{W} = \\begin{bmatrix} \\mathbf{w}_1^T \\\\ \\mathbf{w}_2^T \\\\ \\vdots \\\\ \\mathbf{w}_C^T\\end{bmatrix} = \\begin{bmatrix} W_{11} & W_{12} & \\dots & W_{1d} \\\\ \n",
        "W_{21} & W_{22} & \\dots & W_{2d} \\\\ \n",
        "\\vdots & \\vdots & \\ddots & \\vdots\\\\ \n",
        "W_{C1} & W_{C2} & \\dots & W_{Cd}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "With this notation, our prediction function becomes:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = \\underset{c\\in\\{1...C\\}}{\\text{argmax}}\\ (\\mathbf{x}^T\\mathbf{W}^T)_c, \\quad \\mathbf{W} \\in \\mathbb{R}^{C\\times d}\n",
        "$$\n",
        "\n",
        "## Multi-class decision boundaries\n",
        "\n",
        "If we only have two classes $0$ and $1$, so $C=2$, then this multi-class prediction function reduces to the same as our binary prediction function. We can see this by noting that $x > y \\equiv x-y>0$:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = \\underset{c\\in\\{0,1\\}}{\\text{argmax}}\\ (\\mathbf{x}^T\\mathbf{W}^T)_c = \\mathbb{I}(\\mathbf{x}^T\\mathbf{w}_1 - \\mathbf{x}^T\\mathbf{w}_0 \\geq 0)\n",
        "$$\n",
        "\n",
        "If we factor out $\\mathbf{x}$ we see that we can simply define a new parameter vector in order to get the same decision rule.\n",
        "\n",
        "$$\n",
        "=\\mathbb{I}(\\mathbf{x}^T(\\mathbf{w}_1 - \\mathbf{w}_0) \\geq 0) \\quad \\longrightarrow \\quad \\mathbb{I}(\\mathbf{x}^T\\mathbf{w} \\geq 0), \\quad \\mathbf{w}=\\mathbf{w}_1-\\mathbf{w}_0\n",
        "$$\n",
        "\n",
        "It follows that the decision boundary between any two classes is also linear! We can see this by plotting a prediction function. In this case for the *Iris* dataset we saw in the homework.\n",
        "\n",
        "## Categorical distribution\n",
        "\n",
        "As a first step towards finding the optimal $\\mathbf{W}$ for a multi-class model, let's look at a distribution over multiple discrete outcomes: the **Categorical** distribution.\n",
        "\n",
        "A categorical distribution needs to define a probability for each possible output. We'll use $q_c$ to denote the probability of output $c$.\n",
        "\n",
        "$$\n",
        "p(y=c) = q_c, \\quad y\\in \\{1...C\\}\n",
        "$$\n",
        "\n",
        "We can then denote the vector of all $C$ probabilities as $\\mathbf{q}$. Note that in order for this to be valid, every probability needs to be in the range $[0,1]$ and the total probability of all outcomes needs to be $1$, so:\n",
        "\n",
        "$$\n",
        "\\mathbf{q} \\in \\mathbb{R}^C\\quad q_c \\geq 0\\ \\forall c\\in \\{1...C\\}\\quad \\sum_{c=1}^C q_c=1\n",
        "$$\n",
        "\n",
        "As with the Bernoulli distribution, we can write this in a more compact form. Here we see that the probability of a given outcome is simply the corresponding entry in $\\mathbf{q}$\n",
        "\n",
        "$$\n",
        "p(y)=\\prod q_c^{\\mathbb{I}(y=c)} = q_y\n",
        "$$\n",
        "\n",
        "Thus the log-probability is simply:\n",
        "\n",
        "$$\n",
        "\\log p(y) = \\sum_{c=1}^C \\mathbb{I}(y=c)\\log q_c = \\log q_y\n",
        "$$\n",
        "\n",
        "## A probabilistic model for multi-class classification\n",
        "\n",
        "With the Categorical distribution defined, we can now ask if we can use it to define a linear probabilistic model for multi-class categorical outputs. As with our other models we'll consider making the distribution parameter a linear function of our input.\n",
        "\n",
        "$$\n",
        "y_i\\sim \\mathbf{Categorical}(\\mathbf{q}=?), \\quad \\mathbf{q}=\\mathbf{x}_i^T\\mathbf{W}^T?\n",
        "$$\n",
        "\n",
        "However, we once again run into the issue that the output of our linear function likely won't satisfy the conditions we need for the parameter of a categorical distribution. In particular, the output is not guaranteed to be positive or to sum to $1$.\n",
        "\n",
        "$$\n",
        "\\mathbf{x}^T\\mathbf{W}^T\\in \\mathbb{R}^C,\\quad  q_c \\ngeq 0\\ \\forall c\\in \\{1...C\\}, \\quad \\sum_{c=1}^C q_c\\neq1\n",
        "$$\n",
        "\n",
        "In this case we need a way to map arbitrary vectors to vectors that satisfy these conditions:\n",
        "\n",
        "$$\n",
        "\\textbf{Need }\\ f(\\mathbf{x}):\\ \\mathbb{R}^C \\longrightarrow [0,\\infty)^C,\\ \\sum_{i=1}^Cf(\\mathbf{x})_c = 1\n",
        "$$\n",
        "\n",
        "## Softmax function\n",
        "\n",
        "Such a mapping exists in the **softmax** function. This function maps vectors to positive vectors such that the entries sum to $1$. Entry $c$ of $\\text{softmax}(\\mathbf{x})$ can be written as:\n",
        "\n",
        "$$\n",
        "\\text{softmax}(\\mathbf{x})_c = \\frac{e^{x_c}}{\\sum_{j=1}^Ce^{x_j}}\n",
        "$$\n",
        "\n",
        "We can also define the softmax function using vector notation as:\n",
        "\n",
        "$$\n",
        "\\text{softmax}(\\mathbf{x}) = \\begin{bmatrix}\\frac{e^{x_1}}{\\sum_{j=1}^Ce^{x_j}} \\\\ \\frac{e^{x_2}}{\\sum_{j=1}^Ce^{x_j}} \\\\ \\vdots \\\\ \\frac{e^{x_C}}{\\sum_{j=1}^Ce^{x_j}} \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Intuitively, $e^x$ is positive for any $x$, while dividing by the sum ensure the entries sum to 1 as:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^C \\frac{e^{x_i}}{\\sum_{j=1}^Ce^{x_j}} = \\frac{\\sum_{i=1}^C e^{x_i}}{\\sum_{j=1}^Ce^{x_j}} = 1\n",
        "$$\n",
        "\n",
        "The softmax function also has the nice property that\n",
        "\n",
        "$$\n",
        "\\underset{c\\in\\{1,...,C\\}}{\\text{argmax}}\\ \\mathbf{x}_c = \\underset{c\\in\\{1,...,C\\}}{\\text{argmax}}\\ \\text{softmax}(\\mathbf{x})_c\n",
        "$$\n",
        "\n",
        "## Multonomial logistic regression\n",
        "\n",
        "With the softmax function we can now define our probabilistic model for categorical labels as:\n",
        "\n",
        "$$\n",
        "y_i\\sim \\mathbf{Categorical}\\big(\\text{softmax}(\\mathbf{x}^T\\mathbf{W})\\big)\n",
        "$$\n",
        "\n",
        "We see that under this assumption, the probability of a particular output $(c)$ is:\n",
        "\n",
        "$$\n",
        "p(y_i=c \\mid \\mathbf{x}, \\mathbf{W}) = \\text{softmax}(\\mathbf{x}^T\\mathbf{W})_c=\\frac{e^{\\mathbf{x}^T\\mathbf{w}_c}}{\\sum_{j=1}^Ce^{\\mathbf{x}^T\\mathbf{w}_j}}\n",
        "$$\n",
        "\n",
        "We call this particular probabilistic model: **multinomial logistic regression**\n",
        "\n",
        "## Maximum likelihood estimation for multinomial logistic regression\n",
        "\n",
        "We now have everything we need to define our negative log-likelihood loss for the multi-class classification model. Once again our loss is the negative sum of the log-probability of each observed output:\n",
        "\n",
        "$$\n",
        "\\textbf{Loss}(\\mathbf{W}) =\\textbf{NLL}(\\mathbf{W}, \\mathbf{X}, \\mathbf{y})=- \\sum_{i=1}^N \\log p(y_i \\mid \\mathbf{x}_i, \\mathbf{W}) \n",
        "$$\n",
        "\n",
        "Using the log-probability of the multinomial logistic regression model we get:\n",
        "\n",
        "$$\n",
        "\\textbf{NLL}(\\mathbf{W}, \\mathbf{X}, \\mathbf{y})= -\\sum_{i=1}^N \\log\\ \\text{softmax}(\\mathbf{x}_i^T\\mathbf{W}^T)_{y_i} = -\\sum_{i=1}^N  \\log \\frac{e^{\\mathbf{x}_i^T\\mathbf{w}_{y_i}}}{\\sum_{j=1}^Ce^{\\mathbf{x}_i^T\\mathbf{w}_{j}}}\n",
        "$$\n",
        "\n",
        "We can simplify this further to:\n",
        "\n",
        "$$\n",
        "\\textbf{NLL}(\\mathbf{W}, \\mathbf{X}, \\mathbf{y})=-\\sum_{i=1}^N \\bigg(\\mathbf{x}_i^T\\mathbf{w}_{y_i}- \\log\\sum_{j=1}^Ce^{\\mathbf{x}_i^T\\mathbf{w}_{j}}\\bigg)\n",
        "$$\n",
        "\n",
        "## Gradient descent\n",
        "\n",
        "In this case our parameters are a matrix $\\mathbf{W}$. The concept of a gradient, extends naturally to a matrix; we simply define the gradient matrix such that each element is the partial derivative with respect to the corresponding element of the input. For the multinomial logistic regression loss, the gradient this looks like:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\mathbf{W}} \\mathbf{NLL}(\\mathbf{W}, \\mathbf{X}, \\mathbf{y})= \\begin{bmatrix} \\frac{\\partial \\mathbf{NLL}}{\\partial W_{11}} & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{12}} & \\dots & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{1C}} \\\\ \n",
        "\\frac{\\partial \\mathbf{NLL}}{\\partial W_{21}} & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{22}} & \\dots & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{2C}} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
        "\\frac{\\partial \\mathbf{NLL}}{\\partial W_{d1}} & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{d2}} & \\dots & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{dC}}\n",
        " \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "We can still apply the same gradient descent updates in this case!\n",
        "\n",
        "$$\n",
        "\\mathbf{W}^{(i+1)} \\leftarrow \\mathbf{W}^{(i)} - \\alpha \\nabla_{\\mathbf{W}} \\mathbf{NLL}(\\mathbf{W}^{(i)}, \\mathbf{X}, \\mathbf{y})\n",
        "$$"
      ],
      "id": "85cb9408"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}