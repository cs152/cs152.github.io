---
title: "Lecture 9: Optimization"
format:
    html:
        toc: true
        toc-depth: 3
---

# Playground

Try out the concepts from this lecture in the [Neural Network Playground!](https://cs152-neural-networks-fall-2023.github.io/playground)

# Initialization

So far we've seen how train neural-networks with gradient descent. Recall that the gradient descent update for a weight $\mathbf{w}$ at step $k$ is: $$ \mathbf{w}^{(k+1)} \longleftarrow \mathbf{w}^{(k)} - \alpha \nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y})$$

We subtract the gradient of the loss with respect to $\mathbf{w}$ from the current estimate of $\mathbf{w}$. An important consideration for this algorithm is how to set the initial guess $\mathbf{w}^{(0)}$. We call this process **initialization**.

## Symmetry-breaking

In neural networks, we typically initialize parameters *randomly*. One important reason for random initialization is to make sure that different parameters have different starting values. To see why this is needed, let's consider the prediction function for a simple neural network that takes in 1-dimensional inputs:

$$
f(\mathbf{x}) = \sigma(\mathbf{x}^T\mathbf{W}_1)^T\mathbf{w}_0=\sigma(x_1 w_{11}) w_{01} +\sigma (x_1 w_{12})w_{02}
$$

In this case we have 4 parameters: $w_{01}, w_{02}, w_{11}, w_{12}$. If we initialize all to the same value, say $w_{**} = a$, let's see what happens to the derivatives we compute:

$$
\frac{d}{dw_{01}} f(\mathbf{x}) = \sigma(x_1 w_{11}) = \sigma(x_1 a)
$$

$$
\frac{d}{dw_{02}} f(\mathbf{x}) = \sigma(x_1 w_{12}) = \sigma(x_1 a)
$$

We see that $\frac{d}{dw_{01}} = \frac{d}{dw_{02}}$! Our gradient descent update will set:

$$
w_{01}^{(1)} \longleftarrow w_{01}^{(0)} - \alpha \frac{d}{dw_{01}} = a - \alpha \sigma(x_1 a)
$$

$$
w_{02}^{(1)} \longleftarrow w_{02}^{(0)} - \alpha \frac{d}{dw_{02}} = a - \alpha \sigma(x_1 a)
$$

So after each gradient descent update the two values will continue to be the same! The gradient decent algorithm has no way to distinguish between these two weights and so it is stuck finding solutions where $w_{01} = w_{02}$ and $w_{11}=w_{12}$. We call this the symmetry problem, and it means we no longer get any benefit from having multiple neurons.

We can see this in practice with a simple network:

::: columns
::: {.column width="45%"}
![](images/paste-1.png)\
When the network is initialized with symmetry, the two neurons will always have the same output and our solution is poor.
:::

::: {.column width="10%"}
:::

::: {.column width="45%"}
![](images/paste-2.png)

When initialized randomly, the two neurons can create different transforms and a much better solution is found.
:::
:::

If we plot the loss as a function of two $w_{01}$ and $w_{02}$ we can see what is happening graphically.

::: columns
::: {.column width="45%"}
![](images/paste-3.png)

Initializing the two parameters equal corresponds to sitting on a ridge of the loss surface, there are equally valid solutions on either side, but gradient descent gives us no way to chose between them.
:::

::: {.column width="10%"}
:::

::: {.column width="45%"}
![](images/paste-6.png)

If we plot the (negative) gradient of the loss we see that the gradient of any point on the ridge always points along the ridge. Gradient descent corresponds to following these arrows to find a minimum.
:::
:::

## Visualizing learning rates

As an aside, plotting the gradient as a vector field also gives us an convenient way to visualize the effects of different learning rates. Recall that the learning rate corresponds to how much we *scale* the gradient each time we take a step.

::: columns
::: {.column width="30%"}
![](images/paste-11.png)

A small learning rate means we will move slowly, so It may take a long time to find the minimum.
:::

::: {.column width="5%"}
:::

::: {.column width="30%"}
![](images/paste-10.png)

A well-chosen learning rate lets us find a minimum quickly.
:::

::: {.column width="5%"}
:::

::: {.column width="30%"}
![](images/paste-13.png)

A too-large learning rate means that steps may take us flying past the minimum!
:::
:::

## Scaled initialization

Now that we've seen the benefits of initializing randomly, we need to consider what distribution to initialize from. An obvious choice might be a standard normal distribution, with mean $0$ and standard deviation $1$.

$$w_{i} \sim \mathcal{N}(0, 1) \quad \forall\ w_{i} \in \mathbf{w}$$This has a subtle issue though. To see why let's consider a linear function defined by randomly initialized weights:

$$
f(\mathbf{x}) = \sum_{i=1}^d x_i w_i
$$

Let's consider the mean and variance of this output with respect to $\mathbf{w}$:

$$
\mathbb{E} \big[f(\mathbf{x})\big] = \mathbb{E} \bigg[  \sum_{i=1}^d x_i w_i \bigg] =   \sum_{i=1}^d x_i \mathbb{E} \big[w_i \big] = 0, \quad w_i \sim \mathcal{N}(0, 1)
$$

$$
\text{Var} \big[f(\mathbf{x})\big] = \text{Var}  \bigg[  \sum_{i=1}^d x_i w_i \bigg] =   \sum_{i=1}^d \text{Var} \big[ x_i w_i \big] = \sum_{i=1}^d x_i^2 \text{Var} [w_i] = \sum_{i=1}^d x_i^2 
$$

We see a few things here, the mean is $0$ and the variance depends on $x_i$, which is reasonable. However we see that the variance also depends on $d$, the dimensionality of the input. In particular it's $\mathcal{O}(d)$. Why is this important? Because it means that if we increase the number of neurons at each layer in our network, the variance of the network's predictions will also increase!

If our network has many neurons in each layer (large networks can have 1000's!) the variance of outputs can be extreme, leading to poor initializations that correspond to extremely steep prediction functions. Here we can compare a few intializations from a network with just 8 neurons per layer to a network with 2.

::: columns
::: {.column width="45%"}
![](images/paste-14.png)

![](images/paste-15.png)

![](images/paste-16.png)
:::

::: {.column width="10%"}
:::

::: {.column width="45%"}
![](images/paste-17.png)

![](images/paste-18.png)

![](images/paste-19.png)
:::
:::

In practice this can make gradient descent difficult as these initialization are often very far from the minimum and the gradients are typically large, meaning small learning rates are needed to prevent divergence.

A better approach would keep the variance consistent no matter how many inputs there are. We can reduce the variance by dividing our initial weights by some scale factor $s$.

$$
f(\mathbf{x}) = \sum_{i=1}^d x_i w_i\bigg(\frac{1}{s}\bigg)
$$

If we want the variance to be independent of $d$, then we want:

$$
s = \sqrt{d}
$$

We can verify this by computing the variance:

$$
\text{Var}  \bigg[  \sum_{i=1}^d x_i w_i \bigg(\frac{1}{\sqrt{d}}\bigg) \bigg] =   \sum_{i=1}^d \text{Var} \bigg[ x_i w_i \bigg(\frac{1}{\sqrt{d}}\bigg) \bigg] = \sum_{i=1}^d x_i^2 \bigg(\frac{1}{\sqrt{d}}\bigg)^2 \text{Var} [w_i] = \frac{1}{d}\sum_{i=1}^d x_i^2 
$$

This is equivalent to drawing our initial weights for each layer from a normal distribution with standard deviation equal to 1 over the square root of the number of inputs:

$$w_{i} \sim \mathcal{N}\bigg(0, \frac{1}{\sqrt{d}}\bigg) \quad \forall\ w_{i} \in \mathbf{w},\ \mathbf{w}\in \mathbb{R}^{d}$$

This is known as **Kaiming normal initialization** (sometimes also called **He initialization**, after the inventor Kaiming He).

For neural network layers where the weights are a matrix $\mathbf{W} \in \mathbb{R}^{d \times e}$, this works the same way:

$$w_{ij} \sim \mathcal{N}\bigg(0, \frac{1}{\sqrt{d}}\bigg) \quad \forall\ w_{ij} \in \mathbf{W},\ \mathbf{w}\in \mathbb{R}^{d \times e}$$

A popular alternative scales the distribution according to both the number of inputs and outputs of the layer:

$$w_{ij} \sim \mathcal{N}\bigg(0, \sqrt{\frac{2}{d + e}}\bigg) \quad \forall\ w_{ij} \in \mathbf{W},\ \mathbf{w}\in \mathbb{R}^{d \times e}$$

This is known as **Xavier initialization** (or **Glorot initialization** after the inventor Xavier Glorot).

We can compare initializations from a standard normal with initializations from a Kaiming normal.

::: columns
::: {.column width="45%"}
**Standard normal** $w_{i} \sim \mathcal{N}\bigg(0, 1\bigg)$

![](images/paste-14.png)

![](images/paste-15.png)

![](images/paste-16.png)
:::

::: {.column width="10%"}
:::

::: {.column width="45%"}
**Kaiming normal** $w_{i} \sim \mathcal{N}\bigg(0, \frac{1}{\sqrt{d}}\bigg)$

![](images/paste-24.png)

![](images/paste-25.png)

![](images/paste-26.png)
:::
:::

# Stochastic Gradient Descent

[Visualizer](./viz.qmd)

## Computational complexity

An important consideration for any algorithm is the computational complexity. So far we've ignored the computational cost of training a neural network, but in practice it can be quite significant! It's worthwhile to think about what this complexity is and if there's any way to improve it.

Let's start by reviewing the factors that will determine the cost of running gradient descent to train a network and define some notation for each:

-   **Dataset size (** $N$ **):** As we have previously, we'll use $N$ to denote the number of observations in the dataset that we'll use to train our model (the *training set*).

-   **Dimensionality (** $d$ **):** We'll use the notation $d$ to refer to *both* the **number of input features** *and* the **number of neurons per layer**. In general this will let us conveniently refer to the number of inputs and outputs for any given layer with a single number. In practice the number of input dimensions may not match the number of neurons per layer (and the number of neurons per layer may not actually be constant!), but since we're mostly concerned with an asymptotic upper bound on complexity, we'll just assume $d$ is the size of the largest layer.

-   **Number of layers (** $L$ **):** The other factor of our network architecture is of course the number of layers, which we'll call $L$.

-   **Number of steps (** $S$ **):** Finally how steps of gradient descent we take will of course also have an input on the running time. We'll use $S$ to denote this number.

We can consider one of the networks shown above as a specific example:

![](images/paste-24.png)

Here we see that this network has $8$ neurons per layer, so $d=8$, a total of $6$ hidden layers, so $L=6$ and the training set shown has roughly $100$ observations, so $N=100$. We won't worry about the number of steps ( $S$ ) for now.

In this example our neural network is being used on a regression task, so at each step of gradient descent we'll to compute the mean squared error loss:

$$\underset{\text{MSE}}{\textbf{Loss}} (\mathbf{w}, \mathbf{X}, \mathbf{y}) = \frac{1}{N}\sum_{i=1}^N (f(\mathbf{x}_i, \mathbf{w}) - y_i)^2$$And then it's gradient with respect to the parameters, $\mathbf{w}$.

$$\nabla_{\mathbf{w}}\underset{\text{MSE}}{\textbf{Loss}} (\mathbf{w}, \mathbf{X}, \mathbf{y}) = \nabla_{\mathbf{w}}\frac{1}{N}\sum_{i=1}^N (f(\mathbf{x}_i, \mathbf{w}) - y_i)^2$$

Our analysis would be equivalent for classification problems.

## Computing a single neuron

Let's start simple, what is the cost computing a *single* neuron in our network? We know that the calculation that a single neuron performs is (we'll ignore a bias term because it could be analyzed as a single extra term of the summation) :

$$\phi(\mathbf{x})=\sigma\big(\mathbf{x}^T\mathbf{w} \big) = \sigma\bigg(\sum_{i=1}^d x_i w_i \bigg)$$

We see that computing activation function $\sigma(\cdot)$ is constant time, but the cost of the summation: $\sum_{i=1}^d x_i w_i$ will scale linearly with the dimension $d$, so we can write the running time generally as $\mathcal{O}(d)$.

That was straightforward, but we're not quite done with a single neuron yet! Remember that we're also going to be computing gradients, so we'll need to make sure that the cost of our backward pass update for this neuron won't increase our asymptotic running time.

Recall that in our backward pass update we're provided $\frac{dLoss}{d\phi}$ and we need to compute$\frac{dLoss}{dx_i}$ $\frac{dLoss}{dw_i}$. Once again we see that the activation $\sigma$ only acts on a single value, so the update will simply be: $$\frac{dLoss}{d(\mathbf{x}^T\mathbf{w})}=\frac{dLoss}{d\phi}\frac{d\phi}{d(\mathbf{x}^T\mathbf{w})}$$ As all of these terms are scalars, the cost is once again constant.

Now given the scalar $\frac{dLoss}{d(\mathbf{x}^T\mathbf{w})}$, we compute the derivative with respect to $x_i$ or $w_i$ as:

$$
\frac{dLoss}{dx_i}=\frac{dLoss}{d(\mathbf{x}^T\mathbf{w})}\frac{d(\mathbf{x}^T\mathbf{w})}{dx_i}= \frac{dLoss}{d(\mathbf{x}^T\mathbf{w})} w_i$$ $$
\frac{dLoss}{dw_i}=\frac{dLoss}{d(\mathbf{x}^T\mathbf{w})}\frac{d(\mathbf{x}^T\mathbf{w})}{dw_i}= \frac{dLoss}{d(\mathbf{x}^T\mathbf{w})} x_i
$$

A constant ( $\mathcal{O}(1)$ ) operation per entry. Since $\mathbf{x}$ and $\mathbf{w}$ both have $d$ entries, the cost of the backward pass update will still be $\mathcal{O}(d)$.

## Computing the full loss

Things get a little easier from here. We know that each layer of our network has $d$ neurons, each of which takes $\mathcal{O}(d)$ time to compute. Therefore the total cost for a layer will be $\mathcal{O}(d^2)$.

Our full network has $L$ layers, therefore the full network will take $\mathcal{O}(Ld^2)$ time to compute.

Looking back at our loss:

$$\underset{\text{MSE}}{\textbf{Loss}} (\mathbf{w}, \mathbf{X}, \mathbf{y}) = \frac{1}{N}\sum_{i=1}^N (f(\mathbf{x}_i, \mathbf{w}) - y_i)^2$$

We see that so far we've bounded the time it takes to compute *one* of our predictions: $f(\mathbf{x}_i, \mathbf{w})$. In order to compute the full loss (and therefore gradient), we need to do this for every term in our summation, a total of $N$ times. This give us a total cost to compute the loss of $\mathcal{O}(NLd^2)$. We already saw that our backward pass updates don't increase our asymptotic cost, so in total the cost of a single gradient descent update:

$$ \mathbf{w}^{(k+1)} \longleftarrow \mathbf{w}^{(k)} - \alpha \nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y})$$

Is $\mathcal{O}(NLd^2)$.

Accounting for the number of update steps ( $S$ ) we perform we see that the **total cost of gradient descent is:** $\mathcal{O}(SNLd^2)$.

## Real world costs

Let's asses how bad this running time actually is. To do so we'll need to consider how large each of these factors actually is in practice. We'll start by looking at a state-of-the-art (for 2012) neural network for classifying images: AlexNet.

![](images/paste-29.png){width="960"}

## Estimating loss

Neural network MSE loss:

$$\underset{\text{MSE}}{\textbf{Loss}} (\mathbf{w}, \mathbf{X}, \mathbf{y}) = \frac{1}{N}\sum_{i=1}^N (f(\mathbf{x}_i, \mathbf{w}) - y_i)^2$$

Estimate by sampling:

$$\underset{\text{MSE}}{\textbf{Loss}} (\mathbf{w}, \mathbf{X}, \mathbf{y}) \approx (f(\mathbf{x}_i, \mathbf{w}) - y_i)^2, \quad i \sim \text{Uniform}(1, N)$$

Expectation of sampled loss is the true loss!

$$\mathbb{E}_i[(f(\mathbf{x}_i, \mathbf{w}) - y_i)^2] = \sum_{i=1}^N p(i)(f(\mathbf{x}_i, \mathbf{w}) - y_i)^2 =\frac{1}{N} \sum_{i=1}^N (f(\mathbf{x}_i, \mathbf{w}) - y_i)^2$$

In general any loss that can be written as a mean of individual losses can be estimated in this way:

$$\textbf{Loss} (\mathbf{w}, \mathbf{X}, \mathbf{y}) = \frac{1}{N} \sum_{i=1}^N \textbf{Loss} (\mathbf{w}, \mathbf{x}_i, y_i)$$

$$\textbf{Loss} (\mathbf{w}, \mathbf{X}, \mathbf{y}) = \mathbb{E}[\textbf{Loss} (\mathbf{w}, \mathbf{x}_i, y_i)], \quad i\sim \text{Uniform}(1,N)$$

## Estimating gradients

Gradient descent update:

$$ \mathbf{w}^{(k+1)} \longleftarrow \mathbf{w}^{(k)} - \alpha \nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y})$$

Gradient can be composed into a sum of gradients and estimated the same way!

$$\nabla_{\mathbf{w}} \textbf{Loss} (\mathbf{w}, \mathbf{X}, \mathbf{y}) = \nabla_{\mathbf{w}} \bigg( \frac{1}{N} \sum_{i=1}^N \textbf{Loss} (\mathbf{w}, \mathbf{x}_i, y_i)\bigg)$$

$$=\frac{1}{N} \sum_{i=1}^N  \nabla_{\mathbf{w}} \textbf{Loss} (\mathbf{w}, \mathbf{x}_i, y_i) = \mathbb{E}[\nabla_{\mathbf{w}} \textbf{Loss} (\mathbf{w}, \mathbf{x}_i, y_i)], \quad i\sim \text{Uniform}(1, N)$$

*Stochastic gradient descent update:*

$$ \mathbf{w}^{(k+1)} \longleftarrow \mathbf{w}^{(k)} - \alpha \nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{x}_i, y_i), \quad i\sim \text{Uniform}(1, N)$$

## Minibatch SGD

Can estimate gradients with a *minibatch* of $B$ observations:

$$\text{Batch:}\ \{(\mathbf{x}_{b_1}, y_{b_1}), (\mathbf{x}_{b_2}, y_{b_2}), ...,  (\mathbf{x}_{b_B}, y_{b_B})\}, \quad \{b_1, b_2, ...,b_B\} \sim \text{Uniform}(1, N)$$

$$\nabla_{\mathbf{w}} \textbf{Loss} (\mathbf{w}, \mathbf{X}, \mathbf{y}) \approx \frac{1}{B} \sum_{i=1}^B \nabla_{\mathbf{w}} \textbf{Loss} (\mathbf{w}, \mathbf{x}_{b_i}, y_{b_i}), \quad \{b_1, b_2, ...,b_B\} \sim \text{Uniform}(1, N)$$

This still gives the correct expectation

$$\mathbb{E}\bigg[\frac{1}{B} \sum_{i=1}^B \nabla_{\mathbf{w}} \textbf{Loss} (\mathbf{w}, \mathbf{x}_{b_i}, y_{b_i})\bigg] = \bigg(\frac{1}{B}\bigg) \sum_{i=1}^B\mathbb{E}\bigg[ \nabla_{\mathbf{w}} \textbf{Loss} (\mathbf{w}, \mathbf{x}_{b_i}, y_{b_i})\bigg]$$ $$ = \nabla_{\mathbf{w}} \textbf{Loss} (\mathbf{w}, \mathbf{X}, \mathbf{y})$$

The variance decreases with the size of the batch!

$$\text{Var}\bigg[\frac{1}{B} \sum_{i=1}^B \nabla_{\mathbf{w}} \textbf{Loss} (\mathbf{w}, \mathbf{x}_{b_i}, y_{b_i})\bigg] =  \bigg(\frac{1}{B^2}\bigg) \sum_{i=1}^B\text{Var}\bigg[ \nabla_{\mathbf{w}} \textbf{Loss} (\mathbf{w}, \mathbf{x}_{b_i}, y_{b_i})\bigg]$$ $$= \bigg(\frac{1}{B}\bigg)\text{Var}\bigg[ \nabla_{\mathbf{w}} \textbf{Loss} (\mathbf{w}, \mathbf{x}_{b_i}, y_{b_i})\bigg]$$

# Gradient Descent Extensions

## Momentum

Gradient descent with momentum updates the *average gradient* then uses the running average to take descent steps.

$$ \mathbf{v}^{(k+1)} \longleftarrow \beta \mathbf{v}^{(k)} + (1-\beta) \nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y})$$

$$ \mathbf{w}^{(k+1)} \longleftarrow \mathbf{w}^{(k)} - \alpha v^{(k+1)}$$

## SGD + Momentum

We can apply momentum for stochastic gradient descent as well

$$ \mathbf{v}^{(k+1)} \longleftarrow \beta \mathbf{v}^{(k)} + (1-\beta) \nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{x}_i, y_i), \quad i\sim \text{Uniform}(1,N)$$

$$ \mathbf{w}^{(k+1)} \longleftarrow \mathbf{w}^{(k)} - \alpha v^{(k+1)}$$

$$\nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y}) \approx \sum_{j=1}^k \beta^{k-j}(1-\beta) \nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(j)}, \mathbf{x}_{i^{(j)}}, y_{i^{(j)}})$$

## Adaptive gradients (RMSProp)

$$\nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y}) = \begin{bmatrix} \frac{dL}{dw^{(k)}_1} \\ \frac{dL}{dw^{(k)}_2} \\ \vdots \end{bmatrix}$$

$$\begin{bmatrix} 3.1\\ 2.2 \\ \vdots \end{bmatrix} \leftarrow 
\begin{bmatrix} 5.0 \\ 1.8 \\ \vdots \end{bmatrix}\leftarrow 
\begin{bmatrix} 1.5 \\ 4.4 \\ \vdots \end{bmatrix}...$$

$$\begin{bmatrix} 10.1\\ 0.04 \\ \vdots \end{bmatrix} \leftarrow 
\begin{bmatrix} 8.6 \\ 0.02 \\ \vdots \end{bmatrix}\leftarrow 
\begin{bmatrix} 9.4 \\ 0.009 \\ \vdots \end{bmatrix}...$$

$$ \mathbf{s}^{(k+1)} \longleftarrow \beta \mathbf{s}^{(k)} + (1-\beta) (\nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y}))^2$$

$$ \mathbf{w}^{(k+1)} \longleftarrow \mathbf{w}^{(k)} - \alpha \frac{\nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y})}{\sqrt{\mathbf{s}^{(k+1)} + \epsilon}}$$ $$\epsilon << 1, \quad \text{e.g. } \epsilon = 1e^{-7}$$

$$\frac{\nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y})}{\sqrt{\mathbf{s}^{(k+1)}}} = 
\begin{bmatrix} \frac{\frac{dL}{dw_1}}{\sqrt{\big(\frac{dL}{dw_1}}\big)^2} \\ \frac{\frac{dL}{dw_2}}{\sqrt{\big(\frac{dL}{dw_2}}\big)^2} \\ \vdots \end{bmatrix}  =
\begin{bmatrix} \text{sign}\big(\frac{dL}{dw_1} \big) \\ \text{sign}\big(\frac{dL}{dw_2} \big) \\ \vdots \end{bmatrix} = \begin{bmatrix} +1 \\ -1 \\ \vdots \end{bmatrix} $$

## Adam

$$ \mathbf{v}^{(k+1)} \longleftarrow \beta_1 \mathbf{v}^{(k)} + (1-\beta_1) \nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y})$$ $$ \mathbf{s}^{(k+1)} \longleftarrow \beta_2 \mathbf{s}^{(k)} + (1-\beta_2) (\nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y}))^2$$ $$ \mathbf{w}^{(k+1)} \longleftarrow \mathbf{w}^{(k)} - \alpha \frac{\mathbf{v}^{(k+1)}
}{\sqrt{\mathbf{s}^{(k+1)} + \epsilon}}$$ $$ \mathbf{w}^{(k+1)} \longleftarrow \mathbf{w}^{(k)} - \alpha \frac{\frac{\mathbf{v}^{(k+1)}}{(1-\beta_1^k)}
}{\sqrt{\frac{\mathbf{s}^{(k+1)}}{(1-\beta_2^k)} + \epsilon}}$$ $$\mathbf{v}^{(0)} = \mathbf{0}, \quad \mathbf{s}^{(0)} = \mathbf{0}$$ $$\frac{\mathbf{v}^{(k+1)}}{(1-\beta_1^k)} = \frac{\beta_1 \mathbf{0} + (1-\beta_1)\nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y})}{(1-\beta_1^1)} = \nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y})$$