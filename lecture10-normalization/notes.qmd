---
title: "Lecture 10: Normalization"
format:
    html:
        toc: true
        toc-depth: 3
---

# Gradient Descent for Deep Networks

## Vanishing and exploding gradients

![](images/paste-27.png)

![](images/paste-28.png)

Neural networks are a composition of functions:

$$f(\mathbf{x}) = f_0(f_1(f_2(...f_L(\mathbf{x})...)))$$

$$f(\mathbf{x}) = \text{relu}(\text{relu}(\text{relu}(...\mathbf{x}^T\mathbf{W}_L...)^T\mathbf{W}_2)^T\mathbf{W}_1)^T\mathbf{w}_0$$ $$\text{relu}(x) = \max(x, 0)$$

$$\nabla_{\mathbf{W}_L}f(\mathbf{x})  = \frac{d\mathbf{f}_0}{d\mathbf{f}_1}\frac{d\mathbf{f}_1}{d\mathbf{f}_2}...\frac{d\mathbf{f}_{L-1}}{d\mathbf{f}_L}\frac{d\mathbf{f}_L}{d\mathbf{W}_L}  = \mathbf{x}^T\prod_{l=1}^{L-1}\frac{d\mathbf{f}_{l}}{d\mathbf{f}_{l-1}} $$

Simplified case: $$\frac{df}{dx}  = \frac{d{f}_0}{d{f}_1}\frac{d{f}_1}{d{f}_2}...\frac{d{f}_{L-1}}{d{f}_L}\frac{d{f}_L}{d{w}_L}  = {x}^T\prod_{l=1}^{L-1}\frac{d{f}_{l}}{d{f}_{l-1}} $$

$$w_L, x, \frac{df_0}{df_1},... \in \mathbb{R}, \quad \bigg|\frac{df_{l-1}}{df_l}\bigg| \approx M$$

$$\bigg|\frac{df}{dw_L}\bigg| = |x| \prod_{l=1}^{L-1}\bigg| \frac{df_{l}}{df_{l-1}}\bigg| \approx |x|\big(\textcolor{red}{M^L}\big)$$ Exploding gradients: $$\textbf{If: } M > 1 \longrightarrow \frac{df}{dw_L} >> 1$$ Vanishing gradients: $$\textbf{If: } M < 1 \longrightarrow \frac{df}{dw_L} \approx 0$$

## Gradient clipping

Explicitly clip the gradient to prevent it form becoming too large.

$$\textbf{clip}_{\text{value}}\big(\mathbf{x}, \epsilon\big) = \begin{bmatrix} \min(\max(x_1, -\epsilon), \epsilon) \\ \min(\max(x_2, - \epsilon), \epsilon) \\ \vdots \end{bmatrix}$$

$$\textbf{clip}_{\text{norm}}\big(\mathbf{x}, \epsilon\big) = \begin{cases} 
\frac{\epsilon\mathbf{x} }{\| \mathbf{x} \|_2} \quad & \textbf{if: } \|\mathbf{x}\|_2 > \epsilon \\
\mathbf{x} \  \quad\quad & \textbf{if: } \|\mathbf{x}\|_2 \leq \epsilon
\end{cases}$$

$$ \mathbf{w}^{(k+1)} \longleftarrow \mathbf{w}^{(k)} - \alpha\ \textbf{clip}\big(\nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y})\big)$$

# Normalization

## Batch normalization

Normalize over the batch:

$$\text{BatchNorm}(x) = \frac{ x - \mathbb{E}[x]}{\sqrt{\text{Var}[x] + \epsilon}}$$

Training time: $$\text{Batch: } \{x_1, x_2,...,x_B\}$$ $$\mathbb{E}[x] \approx \bar{x} = \frac{1}{B}\sum_{i=1}^{B} x_i\quad \text{(sample mean)}$$ Biased estimator: $$\text{Var}[x] \approx s^2 = \frac{1}{B}\sum_{i=1}^{B} \bigg(x_i - \bigg(\frac{1}{B}\sum_{i=1}^{B} x_i\bigg)\bigg)^2\quad \text{(sample var.)}$$ Unbiased estimator: $$\text{Var}[x] \approx  s^2 = \frac{1}{B-1}\sum_{i=1}^{B} \bigg(x_i - \bigg(\frac{1}{B}\sum_{i=1}^{B} x_i\bigg)\bigg)^2\quad \text{(sample var.)}$$

$$\underset{\text{train}}{\text{BatchNorm}}(x) = \frac{ x - \bar{x}}{\sqrt{s^2 + \epsilon}}$$

Running estimate: $$\bar{\mu}^{(k+1)} \longleftarrow \beta \bar{\mu}^{(k)} + (1-\beta) \bar{x}^{(k)}$$ $$\bar{\sigma}^{2(k+1)} \longleftarrow \beta \bar{\sigma}^{2(k)} + (1-\beta) s^{2(k)}$$

$$\underset{\text{test}}{\text{BatchNorm}}(x) = \frac{ x - \bar{\mu}}{\sqrt{\bar{\sigma}^2 + \epsilon}}$$

## Layer normalization

Normalize over the layer:

$$\text{LayerNorm}(\mathbf{x}) = \frac{\mathbf{x} - \bar{x}}{\sqrt{s^2 + \epsilon}}, \quad \mathbf{x} = \begin{bmatrix} x_1 \\ \vdots \\ x_d\end{bmatrix}$$

Training & test time: $$\bar{x} = \frac{1}{d}\sum_{i=1}^{d} x_i\quad \text{(output mean)}$$ Biased estimator: $$s^2 = \frac{1}{d}\sum_{i=1}^{d} \bigg(x_i - \bigg(\frac{1}{d}\sum_{i=1}^{d} x_i\bigg)\bigg)^2\quad \text{(output var.)}$$ Unbiased estimator: $$s^2 = \frac{1}{d-1}\sum_{i=1}^{d} \bigg(x_i - \bigg(\frac{1}{d}\sum_{i=1}^{d} x_i\bigg)\bigg)^2\quad \text{(output var.)}$$

## Scaled normalization

$$\text{BatchNorm}(x) = \frac{ x - \mathbb{E}[x]}{\sqrt{\text{Var}[x] + \epsilon}} \gamma + \kappa$$ $$\text{LayerNorm}(\mathbf{x}) = \frac{\mathbf{x} - \bar{x}}{\sqrt{s^2 + \epsilon}} \gamma + \kappa$$