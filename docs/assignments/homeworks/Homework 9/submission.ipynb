{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ak365mIE6LiO"
   },
   "source": [
    "# **Homework 9:** Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR NAME HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborators\n",
    "\n",
    "Please list anyone you discussed or collaborated on this assignment with below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIST COLLABORATORS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course feedback\n",
    "\n",
    "Please submit this week's course survey here: https://forms.gle/ELjvh2PK7iiAHbaC8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8dQ4lKYLFuYC"
   },
   "outputs": [],
   "source": [
    "# Uncomment and run if using Colab!\n",
    "\n",
    "#!wget https://cs152.github.io/assignments/homeworks/Homework%209/hw9_support.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7SlWhxaD6LiR"
   },
   "outputs": [],
   "source": [
    "# This is the path that the dataset for this homework will be downloaded to.\n",
    "# If you are running on the course server or Colab you can keep this line, if you are\n",
    "# running on a personal computer, you may want to change this location.\n",
    "from hw9_support import *\n",
    "data_path = '/tmp/data'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppAssMaM6LiU"
   },
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QttJ8zrm6LiU"
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5UzpSGM6LiV"
   },
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwRGi-pi6LiV"
   },
   "source": [
    "YOUR ANSWER HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPmKUFAR6LiV"
   },
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuKZdiaQ6LiV"
   },
   "outputs": [],
   "source": [
    "def conv1d(sequence, kernel):\n",
    "    kernel_size = kernel.shape[0]\n",
    "    sequence_length = sequence.shape[0]\n",
    "    output_length = ## YOUR CODE HERE\n",
    "\n",
    "    output = np.zeros((output_length,))\n",
    "    ## YOUR CODE HERE\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdJUq6E76LiV"
   },
   "source": [
    "Now we'll try out our convolution on a sequence of data! Below, we'll create an arbitrary input sequence and apply a kernel to it using the convolution function we just wrote. If you're curious, try changing the kernel and see how the result changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wSPPH9sd6LiV"
   },
   "outputs": [],
   "source": [
    "# Create a domain to create our input\n",
    "x = np.linspace(-5, 5, 25)\n",
    "\n",
    "# Create an sequence\n",
    "signal = np.sin(x * 1.5) + np.cos(x / 2) + np.cos(x / 1.3 + 2)\n",
    "\n",
    "# Create a kernel for our convolution\n",
    "kernel = np.array([-1, 1, -1])\n",
    "\n",
    "# Plot the input, kernel and the result of the convolution\n",
    "f, ax = plt.subplots(1, 3, figsize=(7, 2))\n",
    "ax[0].plot(signal)\n",
    "ax[0].set_title('Input')\n",
    "ax[1].plot(kernel)\n",
    "ax[1].set_title('Kernel')\n",
    "ax[2].plot(conv1d(signal, kernel))\n",
    "ax[2].set_title('Convolved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEsbMFW-6LiV"
   },
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "0LhRJ0B36LiV"
   },
   "outputs": [],
   "source": [
    "def conv1d_backward(sequence, kernel, dloss_doutput):\n",
    "    kernel_size = kernel.shape[0]\n",
    "    sequence_length = sequence.shape[0]\n",
    "    output_length = dloss_doutput.shape[0]\n",
    "\n",
    "    dloss_dsequence = np.zeros_like(sequence)\n",
    "    dloss_dkernel = np.zeros_like(kernel)\n",
    "\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "    return dloss_dsequence, dloss_dkernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ri7r4SBS6LiW"
   },
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "Vz88kOHS6LiW"
   },
   "outputs": [],
   "source": [
    "def conv2d(image, kernel):\n",
    "    kernel_size = kernel.shape[0]\n",
    "    image_height, image_width = image.shape[0], image.shape[1]\n",
    "    output_height = # YOUR CODE HERE\n",
    "    output_width = # YOUR CODE HERE\n",
    "\n",
    "    output = np.zeros((output_height, output_width))\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-EIgD8n6LiW"
   },
   "source": [
    "Let's try out our `conv2d` function on an image. As we've used in previous homeworks, we can use PyTorch to load a variety of real datasets. For image datasets, these are available in the `torchvision` library. We can download the MNIST dataset of number images that we've used before with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdTFBXLX6LiW"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "data = torchvision.datasets.MNIST(data_path,       # Specify where to store the data\n",
    "                                  download=True,   # Whether to download the data if we don't have it\n",
    "                                  train=True       # Download the training set (False will download the validation set)\n",
    "                                  )\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBzu49fh6LiW"
   },
   "source": [
    "We see that in the above if we take an observation from this dataset we get a tuple of an image and a corresponding label. The image is given as an object from the Pillow library (`PIL`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ps1Vtr5E6LiW"
   },
   "outputs": [],
   "source": [
    "data[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tSb7C6k6LiW"
   },
   "source": [
    "We can convert Pillow images to torch tensors using a `torchvision.transforms.ToTensor` object, or we can pass such an object directly to the dataset class to transform our data automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZZm7My76LiW"
   },
   "outputs": [],
   "source": [
    "# Create a transform object\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "# Apply the transform to the first image in the dataset (ignoring the label)\n",
    "image = transform(data[0][0])\n",
    "print('Converted image type:', type(image))\n",
    "\n",
    "# Give the transform directly to the MNIST object\n",
    "data = torchvision.datasets.MNIST(data_path, transform=transform)\n",
    "print('Automatically converted image type:', type(data[0][0]))\n",
    "print('Image shape:', data[0][0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrZ9kigi6LiX"
   },
   "source": [
    "If we look at the shape of an image, we see that it has height and width dimensions, but also an extra dimension. This is the *color channel dimension*. Since these are greyscale images with only a single color channel we'll get rid of it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQYngDjW6LiX"
   },
   "outputs": [],
   "source": [
    "# Take just the height and width dimensions of our image\n",
    "image = data[0][0][0]\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odTLr4f56LiX"
   },
   "source": [
    "Fianlly let's apply a few different kernels to the image and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWA_H9jm6LiX"
   },
   "outputs": [],
   "source": [
    "# Create an edge detection kernel\n",
    "kernel = np.array([[ 1,  1,  1],\n",
    "                   [ 0,  0,  0],\n",
    "                   [-1, -1, -1]])\n",
    "\n",
    "# Create the axes for our plots\n",
    "f, ax = plt.subplots(2, 4, figsize=(8, 4))\n",
    "ax[0, 0].imshow(image, cmap='Greys_r')\n",
    "ax[0, 0].set_title('Image')\n",
    "\n",
    "# Apply our convolution and plot the result\n",
    "output = conv2d(image, kernel)\n",
    "ax[0, 1].imshow(output, cmap='Greys_r')\n",
    "ax[0, 1].set_title('Vertical edges')\n",
    "ax[1, 1].imshow(kernel, cmap='Greys_r')\n",
    "\n",
    "# Apply our transposed convolution and plot the result\n",
    "output = conv2d(image, kernel.T)\n",
    "ax[0, 2].imshow(output, cmap='Greys_r')\n",
    "ax[0, 2].set_title('Horizontal edges')\n",
    "ax[1, 2].imshow(kernel.T, cmap='Greys_r')\n",
    "\n",
    "# Apply a kernel of all 1s to blur the image\n",
    "output = conv2d(image, np.ones_like(kernel))\n",
    "ax[0, 3].imshow(output, cmap='Greys_r')\n",
    "ax[0, 3].set_title('Blur')\n",
    "ax[1, 3].imshow(np.ones_like(kernel), cmap='Greys_r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D46ULsCQ6LiX"
   },
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "B61oLBJc6LiX"
   },
   "outputs": [],
   "source": [
    "def conv2d(image, kernel, padding=0):\n",
    "    kernel_size = kernel.shape[0]\n",
    "    image_height, image_width = image.shape[0], image.shape[1]\n",
    "    output_height = # YOUR CODE HERE\n",
    "    output_width = # YOUR CODE HERE\n",
    "\n",
    "    output = np.zeros((output_height, output_width))\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsEGDF9P6LiY"
   },
   "source": [
    "Let's visualize the same convolution with and without padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMZRakwF6LiY"
   },
   "outputs": [],
   "source": [
    "# Create an edge detection kernel\n",
    "kernel = np.array([[ 1,  1,  1],\n",
    "                   [ 0,  0,  0],\n",
    "                   [-1, -1, -1]])\n",
    "\n",
    "# Take just the height and width dimensions of our image\n",
    "image = data[0][0][0]\n",
    "\n",
    "# Create the axes for our plots\n",
    "f, ax = plt.subplots(1, 3,)\n",
    "ax[0].imshow(image, cmap='Greys_r')\n",
    "ax[0].set_title('Image')\n",
    "\n",
    "# Apply our convolution with no padding and plot the result\n",
    "output_0 = conv2d(image, kernel, padding=0)\n",
    "ax[1].imshow(output_0, cmap='Greys_r')\n",
    "ax[1].set_title('Padding=0')\n",
    "\n",
    "# Apply our convolution with a padding of 10 and plot the result\n",
    "output_10 = conv2d(image, kernel, padding=10)\n",
    "ax[2].imshow(output_10, cmap='Greys_r')\n",
    "ax[2].set_title('Padding=10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zT4PHyQc6LiY"
   },
   "source": [
    "While it is convinient to think of images as being matrices, as we've seen, color images actually have 3 values at each location! A red value, a green value as a blue value as shown below. We call these the color *channels*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vbpi-yXL6LiY"
   },
   "outputs": [],
   "source": [
    "import PIL # Pillow\n",
    "# Load an image\n",
    "image = np.array(PIL.Image.open('astronaut.jpg'))[..., :3]\n",
    "\n",
    "# If you're in Colab, you can use the commented line below to load the same image.\n",
    "#image = (skimage.transform.resize(skimage.data.astronaut(), (128, 128, 3)) * 255).astype(int)\n",
    "\n",
    "# Plot the color image\n",
    "f, ax = plt.subplots(1, 4, figsize=(8, 3))\n",
    "ax[0].imshow(image)\n",
    "ax[0].set_title('Image')\n",
    "\n",
    "# Plot each color channel separately\n",
    "ax[1].imshow(image[..., 0], cmap='Greys_r')\n",
    "ax[1].set_title('Red channel')\n",
    "ax[2].imshow(image[..., 1], cmap='Greys_r')\n",
    "ax[2].set_title('Green channel')\n",
    "ax[3].imshow(image[..., 2], cmap='Greys_r')\n",
    "ax[3].set_title('Blue channel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UH9gdDD06LiY"
   },
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "scgNAAY16LiZ"
   },
   "outputs": [],
   "source": [
    "def conv2d(image, kernel, padding=0):\n",
    "    kernel_size = kernel.shape[-1]\n",
    "    channels, image_height, image_width = image.shape[0], image.shape[1], image.shape[2]\n",
    "    output_height = # YOUR CODE HERE\n",
    "    output_width = # YOUR CODE HERE\n",
    "\n",
    "    output = np.zeros((output_height, output_width))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6ReyKTg6LiZ"
   },
   "source": [
    "Let's try applying using our convolution function on a color image! Below we'll load an image using the Python Pillow library and use our function to compute a convolution, using an *edge detection* kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gdZ_MdQ36LiZ"
   },
   "outputs": [],
   "source": [
    "import PIL # Pillow\n",
    "\n",
    "# Create an edge detection kernel\n",
    "kernel = np.array([[ 1,  1,  1],\n",
    "                   [ 0,  0,  0],\n",
    "                   [-1, -1, -1]])\n",
    "# Repeat it for the red, green and blue channels\n",
    "kernel_vertical = np.stack([kernel, kernel, kernel])\n",
    "# Create a transposed version as well\n",
    "kernel_horizontal = np.stack([kernel.T, kernel.T, kernel.T])\n",
    "\n",
    "# Load an image and convert it to a numpy array\n",
    "image = np.array(PIL.Image.open('astronaut.jpg'))[..., :3]\n",
    "\n",
    "# Create the axes for our plots\n",
    "f, ax = plt.subplots(1, 4, figsize=(8, 3))\n",
    "ax[0].imshow(image)\n",
    "\n",
    "# Convert our image from (height x width x channels) to (channels x height x width)\n",
    "image = image.transpose(2, 0, 1)\n",
    "\n",
    "# Apply our convolution and plot the result\n",
    "output_vertical = conv2d(image, kernel_vertical)\n",
    "ax[1].imshow(output_vertical, cmap='Greys_r')\n",
    "\n",
    "# Apply our transposed convolution and plot the result\n",
    "output_horizontal = conv2d(image, kernel_horizontal)\n",
    "ax[2].imshow(output_horizontal, cmap='Greys_r')\n",
    "\n",
    "# Apply a averaging (blur) kernel\n",
    "output_mean = conv2d(image, np.ones_like(kernel_horizontal))\n",
    "ax[3].imshow(output_mean, cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-9le4kb6LiZ"
   },
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "bxcSVycD6LiZ"
   },
   "outputs": [],
   "source": [
    "def conv2d(image, kernel, padding=0):\n",
    "    kernel_size = kernel.shape[-1]\n",
    "    output_channels = kernel.shape[0]\n",
    "    N, channels, image_height, image_width = image.shape[0], image.shape[1], image.shape[2], image.shape[3]\n",
    "    output_height = # YOUR CODE HERE\n",
    "    output_width = # YOUR CODE HERE\n",
    "\n",
    "    output = np.zeros((N, output_channels, output_height, output_width))\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzoYq4fa6LiZ"
   },
   "source": [
    "PyTorch provides a convinient way to load *batches* of images from a dataset via the `DataLoader` class. The dataloader will automatically group images and labels into tensors for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QgnNuYw36LiZ"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(data,            # The dataset\n",
    "                    batch_size=8,    # The batch size\n",
    "                    shuffle=True,    # Tell PyTorch to randomize the order of the data\n",
    "                    )\n",
    "\n",
    "for batch in loader:\n",
    "    images, labels = batch\n",
    "    print('Image batch shape:', images.shape)\n",
    "    print('Label batch shape:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnKXED-z6LiZ"
   },
   "source": [
    "Now we can try applying 3 kernels to a batch of data simultaniously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wxT9a-6B6LiZ"
   },
   "outputs": [],
   "source": [
    "# Create an edge detection kernel\n",
    "kernel = np.array([[ 1,  1,  1],\n",
    "                   [ 0,  0,  0],\n",
    "                   [-1, -1, -1]])\n",
    "# Repeat it for the red, green and blue channels\n",
    "kernel_vertical = np.stack([kernel])\n",
    "# Create a transposed version as well\n",
    "kernel_horizontal = np.stack([kernel.T])\n",
    "# Create a blur kernel\n",
    "kernel_blur = np.ones_like(kernel_horizontal)\n",
    "\n",
    "# Put all 3 kernels together into a single array\n",
    "kernel = np.stack([kernel_vertical, kernel_horizontal, kernel_blur])\n",
    "output = conv2d(images, kernel)\n",
    "\n",
    "# Create the axes for our plots\n",
    "f, ax = plt.subplots(1, 5, figsize=(10, 3))\n",
    "ax[0].imshow(images[0][0], cmap='Greys_r')\n",
    "ax[0].set_title('Image')\n",
    "\n",
    "# Plot the result of the first kernel\n",
    "ax[1].imshow(output[0][0], cmap='Greys_r')\n",
    "ax[1].set_title('Kernel 1')\n",
    "\n",
    "# Plot the result of the second kernel\n",
    "ax[2].imshow(output[0][1], cmap='Greys_r')\n",
    "ax[2].set_title('Kernel 2')\n",
    "\n",
    "# Plot the result of the third kernel\n",
    "ax[3].imshow(output[0][2], cmap='Greys_r')\n",
    "ax[3].set_title('Kernel 3')\n",
    "\n",
    "# Plot the result of all three kernels as the rgb channels of an image\n",
    "full_output = output[0].transpose(1, 2, 0)\n",
    "full_output = full_output - full_output.min()\n",
    "full_output = full_output / full_output.max()\n",
    "ax[4].imshow(full_output)\n",
    "ax[4].set_title('All kernels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTWH7sGa6Lib"
   },
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "so4OfRox6Lib"
   },
   "outputs": [],
   "source": [
    "def maxpool1d(sequence, pool_size=3, stride=1):\n",
    "    sequence_length = sequence.shape[0]\n",
    "    output_length = # YOUR CODE HERE\n",
    "\n",
    "    output = np.ones((output_length,)) * -np.inf\n",
    "    \n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHkRueMu6Lic"
   },
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3q2fCYmZ6Lic"
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    ## YOUR CODE HERE\n",
    ")\n",
    "\n",
    "run_model(data_path, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q11**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the axes for our plots\n",
    "f, ax = plt.subplots(2, 6, figsize=(12, 4))\n",
    "\n",
    "# We're using a nn.Sequential model, so we can access the first layer as follows\n",
    "layer = model[0]\n",
    "\n",
    "\n",
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-w56nPDi6Lic"
   },
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q12**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTMGoETt6Lic"
   },
   "outputs": [],
   "source": [
    "improved_model = nn.Sequential(\n",
    "    ## YOUR CODE HERE\n",
    ")\n",
    "\n",
    "run_model(data_path, improved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q13**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = nn.Sequential(\n",
    "    ## YOUR CODE HERE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convinience, we'll create an autoencoder model that lets us combine the encoder and decoder in to one object.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.decoder(self.encoder(X))\n",
    "    \n",
    "ae_model = Autoencoder(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q14**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_output_and_mseloss(model, X,  y):\n",
    "    '''\n",
    "    Inputs:\n",
    "        model: nn.Module, an autoencoder model.\n",
    "        X: tensor (float), an N x C x H x W tensor of images\n",
    "        y: tensor (float), an N x classes tensor of labels (unused)\n",
    "\n",
    "    Outputs:\n",
    "        output: tensor (float), an N x C x H x W tensor of reconstructed images.\n",
    "        loss: float, the mean squared error loss between the input and reconstructions.\n",
    "    '''\n",
    "    \n",
    "    ## YOUR CODE HERE\n",
    "    return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_output_and_maeloss(model, X,  y):\n",
    "    '''\n",
    "    Inputs:\n",
    "        model: nn.Module, an autoencoder model.\n",
    "        X: tensor (float), an N x C x H x W tensor of images\n",
    "        y: tensor (float), an N x classes tensor of labels (unused)\n",
    "\n",
    "    Outputs:\n",
    "        output: tensor (float), an N x C x H x W tensor of reconstructed images.\n",
    "        loss: float, the mean squared error loss between the input and reconstructions.\n",
    "    '''\n",
    "    \n",
    "    ## YOUR CODE HERE\n",
    "    return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_output_and_loss = autoencoder_output_and_maeloss# MSE or MAE\n",
    "run_model(data_path, ae_model, learning_rate=0.003, get_output_and_loss=get_output_and_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ae_model(images)\n",
    "\n",
    "# Convert to numpy\n",
    "output = output.detach().numpy()\n",
    "\n",
    "# Create the axes for our plots\n",
    "f, ax = plt.subplots(4, 2, figsize=(4, 8))\n",
    "for r in range(4):\n",
    "    ax[r, 0].imshow(images[r][0], cmap='Greys_r')\n",
    "    ax[r, 0].set_title('Image')\n",
    "    ax[r, 0].axis('off')\n",
    "    ax[r, 1].imshow(output[r][0], cmap='Greys_r')\n",
    "    ax[r, 1].set_title('Reconstruction')\n",
    "    ax[r, 1].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q15**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q16**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = nn.Sequential(\n",
    "    ## YOUR CODE HERE                              \n",
    ")\n",
    "\n",
    "decoder = nn.Sequential(\n",
    "    ## YOUR CODE HERE\n",
    ")\n",
    "\n",
    "ae_model = Autoencoder(encoder, decoder)\n",
    "run_model(data_path, ae_model, learning_rate=0.003, get_output_and_loss=autoencoder_output_and_maeloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ae_model(images)\n",
    "\n",
    "# Convert to numpy\n",
    "output = output.detach().numpy()\n",
    "\n",
    "# Create the axes for our plots\n",
    "f, ax = plt.subplots(4, 2, figsize=(4, 8))\n",
    "for r in range(4):\n",
    "    ax[r, 0].imshow(images[r][0], cmap='Greys_r')\n",
    "    ax[r, 0].set_title('Image')\n",
    "    ax[r, 0].axis('off')\n",
    "    ax[r, 1].imshow(output[r][0], cmap='Greys_r')\n",
    "    ax[r, 1].set_title('Reconstruction')\n",
    "    ax[r, 1].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q17**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_0 = images[2, 0] # Select image 0 and remove unused channel dimension\n",
    "image_1 = images[1, 0] # Same for second image\n",
    "\n",
    "f, ax = plt.subplots(1, 10, figsize=(15, 1.5))\n",
    "for i, t in enumerate(np.linspace(0, 1, 10)):\n",
    "    # Perform the interpolation and show each intermediate result.\n",
    "    \n",
    "    image_t = ## YOUR CODE HERE\n",
    "    ax[i].imshow(image_t, cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
