<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.319">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>CS 152: Neural Networks - Homework 5: PyTorch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">CS 152: Neural Networks</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../calendar/calendar.html" rel="" target="">
 <span class="menu-text">Calendar</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/orgs/CS152-Neural-Networks-Fall-2023/repositories" rel="" target="">
 <span class="menu-text">Homeworks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://harveymuddcollege.instructure.com/courses/615/" rel="" target="">
 <span class="menu-text">Canvas</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/orgs/CS152-Neural-Networks-Fall-2023/" rel="" target="">
 <span class="menu-text">Github</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#part-1-vector-jacobian-products" id="toc-part-1-vector-jacobian-products" class="nav-link active" data-scroll-target="#part-1-vector-jacobian-products"><strong>Part 1:</strong> Vector-Jacobian Products</a></li>
  <li><a href="#part-2-introduction-to-pytorch" id="toc-part-2-introduction-to-pytorch" class="nav-link" data-scroll-target="#part-2-introduction-to-pytorch">Part 2: Introduction to PyTorch</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><strong>Homework 5:</strong> PyTorch</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="part-1-vector-jacobian-products" class="level2">
<h2 class="anchored" data-anchor-id="part-1-vector-jacobian-products"><strong>Part 1:</strong> Vector-Jacobian Products</h2>
<p>In the last homework we implemented automatic differentiation in terms of scalar values. For reverse mode, we saw that the update for a node <span class="math inline">\(w\)</span> from a its child <span class="math inline">\(a\)</span> could be written as: <span class="math display">\[\frac{dL}{dw} = \frac{dL}{da}\frac{da}{dw}\]</span></p>
<p>In this case all three values in this equation were scalars and therefore the computation was quite simple. However we saw that this approach was quite inefficient when dealing with large arrays of numbers. A better approach that we saw in class was to have each node correspond to a <em>vector</em> (or possible a matrix). In this case if <span class="math inline">\(\mathbf{w}\)</span> is a length <span class="math inline">\(n\)</span> vector and <span class="math inline">\(\mathbf{a}\)</span> is a length <span class="math inline">\(m\)</span> vector we would say that: <span class="math display">\[\mathbf{w} \in \mathbb{R}^n, \quad \mathbf{a} \in \mathbb{R}^m\]</span> corresponding update equation would be written as: <span class="math display">\[ \frac{dL}{d\mathbf{w}} = \frac{dL}{d\mathbf{a}}^T\frac{da}{d\mathbf{w}}\]</span></p>
<p>Recall that we called this update a <em>vector-jacobian product</em>. #### <strong>Q1</strong> Assuming that <span class="math inline">\(L\)</span> is still a scalar (<span class="math inline">\(L\in \mathbb{R}\)</span>), what is the <em>shape</em> of each term in the equation above?</p>
<p><em>Your answer should be as matrix/vector dimensions e.g.&nbsp;<span class="math inline">\((a \times b)\)</span></em></p>
<p><span class="math display">\[\frac{dL}{d\mathbf{w}} = n\]</span> <span class="math display">\[\frac{dL}{d\mathbf{a}} = m\]</span> <span class="math display">\[\frac{d\mathbf{a}}{d\mathbf{w}} = m \times n\]</span></p>
<section id="q2" class="level4">
<h4 class="anchored" data-anchor-id="q2"><strong>Q2</strong></h4>
<p>Assume that we have the following formula for <span class="math inline">\(\mathbf{a}\)</span>: <span class="math display">\[\mathbf{a}= \exp \big( \mathbf{A} \mathbf{w} \big) + \mathbf{w}^2\]</span> Where <span class="math inline">\(\mathbf{A}\)</span> is a constant <span class="math inline">\(n \times n\)</span> matrix. Given the gradient of <span class="math inline">\(L\)</span> with respect to <span class="math inline">\(\mathbf{a}\)</span>: <span class="math inline">\(\frac{dL}{d\mathbf{a}}\)</span>, what is the gradient of <span class="math inline">\(L\)</span> with respect to <span class="math inline">\(\mathbf{w}\)</span> (<span class="math inline">\(\frac{dL}{d\mathbf{w}}\)</span>) in terms of: <span class="math inline">\(\frac{dL}{d\mathbf{a}}\)</span>, <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{w}\)</span>?</p>
<p><em><strong>Hint:</strong> You do <strong>not</strong> need to write a formula for the Jacobian (<span class="math inline">\(\frac{d\mathbf{a}}{d\mathbf{w}}\)</span>). As we saw in class many VJPs can be written without definining the Jacobian explicitly (e.g.&nbsp;for element-wise functions). You can use <span class="math inline">\(\odot\)</span> (<code>\odot</code>) to denote an element-wise product between vectors:</em> <span class="math display">\[ \mathbf{a} \odot \mathbf{b} = \begin{bmatrix}
a_1 b_1 \\
a_2 b_3 \\
\vdots \\
a_d b_d \\
\end{bmatrix}
\]</span></p>
<p>Here we’ll start by writing out the vector-Jacobian product we’re looking for: <span class="math display">\[\frac{dL}{d\mathbf{w}} = \frac{dL}{d\mathbf{a}}^T\frac{d\mathbf{a}}{d\mathbf{w}} = \frac{dL}{d\mathbf{a}}^T\frac{d}{d\mathbf{w}}\bigg( \exp \big( \mathbf{A} \mathbf{w} \big) + \mathbf{w}^2\bigg)\]</span> Using the addition rule, we can distribute the derivative: <span class="math display">\[=\frac{dL}{d\mathbf{a}}^T\frac{d}{d\mathbf{w}}\bigg( \exp \big( \mathbf{A} \mathbf{w} \big)\bigg) + \frac{dL}{d\mathbf{a}}^T\frac{d}{d\mathbf{w}}\bigg(\mathbf{w}^2\bigg)\]</span></p>
<p>For simplicity, let’s substitute <span class="math inline">\(\mathbf{b} = \mathbf{A}\mathbf{w}\)</span> for now, so we can apply the chain rule <span class="math display">\[=\frac{dL}{d\mathbf{a}}^T\frac{d}{d\mathbf{b}}\bigg( \exp \big( \mathbf{b} \big)\bigg)\frac{d\mathbf{b}}{d\mathbf{w}} + \frac{dL}{d\mathbf{a}}^T\frac{d}{d\mathbf{w}}\bigg(\mathbf{w}^2\bigg)\]</span></p>
<p>We see that <span class="math inline">\(\exp(w)\)</span> and <span class="math inline">\(w^2\)</span> are both <em>element-wise</em> operations, so we can use the rule we learned in class:</p>
<p><span class="math display">\[  = \bigg(\frac{dL}{d\mathbf{a}} \odot \exp(d\mathbf{b})\bigg)^T \frac{d\mathbf{b}}{d\mathbf{w}} + 2 \bigg( \frac{dL}{d\mathbf{a}}  \odot \mathbf{w} \bigg)\]</span></p>
<p>Finally we know that <span class="math inline">\(\mathbf{b} = \mathbf{A}\mathbf{w}\)</span>, therefore <span class="math inline">\(\frac{d\mathbf{b}}{d\mathbf{w}} = \mathbf{A}\)</span></p>
<p><span class="math display">\[ \frac{dL}{d\mathbf{w}}  = \bigg(\frac{dL}{d\mathbf{a}} \odot \exp(\mathbf{A}\mathbf{w})\bigg)^T \mathbf{A} + 2 \bigg( \frac{dL}{d\mathbf{a}}  \odot \mathbf{w} \bigg)\]</span></p>
<p>We can verify our answer using PyTorch!</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">0</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup the variables</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> torch.randn(<span class="dv">5</span>, <span class="dv">5</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> torch.randn(<span class="dv">5</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>w.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the gradient with autograd</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.exp(torch.matmul(A, w)) <span class="op">+</span> w <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> torch.<span class="bu">sum</span>(a)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>L.backward()</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>wgrad <span class="op">=</span> w.grad</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>wgrad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>tensor([ 2.5438, -4.3855,  1.7762, -0.3767,  4.9248])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute your VJP</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>dL_da <span class="op">=</span> torch.ones_like(a)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>dL_dw <span class="op">=</span> torch.matmul(dL_da <span class="op">*</span> torch.exp(torch.matmul(A, w)), A) <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> w <span class="op">*</span> dL_da <span class="co"># YOUR CODE HERE</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> torch.<span class="bu">all</span>(torch.isclose(wgrad, dL_dw))</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>dL_dw</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>tensor([ 2.5438, -4.3855,  1.7762, -0.3767,  4.9248], grad_fn=&lt;AddBackward0&gt;)</code></pre>
</div>
</div>
</section>
</section>
<section id="part-2-introduction-to-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="part-2-introduction-to-pytorch">Part 2: Introduction to PyTorch</h2>
<p>Now that we’ve successfully built our own tool for automatic differentiation and neural networks, let’s look at an industry-standard tool for accomplishing the same tasks: <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">PyTorch</a>.</p>
<p>Throughout this homework to may find it helpful to refer to the <a href="https://pytorch.org/docs/stable/index.html">PyTorch documentation</a>, as well as the <a href="https://colab.research.google.com/drive/1hJoRYXlGT-nRE4LnA9LEjQcy-c2IEy0p?usp=sharing#scrollTo=cmK5TWhhmePR">lecture notebook on Pytorch</a>.</p>
<p>We saw in class that we can create a function with <em>parameters</em> in PyTorch using the <code>torch.nn</code> module that we’ll import as just <code>nn</code>. We can do this by creating a subclass of <code>nn.Module</code> and defining the parameters with <code>nn.Parameter</code>.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> hw5_support <span class="im">import</span> <span class="op">*</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LinearZeros(nn.Module):</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co">    A PyTorch module representing a linear/affine function with weights W and bias b: </span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co">        f(X) = XW + b</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co">    W is an (in_dimensions x out_dimensions) matrix and b is an (out_dimensions) vector.</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co">    This version of the Linear module initializes the parameters to 0.</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_dimensions, out_dimensions):</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Call the nn.Module __init__ function</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create parameters that we can fit with gradient descent.</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> nn.Parameter(torch.zeros(in_dimensions, out_dimensions))</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> nn.Parameter(torch.zeros(out_dimensions))</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the function. Note that we use torch.matmul rather than torch.dot!</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This assumes X is 2-dimensional (a matrix)!</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.matmul(x, <span class="va">self</span>.weights) <span class="op">+</span> <span class="va">self</span>.bias</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can create a 1-dimensional linear function by creating a <code>LinearZeros</code> object, specifying that both the input and output dimensions should be <code>1</code>. The method <code>model.parameters()</code> will give us access to all the weights we can fit with gradient descent.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearZeros(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(model.parameters())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>[Parameter containing:
 tensor([[0.]], requires_grad=True),
 Parameter containing:
 tensor([0.], requires_grad=True)]</code></pre>
</div>
</div>
<p>We can also call this model just like any other function.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create 4 1-dimensional inputs</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.ones((<span class="dv">4</span>, <span class="dv">1</span>))</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>model(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>tensor([[0.],
        [0.],
        [0.],
        [0.]], grad_fn=&lt;AddBackward0&gt;)</code></pre>
</div>
</div>
<p>Let’s start by creating a simple dataset to use for the next few problems. We’ll use a regression dataset similar to the one we saw in class. In this case, I’ve provied data already split into training and validation sets.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the training inputs and labels</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">0</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.rand(<span class="dv">200</span>, <span class="dv">1</span>) <span class="op">*</span> <span class="dv">10</span> <span class="op">-</span> <span class="fl">5.</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> x <span class="op">**</span> <span class="dv">2</span> <span class="op">/</span> <span class="dv">2</span> <span class="op">+</span> torch.sin(x <span class="op">*</span> <span class="dv">5</span>) <span class="op">-</span> <span class="dv">5</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the validation inputs and labels</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>xvalid <span class="op">=</span> torch.rand(<span class="dv">200</span>, <span class="dv">1</span>) <span class="op">*</span> <span class="dv">10</span> <span class="op">-</span> <span class="fl">5.</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>yvalid <span class="op">=</span> xvalid <span class="op">**</span> <span class="dv">2</span> <span class="op">/</span> <span class="dv">2</span> <span class="op">+</span> torch.sin(xvalid <span class="op">*</span> <span class="dv">5</span> <span class="op">+</span> torch.pi) <span class="op">-</span> <span class="dv">5</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>plotRegression(x, y, xvalid, yvalid) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-7-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>We can make predictions for our data using the model we just definied:</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> model(x)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>predictions[:<span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>tensor([[0.],
        [0.],
        [0.],
        [0.],
        [0.]], grad_fn=&lt;SliceBackward0&gt;)</code></pre>
</div>
</div>
<p>However if we plot the prediction function, we’ll see that it isn’t very good as we haven’t optimized the parameters yet:</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>plotRegression(x, y, xvalid, yvalid, model<span class="op">=</span>model) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-9-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The first thing we’ll need to optimize our model is a loss function. As we saw in class, the convention in PyTorch is to separate the loss from the model, so we’ll write a simple function that takes in predictions and labels, returning the <em>mean squared error</em> loss.</p>
<section id="q3" class="level4">
<h4 class="anchored" data-anchor-id="q3"><strong>Q3</strong></h4>
<p>Complete the <code>mse_loss</code> function below. The function should compute the same MSE loss we’ve seen in previous homeworks, but using <strong>PyTorch</strong> operations.</p>
<p><span class="math display">\[\textbf{Loss}(\mathbf{a}, \mathbf{y}) = \frac{1}{N}\sum_{i=1}^N(a_i - y_i)^2\]</span></p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_loss(prediction, labels):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.mean((prediction.reshape((<span class="op">-</span><span class="dv">1</span>,)) <span class="op">-</span> labels.reshape((<span class="op">-</span><span class="dv">1</span>,))) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Test to check </span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">0</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> torch.isclose(mse_loss(torch.randn(<span class="dv">10</span>, <span class="dv">1</span>), torch.randn(<span class="dv">10</span>, <span class="dv">1</span>)), torch.tensor(<span class="fl">1.1550</span>), <span class="fl">1e-3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>With our loss in hand, we can run gradient descent to optimize our model’s parameters. This time, we’ll use the <code>torch.optim</code> module, which includes many useful variations of gradient descent.</p>
</section>
<section id="q4" class="level4">
<h4 class="anchored" data-anchor-id="q4"><strong>Q4</strong></h4>
<p>Complete the gradient descent function below. The function should: - Create an <code>optim.SGD</code> optimizer for the model’s parameters with the specified learning rate - At each step: - Compute the model output and loss (<code>loss_func</code>) on the training data - Compute the gradients of the loss with respect to the model parameters - Take a gradient descent step - Reset the parameter gradients to 0 - Compute the validation loss</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> optim</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent(model, loss_func, x, y, xvalid, yvalid, lr<span class="op">=</span><span class="fl">0.1</span>, steps<span class="op">=</span><span class="dv">5000</span>):</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    valid_losses <span class="op">=</span> []</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> tqdm.trange(steps):</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(x)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_func(output, y)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>        valid_loss <span class="op">=</span> loss_func(model(xvalid), yvalid)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>        losses.append(loss.detach().numpy())</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        valid_losses.append(valid_loss.detach().numpy())</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, valid_losses</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>test_gradient_descent(gradient_descent, mse_loss, x, y, xvalid, yvalid)</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearZeros(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>losses, valid_losses <span class="op">=</span> gradient_descent(model, mse_loss, x, y, xvalid, yvalid)</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>plotRegression(x, y, xvalid, yvalid, loss_history<span class="op">=</span>losses, valid_loss_history<span class="op">=</span>valid_losses, model<span class="op">=</span>model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 1/1 [00:00&lt;00:00, 297.11it/s]
100%|██████████| 5000/5000 [00:00&lt;00:00, 9927.69it/s] </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Passed!</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-11-output-3.png" class="img-fluid"></p>
</div>
</div>
<p>Now that we have a function to train a PyTorch model, we can try something bigger and more exciting! Let’s train a neural network.</p>
</section>
<section id="q5" class="level4">
<h4 class="anchored" data-anchor-id="q5"><strong>Q5</strong></h4>
<p>Create a PyTorch model for a neural network with the following specification: - The network should have 4 hidden layers, each with 20 neurons - The network should take 1-dimensional inputs as above - Each layer should use the LinearZeros module we just wrote - Each linear layer should be followed by a <em>ReLU</em> activation (except the output), use the <code>nn.ReLU()</code> module.</p>
<p><em><strong>Hint:</strong> Remember that you can use the <code>nn.Sequential</code> class to easily compose a sequence of functions in PyTorch.</em></p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>            LinearZeros(<span class="dv">1</span>, <span class="dv">20</span>),</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>            LinearZeros(<span class="dv">20</span>, <span class="dv">20</span>),</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>            LinearZeros(<span class="dv">20</span>, <span class="dv">20</span>),</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>            LinearZeros(<span class="dv">20</span>, <span class="dv">20</span>),</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>            LinearZeros(<span class="dv">20</span>, <span class="dv">1</span>),</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Test the model build</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>test_build(model, LinearZeros, dropout_type<span class="op">=</span><span class="va">None</span>, <span class="bu">type</span><span class="op">=</span><span class="st">'zeros'</span>)</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the model</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>losses, valid_losses <span class="op">=</span> gradient_descent(model, mse_loss, x, y, xvalid, yvalid)</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>plotRegression(x, y, xvalid, yvalid, loss_history<span class="op">=</span>losses, valid_loss_history<span class="op">=</span>valid_losses, model<span class="op">=</span>model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Passed!</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 5000/5000 [00:01&lt;00:00, 3225.93it/s]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-12-output-3.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="q6" class="level4">
<h4 class="anchored" data-anchor-id="q6"><strong>Q6</strong></h4>
<p>What happened when you attempted to train the model above? Why did this happen? Give a short 1-2 sentence answer.</p>
<p>Nothing happened because we initialized all weights to 0.</p>
<p>Let’s try modifying our Linear module with a different strategy for initialization.</p>
</section>
<section id="q7" class="level4">
<h4 class="anchored" data-anchor-id="q7"><strong>Q7</strong></h4>
<p>Modify the <code>LinearZeros</code> implementation from above to initialize the <code>weights</code> and <code>bias</code> parameters from a <em>standard normal</em> distribution <span class="math inline">\(w,b \sim \mathcal{N}(0, 1)\)</span>. Then modify your model from <strong>Q6</strong> to use this new module.</p>
<p><em><strong>Hint:</strong> You may find the <code>torch.randn</code> function useful here. You might also find that the model doesn’t train! We’ll address this in the next question.</em></p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LinearNormal(nn.Module):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_dimensions, out_dimensions):</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> nn.Parameter(torch.randn(in_dimensions, out_dimensions))</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> nn.Parameter(torch.randn(out_dimensions))</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.matmul(x, <span class="va">self</span>.weights) <span class="op">+</span> <span class="va">self</span>.bias</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>            LinearNormal(<span class="dv">1</span>, <span class="dv">20</span>),</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>            LinearNormal(<span class="dv">20</span>, <span class="dv">20</span>),</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>            LinearNormal(<span class="dv">20</span>, <span class="dv">20</span>),</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>            LinearNormal(<span class="dv">20</span>, <span class="dv">20</span>),</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>            LinearNormal(<span class="dv">20</span>, <span class="dv">1</span>),</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Test the model build and LinearKaiming</span></span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>test_normal(LinearNormal)</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>test_build(model, LinearNormal, dropout_type<span class="op">=</span><span class="va">None</span>, <span class="bu">type</span><span class="op">=</span><span class="st">'normal'</span>)</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>losses, valid_losses <span class="op">=</span> gradient_descent(model, mse_loss, x, y, xvalid, yvalid, lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>plotRegression(x, y, xvalid, yvalid, loss_history<span class="op">=</span>losses, valid_loss_history<span class="op">=</span>valid_losses, model<span class="op">=</span>model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Passed!
Passed!</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 5000/5000 [00:01&lt;00:00, 3264.70it/s]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-13-output-3.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="q8" class="level4">
<h4 class="anchored" data-anchor-id="q8"><strong>Q8</strong></h4>
<p>In the previous question you might have found that gradient descent didn’t work. This could suggest that our learning rate is set wrong. Think about a strategy that you might use to find an appropriate learning rate for fitting this model and try it out below. Then explain the strategy that you used. Is there any way you could improve this strategy to make finding a learning rate quicker?</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Modify this code to choose a good learning rate</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>            LinearNormal(<span class="dv">1</span>, <span class="dv">20</span>),</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>            LinearNormal(<span class="dv">20</span>, <span class="dv">20</span>),</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>            LinearNormal(<span class="dv">20</span>, <span class="dv">20</span>),</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>            LinearNormal(<span class="dv">20</span>, <span class="dv">20</span>),</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>            LinearNormal(<span class="dv">20</span>, <span class="dv">1</span>),</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.00001</span></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>losses, valid_losses <span class="op">=</span> gradient_descent(model, mse_loss, x, y, xvalid, yvalid, lr<span class="op">=</span>lr)</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>plotRegression(x, y, xvalid, yvalid, loss_history<span class="op">=</span>losses, valid_loss_history<span class="op">=</span>valid_losses, model<span class="op">=</span>model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 5000/5000 [00:01&lt;00:00, 3348.33it/s]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-14-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>EXPLAIN YOUR APPROACH HERE</p>
</section>
<section id="q9" class="level4">
<h4 class="anchored" data-anchor-id="q9"><strong>Q9</strong></h4>
<p>We saw in class that a common, useful approach for initializing neural networks is to use a <em>Kaiming normal</em> initialization. In this approach we draw each initial weight from a normal distribution where the standard deviation is scaled by the square root of the number of input dimensions to the layer. If <span class="math inline">\(\mathbf{W} \in \mathbb{R}^{d\times e}\)</span> then: <span class="math display">\[w_{ij} \sim \mathcal{N}\bigg(0, \frac{1}{\sqrt{d}}\bigg) \quad \mathbf{W}: d \times e\]</span> <span class="math display">\[b_j = 0 \quad \mathbf{b}: e\]</span> We’ll initialize the biases to <span class="math inline">\(0\)</span>. Below, implement a linear module using the Kaiming normal initialization, then repeat <strong>Q5</strong> using the <code>LinearKaiming</code> class and the learning rate you chose in <strong>Q8</strong>. Then if needed, adjust the learning rate until your model almost perfectly fits the <em>training data</em>.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LinearKaiming(nn.Module):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_dimensions, out_dimensions):</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> nn.Parameter(torch.randn(in_dimensions, out_dimensions) <span class="op">/</span> np.sqrt(in_dimensions))</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span>nn.Parameter(torch.zeros(out_dimensions))</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.matmul(x, <span class="va">self</span>.weights) <span class="op">+</span> <span class="va">self</span>.bias</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>            LinearKaiming(<span class="dv">1</span>, <span class="dv">20</span>),</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>            LinearKaiming(<span class="dv">20</span>, <span class="dv">20</span>),</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>            LinearKaiming(<span class="dv">20</span>, <span class="dv">20</span>),</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>            LinearKaiming(<span class="dv">20</span>, <span class="dv">20</span>),</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>            LinearKaiming(<span class="dv">20</span>, <span class="dv">1</span>),</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Test the model build and LinearKaiming</span></span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>test_kaiming(LinearKaiming)</span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>test_build(model, LinearKaiming, dropout_type<span class="op">=</span><span class="va">None</span>, <span class="bu">type</span><span class="op">=</span><span class="st">'normal'</span>)</span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a>losses, valid_losses <span class="op">=</span> gradient_descent(model, mse_loss, x, y, xvalid, yvalid, lr<span class="op">=</span>lr)</span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a>plotRegression(x, y, xvalid, yvalid, loss_history<span class="op">=</span>losses, valid_loss_history<span class="op">=</span>valid_losses, model<span class="op">=</span>model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Passed!
Passed!</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 5000/5000 [00:01&lt;00:00, 3270.61it/s]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-15-output-3.png" class="img-fluid"></p>
</div>
</div>
<p>If all went well so far, we should find that our model fits our data well, but perhaps a little bit <em>too</em> well. Let’s try out some of the strategies we’ve seen to reduce overfitting, starting with early stopping.</p>
</section>
<section id="q10" class="level4">
<h4 class="anchored" data-anchor-id="q10"><strong>Q10</strong></h4>
<p>Modify your gradient descent algorithm to implment a basic form of early stopping: stop gradient descent as soon as the validation loss increases from the previous iteration. Test this approach with the same model from <strong>Q9</strong>.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent_early_stopping(model, loss_func, x, y, xvalid, yvalid, lr<span class="op">=</span><span class="fl">0.1</span>, steps<span class="op">=</span><span class="dv">5000</span>):</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    valid_losses <span class="op">=</span> []</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> tqdm.trange(steps):</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(x)</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_func(output, y)</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>        valid_loss <span class="op">=</span> loss_func(model(xvalid), yvalid)</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>        losses.append(loss.detach().numpy())</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>        valid_losses.append(valid_loss.detach().numpy())</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(valid_losses) <span class="op">&gt;</span> <span class="dv">1</span> <span class="kw">and</span> valid_losses[<span class="op">-</span><span class="dv">2</span>] <span class="op">&lt;</span> valid_losses[<span class="op">-</span><span class="dv">1</span>]:</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, valid_losses</span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>            LinearKaiming(<span class="dv">1</span>, <span class="dv">20</span>),</span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>            LinearKaiming(<span class="dv">20</span>, <span class="dv">20</span>),</span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a>            LinearKaiming(<span class="dv">20</span>, <span class="dv">20</span>),</span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a>            LinearKaiming(<span class="dv">20</span>, <span class="dv">20</span>),</span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a>            LinearKaiming(<span class="dv">20</span>, <span class="dv">1</span>),</span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb29-33"><a href="#cb29-33" aria-hidden="true" tabindex="-1"></a>losses, valid_losses <span class="op">=</span> gradient_descent_early_stopping(model, mse_loss, x, y, xvalid, yvalid, lr<span class="op">=</span>lr)</span>
<span id="cb29-34"><a href="#cb29-34" aria-hidden="true" tabindex="-1"></a>plotRegression(x, y, xvalid, yvalid, loss_history<span class="op">=</span>losses, valid_loss_history<span class="op">=</span>valid_losses, model<span class="op">=</span>model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>  1%|          | 32/5000 [00:00&lt;00:02, 2312.30it/s]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-16-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="q11" class="level4">
<h4 class="anchored" data-anchor-id="q11"><strong>Q11</strong></h4>
<p>Did this approach work as intended? Why or why not? Think about how you might improve this approach and explain any ideas you have in 1-2 sentences.</p>
<p>No, it stopped too early.</p>
</section>
<section id="q12" class="level4">
<h4 class="anchored" data-anchor-id="q12"><strong>Q12</strong></h4>
<p>Modify your early stopping gradient descent so that it always runs for at least 50 iterations. Then after 50 iterations stop if at any point the validation loss is larger than the <em>average validation loss for the previous 50 iterations</em>.</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent_patient_early_stopping(model, loss_func, x, y, xvalid, yvalid, lr<span class="op">=</span><span class="fl">0.1</span>, steps<span class="op">=</span><span class="dv">5000</span>):</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    valid_losses <span class="op">=</span> []</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> tqdm.trange(steps):</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(x)</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_func(output, y)</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>        valid_loss <span class="op">=</span> loss_func(model(xvalid), yvalid)</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>        losses.append(loss.detach().numpy())</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>        valid_losses.append(valid_loss.detach().numpy())</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(valid_losses) <span class="op">&gt;</span> <span class="dv">50</span> <span class="kw">and</span> np.mean(valid_losses[<span class="op">-</span><span class="dv">50</span>:<span class="op">-</span><span class="dv">1</span>]) <span class="op">&lt;</span> valid_losses[<span class="op">-</span><span class="dv">1</span>]:</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, valid_losses</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>            LinearKaiming(<span class="dv">1</span>, <span class="dv">20</span>),</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>            LinearKaiming(<span class="dv">20</span>, <span class="dv">20</span>),</span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a>            LinearKaiming(<span class="dv">20</span>, <span class="dv">20</span>),</span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a>            LinearKaiming(<span class="dv">20</span>, <span class="dv">20</span>),</span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a>            LinearKaiming(<span class="dv">20</span>, <span class="dv">1</span>),</span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.003</span></span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true" tabindex="-1"></a>losses, valid_losses <span class="op">=</span> gradient_descent_patient_early_stopping(model, mse_loss, x, y, xvalid, yvalid, lr<span class="op">=</span>lr)</span>
<span id="cb31-34"><a href="#cb31-34" aria-hidden="true" tabindex="-1"></a>plotRegression(x, y, xvalid, yvalid, loss_history<span class="op">=</span>losses, valid_loss_history<span class="op">=</span>valid_losses, model<span class="op">=</span>model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>  2%|▏         | 104/5000 [00:00&lt;00:01, 2870.24it/s]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-17-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Now let’s try out L2 regualrization! We will consider a scaled version of L2 regularization, where for a <span class="math inline">\(d \times e\)</span> weight matrix <span class="math inline">\(\mathbf{W}\)</span> we will define the L2 loss as: <span class="math display">\[\textbf{Loss}_{L2}(\mathbf{W})= \frac{\lambda}{d e}\sum_{i=1}^d\sum_{j=1}^e w_{ij}^2 \quad \mathbf{W}: d \times e\]</span> Here <span class="math inline">\(\lambda\)</span> is a value that we can choose to control how much weight we put on our L2 loss (we’ll call it <code>l2_weight</code> below).</p>
</section>
<section id="q13" class="level4">
<h4 class="anchored" data-anchor-id="q13"><strong>Q13</strong></h4>
<p>Modify your original gradient descent algorithm from <strong>Q4</strong> (no early stopping) to add the L2 loss for each parameter in the model to the loss.</p>
<p><em><strong>Hint:</strong> Recall that we can access every parameter in the model using the <code>model.parameters()</code> method. In this question you do not need to worry about distinguishing between weights and biases, you can apply L2 regularization to biases as well if it simplifies your approach. Your validation loss should not include the regularization terms.</em></p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> optim</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent_l2(model, loss_func, x, y, xvalid, yvalid, lr<span class="op">=</span><span class="fl">0.1</span>, l2_weight<span class="op">=</span><span class="fl">1.</span>, steps<span class="op">=</span><span class="dv">5000</span>):</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>    valid_losses <span class="op">=</span> []</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> tqdm.trange(steps):</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(x)</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_func(output, y)</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>        losses.append(loss.detach().numpy()) </span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>        l2_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> p <span class="kw">in</span> model.parameters():</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>            l2_loss <span class="op">=</span> l2_loss <span class="op">+</span> torch.mean(p <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss <span class="op">+</span> l2_weight <span class="op">*</span> l2_loss        </span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>        valid_loss <span class="op">=</span> loss_func(model(xvalid), yvalid)</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>        valid_losses.append(valid_loss.detach().numpy())</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, valid_losses</span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Test our function</span></span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>test_gradient_descent(gradient_descent_l2, mse_loss, x, y, xvalid, yvalid, l2<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>TypeError: test_gradient_descent() got an unexpected keyword argument 'l2'</code></pre>
</div>
</div>
</section>
<section id="q14" class="level4">
<h4 class="anchored" data-anchor-id="q14"><strong>Q14</strong></h4>
<p>Apply <code>gradient_descent_l2</code> as in previous problems. Find an appropriate setting of <code>l2_weight</code> that minimizes the validation loss.</p>
<p><em><strong>Hint:</strong> How you go about choosing <code>l2_weight</code> is up to you! Your validation loss should be lower than the validation loss without regularization.</em></p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>            LinearKaiming(<span class="dv">1</span>, <span class="dv">20</span>),</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>            LinearKaiming(<span class="dv">20</span>, <span class="dv">20</span>),</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>            LinearKaiming(<span class="dv">20</span>, <span class="dv">20</span>),</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>            LinearKaiming(<span class="dv">20</span>, <span class="dv">20</span>),</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>            LinearKaiming(<span class="dv">20</span>, <span class="dv">1</span>),</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.003</span></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>l2_weight <span class="op">=</span> <span class="fl">1.</span></span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>losses, valid_losses <span class="op">=</span> gradient_descent_l2(model, mse_loss, x, y, xvalid, yvalid, lr<span class="op">=</span>lr, l2_weight<span class="op">=</span>l2_weight)</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>plotRegression(x, y, xvalid, yvalid, loss_history<span class="op">=</span>losses, valid_loss_history<span class="op">=</span>valid_losses, model<span class="op">=</span>model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 5000/5000 [00:02&lt;00:00, 1883.76it/s]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-19-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Finally let’s try out <em>dropout</em> regularization. We will implement dropout as its own module, so we can think of it as a function that transforms a vector or matix into a vector or matrix of the same shape with elements randomly set to <span class="math inline">\(0\)</span>. In this case we can write the dropout function as: <span class="math display">\[\text{Dropout}(\mathbf{X}, r) = \mathbf{D} \odot \mathbf{X}, \quad \mathbf{D} =
\begin{bmatrix}
d_{11} &amp; d_{12} &amp; \dots &amp; d_{1n} \\
d_{21} &amp; d_{22} &amp; \dots &amp; d_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
d_{m1} &amp; d_{m2} &amp; \dots &amp;  d_{mn}
\end{bmatrix},\ d_{ij} \sim \text{Bernoulli}(1-r)\]</span></p>
<p>Here <span class="math inline">\(\odot\)</span> denotes element-wise multiplication (so <span class="math inline">\(\mathbf{D}\)</span> and <span class="math inline">\(\mathbf{X}\)</span> are the same shape), <span class="math inline">\(r\)</span> is the dropout rate so <span class="math inline">\(p(d_{ij}=0)=r\)</span>.</p>
<p>At evaluation time, we do not want to randomly drop elements. Instead we will scale <span class="math inline">\(\mathbf{X}\)</span> by <span class="math inline">\((1-r)\)</span>: <span class="math display">\[\text{Dropout}_{\text{eval}}(\mathbf{X}, r) = (1-r)\mathbf{X}\]</span></p>
</section>
<section id="q15" class="level4">
<h4 class="anchored" data-anchor-id="q15"><strong>Q15</strong></h4>
<p>Complete the implementation of the <code>Dropout</code> module below.</p>
<p><em><strong>Hint:</strong> The built-in <code>training</code> property of an <code>nn.Module</code> instance specifies if our model is in training mode or evaluation mode. By default models are in training mode (<code>training == True</code>), but we can set a model to evaluation mode by calling <code>model.eval()</code>. Then we can use <code>model.train()</code> to set it back to training mode.</em></p>
<p><em>You may find the function <code>torch.rand_like()</code> helpful for this problem. You might also find it helpful to know that you can convert and <code>boolean</code> tensor <code>X</code> into a <code>float</code> tensor by calling <code>X.float()</code> (<code>True</code> becomes <code>1.</code>, <code>False</code> becomes <code>0.</code>)</em></p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Dropout(nn.Module):</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, rate<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Rate specifies the dropout rate (r)</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rate <span class="op">=</span> rate</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.training:</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x <span class="op">*</span> (torch.rand_like(x) <span class="op">&gt;</span> <span class="va">self</span>.rate).<span class="bu">float</span>()</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.rate)</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a>test_dropout(Dropout)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Passed!</code></pre>
</div>
</div>
</section>
<section id="q16" class="level4">
<h4 class="anchored" data-anchor-id="q16"><strong>Q16</strong></h4>
<p>Modify your <code>gradient_descent</code> function to put the model into <code>train</code> mode before calculating the training loss and into <code>eval</code> mode before calculating the validation loss. Then create a model based on your network from <strong>Q9</strong>, but this time add a <code>Dropout</code> layer before each <code>LinearKaiming</code> layer. You can use the default dropout rate of <code>0.01</code> or try something different! Verify that dropout gives different results to previous approaches.</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent(model, loss_func, x, y, xvalid, yvalid, lr<span class="op">=</span><span class="fl">0.1</span>, steps<span class="op">=</span><span class="dv">5000</span>):</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>    valid_losses <span class="op">=</span> []</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> tqdm.trange(steps):</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>        model.train()</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(x)</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_func(output, y)</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>        model.<span class="bu">eval</span>()</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>        valid_loss <span class="op">=</span> loss_func(model(xvalid), yvalid)</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>        losses.append(loss.detach().numpy())</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>        valid_losses.append(valid_loss.detach().numpy())</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, valid_losses</span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>            Dropout(),</span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a>            LinearKaiming(<span class="dv">1</span>, <span class="dv">20</span>),</span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a>            Dropout(),</span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a>            LinearKaiming(<span class="dv">20</span>, <span class="dv">20</span>),</span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a>            Dropout(),</span>
<span id="cb39-29"><a href="#cb39-29" aria-hidden="true" tabindex="-1"></a>            LinearKaiming(<span class="dv">20</span>, <span class="dv">20</span>),</span>
<span id="cb39-30"><a href="#cb39-30" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb39-31"><a href="#cb39-31" aria-hidden="true" tabindex="-1"></a>            Dropout(),</span>
<span id="cb39-32"><a href="#cb39-32" aria-hidden="true" tabindex="-1"></a>            LinearKaiming(<span class="dv">20</span>, <span class="dv">20</span>),</span>
<span id="cb39-33"><a href="#cb39-33" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb39-34"><a href="#cb39-34" aria-hidden="true" tabindex="-1"></a>            Dropout(),</span>
<span id="cb39-35"><a href="#cb39-35" aria-hidden="true" tabindex="-1"></a>            LinearKaiming(<span class="dv">20</span>, <span class="dv">1</span>),</span>
<span id="cb39-36"><a href="#cb39-36" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb39-37"><a href="#cb39-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-38"><a href="#cb39-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Test our model build</span></span>
<span id="cb39-39"><a href="#cb39-39" aria-hidden="true" tabindex="-1"></a>test_build(model, LinearKaiming, dropout_type<span class="op">=</span>Dropout, <span class="bu">type</span><span class="op">=</span><span class="st">'normal'</span>)</span>
<span id="cb39-40"><a href="#cb39-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-41"><a href="#cb39-41" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.003</span></span>
<span id="cb39-42"><a href="#cb39-42" aria-hidden="true" tabindex="-1"></a>losses, valid_losses <span class="op">=</span> gradient_descent(model, mse_loss, x, y, xvalid, yvalid, lr<span class="op">=</span>lr)</span>
<span id="cb39-43"><a href="#cb39-43" aria-hidden="true" tabindex="-1"></a>plotRegression(x, y, xvalid, yvalid, loss_history<span class="op">=</span>losses, valid_loss_history<span class="op">=</span>valid_losses, model<span class="op">=</span>model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Passed!</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 5000/5000 [00:02&lt;00:00, 1964.57it/s]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-21-output-3.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>test_build(model, LinearKaiming, dropout_type<span class="op">=</span>Dropout, <span class="bu">type</span><span class="op">=</span><span class="st">'normal'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Passed!</code></pre>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>