<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.319">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>CS 152: Neural Networks - Lecture 2: Linear regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">CS 152: Neural Networks</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../calendar/calendar.html" rel="" target="">
 <span class="menu-text">Calendar</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/orgs/CS152-Neural-Networks-Fall-2023/repositories" rel="" target="">
 <span class="menu-text">Homeworks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../assignments/final-project/outline.html" rel="" target="">
 <span class="menu-text">Project</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/orgs/CS152/" rel="" target="">
 <span class="menu-text">Github</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://www.gradescope.com/courses/710173" rel="" target="">
 <span class="menu-text">Gradescope</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#linear-regression" id="toc-linear-regression" class="nav-link active" data-scroll-target="#linear-regression">Linear regression</a>
  <ul class="collapse">
  <li><a href="#functions-revisited" id="toc-functions-revisited" class="nav-link" data-scroll-target="#functions-revisited">Functions revisited</a></li>
  <li><a href="#linear-functions" id="toc-linear-functions" class="nav-link" data-scroll-target="#linear-functions">Linear Functions</a></li>
  <li><a href="#parameterized-functions" id="toc-parameterized-functions" class="nav-link" data-scroll-target="#parameterized-functions">Parameterized Functions</a></li>
  <li><a href="#handling-bias-compactly" id="toc-handling-bias-compactly" class="nav-link" data-scroll-target="#handling-bias-compactly">Handling bias compactly</a></li>
  <li><a href="#datasets-and-observations" id="toc-datasets-and-observations" class="nav-link" data-scroll-target="#datasets-and-observations">Datasets and observations</a></li>
  <li><a href="#prediction-functions" id="toc-prediction-functions" class="nav-link" data-scroll-target="#prediction-functions">Prediction functions</a></li>
  <li><a href="#linear-interpolation" id="toc-linear-interpolation" class="nav-link" data-scroll-target="#linear-interpolation">Linear interpolation</a></li>
  <li><a href="#linear-regression-1" id="toc-linear-regression-1" class="nav-link" data-scroll-target="#linear-regression-1">Linear regression</a></li>
  <li><a href="#residuals-and-error" id="toc-residuals-and-error" class="nav-link" data-scroll-target="#residuals-and-error">Residuals and error</a></li>
  <li><a href="#mean-squared-error" id="toc-mean-squared-error" class="nav-link" data-scroll-target="#mean-squared-error">Mean squared error</a></li>
  <li><a href="#loss-functions" id="toc-loss-functions" class="nav-link" data-scroll-target="#loss-functions">Loss functions</a></li>
  <li><a href="#visualizing-loss" id="toc-visualizing-loss" class="nav-link" data-scroll-target="#visualizing-loss">Visualizing loss</a></li>
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link" data-scroll-target="#gradient-descent">Gradient descent</a></li>
  <li><a href="#gradient-descent-convergence" id="toc-gradient-descent-convergence" class="nav-link" data-scroll-target="#gradient-descent-convergence">Gradient descent convergence</a></li>
  <li><a href="#step-sizes" id="toc-step-sizes" class="nav-link" data-scroll-target="#step-sizes">Step sizes</a></li>
  <li><a href="#optimizing-linear-regression" id="toc-optimizing-linear-regression" class="nav-link" data-scroll-target="#optimizing-linear-regression">Optimizing linear regression</a></li>
  <li><a href="#optimizing-linear-regression-directly" id="toc-optimizing-linear-regression-directly" class="nav-link" data-scroll-target="#optimizing-linear-regression-directly">Optimizing linear regression directly</a></li>
  </ul></li>
  <li><a href="#maximum-likelihood-estimation" id="toc-maximum-likelihood-estimation" class="nav-link" data-scroll-target="#maximum-likelihood-estimation">Maximum likelihood estimation</a>
  <ul class="collapse">
  <li><a href="#normal-distributions" id="toc-normal-distributions" class="nav-link" data-scroll-target="#normal-distributions">Normal distributions</a></li>
  <li><a href="#linear-regression-as-a-probabilistic-model" id="toc-linear-regression-as-a-probabilistic-model" class="nav-link" data-scroll-target="#linear-regression-as-a-probabilistic-model">Linear regression as a probabilistic model</a></li>
  <li><a href="#maximum-likelihood-estimation-1" id="toc-maximum-likelihood-estimation-1" class="nav-link" data-scroll-target="#maximum-likelihood-estimation-1">Maximum likelihood estimation</a></li>
  <li><a href="#section" id="toc-section" class="nav-link" data-scroll-target="#section"></a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Lecture 2: Linear regression</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="cell" data-execution_count="1">
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Manim Community <span style="color: #008000; text-decoration-color: #008000">v0.18.0</span>

</pre>
</div>
</div>
<section id="linear-regression" class="level1">
<h1>Linear regression</h1>
<section id="functions-revisited" class="level2">
<h2 class="anchored" data-anchor-id="functions-revisited">Functions revisited</h2>
<p>In the previous lecture we reviewed the concept of a <em>function</em>, which is a mapping from a set of possible inputs to a corresponding set of outputs. Here we’ll consider functions with vector inputs and scalar outputs.</p>
<p><span class="math display">\[
y=f(\mathbf{x}), \quad \text{Input: } \mathbf{x} \in\mathbb{R}^n \longrightarrow \text{ Output: }y \in\mathbb{R}
\]</span></p>
<p>Mathematically, we can easily definite a function using a sequence of basic operations.</p>
<div class="cell" data-execution_count="2">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-3-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>This function gives us the relationship between <em>inputs</em> <span class="math inline">\(\mathbf{x}\)</span> and outputs <span class="math inline">\(f(\mathbf{x})\)</span>. That is, for any given input <span class="math inline">\(x\)</span>, we can find the corresponding output <span class="math inline">\(y\)</span> by applying our function <span class="math inline">\(f(\mathbf{x})\)</span>.</p>
</section>
<section id="linear-functions" class="level2">
<h2 class="anchored" data-anchor-id="linear-functions">Linear Functions</h2>
<p>A <em>linear function</em> is any function <span class="math inline">\(f\)</span> where the following conditions always hold: <span class="math display">\[ f(\mathbf{x} + \mathbf{y}) =f(\mathbf{x}) + f(\mathbf{y})\]</span> and <span class="math display">\[ f(a\mathbf{x}) = a f(\mathbf{x})\]</span>For a linear function, the output can be defined as a weighted sum of the inputs. In other words a linear function of a vector can always be written as:</p>
<p><span class="math display">\[
f(\mathbf{x}) = \sum_{i=1}^n x_iw_i
\]</span></p>
<p>We can add an offset <span class="math inline">\(b\)</span> to create an <em>affine</em> function:</p>
<p><span class="math display">\[
f(\mathbf{x}) = \sum_{i=1}^n x_iw_i +b
\]</span></p>
<p>We can also write this using a dot-product between our input <span class="math inline">\(\mathbf{x}\)</span> and parameter vector <span class="math inline">\(\mathbf{w}\)</span> as:</p>
<p><span class="math display">\[
f(\mathbf{x}) = \mathbf{x} \cdot \mathbf{w} + b \quad \text{or} \quad f(\mathbf{x}) = \mathbf{x}^T  \mathbf{w} + b
\]</span></p>
<p>In one dimension, a linear (or affine) function is always a line, for example:</p>
<div class="cell" data-execution_count="3">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-4-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>In higher dimensions, it is a plane or hyperplane:</p>
<div class="cell" data-execution_count="4">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-5-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>In numpy we can easily write a function of this form:</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> np.array([<span class="op">-</span><span class="fl">0.6</span>, <span class="op">-</span><span class="fl">0.2</span>])</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.dot(x, w) <span class="op">+</span> b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="parameterized-functions" class="level2">
<h2 class="anchored" data-anchor-id="parameterized-functions">Parameterized Functions</h2>
<p>Linear and affine functions are examples of <strong>classes of functions</strong>, they define a general form for many different functions. Using the affine example, we see that we can define a particular function by choosing values for <span class="math inline">\(w_i\)</span> and <span class="math inline">\(b\)</span>.</p>
<p><span class="math display">\[
f(\mathbf{x}) = \sum_{i=1}^n x_iw_i +b
\]</span></p>
<p>We will refer to the values that define a function within our class (e.g.&nbsp;<span class="math inline">\(w_i\)</span> and <span class="math inline">\(b\)</span>) as the <strong>parameters</strong> of the function, by changing these values, we can change the function.</p>
<p>We typically refer to <span class="math inline">\(\mathbf{w}\)</span> specifically as the <strong>weight vector</strong> (or weights) and <span class="math inline">\(b\)</span> as the <strong>bias</strong>. To summarize:</p>
<p><span class="math display">\[
\textbf{Affine function:  }f(\mathbf{x}) = \mathbf{x}^T\mathbf{w}+b,\quad \textbf{Parameters:}\quad \big(\text{Weights:  } \mathbf{w},\ \text{Bias:  } b \big)
\]</span></p>
</section>
<section id="handling-bias-compactly" class="level2">
<h2 class="anchored" data-anchor-id="handling-bias-compactly">Handling bias compactly</h2>
<p>Notationally, it can be tedious to always write the bias term. A common approach to compactly describing linear or affine functions is to use <em>augmented</em> inputs and weights, such that for <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{w}\in \mathbb{R}^n\)</span>, we add <span class="math inline">\(x_{n+1}=1\)</span> and <span class="math inline">\(w_{n+1}=b\)</span>. So:</p>
<p><span class="math display">\[
\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} \longrightarrow \mathbf{x}_{aug}= \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \\ 1 \end{bmatrix} \quad \text{and} \quad \mathbf{w} = \begin{bmatrix} w_1 \\ w_2 \\ \vdots \\ w_n \end{bmatrix} \longrightarrow \mathbf{w}_{aug}=  \begin{bmatrix} w_1 \\ w_2 \\ \vdots \\ w_n \\ b \end{bmatrix}
\]</span></p>
<p>We can easily see then that using this notation:</p>
<p><span class="math display">\[
f(\mathbf{x}) = \mathbf{x}^T \mathbf{w} +b = \mathbf{x}_{aug}^T \mathbf{w}_{aug}
\]</span></p>
<p>This approach is common enough that we typically won’t bother with the <span class="math inline">\(aug\)</span> notation and just assume that any function defined as <span class="math inline">\(f(\mathbf{x})=\mathbf{x}^T\mathbf{w}\)</span> can be defined to include a bias implicitly. Note that in this case the function is a <em>linear function</em> of the augmented input, thus we will still typically refer to functions of this form as linear functions.</p>
<p>In numpy this is similarly straightforward:</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> np.array([<span class="op">-</span><span class="fl">0.6</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.pad(x, ((<span class="dv">0</span>,<span class="dv">1</span>),), constant_values<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.dot(x, w)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="datasets-and-observations" class="level2">
<h2 class="anchored" data-anchor-id="datasets-and-observations">Datasets and observations</h2>
<p>In the real-world we often have access to inputs and outputs in the form of <em>data</em>, but not to an actual function that we can evaluate.</p>
<p>Specifically we will say that we have access to a <strong>dataset</strong> <span class="math inline">\(\mathcal{D}\)</span> made up of <span class="math inline">\(N\)</span> pairs of inputs ( <span class="math inline">\(\mathbf{x}\)</span> ) and outputs ( <span class="math inline">\(y\)</span> ):</p>
<p><span class="math display">\[
\mathcal{D} = \{ (\mathbf{x}_1, y_1),\ (\mathbf{x}_2, y_2),\ ...\ (\mathbf{x}_N, y_N)\}
\]</span></p>
<p>We call each of these pairs an <strong>observation</strong>. Let’s take a look at a real world example of a dataset.</p>
<section id="fuel-efficiency" class="level4">
<h4 class="anchored" data-anchor-id="fuel-efficiency">Fuel efficiency</h4>
<p>Let’s imagine we’re designing a car and we would like to know what the fuel efficiency of the car we’re designing will be in <em>miles per gallon</em> (MPG). We know some properties of our current design, such as the weight and horsepower, that we know should affect the efficiency. Ideally we would have access to a function that would give us the MPG rating if we provide these <strong>features</strong>.</p>
<p><span class="math display">\[
\text{mpg} = f(\text{weight},\ \text{horsepower}...)
\]</span></p>
<p>Unfortunately we don’t know the exact relationship between a car’s features and fuel efficiency. However, we can look at other cars on the market and see what the corresponding inputs and outputs would be:</p>
<p><span class="math display">\[
\text{Honda Accord: } \begin{bmatrix} \text{Weight:} &amp; \text{2500 lbs} \\ \text{Horsepower:} &amp; \text{ 123 HP} \\ \text{Displacement:} &amp; \text{ 2.4 L} \\ \text{0-60mph:} &amp; \text{ 7.8 Sec} \end{bmatrix} \longrightarrow \text{   MPG: 33mpg}
\]</span></p>
<p><span class="math display">\[
\text{Dodge Aspen: } \begin{bmatrix} \text{Weight:} &amp; \text{3800 lbs} \\ \text{Horsepower:} &amp; \text{ 155 HP} \\ \text{Displacement:} &amp; \text{ 3.2 L} \\ \text{0-60mph:} &amp; \text{ 6.8 Sec} \end{bmatrix} \longrightarrow \text{   MPG: 21mpg}
\]</span></p>
<p><span class="math display">\[
\vdots \quad \vdots
\]</span></p>
<p>Our dataset will be this collection of data that we have for all other cars. In general, each <em>observation</em> in this dataset will correspond to a car.</p>
<p><span class="math display">\[
\text{Dataset: } \mathcal{D}=\{(\mathbf{x}_i,\ y_i) \text{  for  } i\in 1...N\}
\]</span></p>
<p><span class="math display">\[
\text{Input: } \mathbf{x}_i= \begin{bmatrix} \text{Weight} \\ \text{Horsepower} \\ \text{Displacement} \\ \text{0-60mph}  \end{bmatrix}, \quad \text{Output: } y_i = MPG
\]</span></p>
<p>Just as with a known function, we can plot the inputs vs the outputs, however in this case, we only know the outputs for the inputs we’ve seen in our dataset. Let’s take a look at a single feature: <em>the weight of a car</em>.</p>
<div class="cell" data-execution_count="7">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-8-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="prediction-functions" class="level2">
<h2 class="anchored" data-anchor-id="prediction-functions">Prediction functions</h2>
<p>Our dataset gives us a set of known inputs and outputs for our unknown functions. The central question we will address in this course is then:</p>
<section id="how-do-we-predict-the-output-for-an-input-that-we-havent-seen-before" class="level4">
<h4 class="anchored" data-anchor-id="how-do-we-predict-the-output-for-an-input-that-we-havent-seen-before"><em>How do we <strong>predict</strong> the output for an input that we haven’t seen before?</em></h4>
<p>For example, in our car scenario, we might know that the car that we’re designing will weigh 3100 lbs. In our dataset we’ve haven’t seen a car that weighs exactly 3100 lbs, so we need a way to <em>predict</em> the output of the function at input 3100 lbs.</p>
<div class="cell" data-execution_count="8">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-9-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>In general, our approach to this problem will be to <strong>model</strong> our unknown function with a known function that we can evaluate at any input. We want to chose a function <span class="math inline">\(f\)</span> such that for any observation our dataset, the output of this function <em>approximates</em> the true <strong>target</strong> output that we observed.</p>
<p><span class="math display">\[
f(\mathbf{x}_i) \approx y_i, \quad \forall (\mathbf{x}_i, y_i) \in \mathcal{D}
\]</span></p>
</section>
</section>
<section id="linear-interpolation" class="level2">
<h2 class="anchored" data-anchor-id="linear-interpolation">Linear interpolation</h2>
<p>One reasonable approach we might consider is <em>linear interpolation</em>. In this approach, we simply connect all the neighboring points with straight lines:</p>
<div class="cell" data-execution_count="9">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-10-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>In some cases this can be a reasonable approach! In fact it’s how the <code>plt.plot</code> function works. Real data however tends to be <em>messy</em>. The measurements in our dataset might not be 100% accurate or they might even conflict! What do we do if we have two observations with the same input and different outputs?</p>
<p><span class="math display">\[(\text{Weight: }3100, \text{MPG: } 34), \quad (\text{Weight: }3100, \text{MPG: } 23) \longrightarrow f(3100) = ?\]</span></p>
<p>As the size and number of features in our inputs gets larger, this become even more complex. We can see this if we try to apply interpolation to a much larger MPG dataset:</p>
<div class="cell" data-execution_count="10">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-11-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="linear-regression-1" class="level2">
<h2 class="anchored" data-anchor-id="linear-regression-1">Linear regression</h2>
<p><strong>Linear regression</strong> is the approach of modeling an unknown function with a linear function. From our discussion of linear functions, we know that this means that we will make predictions using a function of the form:</p>
<p><span class="math display">\[
f(\mathbf{x}) = \mathbf{x}^T\mathbf{w} = \sum_{i=1}^n x_i w_i
\]</span></p>
<p>Meaning that the output will be a weighted sum of the <em>features</em> of the input. In the case of our car example, we will make predictions as:</p>
<p><span class="math display">\[
\text{Predicted MPG} = f(\mathbf{x})=
\]</span></p>
<p><span class="math display">\[
(\text{weight})w_1 + (\text{horsepower})w_2 + (\text{displacement})w_3 + (\text{0-60mph})w_4 + b
\]</span></p>
<p>Or in matrix notation:</p>
<p><span class="math display">\[
f(\mathbf{x})= \begin{bmatrix} \text{Weight} \\ \text{Horsepower} \\ \text{Displacement} \\ \text{0-60mph} \\ 1 \end{bmatrix} \cdot \begin{bmatrix} w_1 \\ w_2\\ w_3 \\ w_4\\ b\end{bmatrix}
\]</span></p>
<p>We see that under this approach each <strong>weight</strong> <span class="math inline">\((w_1, w_2…)\)</span> tells us how much our prediction changes as we change the corresponding feature. For example, if we were to increase the weight of our car by 1 lb, the predicted MPG would change by <span class="math inline">\(w_1\)</span>.</p>
<p>The set of weights defines the particular linear regression function. In numpy we can define a generic class for linear regression:</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Regression:</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, weights):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> weights</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.dot(x, <span class="va">self</span>.weights)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Regression(np.array([<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>]))</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>model.predict(np.array([<span class="dv">5</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">1</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>14</code></pre>
</div>
</div>
<p>If we again look at our plot of weight vs.&nbsp;MPG, we see we could chose many different linear functions to make predictions:</p>
<div class="cell" data-execution_count="12">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-13-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="residuals-and-error" class="level2">
<h2 class="anchored" data-anchor-id="residuals-and-error">Residuals and error</h2>
<p>The <strong>residual</strong> or <strong>error</strong> of a prediction is the difference between the prediction and the true output:</p>
<p><span class="math display">\[
e_i = y_i - f(\mathbf{x}_i)
\]</span></p>
<div class="cell" data-execution_count="13">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-14-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="mean-squared-error" class="level2">
<h2 class="anchored" data-anchor-id="mean-squared-error">Mean squared error</h2>
<p>In deciding what linear function to use, we need a measure of error for the <em>entire dataset</em>. A computationally convenient measure is <strong>mean squared error (MSE)</strong>. The mean squared error is the averaged of the error squared for each observation in our dataset:</p>
<p><span class="math display">\[
MSE = \frac{1}{N}\sum_{i=1}^N (f(\mathbf{x}_i) - y_i)^2 = \frac{1}{N}\sum_{i=1}^N (\mathbf{x}_i^T\mathbf{w} - y_i)^2
\]</span>It follows that the best choice of linear function <span class="math inline">\(f^*\)</span> is the one that <em>minimizes</em> the mean squared error for our dataset. Since each linear function is defined by a parameter vector <span class="math inline">\(\mathbf{w}\)</span>, this is equivalent to finding <span class="math inline">\(\mathbf{w}^*\)</span>, the parameters vector that minimizes the mean squared error. <span class="math display">\[\mathbf{w}^* = \underset{\mathbf{w}}{\text{argmin}} \frac{1}{N}\sum_{i=1}^N (\mathbf{x}_i^T\mathbf{w} - y_i)^2 \]</span></p>
</section>
<section id="loss-functions" class="level2">
<h2 class="anchored" data-anchor-id="loss-functions">Loss functions</h2>
<p>Note that the mean squared error depends on the data inputs <span class="math inline">\((\mathbf{x}_1,…,\mathbf{x}_N)\)</span>, the data targets <span class="math inline">\((y_1,…,y_N)\)</span> <em>and</em> the parameters <span class="math inline">\((\mathbf{w})\)</span>. So we can express the MSE as a <em>function</em> of all three:</p>
<p><span class="math display">\[
MSE(\mathbf{w}, \mathbf{X}, \mathbf{y}) = \frac{1}{N}\sum_{i=1}^N (\mathbf{x}_i^T\mathbf{w} - y_i)^2
\]</span></p>
<p>Here we have used <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> to refer to the entire collection of inputs and outputs from our dataset <span class="math inline">\((\mathcal{D})\)</span> respectively, so:</p>
<p><span class="math display">\[
\mathbf{X} = \begin{bmatrix}\mathbf{x}_1 \\ \mathbf{x}_2 \\ \vdots \\ \mathbf{x}_N \end{bmatrix} = \begin{bmatrix}x_{11} &amp; x_{12} &amp; \dots &amp; x_{1n} \\ x_{21} &amp; x_{22} &amp; \dots &amp; x_{2n}\\ \vdots &amp; \vdots  &amp; \ddots &amp; \vdots \\ x_{N1} &amp; x_{N2} &amp; \dots &amp; x_{Nn} \end{bmatrix}, \quad \mathbf{y} = \begin{bmatrix}y_1 \\ y_2 \\ \vdots \\ y_N \end{bmatrix}
\]</span></p>
<p>This is an example of <strong>loss function</strong>, for our given dataset this function tells us how much error (loss) we are incurring for a given choice of <span class="math inline">\(\mathbf{w}\)</span>. If we assume our dataset is fixed we can drop the explicit dependence on <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>, looking at the loss as purely a function of our choice of parameters:</p>
<p><span class="math display">\[
\textbf{Loss}(\mathbf{w})= MSE(\mathbf{w}, \mathbf{X}, \mathbf{y}) = \frac{1}{N}\sum_{i=1}^N (\mathbf{x}_i^T\mathbf{w} - y_i)^2
\]</span></p>
<p>Again, if our goal is to minimize error, we want to choose the parameters <span class="math inline">\(\mathbf{w}^*\)</span> that <em>minimize</em> this loss:</p>
<p><span class="math display">\[
\mathbf{w}^* = \underset{\mathbf{w}}{\text{argmin}}\ \textbf{Loss}(\mathbf{w})= \underset{\mathbf{w}}{\text{argmin}} \frac{1}{N}\sum_{i=1}^N (\mathbf{x}_i^T\mathbf{w} - y_i)^2
\]</span></p>
</section>
<section id="visualizing-loss" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-loss">Visualizing loss</h2>
<p>If we consider the case where our inputs are 1-dimensional, as in the weight example above, then our parameter vector <span class="math inline">\(\mathbf{w}\)</span> only has 2 entries: <span class="math inline">\(w_1\)</span> and <span class="math inline">\(b\)</span>. In this case, we can actually plot our loss function directly!</p>
<div class="cell" data-execution_count="14">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-15-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="15">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-16-output-1.png" width="558" height="428"></p>
</div>
</div>
<p>We see that point where the loss is lowest, corresponds to the line that best fits our data!</p>
</section>
<section id="gradient-descent" class="level2">
<h2 class="anchored" data-anchor-id="gradient-descent">Gradient descent</h2>
<p>Now that we have a way to determine the quality of a choice of parameters <span class="math inline">\(\mathbf{w}\)</span>, using our <em>loss</em> function, we need a way to actually find the <span class="math inline">\(\mathbf{w}^*\)</span> that minimizes our loss. To do this we will turn to an algorithm called <strong>gradient descent</strong>. In this lecture we will introduce gradient descent, but we will go into much more depth in a future lecture.</p>
<p>We’ll introduce gradient descent as a method to find the minimum of a generic function. We have some function <span class="math inline">\(f(\mathbf{\cdot})\)</span> and we would like find the input <span class="math inline">\(\mathbf{w}^*\)</span> that minimizes the output of the function:</p>
<p><span class="math display">\[
\text{Find: } \mathbf{w}^* = \underset{\mathbf{w}}{\text{argmin}}\ f(\mathbf{w})
\]</span></p>
<p>We don’t know how to find <span class="math inline">\(\mathbf{w}^*\)</span> directly, but if we have an initial guess <span class="math inline">\(\mathbf{w}^{(0)}\)</span>, we can try to update our guess to improve it.</p>
<p><span class="math display">\[
\mathbf{w}^{(1)} \leftarrow \mathbf{w}^{(0)} + \mathbf{g}
\]</span></p>
<div class="cell" data-execution_count="16">
<div class="cell-output cell-output-stdout">
<pre><code>Autograd ArrayBox with value [[-5.0059363  -4.47709947 -2.89058898 -4.09935888 -3.04168522 -2.36175215
  -2.52795801 -2.13510779]
 [-4.33221987 -3.80338304 -2.21687255 -3.42564245 -2.36796879 -1.68803572
  -1.85424158 -1.46139136]
 [-4.04994516 -3.52110833 -1.93459784 -3.14336774 -2.08569408 -1.40576101
  -1.57196687 -1.17911665]
 [-3.58364606 -3.05480923 -1.46829873 -2.67706863 -1.61939497 -0.9394619
  -1.10566776 -0.71281755]
 [-3.13138271 -2.60254588 -1.01603539 -2.22480529 -1.16713163 -0.48719856
  -0.65340442 -0.2605542 ]
 [-2.54188083 -2.013044   -0.42653351 -1.63530341 -0.57762975  0.10230332
  -0.06390254  0.32894768]
 [-1.87596204 -1.34712521  0.23938528 -0.96938462  0.08828904  0.76822211
   0.60201625  0.99486647]
 [-0.98703064 -0.45819381  1.12831668 -0.08045321  0.97722045  1.65715351
   1.49094765  1.88379787]]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-17-output-2.png" width="558" height="428"></p>
</div>
</div>
<p>Here we are changing <span class="math inline">\(\mathbf{w}^{(0)}\)</span> by moving in the direction of <span class="math inline">\(\mathbf{g}\)</span>. If we recall that the <em>gradient</em> of a function at point <span class="math inline">\(\mathbf{x}\)</span> corresponds to the <em>slope</em> of <span class="math inline">\(f\)</span> at <span class="math inline">\(\mathbf{w}\)</span>, or equivalently the direction of maximum change. This gives us a natural choice for the update to our guess.</p>
<p><span class="math display">\[
\mathbf{w}^{(1)} \leftarrow \mathbf{w}^{(0)} - \nabla f(\mathbf{w}^{(0)})
\]</span></p>
<p>Note that because the gradient corresponds to the direction that maximally <em>increases</em> <span class="math inline">\(f(\mathbf{w})\)</span>, we actually need to subtract the gradient in order to minimize our function. We can repeat this process many times, continuously updating our estimate.</p>
<p><span class="math display">\[
\text{For }i \text{ in 1,...,T}\text{ :} \\
\quad \mathbf{w}^{(i+1)} \leftarrow \mathbf{w}^{(i)} - \nabla f(\mathbf{w}^{(i)})
\]</span></p>
<div class="cell" data-execution_count="17">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-18-output-1.png" width="558" height="428"></p>
</div>
</div>
</section>
<section id="gradient-descent-convergence" class="level2">
<h2 class="anchored" data-anchor-id="gradient-descent-convergence">Gradient descent convergence</h2>
<p>Recall that it’s minimum value <span class="math inline">\(\mathbf{w}^*\)</span>, a function <span class="math inline">\(f\)</span> <em>must</em> have a gradient of <span class="math inline">\(\mathbf{0}\)</span>.</p>
<p><span class="math display">\[
\nabla f(\mathbf{w}^*) = \mathbf{0}
\]</span></p>
<p>It follows that:</p>
<p><span class="math display">\[
\mathbf{w}^{*} = \mathbf{w}^{*} - \nabla f(\mathbf{w}^{*})
\]</span></p>
<p>This means that if our gradient descent reaches the minimum, it will stop updating the guess and we know that we can stop our iteration. So we could write our algorithm to account for this:</p>
<p><span class="math display">\[
\text{While } \nabla f(\mathbf{w}^{(i)}) \neq \mathbf{0} \text{ :} \\
\quad \mathbf{w}^{(i+1)} \leftarrow \mathbf{w}^{(i)} - \nabla f(\mathbf{w}^{(i)})
\]</span></p>
<p>In practice though, it could take infinitely many updates to find the <em>exact</em> minimum. A more common approach is to define a convergence criteria that stops the iteration when the gradient magnitude is sufficiently small:</p>
<p><span class="math display">\[
\text{While } ||\nabla f(\mathbf{w}^{(i)})||_2 &gt; \epsilon \text{ :} \\
\quad \mathbf{w}^{(i+1)} \leftarrow \mathbf{w}^{(i)} - \nabla f(\mathbf{w}^{(i)})
\]</span></p>
</section>
<section id="step-sizes" class="level2">
<h2 class="anchored" data-anchor-id="step-sizes">Step sizes</h2>
<p>Notice that the gradient descent algorithm we’ve defined so far not only says that we want to update our guess in the <em>direction</em> of the gradient, it also say that we want to move in that direction a distance equal to the <em>magnitude</em> of the gradient. It turns out this is often a very bad idea!</p>
<p>This approach says that when the magnitude of the gradient is large, we should take a large step, and vice-versa. This is desirable for many functions as it means when we’re far from the minimum we take large steps, moving toward the minimum more quickly. While when we’re close to the minimum we take small steps to refine our guess more precisely.</p>
<div class="cell" data-execution_count="18">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-19-output-1.png" width="558" height="428"></p>
</div>
</div>
<p>However, if we take too large a step, we can overshoot the minimum entirely! In the worst case, this can lead to <em>divergence,</em> where gradient descent overshoots the minimum more and more at each step.</p>
<div class="cell" data-execution_count="19">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-20-output-1.png" width="558" height="428"></p>
</div>
</div>
<p>Remember that the gradient is making a <em>linear approximation</em> to the function. A strait line has no minimum, so the gradient has no information about where along the approximation the true minimum will be.</p>
<div class="cell" data-execution_count="20">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-21-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Also remember that the gradient gives us the direction of maximum change of the function, but this is only true in the <em>limit</em> of a very small step.</p>
<p><span class="math display">\[
\frac{df}{d\mathbf{w}}= \underset{\gamma \rightarrow 0}{\lim}\ \underset{\|\mathbf{\epsilon}\|_2 &lt; \gamma}{\max} \frac{f(\mathbf{w} + \mathbf{\epsilon}) - f(\mathbf{w})}{\|\mathbf{\epsilon}\|_2}
\]</span></p>
<p>So in higher dimensions, the gradient may not point directly to the minimum.</p>
<p>All of these issues motivate the need to control the size of our updates. We will typically do this by introducing an additional control to our algorithm: a <strong>step size</strong> or <strong>learning rate</strong>. This is a small constant <span class="math inline">\(\alpha\)</span>, that we will multiply the gradient by in each of our updates.</p>
<p><span class="math display">\[
\mathbf{w}^{(i+1)} \leftarrow \mathbf{w}^{(i)} - \alpha \nabla f(\mathbf{w}^{(i)})
\]</span></p>
<p>Using a small learning rate <span class="math inline">\((\alpha &lt;&lt; 1)\)</span> will make gradient descent slower, but <em>much</em> more reliable. Later on in the semester we will explore how to choose <span class="math inline">\(\alpha\)</span> (and even update it during optimization).</p>
<div class="cell" data-execution_count="21">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-22-output-1.png" width="558" height="427"></p>
</div>
</div>
<div class="cell" data-execution_count="22">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-23-output-1.png" width="558" height="428"></p>
</div>
</div>
</section>
<section id="optimizing-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="optimizing-linear-regression">Optimizing linear regression</h2>
<p>We can apply gradient descent to linear regression in order to find the parameters that minimize the mean squared error loss. To do this we need to find the gradient of the mean squared error with respect to the parameters:</p>
<p><span class="math display">\[
\nabla_{\mathbf{w}} \textbf{MSE}(\mathbf{w}, \mathbf{X}, \mathbf{y}) =
\frac{d}{d\mathbf{w}}\bigg( \frac{1}{N}\sum_{i=1}^N (\mathbf{x}_i^T\mathbf{w} - y_i)^2 \bigg)
\]</span></p>
<p><span class="math display">\[
= \frac{2}{N}\sum_{i=1}^N (\mathbf{x}_i^T\mathbf{w} - y_i)\mathbf{x}_i
\]</span></p>
<p>With this gradient our gradient descent update becomes:</p>
<p><span class="math display">\[
\mathbf{w}^{(i+1)} \longleftarrow \mathbf{w}^{(i)} - \alpha\bigg(\frac{2 }{N}\bigg)\sum_{i=1}^N  (\mathbf{x}_i^T\mathbf{w}^{(i)} - y_i)\mathbf{x}_i
\]</span></p>
<p>We can see that this update is a sum of all the inputs weighted by their corresponding residual given the current value of the parameters.</p>
<p>We can see how the the parameters of our regression model change as we run gradient descent.</p>
</section>
<section id="optimizing-linear-regression-directly" class="level2">
<h2 class="anchored" data-anchor-id="optimizing-linear-regression-directly">Optimizing linear regression directly</h2>
<p>Gradient descent works well for finding the optimal parameters for a linear regression model, but in fact we can actually find the optimal set of parameters directly, without needing to run an iterative algorithm.</p>
<p>We know that at the minimum, the gradient must be <span class="math inline">\(\mathbf{0}\)</span>, so the following condition must hold:</p>
<p><span class="math display">\[
\mathbf{0} = \bigg( \frac{2}{N}\bigg)\sum_{i=1}^N  (\mathbf{x}_i^T\mathbf{w} - y_i)\mathbf{x}_i
\]</span></p>
<p>We can solve for a <span class="math inline">\(\mathbf{w}\)</span> that satisfied this condition by first dropping the constant <span class="math inline">\(\frac{2}{N}\)</span>.</p>
<p><span class="math display">\[
\mathbf{0} = \sum_{i=1}^N  (\mathbf{x}_i^T\mathbf{w} - y_i)\mathbf{x}_i
\]</span></p>
<p><span class="math display">\[
\mathbf{0} = \sum_{i=1}^N \big( \mathbf{x}_i\mathbf{x}_i^T\mathbf{w} - y_i \mathbf{x}_i \big)
\]</span></p>
<p><span class="math display">\[
\sum_{i=1}^N  y_i \mathbf{x}_i  =\bigg(\sum_{i=1}^N  \mathbf{x}_i\mathbf{x}_i^T \bigg)  \mathbf{w}
\]</span></p>
<p>Note that <span class="math inline">\(\mathbf{x}_i \mathbf{x}_i^T\)</span> is a vector <em>outer product</em>:</p>
<p><span class="math display">\[
\mathbf{x}_i \mathbf{x}_i^T = \begin{bmatrix} x_{i1} \\ x_{i2} \\ \vdots \\  x_{in}\end{bmatrix} \begin{bmatrix} x_{i1} &amp; x_{i2} &amp; \dots &amp;  x_{in}\end{bmatrix} =
\begin{bmatrix} x_{i1} x_{i1} &amp; x_{i1} x_{i2} &amp; \dots &amp; x_{i1} x_{in} \\
x_{i2} x_{i1} &amp; x_{i2} x_{i2} &amp; \dots &amp; x_{i2} x_{in} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{in} x_{i1} &amp; x_{in} x_{i2} &amp; \dots &amp; x_{in} x_{in} \\
\end{bmatrix}
\]</span></p>
<p>Thus <span class="math inline">\(\bigg(\sum_{i=1}^N \mathbf{x}_i\mathbf{x}_i^T \bigg)\)</span> is a matrix. Multiplying both sides by the inverse <span class="math inline">\(\bigg(\sum_{i=1}^N \mathbf{x}_i\mathbf{x}_i^T \bigg)^{-1}\)</span> we get:</p>
<p><span class="math display">\[
\bigg(\sum_{i=1}^N  \mathbf{x}_i\mathbf{x}_i^T \bigg)^{-1} \bigg(\sum_{i=1}^N  y_i \mathbf{x}_i\bigg)  =  \mathbf{w}^*
\]</span></p>
<p>We can write this more compactly using the stacked input matrix and label vector notation we saw in homework 1.</p>
<p><span class="math display">\[
\mathbf{X} = \begin{bmatrix} \mathbf{x}_{1} \\ \mathbf{x}_{2} \\ \vdots \\  \mathbf{x}_{N} \end{bmatrix},\quad \mathbf{y} = \begin{bmatrix} y_{1} \\ y_{2} \\ \vdots \\  y_{N} \end{bmatrix}
\]</span></p>
<p>In this case, the expression becomes:</p>
<p><span class="math display">\[\mathbf{w}^* = \big( \mathbf{X}^T \mathbf{X} \big)^{-1} \big(\mathbf{y}\mathbf{X}\big)\]</span></p>
</section>
</section>
<section id="maximum-likelihood-estimation" class="level1">
<h1>Maximum likelihood estimation</h1>
<p>Let’s now take a look a linear regression from a slightly different perspective: the probabilistic view. We’ll get to the exact same approach, but with a motivation guided by statistics.</p>
<section id="normal-distributions" class="level2">
<h2 class="anchored" data-anchor-id="normal-distributions">Normal distributions</h2>
<p>The <strong>Normal</strong> distribution (also known as the <strong>Gaussian</strong> distribution) is a continuous probability distribution with the following probability density function:</p>
<p><span class="math display">\[
p(y) = \frac{1}{\sigma \sqrt{2 \pi}} \text{exp}\bigg(-\frac{1}{2\sigma^2} (y -\mu)^2\bigg)
\]</span></p>
<p>The normal distribution shows up almost everywhere in probability and statistics. Most notably, the <em>central limit theorem</em> tells us that the mean of many independent and identically distributed random outcomes tends towards a normal distribution.</p>
</section>
<section id="linear-regression-as-a-probabilistic-model" class="level2">
<h2 class="anchored" data-anchor-id="linear-regression-as-a-probabilistic-model">Linear regression as a probabilistic model</h2>
<p>Our function approximation view of linear regression says that we can <em>approximate</em> an unknown function with a linear function. An alternate approach is to define a <em>distribution</em> over the output <span class="math inline">\(y_i\)</span> of our unknown function given an input <span class="math inline">\(\mathbf{x}_i\)</span>. In particular, the probabilistic model for linear regression will make the assumption that the output is <em>normally distributed</em> conditioned on the input:</p>
<p><span class="math display">\[
y_i \sim \mathcal{N}\big(\mathbf{x}_i^T \mathbf{w},\ \sigma^2\big)
\]</span></p>
<p>Here we see the assumption we’re making is that the <em>mean</em> of the distribution is a linear function of the input, while the variance is fixed. Under this model, we can write the conditional probability or <strong>likelihood</strong> of an output as:</p>
<p><span class="math display">\[
p(y_i\mid\mathbf{x}_i, \mathbf{w}) =  \frac{1}{\sigma \sqrt{2 \pi}} \text{exp}\bigg(-\frac{1}{2\sigma^2} (y_i - \mathbf{x}_i^T\mathbf{w})^2\bigg)
\]</span></p>
<p>Why view linear regression as a probabilistic model? Well, generally for real data we can’t know if there actually <em>is</em> a function that perfectly maps inputs to outputs. It could be that there are variables we’re not accounting for, that there errors in our measurements for the data we collected or simply that there is some inherent randomness in the outputs. This view of linear regression makes the uncertainty in our predictions explicit.</p>
</section>
<section id="maximum-likelihood-estimation-1" class="level2">
<h2 class="anchored" data-anchor-id="maximum-likelihood-estimation-1">Maximum likelihood estimation</h2>
<p>With this view of linear regression in mind, let’s ask again how we find the optimal value for <span class="math inline">\(\mathbf{w}\)</span>. Possibly the most widely used approach to this is to simply choose the <span class="math inline">\(\mathbf{w}\)</span> that <em>maximizes the likelihood</em> (conditional probability) of all of the outputs in our dataset:</p>
<p><span class="math display">\[
\mathbf{w}^* = \underset{\mathbf{w}}{\text{argmax}} \ p(\mathbf{y} \mid \mathbf{X}, \mathbf{w}) =\underset{\mathbf{w}}{\text{argmax}} \ p(y_1,...,y_N \mid \mathbf{x}_1, ...,\mathbf{x}_N, \mathbf{w}) \]</span></p>
<p>Generally our model also assumes <em>conditional independence</em> across observations so:</p>
<p><span class="math display">\[
p(y_1,...,y_N \mid \mathbf{x}_1, ...,\mathbf{x}_N, \mathbf{w}) = \prod_{i=1}^N p(y_i\mid \mathbf{x}_i, \mathbf{w})
\]</span></p>
<p>For convenience, it is typical to frame the optimal value in terms of the <em>negative log-likelihood</em> rather than the likelihood, but the two are equivalent.</p>
<p><span class="math display">\[
\underset{\mathbf{w}}{\text{argmax}} \prod_{i=1}^N p(y_i\mid \mathbf{x}_i, \mathbf{w}) = \underset{\mathbf{w}}{\text{argmin}} - \sum_{i=1}^N \log p(y_i \mid \mathbf{x}_i, \mathbf{w}) = \textbf{NLL}(\mathbf{w}, \mathbf{X}, \mathbf{y})
\]</span></p>
<p>We see that the negative log-likelihood is a natural <em>loss function</em> to optimize to find <span class="math inline">\(\mathbf{w}^*\)</span>.</p>
<p><span class="math display">\[
\textbf{Loss}(\mathbf{w}) =\textbf{NLL}(\mathbf{w}, \mathbf{X}, \mathbf{y})=- \sum_{i=1}^N \log p(y_i \mid \mathbf{x}_i, \mathbf{w})
\]</span></p>
<p>We can write out the negative log-likelihood explicitly using the normal PDF:</p>
<p><span class="math display">\[
\textbf{NLL}(\mathbf{w}, \mathbf{X}, \mathbf{y}) = -\sum_{i=1}^N\log\bigg[\frac{1}{\sigma \sqrt{2 \pi}} \text{exp}\bigg(-\frac{1}{2\sigma^2} (y_i - \mathbf{x}_i^T\mathbf{w})^2\bigg)\bigg]
\]</span></p>
<p><span class="math display">\[
= \frac{1}{2\sigma^2} \sum_{i=1}^N(y_i - \mathbf{x}_i^T\mathbf{w})^2 + N \log \sigma \sqrt{2 \pi}
\]</span></p>
<p>We see that this loss is very similar to the MSE loss. Taking the gradient this becomes even more clear.</p>
<p><span class="math display">\[
\nabla_{\mathbf{w}}\textbf{NLL}(\mathbf{w}, \mathbf{X}, \mathbf{y}) =
\frac{d}{d\mathbf{w}}\bigg( \frac{1}{2\sigma^2} \sum_{i=1}^N(y_i - \mathbf{x}_i^T\mathbf{w})^2 + N \log \sigma \sqrt{2 \pi} \bigg)
\]</span></p>
<p><span class="math display">\[
= \frac{1}{2\sigma^2}\sum_{i=1}^N (\mathbf{x}_i^T\mathbf{w} - y_i)\mathbf{x}_i
\]</span></p>
<p>As we saw in the MSE case, the optimal value <span class="math inline">\(\mathbf{w}^*\)</span> does not depend on the constant value outside the summation. This means that the optimal value for <span class="math inline">\(\mathbf{w}\)</span> is the same for both MSE and negative log-likelihood and the optimal value does not depend on <span class="math inline">\(\sigma^2\)</span>!</p>
<p><span class="math display">\[
\underset{\mathbf{w}}{\text{argmin}}\  MSE(\mathbf{w}, \mathbf{X}, \mathbf{y}) = \underset{\mathbf{w}}{\text{argmin}}\ \textbf{NLL}(\mathbf{w}, \mathbf{X}, \mathbf{y})
\]</span></p>
<pre><code></code></pre>
</section>
<section id="section" class="level2">
<h2 class="anchored" data-anchor-id="section"></h2>
<pre><code></code></pre>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>