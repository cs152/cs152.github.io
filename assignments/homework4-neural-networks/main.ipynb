{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 4:** Automatic Differentiation and Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborators\n",
    "\n",
    "Please list anyone you discussed or collaborated on this assignment with below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIST COLLABORATORS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course feedback\n",
    "\n",
    "Please submit this week's course survey here: https://forms.gle/PJjGy8wrhhQnAhbb6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this homework we will build a tiny neural network libarary from scratch and start on an implementation of automatic differentiation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if using Colab!\n",
    "\n",
    "#import urllib.request\n",
    "#remote_url = 'https://gist.githubusercontent.com/gabehope/cb9e69f642104f107f25826a0931629a/raw/163f9cf5325db28826f4103d0f168702c77dfca1/hw4_support.py'\n",
    "#with urllib.request.urlopen(remote_url) as remote, open('hw4_support.py', 'w') as local:\n",
    "#  [local.write(str(line, encoding='utf-8')) for line in remote]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run me first!\n",
    "from hw4_support import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python features\n",
    "This homework makes use of a few fancy features in Python that are worth knowing about if you are unfamiliar.\n",
    "- [Variable length arguments](https://book.pythontips.com/en/latest/args_and_kwargs.html) (e.g. `*args`)\n",
    "- [List comprehensions](https://book.pythontips.com/en/latest/comprehensions.html#list-comprehensions) (e.g. `[a**2 for a in range(5)]`)\n",
    "- [Magic methods](https://rszalski.github.io/magicmethods/) (e.g. `__add__`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Autograd\n",
    "\n",
    "In this homework we will be using a special version of Numpy from a package called `Autograd`. Assuming it is installed (`pip install autograd`), we can import it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This special version of Numpy behaives exactly like normal numpy. We can create and do calculations with arrays just like we would before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([3., 2., 1])\n",
    "print('x:\\t', x)\n",
    "print('x^2:\\t', x ** 2)\n",
    "print('sum(x):\\t', np.sum(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, Autograd also has a very important trick up its sleeve: it can take derivatives (and gradients) for us! This functionality can be accessed through the `grad` function. Let's start by seeing it in action with a very simple example, where we know the correct answer. The square function and its derivative can be written as:\n",
    "\n",
    "$f(x) = x^2, \\quad f'(x) = 2x$\n",
    "\n",
    "The following code uses Autograd to compute this derivative automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import grad\n",
    "\n",
    "# Define a function\n",
    "def f(x):\n",
    "    return x ** 2\n",
    "\n",
    "# Use 'grad' to compute the derivative function\n",
    "f_prime = grad(f)\n",
    "\n",
    "# Verify that we get the correct answer\n",
    "x = 5.\n",
    "print('x:\\t', x)\n",
    "print('f(x):\\t', f(x))\n",
    "print(\"f'(x):\\t\", f_prime(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start to see how `grad` operates. `grad` takes as input a function (e.g. $f(x)$) and returns a new function that computes the derivative of $f$ at $x$. ($f'(x)$). So:\n",
    "\n",
    "$\\text{grad}(f) \\longrightarrow f'$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1\n",
    "\n",
    "Define the following function in python:\n",
    "\n",
    "$f(x) = \\log(\\sin(x^3) + 3 x)$\n",
    "\n",
    "Use `grad` to compute the derivative of $f$ at $1.5$ (i.e. compute $f'(1.5)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "\n",
    "answer = \n",
    "print(\"f'(1.5)=\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name would imply, `grad` can more generally be used to compute the *gradient* of a function of the form $f(\\mathbf{x}): \\mathbb{R}^d\\rightarrow \\mathbb{R}$. Remember that for a function that takes in a vector and outputs a scalar, the gradient is vector of all partial derivatives of the output with respect to each input. For example, consider a function that gives the square of the 2-norm of a vector:\n",
    "\n",
    "$f(\\mathbf{x}) = ||\\mathbf{x}||^2_2 = \\mathbf{x}^T\\mathbf{x} = \\sum_{i=1}^d x_i^2$\n",
    "\n",
    "*Think about why these expressions are equivalent!*\n",
    "\n",
    "As we've seen, the gradient of this function can be written as:\n",
    "\n",
    "$\\nabla f(\\mathbf{x}) = 2\\mathbf{x} = \\begin{bmatrix}2x_1 \\\\ 2x_2 \\\\ \\vdots \\\\ 2x_d \\end{bmatrix}$\n",
    "\n",
    "Let's see what Autograd gives us in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function\n",
    "def f(x):\n",
    "    return np.sum(x ** 2)\n",
    "\n",
    "# Use 'grad' to compute the derivative function\n",
    "grad_f = grad(f)\n",
    "\n",
    "# Verify that we get the correct answer\n",
    "x = np.array([1., 2., 3])\n",
    "print('x:\\t', x)\n",
    "print('f(x):\\t', f(x))\n",
    "print(\"f'(x):\\t\", grad_f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the gradient has the same shape as the input. So the gradient function is of the form: $\\mathbb{R}^d \\rightarrow \\mathbb{R}^d$\n",
    "\n",
    "This makes sense as the gradient should have exactly one partial derivative for each entry in the input to the function. As discussed, this even extends beyond vectors! We could have a function that takes in any datastructure and computes the set of partial derivatives with respect to each entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2\n",
    "\n",
    "Write a function that takes a `list` of vectors and computes the sum of the squared 2-norm for each vector. That is:\n",
    "\n",
    "$f([\\mathbf{a}, \\mathbf{b}, \\mathbf{c}...]) = ||\\mathbf{a}||^2 + ||\\mathbf{b}||^2 + ||\\mathbf{c}||^2+...$\n",
    "\n",
    "*Recall from above how we can compute each term in this sum!*\n",
    "\n",
    "Then use `grad` to compute the gradient of this function with respect to the given input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function\n",
    "def f(x):\n",
    "    '''\n",
    "    Compute the sum of squared 2-norms for a list of vectors\n",
    "\n",
    "    Args:\n",
    "        x (list of arrays): A list of 1-dimensional arrays\n",
    "    Returns:\n",
    "        output (float): The result\n",
    "    '''\n",
    "    ## YOUR CODE HERE\n",
    "    s = 0.\n",
    "    for xi in x:\n",
    "        s += np.dot(xi, xi)\n",
    "    return s\n",
    "\n",
    "# Use 'grad' to compute the derivative function\n",
    "grad_f = grad(f)\n",
    "\n",
    "# Verify that we get the correct answer\n",
    "x = [np.array([1., 2., 3]), np.array([7., 2.]), np.array([6.])]\n",
    "print('x:\\t', x)\n",
    "print('f(x):\\t', f(x))\n",
    "print(\"f'(x):\\t\", grad_f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful argument that we can give to `grad` is `argnum`. If our function takes more than one argument `argnum` lets us specify which one to take the gradient with respect to. For example, if we have the function:\n",
    "\n",
    "$f(x, y) = x^2y$\n",
    "\n",
    "Then:\n",
    "\n",
    "$f'_x(x,y)=2xy, \\quad f'_y(x, y)=x^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    return x ** 2 * y\n",
    "\n",
    "print('f(3, 5) = ', f(3., 5.))\n",
    "\n",
    "df_dx = grad(f, argnum=0)(3., 5.)\n",
    "df_dy = grad(f, argnum=1)(3., 5.)\n",
    "\n",
    "print('df_dx = ', df_dx)\n",
    "print('df_dy = ', df_dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementing a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have everything we need to apply automatic differentiation to train a neural network!\n",
    "\n",
    "Before we do that though, let's try out our automatic differentiation for logistic regression. Below is a slight modification of LogisticRegression implementation we saw in the last homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x):\n",
    "    # Computes the sigmoid function\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, dims):\n",
    "        '''\n",
    "        Args:\n",
    "            dims (int): d, the dimension of each input\n",
    "        '''\n",
    "        self.weights = np.zeros((dims + 1, 1))\n",
    "\n",
    "    def prediction_function(self, X, w):\n",
    "        '''\n",
    "        Get the result of our base function for prediction (i.e. x^t w)\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "            w (array): A (d+1) x 1 vector of weights.\n",
    "        Returns:\n",
    "            pred (array): A length N vector of f(X).\n",
    "        '''\n",
    "        X = np.pad(X, ((0,0), (0,1)), constant_values=1.)\n",
    "        return np.dot(X, w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict labels given a set of inputs.\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "        Returns:\n",
    "            pred (array): An N x 1 column vector of predictions in {0, 1}\n",
    "        '''\n",
    "        return (self.prediction_function(X, self.weights) > 0)\n",
    "    \n",
    "    def predict_probability(self, X):\n",
    "        '''\n",
    "        Predict the probability of each class given a set of inputs\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "        Returns:\n",
    "            probs (array): An N x 1 column vector of predicted class probabilities\n",
    "        '''\n",
    "        return sigmoid(self.prediction_function(X, self.weights))\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        '''\n",
    "        Compute the accuracy of the model's predictions on a dataset\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "            y (array): A length N vector of labels.\n",
    "        Returns:\n",
    "            acc (float): The accuracy of the classifier\n",
    "        '''\n",
    "        y = y.reshape((-1, 1))\n",
    "        return (self.predict(X) == y).mean()\n",
    "\n",
    "    def nll(self, X, y, w=None):\n",
    "        '''\n",
    "        Compute the negative log-likelihood loss.\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "            y (array): A length N vector of labels.\n",
    "            w (array, optional): A (d+1) x 1 matrix of weights.\n",
    "        Returns:\n",
    "            nll (float): The NLL loss\n",
    "        '''\n",
    "        if w is None:\n",
    "            w = self.weights\n",
    "\n",
    "        y = y.reshape((-1, 1))\n",
    "        xw = self.prediction_function(X, w)\n",
    "        py = sigmoid((2 * y - 1) * xw)\n",
    "        return -(np.log(py)).sum()\n",
    "    \n",
    "    def nll_gradient(self, X, y):\n",
    "        '''\n",
    "        Compute the gradient of the negative log-likelihood loss.\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "            y (array): A length N vector of labels.\n",
    "        Returns:\n",
    "            grad (array): A length (d + 1) vector with the gradient\n",
    "        '''\n",
    "        y = y.reshape((-1, 1))\n",
    "        xw = self.prediction_function(X, self.weights)\n",
    "        py = sigmoid((2 * y - 1) * xw)\n",
    "        grad = ((1 - py) * (2 * y - 1)).reshape((-1, 1)) * np.pad(X, [(0,0), (0,1)], constant_values=1.)\n",
    "        return -np.sum(grad, axis=0)\n",
    "    \n",
    "    def nll_and_grad_no_autodiff(self, X, y):\n",
    "        # Compute nll_and_grad without automatic diferentiation\n",
    "        return self.nll(X, y), self.nll_gradient(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3 \n",
    "Write the method `nll_and_grad` for the LogisticRegression class using the `grad` function from Autograd. Verify that it gives a similar answer to `nll_and_grad_no_autodiff`.\n",
    "\n",
    "***Hint:** Note that the `nll` function can optionally take in the parameters. You can use this functionality and the `argnum` argument of `grad` in your answer. You can assume that `self` refers to the model object, so you can access the weights via `self.weights`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_and_grad(self, X, y):\n",
    "    ## YOUR CODE HERE\n",
    "    loss =\n",
    "    grads = \n",
    "    return loss, grads\n",
    "\n",
    "LogisticRegression.nll_and_grad = nll_and_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation quite inefficient (we'll fix this in the future!), so we'll test our model on a very small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(100, noise=0.1)\n",
    "model = LogisticRegression(2)\n",
    "gradient_descent(model, X, y, lr=3e-2, steps=250)\n",
    "\n",
    "print('Model accuracy: %.3f' % model.accuracy(X, y))\n",
    "plot_boundary(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extend our model to be a neural network! We'll create a neural network class that extends our logistic regression class. First we'll setup the needed weight matrices.\n",
    "\n",
    "#### Q4\n",
    "Fill in the Neural Network `__init__` method below. The method should take in the input data dimension and a list of integers specifying the size of each hidden layer (the number of neurons in each layer). The function should create a list of numpy arrays of the appropriate shapes for the weight matrices. \n",
    "\n",
    "For example if `dims` is `2` and `hidden_sizes` is `[4, 4]`, then `self.weights` should have 3 entries of shapes `[(4x2), (4x4), (1x4)]`. This network is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div scale=0.5><svg xmlns=\"http://www.w3.org/2000/svg\" style=\"cursor: move;\" viewbox=\"100 100 1660 899\" width=\"600\" height=\"400\"><g transform=\"translate(-1450.305465592915,-694.5417988897334) scale(2.441893025338307)\"><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,469.5, 846.6666666666666,429.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,469.5, 846.6666666666666,469.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,469.5, 846.6666666666666,509.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,469.5, 846.6666666666666,549.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,509.5, 846.6666666666666,429.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,509.5, 846.6666666666666,469.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,509.5, 846.6666666666666,509.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,509.5, 846.6666666666666,549.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,429.5, 1026.6666666666667,429.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,429.5, 1026.6666666666667,469.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,429.5, 1026.6666666666667,509.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,469.5, 1026.6666666666667,429.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,469.5, 1026.6666666666667,469.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,469.5, 1026.6666666666667,509.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,509.5, 1026.6666666666667,429.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,509.5, 1026.6666666666667,469.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,509.5, 1026.6666666666667,509.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,549.5, 1026.6666666666667,429.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,549.5, 1026.6666666666667,469.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,549.5, 1026.6666666666667,509.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M1026.6666666666667,429.5, 1206.6666666666667,489.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M1026.6666666666667,469.5, 1206.6666666666667,489.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M1026.6666666666667,509.5, 1206.6666666666667,489.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,429.5, 1026.6666666666667,549.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,469.5, 1026.6666666666667,549.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,509.5, 1026.6666666666667,549.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,549.5, 1026.6666666666667,549.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M1026.6666666666667,549.5, 1206.6666666666667,489.5\"></path><circle r=\"10\" class=\"node\" id=\"0_0\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"666.6666666666666\" cy=\"469.5\"></circle><circle r=\"10\" class=\"node\" id=\"0_1\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"666.6666666666666\" cy=\"509.5\"></circle><circle r=\"10\" class=\"node\" id=\"1_0\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"846.6666666666666\" cy=\"429.5\"></circle><circle r=\"10\" class=\"node\" id=\"1_1\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"846.6666666666666\" cy=\"469.5\"></circle><circle r=\"10\" class=\"node\" id=\"1_2\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"846.6666666666666\" cy=\"509.5\"></circle><circle r=\"10\" class=\"node\" id=\"1_3\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"846.6666666666666\" cy=\"549.5\"></circle><circle r=\"10\" class=\"node\" id=\"2_0\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1026.6666666666667\" cy=\"429.5\"></circle><circle r=\"10\" class=\"node\" id=\"2_1\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1026.6666666666667\" cy=\"469.5\"></circle><circle r=\"10\" class=\"node\" id=\"2_2\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1026.6666666666667\" cy=\"509.5\"></circle><text class=\"text\" dy=\".35em\" style=\"font-size: 12px;\" x=\"631.6666666666666\" y=\"589.5\">Input Layer ∈ ℝ²</text><text class=\"text\" dy=\".35em\" style=\"font-size: 12px;\" x=\"811.6666666666666\" y=\"589.5\">Hidden Layer ∈ ℝ⁴</text><text class=\"text\" dy=\".35em\" style=\"font-size: 12px;\" x=\"991.6666666666667\" y=\"589.5\">Hidden Layer ∈ ℝ⁴</text><circle r=\"10\" class=\"node\" id=\"2_3\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1026.6666666666667\" cy=\"549.5\"></circle><circle r=\"10\" class=\"node\" id=\"3_0\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1206.6666666666667\" cy=\"489.5\"></circle><text class=\"text\" dy=\".35em\" style=\"font-size: 12px;\" x=\"1171.6666666666667\" y=\"589.5\">Output Layer ∈ ℝ¹</text></g><defs><marker id=\"arrow\" viewBox=\"0 -5 10 10\" markerWidth=\"7\" markerHeight=\"7\" orient=\"auto\" refX=\"40\"><path d=\"M0,-5L10,0L0,5\" style=\"stroke: rgb(80, 80, 80); fill: none;\"></path></marker></defs></svg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you find it easier you could also define the weights in terms of $W^T$ instead, in which case the shapes would be: `[(2x4), (4x4), (4x1)]`. You could also consider how to add a bias term at each layer as in logistic regression (but this isn't nessecary for full credit).\n",
    "\n",
    "The values in each array should be drawn from a normal distribution with standard deviation 1. You can create such a matrix in numpy using:\n",
    "\n",
    "```\n",
    "np.random.normal(scale=1., size=shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(LogisticRegression):\n",
    "    def __init__(self, dims, hidden_sizes=[]):\n",
    "        ## YOUR CODE HERE\n",
    "        self.weights = \n",
    "\n",
    "test_nn_constructor(NeuralNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that for logistic regression the prediction function (before threholding or sigmoid) was $\\mathbf{X}\\mathbf{w}$. We now want to implement the prediction function for our neural network class. This function should perform the appropriate feature transforms and multiply by the regression weights. For a neural network with a single hidden layer this will look like:\n",
    "\n",
    "$f(\\mathbf{X}) = \\sigma(\\mathbf{X}\\mathbf{W}_1^T)\\mathbf{w}_0$\n",
    "\n",
    "Use the **sigmoid** activation function for this problem.\n",
    "\n",
    "For multiple layers we can also think of this a a **chain** of feature transforms:\n",
    "$$\\Phi_1 = \\sigma(\\mathbf{X}\\mathbf{W}_1^T)$$\n",
    "$$\\Phi_2 = \\sigma(\\Phi_1 \\mathbf{W}_2^T)$$\n",
    "$$...$$\n",
    "$$\\Phi_l = \\sigma(\\Phi_{l-1} \\mathbf{W}_l^T)$$\n",
    "$$f(\\mathbf{X}) = \\Phi_l\\mathbf{w}_0$$\n",
    "Where $\\Phi_i$ is just the variable that represents the neurons at layer $i$ (the result of the first $i$ transforms applied to $\\mathbf{X}$).\n",
    "\n",
    "\n",
    "#### Q5 \n",
    "Implement the prediction function as described above. Note that the prediction function should use the weights passed into the `w` argument rather than `self.weights`, this will make it easier to implement the next question.\n",
    "\n",
    "***Hint:** Note that this function should **not** apply a final sigmoid or thresholding, instead it should be the equivalent of `linear_function` from the previous homework*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_function(self, X, w):\n",
    "    '''\n",
    "    Get the result of our base function for prediction (i.e. x^t w)\n",
    "\n",
    "    Args:\n",
    "        X (array): An N x d matrix of observations.\n",
    "        w (list of arrays): A list of weight matrices\n",
    "    Returns:\n",
    "        pred (array): An N x 1 matrix of f(X).\n",
    "    '''\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "    return pred.reshape((-1, 1))\n",
    "\n",
    "NeuralNetwork.prediction_function = prediction_function\n",
    "test_nn_prediction_function(NeuralNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6\n",
    "Implement an `nll_and_grad` method for the `NeuralNetwork` class using Autograd to compute the gradient with respect to each weight matrix.\n",
    "\n",
    "***Hint:** If you use `np.pad` anywhere in your implementation, Autograd may complain if you don't include the keyword argument `mode='constant'`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_and_grad(self, X, y):\n",
    "    '''\n",
    "    Get the negative log-likelihood loss and its gradient\n",
    "\n",
    "    Args:\n",
    "        X (array): An N x d matrix of observations.\n",
    "        y (array): A length N vector of labels\n",
    "    Returns:\n",
    "        nll (float): The negative log-likelihood\n",
    "        grads (list of arrays): A list of the gradient of the nll with respect\n",
    "                                to each value in self.weights.\n",
    "    '''\n",
    "    ## YOUR CODE HERE\n",
    "    loss =\n",
    "    grads = \n",
    "    return loss, grads\n",
    "\n",
    "NeuralNetwork.nll_and_grad = nll_and_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything in place to train a neural network from scratch! Let's try it on our tiny dataset. Feel free to change the inputs.\n",
    "\n",
    "***Hint**: If this give very poor results and/or runs very slowly, make sure to carefully check the shape of each operation in your code to make sure it matches your expectation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(100, noise=0.1)\n",
    "model = NeuralNetwork(2, [5, 5])\n",
    "gradient_descent(model, X, y, lr=3e-2, steps=250)\n",
    "\n",
    "print('Model accuracy: %.3f' % model.accuracy(X, y))\n",
    "plot_boundary(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Forward-mode automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by developing an automatic differentiation class that uses *forward-mode automatic differentiation*. \n",
    "\n",
    "Recall that for this version of automatic differentiaion each operation needs to keep track of the derivative of it's value with respect *each* original input. Since for every operation we need to store these extra pieces of data and functions for computing both the operation and its derivative, it makes sense to define a *class* to represent the result of an operation.\n",
    "\n",
    "For example, if we want to make a class that represents the operation `c=a+b` our class needs several properties:\n",
    "- `value`: The value of the operation (`c`)\n",
    "- `forward_grads`: A dictionary that contains the derivatives with respect to each original input (e.g. ($\\frac{dc}{da}$ and $\\frac{dc}{db}$)).\n",
    "- `func`: A function that computes the operation (`a+b`)\n",
    "- `grads`: A function that *computes* the derivatives of the operation ($\\frac{dc}{da}$ and $\\frac{dc}{db}$)\n",
    "\n",
    "For this homework, we've provided the outline of such a class, called `ForwardValue`. This will be the base class for all of our possible operations and represents declaring a variable with a value (`a = 5`). This is useful because it lets us define values that we might want to find derivatives with respect to.\n",
    "\n",
    "Let's see how this will work in practice. If we want to take derivatives we will first define the inputs using `ForwardValue`.\n",
    "\n",
    "```{python}\n",
    "a = ForwardValue(5)\n",
    "b = ForwardValue(2)\n",
    "```\n",
    "Then we can perform whatever operations we want on these inputs:\n",
    "```{python}\n",
    "c = a + b\n",
    "L = log(c)\n",
    "```\n",
    "Each of these operations will produce a new `ForwardValue` object representing the result of that operation. \n",
    "\n",
    "As each result should maintain the derivatives with respect to each *original* inputs, we can access the final derivatives we're interested ($\\frac{dL}{da}$ and $\\frac{dL}{db}$) in from `L`. \n",
    "\n",
    "```{python}\n",
    "dL_da = L.forward_grads[a]\n",
    "dL_db = L.forward_grads[b]\n",
    "```\n",
    "\n",
    "\n",
    "We'll also be able to compute operations with non-AutogradValue numbers, but obviously won't be able to compute derivaitives with respect to these values.\n",
    "```{python}\n",
    "s = 4\n",
    "L = s * a\n",
    "dL_da = L.forward_grads[a] # Will work because a is an ForwardValue\n",
    "dL_ds = L.forward_grads[s] # Will give an error because s is not an ForwardValue\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen what our final product will look like, let's define our `ForwardValue` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardValue:\n",
    "    '''\n",
    "    Base class for automatic differentiation operations. Represents variable delcaration.\n",
    "    Subclasses will overwrite func and grads to define new operations.\n",
    "\n",
    "    Properties:\n",
    "        parent_values    (list): A list of raw values of each input (as floats)\n",
    "        value   (float): The value of the result of this operation\n",
    "        forward_grads (dict): A dictionary mapping inputs to gradients\n",
    "    '''\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        self.parent_values = [arg.value if isinstance(arg, ForwardValue) else arg for arg in args]\n",
    "        self.value = self.forward_pass(args)\n",
    "        \n",
    "        if len(self.forward_grads.keys()) == 0:\n",
    "            self.forward_grads = {self: 1}\n",
    "        \n",
    "\n",
    "    def func(self, input):\n",
    "        '''\n",
    "        Compute the value of the operation given the inputs.\n",
    "        For declaring a variable, this is just the identity function (return the input).\n",
    "\n",
    "        Args:\n",
    "            input (float): The input to the operation\n",
    "        Returns:\n",
    "            value (float): The result of the operation\n",
    "        '''\n",
    "        return input\n",
    "\n",
    "    def grads(self, *args):\n",
    "        '''\n",
    "        Compute the derivative of the operation with respect to each input.\n",
    "        In the base case the derivative of the identity function is just 1. (da/da = 1).\n",
    "\n",
    "        Args:\n",
    "            input (float): The input to the operation\n",
    "        Returns:\n",
    "            grads (tuple): The derivative of the operation with respect to each input\n",
    "                            Here there is only a single input, so we return a length-1 tuple.\n",
    "        '''\n",
    "        return (1,)\n",
    "    \n",
    "    def forward_pass(self, args):\n",
    "        # Calls func to compute the value of this operation \n",
    "        self.forward_grads = {}\n",
    "        return self.func(*self.parent_values)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        # Python magic function for string representation.\n",
    "        return str(self.value)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the base case, when we declare a variable the derivative with respect to itself is just 1 ($\\frac{da}{da}=1$)\n",
    "```{python}\n",
    "da_da = a.forward_grads[a] # Will be 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined the framework for an operation that can be used in automatic differentiation, we need to define some actual useful operations by subclassing `ForwardValue`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7\n",
    "Fill out the `func` and `grads` methods of each subclass below. Recall that `func` should always return the result of the operation and `grads` should always return a `tuple` of the derivative with respect to each input.\n",
    "\n",
    "***Hint:** Look at the `_add` and `_neg` examples as a template!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _add(ForwardValue):\n",
    "    # Addition operator (a + b)\n",
    "    def func(self, a, b):\n",
    "        return a + b\n",
    "    \n",
    "    def grads(self, a, b):\n",
    "        return 1., 1.\n",
    "    \n",
    "class _neg(ForwardValue):\n",
    "    # Negation operator (-a)\n",
    "    def func(self, a):\n",
    "        return -a\n",
    "    \n",
    "    def grads(self, a):\n",
    "        return (-1.,)\n",
    "\n",
    "class _sub(ForwardValue):\n",
    "    # Subtraction operator (a - b)\n",
    "    def func(self, a, b):\n",
    "        # Your code here\n",
    "    \n",
    "    def grads(self, a, b):\n",
    "        # Your code here\n",
    "\n",
    "class _mul(ForwardValue):\n",
    "    # Multiplication operator (a * b)\n",
    "    def func(self, a, b):\n",
    "        # Your code here\n",
    "    \n",
    "    def grads(self, a, b):\n",
    "        # Your code here\n",
    "\n",
    "class _div(ForwardValue):\n",
    "    # Division operator (a / b)\n",
    "    def func(self, a, b):\n",
    "        # Your code here\n",
    "    \n",
    "    def grads(self, a, b):\n",
    "        # Your code here\n",
    "    \n",
    "class _exp(ForwardValue):\n",
    "    # Exponent operator (e^a, or exp(a))\n",
    "    def func(self, a):\n",
    "        # Your code here\n",
    "    \n",
    "    def grads(self, a):\n",
    "        # Your code here\n",
    "\n",
    "class _log(ForwardValue):\n",
    "    # (Natural) log operator (log(a))\n",
    "    def func(self, a):\n",
    "        # Your code here\n",
    "    \n",
    "    def grads(self, a):\n",
    "        # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we'll define our basic functions and operators in terms of the operator classes we just wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp(a):\n",
    "    return _exp(a) if isinstance(a, ForwardValue) else math.exp(a)\n",
    "def log(a):\n",
    "    return _log(a) if isinstance(a, ForwardValue) else math.log(a)\n",
    "\n",
    "# Note: Remember that above we defined a class for each type of operation\n",
    "# so in this code we are overriding the basic operators for AutogradValue\n",
    "# such that they construct a new object of the class corresponding to the\n",
    "# given operation and return it. \n",
    "# (You don't need to everything that's happening here to do the HW)\n",
    "ForwardValue.exp = lambda a: _exp(a)\n",
    "ForwardValue.log = lambda a: _log(a)\n",
    "ForwardValue.__add__ = lambda a, b: _add(a, b)\n",
    "ForwardValue.__radd__ = lambda a, b: _add(b, a)\n",
    "ForwardValue.__sub__ = lambda a, b: _sub(a, b)\n",
    "ForwardValue.__rsub__ = lambda a, b: _sub(b, a)\n",
    "ForwardValue.__neg__ = lambda a: _neg(a)\n",
    "ForwardValue.__mul__ = lambda a, b: _mul(a, b)\n",
    "ForwardValue.__rmul__ = lambda a, b: _mul(b, a)\n",
    "ForwardValue.__truediv__ = lambda a, b: _div(a, b)\n",
    "ForwardValue.__rtruediv__ = lambda a, b: _div(b, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now be able to use our `AutogradValue` objects as if they are numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ForwardValue(5)\n",
    "b = ForwardValue(2)\n",
    "\n",
    "print((a + 5) * b)\n",
    "print(log(b))\n",
    "\n",
    "test_operators(ForwardValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see now that our `forward_pass` method needs to update `forward_grads` (e.g. to compute $\\frac{dg}{da}$ and $\\frac{dg}{db}$) using the `forward_grads` values of its parents (e.g. $\\frac{dc}{da}$ and $\\frac{dc}{db}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8\n",
    "Update the `forward_pass` method below for forward-mode automatic differentiation. This method should update the `foward_grads` property of the operation such that:\n",
    "- `foward_grads` has an entry for every input that appears in `foward_grads` of *any* parent operation.\n",
    "- If an input appears in more than 1 parent, make sure to *add* the gradients appropritately (if `g` has parents `b` and `c` then $\\frac{dg}{da} = \\frac{dg}{db}\\frac{db}{da} + \\frac{dg}{dc}\\frac{dc}{da}$  )\n",
    "- Parents that are not `AutogradValue` objects are ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our `forward_pass` method is working correctly, we should have the following behaivior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our inputs as ForwardValue objects\n",
    "a = ForwardValue(5)\n",
    "b = ForwardValue(2)\n",
    "\n",
    "# Perform operations\n",
    "c = a * b\n",
    "g = 3 * c + a\n",
    "\n",
    "\n",
    "# We should have the following in the forward_grads property of c and d (note that the keys are ForwardValue objects!)\n",
    "c.forward_grads = {a: 2, b: 5}  # dc/da and dc/db\n",
    "g.forward_grads = {a: 3 * 2 + 1, b: 3 * 5} # dg/da = dg/dc dc/da + dg/da, dg/db = dg/dc dc/db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the method below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(self, args):\n",
    "    # Clear forward_grads if it exists\n",
    "    self.forward_grads = {} \n",
    "    ## YOUR CODE HERE\n",
    "    \n",
    "\n",
    "    # Make sure to still return the operation's value\n",
    "    return self.func(*self.parent_values)\n",
    "\n",
    "# Overwrite the AutogradValue method so that operators still work\n",
    "ForwardValue.forward_pass = forward_pass\n",
    "test_forward_mode(ForwardValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take derivates of functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ForwardValue(5)\n",
    "b = ForwardValue(2)\n",
    "L = -log(5 *b + a)\n",
    "\n",
    "dL_da = L.forward_grads[a]\n",
    "dL_db = L.forward_grads[b]\n",
    "print('dL/da = %.3f, dL/db = %.3f' % (dL_da, dL_db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also implement our own very simple version of Autograd's `grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(f):\n",
    "    def ad_function(x, *args):\n",
    "        x = ForwardValue(x)\n",
    "        output = f(x, *args)\n",
    "        return output.forward_grads[x]\n",
    "    return ad_function\n",
    "\n",
    "# Define a function\n",
    "def f(x):\n",
    "    return x ** 2\n",
    "\n",
    "# Use 'grad' to compute the derivative function\n",
    "f_prime = grad(f)\n",
    "\n",
    "# Verify that we get the correct answer\n",
    "x = 5.\n",
    "print('x:\\t', x)\n",
    "print('f(x):\\t', f(x))\n",
    "print(\"f'(x):\\t\", f_prime(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next homework, we'll combine our forward-mode AD implementation with our neural network class. Then we'll look at how to do automatic-differentiation more efficiently with reverse-mode AD."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
