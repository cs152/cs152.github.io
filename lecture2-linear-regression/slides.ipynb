{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Lecture 2: Linear regression\"\n",
        "format: \n",
        "  revealjs:\n",
        "    width: 1920\n",
        "    height: 1080\n",
        "    slide-number: true\n",
        "    incremental: false\n",
        "    echo: true\n",
        "    theme: [\"theme.scss\"]\n",
        "    revealjs-plugins:\n",
        "---"
      ],
      "id": "c5d1808b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Logistics\n"
      ],
      "id": "3e26baa5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from manim import *\n",
        "import autograd.numpy as np\n",
        "\n",
        "class LectureScene(Scene):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.camera.background_color = \"#ffffff\"\n",
        "        self.template = TexTemplate()\n",
        "        self.template.add_to_preamble(r\"\\usepackage{amsmath}\")\n",
        "\n",
        "class ThreeDLectureScene(ThreeDScene):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.camera.background_color = \"#ffffff\"\n",
        "        self.template = TexTemplate()\n",
        "        self.template.add_to_preamble(r\"\\usepackage{amsmath}\")\n",
        "    \n",
        "\n",
        "class VectorScene(LectureScene):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.ax = Axes(\n",
        "            x_range=[-7.5, 7.5, 1],\n",
        "            y_range=[-5, 5, 1],\n",
        "            x_length=12,\n",
        "            y_length=8,\n",
        "            axis_config={\"color\": GREY},\n",
        "        )\n",
        "        \n",
        "        #axes_labels.set_color(GREY)\n",
        "        self.add(self.ax)\n",
        "\n",
        "class PositiveVectorScene(LectureScene):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.ax = Axes(\n",
        "            x_range=[-2.5, 12.5, 1],\n",
        "            y_range=[-1, 9, 1],\n",
        "            x_length=12,\n",
        "            y_length=8,\n",
        "            axis_config={\"color\": GREY},\n",
        "        )\n",
        "                #axes_labels.set_color(GREY)\n",
        "        self.add(self.ax)\n",
        "\n",
        "class ComparisonVectorScene(LectureScene):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.ax1 = Axes(\n",
        "            x_range=[-5, 5, 1],\n",
        "            y_range=[-5, 5, 1],\n",
        "            x_length=6,\n",
        "            y_length=6,\n",
        "            axis_config={\"color\": GREY},\n",
        "        )\n",
        "        self.ax2 = Axes(\n",
        "            x_range=[-5, 5, 1],\n",
        "            y_range=[-5, 5, 1],\n",
        "            x_length=6,\n",
        "            y_length=6,\n",
        "            axis_config={\"color\": GREY},\n",
        "        )\n",
        "        axgroup = Group(self.ax1, self.ax2)\n",
        "        axgroup.arrange_in_grid(buf=2)\n",
        "        \n",
        "        #axes_labels.set_color(GREY)\n",
        "        self.add(axgroup)"
      ],
      "id": "0c8668f5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logistics\n",
        "\n",
        "-   Homework 1 due **next Tuesday 9/5 at 11:59pm**\n",
        "\n",
        "-   Fill out Github username survey!\n",
        "\n",
        "-   Office hours **tomorrow 4-5:30pm in MacGregor 322**\n",
        "\n",
        "-   [Course calendar](https://harveymuddcollege.instructure.com/courses/615/pages/course-calendar) added to website with lecture notes.\n",
        "\n",
        "-   Lecture notes are [open source](https://github.com/CS152-Neural-Networks-Fall-2023/CS152-Neural-Networks-Fall-2023.github.io)!\n",
        "\n",
        "# Linear regression\n",
        "\n",
        "## Functions revisited\n",
        "\n",
        "$$\n",
        "y=f(\\mathbf{x}), \\quad \\text{Input: } \\mathbf{x} \\in\\mathbb{R}^n \\longrightarrow \\text{ Output: }y \\in\\mathbb{R}\n",
        "$$\n"
      ],
      "id": "b32f6f66"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "%%manim -sqh -v CRITICAL --progress_bar none BasicFunction\n",
        "\n",
        "class BasicFunction(PositiveVectorScene):\n",
        "    def construct(self):\n",
        "        fx = self.ax.plot(lambda x: 2 * (x / 5) ** 3 - 5 * (x / 6) ** 2 + 4, color=RED)\n",
        "        eq = MathTex(r'f(\\mathbf{x})=2(\\frac{x_1}{5})^3 -5 (\\frac{x_1}{6})^2 + 4', color=BLACK, tex_template=self.template).to_edge(UP + 2 * UP)\n",
        "        #eq.move_to(*self.ax.c2p([8, 7, 0]))\n",
        "        labels = self.ax.get_axis_labels(x_label=\"x_1\", y_label=\"y = f(\\mathbf{x})\")\n",
        "        labels.set_color(GREY)\n",
        "        labels.set_tex_template(self.template)\n",
        "        self.add(fx, eq, labels)\n"
      ],
      "id": "b416727d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Functions\n",
        "\n",
        "A *linear function* is any function $f$ where the following conditions always hold: $$ f(\\mathbf{x} + \\mathbf{y}) =f(\\mathbf{x}) + f(\\mathbf{y})$$ and $$ f(a\\mathbf{x}) = a f(\\mathbf{x})$$ For a linear function, the output can be defined as a weighted sum of the inputs.\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = \\sum_{i=1}^n x_iw_i + b\n",
        "$$\n",
        "\n",
        "Here, $w_i$ and $b$ are the **parameters** of the function.\n",
        "\n",
        "## Linear Functions\n",
        "\n",
        "We can also write a linear function using a dot-product between our input $\\mathbf{x}$ and parameter vector $\\mathbf{w}$ as:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = \\mathbf{x} \\cdot \\mathbf{w} + b \\quad \\text{or} \\quad f(\\mathbf{x}) = \\mathbf{x}^T  \\mathbf{w} + b\n",
        "$$\n",
        "\n",
        "We typically refer to $\\mathbf{w}$ specifically as the **weight vector** (or weights) and $b$ as the **bias**.\n",
        "\n",
        "$$\n",
        "\\textbf{Linear function:  }f(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{w}+b,\\quad \\textbf{Parameters:}\\quad \\big(\\text{Weights:  } \\mathbf{w},\\ \\text{Bias:  } b \\big)\n",
        "$$\n",
        "\n",
        "## Linear Functions\n",
        "\n",
        "In one dimension, a linear function is always a line, for example:\n"
      ],
      "id": "eafbef3a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "%%manim -sqh -v CRITICAL --progress_bar none LinearFunction\n",
        "\n",
        "class LinearFunction(PositiveVectorScene):\n",
        "    def construct(self):\n",
        "        fx = self.ax.plot(lambda x: 0.5 * x + 1, color=RED)\n",
        "        eq = MathTex(r'f(\\mathbf{x})= \\frac{1}{2} x_1 + 1', color=BLACK, tex_template=self.template).to_edge(UP + 2 * UP)\n",
        "        #eq.move_to(*self.ax.c2p([8, 7, 0]))\n",
        "        labels = self.ax.get_axis_labels(x_label=\"x_1\", y_label=\"y = f(\\mathbf{x})\")\n",
        "        labels.set_color(GREY)\n",
        "        labels.set_tex_template(self.template)\n",
        "        self.add(fx, eq, labels)\n"
      ],
      "id": "474a13e7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Functions\n",
        "\n",
        "In higher dimensions, it is a plane or hyperplane:\n"
      ],
      "id": "7015d260"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "%%manim -sqh -v CRITICAL --progress_bar none ThreeDSurfacePlot\n",
        "\n",
        "class ThreeDSurfacePlot(ThreeDLectureScene):\n",
        "    def construct(self):\n",
        "        resolution_fa = 24\n",
        "        self.set_camera_orientation(phi=75 * DEGREES, theta=-30 * DEGREES)\n",
        "        \n",
        "\n",
        "        def param_gauss(u, v):\n",
        "            x = u\n",
        "            y = v\n",
        "            return np.array([x, y, -0.6 * x + -0.2 * y - 1.])\n",
        "\n",
        "        gauss_plane = Surface(\n",
        "            param_gauss,\n",
        "            resolution=(resolution_fa, resolution_fa),\n",
        "            v_range=[-3, +3],\n",
        "            u_range=[-3, +3]\n",
        "        )\n",
        "\n",
        "        gauss_plane.scale(1, about_point=ORIGIN)\n",
        "        gauss_plane.set_style(fill_opacity=1, stroke_color=BLACK)\n",
        "        gauss_plane.set_fill_by_checkerboard(RED, GREY, opacity=0.5)\n",
        "        axes = ThreeDAxes()\n",
        "        axes.set_color(GREY)\n",
        "        labels = axes.get_axis_labels(x_label=\"x_1\", y_label=\"x_2\", z_label=\"y = f(\\mathbf{x})\")\n",
        "        labels.set_color(GREY)\n",
        "        #labels[0].rotate(75 * DEGREES, RIGHT)\n",
        "        #labels[0].rotate(-30 * DEGREES, IN)\n",
        "        #\n",
        "        #labels[1].rotate(-30 * DEGREES, IN)\n",
        "\n",
        "        eq = MathTex(r'f(\\mathbf{x})= \\frac{-3}{5} x_1 - \\frac{1}{5} x_2 - 1', color=BLACK, tex_template=self.template)\n",
        "        eq.to_corner(UL)\n",
        "        eq.scale(0.8)\n",
        "        self.add_fixed_in_frame_mobjects(eq)\n",
        "        self.add_fixed_orientation_mobjects(labels[0])\n",
        "        labels[1].rotate(90 * DEGREES, IN)\n",
        "        self.add_fixed_orientation_mobjects(labels[1])\n",
        "        labels[2].rotate(90 * DEGREES, LEFT)\n",
        "        self.add_fixed_orientation_mobjects(labels[2])\n",
        "        self.add( gauss_plane, axes, )\n"
      ],
      "id": "bed03456",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Functions\n",
        "\n",
        "In numpy we can easily write a linear function of this form:\n"
      ],
      "id": "9ef67b31"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def f(x):\n",
        "    w = np.array([-0.6, -0.2])\n",
        "    b = -1\n",
        "    return np.dot(x, w) + b"
      ],
      "id": "c9d17d29",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Handling bias compactly\n",
        "\n",
        "$$\n",
        "\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\longrightarrow \\mathbf{x}_{aug}= \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\\\ 1 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{bmatrix} \\longrightarrow \\mathbf{w}_{aug}=  \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\\\ b \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "We can easily see then that using this notation:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = \\mathbf{x}^T \\mathbf{w} +b = \\mathbf{x}_{aug}^T \\mathbf{w}_{aug}\n",
        "$$\n",
        "\n",
        "We won't bother with the $aug$ notation and just assume that any linear function defined as $f(\\mathbf{x})=\\mathbf{x}^T\\mathbf{w}$ can be defined to include a bias implicitly.\n",
        "\n",
        "In numpy this is similarly straightforward:\n"
      ],
      "id": "cee7b997"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def f(x):\n",
        "    w = np.array([-0.6, -0.2, -1])\n",
        "    x = np.pad(x, ((0,1),), constant_values=1)\n",
        "    return np.dot(x, w)"
      ],
      "id": "2bb374d3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Datasets and observations\n",
        "\n",
        "**Dataset** $\\mathbf{D}$ made up of $N$ pairs of inputs ( $\\mathbf{x}$ ) and outputs ( $y$ ):\n",
        "\n",
        "$$\n",
        "\\mathbf{D} = \\{ (\\mathbf{x}_1, y_1),\\ (\\mathbf{x}_2, y_2),\\ ...\\ (\\mathbf{x}_N, y_N)\\}\n",
        "$$\n",
        "\n",
        "We call each of these pairs an **observation**.\n",
        "\n",
        "## Fuel efficiency\n",
        "\n",
        "Let's imagine we're designing a car and we would like to know what the fuel efficiency of the car we're designing will be in *miles per gallon* (MPG). Ideally we would have access to a function that would give us the MPG rating if we provide some **features**.\n",
        "\n",
        "$$\n",
        "\\text{mpg} = f(\\text{weight},\\ \\text{horsepower}...)\n",
        "$$\n",
        "\n",
        "We don't know the exact relationship between a car's features and fuel efficiency. However, we can look at other cars on the market and see what the corresponding inputs and outputs would be:\n",
        "\n",
        "$$\n",
        "\\text{Honda Accord: } \\begin{bmatrix} \\text{Weight:} & \\text{2500 lbs} \\\\ \\text{Horsepower:} & \\text{ 123 HP} \\\\ \\text{Displacement:} & \\text{ 2.4 L} \\\\ \\text{0-60mph:} & \\text{ 7.8 Sec} \\end{bmatrix} \\longrightarrow \\text{   MPG: 33mpg}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{Dodge Aspen: } \\begin{bmatrix} \\text{Weight:} & \\text{3800 lbs} \\\\ \\text{Horsepower:} & \\text{ 155 HP} \\\\ \\text{Displacement:} & \\text{ 3.2 L} \\\\ \\text{0-60mph:} & \\text{ 6.8 Sec} \\end{bmatrix} \\longrightarrow \\text{   MPG: 21mpg}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\vdots \\quad \\vdots\n",
        "$$\n",
        "\n",
        "## Fuel efficiency\n",
        "\n",
        "Our dataset will be this collection of data that we have for all other cars. In general, each *observation* in this dataset will correspond to a car.\n",
        "\n",
        "$$\n",
        "\\text{Dataset: } \\mathbf{D}=\\{(\\mathbf{x}_i,\\ y_i) \\text{  for  } i\\in 1...N\\}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{Input: } \\mathbf{x}_i= \\begin{bmatrix} \\text{Weight} \\\\ \\text{Horsepower} \\\\ \\text{Displacement} \\\\ \\text{0-60mph}  \\end{bmatrix}, \\quad \\text{Output: } y_i = MPG\n",
        "$$\n",
        "\n",
        "## Fuel efficiency\n",
        "\n",
        "Let's take a look at a single feature: *the weight of a car*.\n"
      ],
      "id": "f702196a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "%%manim -sqh -v CRITICAL --progress_bar none HP_MPG\n",
        "import pandas as pd\n",
        "mpg_data = pd.read_csv('data/auto-mpg.csv')\n",
        "data = mpg_data.sort_values(by=['weight'])[::50]\n",
        "\n",
        "class HP_MPG(Scene):\n",
        "    def construct(self):\n",
        "        self.camera.background_color = \"#ffffff\"\n",
        "        self.ax = Axes(\n",
        "            x_range=[1000, 5000, 500],\n",
        "            y_range=[10, 40, 10],\n",
        "            x_length=12,\n",
        "            y_length=8,\n",
        "            axis_config={\"color\": GREY},\n",
        "        )\n",
        "        self.add(self.ax)\n",
        "        \n",
        "        \n",
        "        coords = [[c1, c2] for (c1, c2) in zip(data['weight'], data['mpg'])]\n",
        "        dots = VGroup(*[Dot(color=BLACK).move_to(self.ax.c2p(coord[0],coord[1])) for coord in coords])\n",
        "        labels = self.ax.get_axis_labels(x_label=\"Weight\\ (lbs)\", y_label=\"MPG\")\n",
        "        labels.set_color(GREY)\n",
        "        self.add(dots,  labels)\n",
        "        for coord, name in list(zip(coords, data['car name'])):\n",
        "            if name in ['honda accord cvcc', 'dodge aspen']:\n",
        "                d = Dot(color=RED).move_to(self.ax.c2p(coord[0], coord[1]))\n",
        "                label = Text('  '.join([n.capitalize() for n in name.split()[:2]]), color=RED, font_size=24).next_to(d, UR)\n",
        "                self.add(d, label)"
      ],
      "id": "ba14ef64",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction functions\n",
        "\n",
        "#### *How do we **predict** the output for an input that we haven't seen before?*\n"
      ],
      "id": "7bdc90dd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "%%manim -sqh -v CRITICAL --progress_bar none HP_MPG\n",
        "import pandas as pd\n",
        "mpg_data = pd.read_csv('data/auto-mpg.csv')\n",
        "data = mpg_data.sort_values(by=['weight'])[::50]\n",
        "\n",
        "class HP_MPG(Scene):\n",
        "    def construct(self):\n",
        "        self.camera.background_color = \"#ffffff\"\n",
        "        self.ax = Axes(\n",
        "            x_range=[1000, 5000, 500],\n",
        "            y_range=[10, 40, 10],\n",
        "            x_length=12,\n",
        "            y_length=8,\n",
        "            axis_config={\"color\": GREY},\n",
        "        )\n",
        "        self.add(self.ax)\n",
        "        \n",
        "        \n",
        "        coords = [[c1, c2] for (c1, c2) in zip(data['weight'], data['mpg'])]\n",
        "        dots = VGroup(*[Dot(color=BLACK).move_to(self.ax.c2p(coord[0],coord[1])) for coord in coords])\n",
        "\n",
        "        query = Line(self.ax.c2p(3100, 10), self.ax.c2p(3100, 40), color=GREEN)\n",
        "        qdot = Dot(self.ax.c2p(3100, 40))\n",
        "        label = Text('Weight = 3100 lbs\\nMPG = ?', color=GREEN, font_size=24).next_to(qdot, DR)\n",
        "        self.add(label)\n",
        "        \n",
        "        self.add(query)\n",
        "        labels = self.ax.get_axis_labels(x_label=\"Weight\\ (lbs)\", y_label=\"MPG\")\n",
        "        labels.set_color(GREY)\n",
        "        self.add(dots,  labels)\n",
        "        for coord, name in list(zip(coords, data['car name'])):\n",
        "            if name in ['honda accord cvcc', 'dodge aspen']:\n",
        "                d = Dot(color=RED).move_to(self.ax.c2p(coord[0], coord[1]))\n",
        "                label = Text('  '.join([n.capitalize() for n in name.split()[:2]]), color=RED, font_size=24).next_to(d, UR)\n",
        "                self.add(d, label)\n"
      ],
      "id": "2ec22728",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction functions\n",
        "\n",
        "**Model** our unknown function with a known function that we can evaluate at any input. Chose a function $f$ such that for any observation our dataset, the output of this function *approximates* the true **target** output that we observed.\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}_i) \\approx y_i, \\quad \\forall (\\mathbf{x}_i, y_i) \\in \\mathbf{D}\n",
        "$$\n",
        "\n",
        "## Linear interpolation\n"
      ],
      "id": "bb50a6c9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "%%manim -sqh -v CRITICAL --progress_bar none HP_MPG\n",
        "import pandas as pd\n",
        "mpg_data = pd.read_csv('data/auto-mpg.csv')\n",
        "data = mpg_data.sort_values(by=['weight'])[::50]\n",
        "\n",
        "class HP_MPG(Scene):\n",
        "    def construct(self):\n",
        "        self.camera.background_color = \"#ffffff\"\n",
        "        self.ax = Axes(\n",
        "            x_range=[1000, 5000, 500],\n",
        "            y_range=[10, 40, 10],\n",
        "            x_length=12,\n",
        "            y_length=8,\n",
        "            axis_config={\"color\": GREY},\n",
        "        )\n",
        "        self.add(self.ax)\n",
        "        \n",
        "        \n",
        "        coords = [[c1, c2] for (c1, c2) in zip(data['weight'], data['mpg'])]\n",
        "        all_dots = [Dot(color=BLACK).move_to(self.ax.c2p(coord[0],coord[1])) for coord in coords]\n",
        "        dots = VGroup(*all_dots)\n",
        "        labels = self.ax.get_axis_labels(x_label=\"Weight\\ (lbs)\", y_label=\"MPG\")\n",
        "        labels.set_color(GREY)\n",
        "        for d1, d2 in zip(all_dots[:-1], all_dots[1:]):\n",
        "            self.add(Line(d1.get_center(), d2.get_center(), color=GREY))\n",
        "        self.add(dots,  labels)\n",
        "        for coord, name in list(zip(coords, data['car name'])):\n",
        "            if name in ['honda accord cvcc', 'dodge aspen']:\n",
        "                d = Dot(color=RED).move_to(self.ax.c2p(coord[0], coord[1]))\n",
        "                label = Text('  '.join([n.capitalize() for n in name.split()[:2]]), color=RED, font_size=24).next_to(d, UR)\n",
        "                self.add(d, label)"
      ],
      "id": "56df1d03",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In some cases this can be a reasonable approach! In fact it's how the `plt.plot` function works.\n",
        "\n",
        "## Linear interpolation\n",
        "\n",
        "Real data however is *messy*. Measurements in our dataset might not be 100% accurate or might even conflict!\n",
        "\n",
        "$$(\\text{Weight: }3100, \\text{MPG: } 34), \\quad (\\text{Weight: }3100, \\text{MPG: } 23) \\longrightarrow f(3100) = ?$$\n",
        "\n",
        "## Linear interpolation\n"
      ],
      "id": "7f9e8f39"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "%%manim -sqh -v CRITICAL --progress_bar none HP_MPG\n",
        "import pandas as pd\n",
        "mpg_data = pd.read_csv('data/auto-mpg.csv')\n",
        "data = mpg_data.sort_values(by=['weight'])\n",
        "\n",
        "class HP_MPG(Scene):\n",
        "    def construct(self):\n",
        "        self.camera.background_color = \"#ffffff\"\n",
        "        self.ax = Axes(\n",
        "            x_range=[1000, 5500, 500],\n",
        "            y_range=[0, 50, 10],\n",
        "            x_length=12,\n",
        "            y_length=8,\n",
        "            axis_config={\"color\": GREY},\n",
        "        )\n",
        "        self.add(self.ax)\n",
        "        \n",
        "        \n",
        "        coords = [[c1, c2] for (c1, c2) in zip(data['weight'], data['mpg'])]\n",
        "        all_dots = [Dot(color=BLACK).move_to(self.ax.c2p(coord[0],coord[1])) for coord in coords]\n",
        "        dots = VGroup(*all_dots)\n",
        "        labels = self.ax.get_axis_labels(x_label=\"Weight\\ (lbs)\", y_label=\"MPG\")\n",
        "        labels.set_color(GREY)\n",
        "        for d1, d2 in zip(all_dots[:-1], all_dots[1:]):\n",
        "            self.add(Line(d1.get_center(), d2.get_center(), color=GREY))\n",
        "        self.add(dots,  labels)\n",
        "        names = ['honda accord cvcc', 'dodge aspen']\n",
        "        for coord, name in list(zip(coords, data['car name'])):\n",
        "            \n",
        "            if name in names:\n",
        "                names.remove(name)\n",
        "                d = Dot(color=RED).move_to(self.ax.c2p(coord[0], coord[1]))\n",
        "                label = Text('  '.join([n.capitalize() for n in name.split()[:2]]), color=RED, font_size=24).next_to(d, UR)\n",
        "                self.add(d, label)"
      ],
      "id": "8c0f76cd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear regression\n",
        "\n",
        "**Linear regression** *models* an unknown function with a linear function.\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{w} = \\sum_{i=1}^n x_i w_i\n",
        "$$\n",
        "\n",
        "Meaning that the output will be a weighted sum of the *features* of the input.\n",
        "\n",
        "$$\n",
        "\\text{Predicted MPG} = f(\\mathbf{x})= \n",
        "$$\n",
        "\n",
        "$$\n",
        "(\\text{weight})w_1 + (\\text{horsepower})w_2 + (\\text{displacement})w_3 + (\\text{0-60mph})w_4 + b\n",
        "$$\n",
        "\n",
        "Or in matrix notation:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x})= \\begin{bmatrix} \\text{Weight} \\\\ \\text{Horsepower} \\\\ \\text{Displacement} \\\\ \\text{0-60mph} \\\\ 1 \\end{bmatrix} \\cdot \\begin{bmatrix} w_1 \\\\ w_2\\\\ w_3 \\\\ w_4\\\\ b\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "## Linear regression\n"
      ],
      "id": "9bf18368"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Regression:\n",
        "    def __init__(self, weights):\n",
        "        self.weights = weights\n",
        "    \n",
        "    def predict(self, x):\n",
        "        return np.dot(x, self.weights)\n",
        "\n",
        "model = Regression(np.array([1, 1, 1, 1, 1]))\n",
        "model.predict(np.array([5, 2, 3, 3, 1]))"
      ],
      "id": "a3bb023e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear regression\n",
        "\n",
        "We could chose many different linear functions to make predictions:\n"
      ],
      "id": "3fa573ab"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "%%manim -sqh -v CRITICAL --progress_bar none HP_MPG\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "mpg_data = pd.read_csv('data/auto-mpg.csv')\n",
        "data = mpg_data.sort_values(by=['weight'])[::50]\n",
        "\n",
        "x = np.array(data['weight'])[:, np.newaxis]\n",
        "y = np.array(data['mpg'])\n",
        "model = LinearRegression().fit(x, y)\n",
        "\n",
        "other_lines = [(model.coef_.item() * 1.2, model.intercept_.item() * 0.95), (model.coef_.item() * 0.7, model.intercept_.item() * 0.9), (model.coef_.item() * 1.8, model.intercept_.item() * 1.4)]\n",
        "\n",
        "class HP_MPG(Scene):\n",
        "    def construct(self):\n",
        "        self.camera.background_color = \"#ffffff\"\n",
        "        self.ax = Axes(\n",
        "            x_range=[1000, 5000, 500],\n",
        "            y_range=[10, 40, 10],\n",
        "            x_length=12,\n",
        "            y_length=8,\n",
        "            axis_config={\"color\": GREY},\n",
        "        )\n",
        "        self.add(self.ax)\n",
        "        \n",
        "        \n",
        "        coords = [[c1, c2] for (c1, c2) in zip(data['weight'], data['mpg'])]\n",
        "        dots = VGroup(*[Dot(color=BLACK).move_to(self.ax.c2p(coord[0],coord[1])) for coord in coords])\n",
        "        labels = self.ax.get_axis_labels(x_label=\"Weight\\ (lbs)\", y_label=\"MPG\")\n",
        "        labels.set_color(GREY)\n",
        "        self.add(dots,  labels)\n",
        "        for coord, name in list(zip(coords, data['car name'])):\n",
        "            if name in ['honda accord cvcc', 'dodge aspen']:\n",
        "                d = Dot(color=RED).move_to(self.ax.c2p(coord[0], coord[1]))\n",
        "                label = Text('  '.join([n.capitalize() for n in name.split()[:2]]), color=RED, font_size=24).next_to(d, UR)\n",
        "                self.add(d, label)\n",
        "\n",
        "        # Add Regression line\n",
        "        plot = self.ax.plot(lambda x: model.predict(np.atleast_2d(x)).item(), x_range=[1300, 4300], color=BLACK)\n",
        "        self.add(plot)\n",
        "\n",
        "        # Add other candidate lines\n",
        "        for param, color in zip(other_lines, [BLUE, ORANGE, GREEN, PURPLE]):\n",
        "            w, b = param\n",
        "            plot = self.ax.plot(lambda x: x * w + b, x_range=[1300, 4300], color=color)\n",
        "            self.add(plot)\n",
        "        \n",
        "\n",
        "        eq = Tex(r'$f(x)=wx+b$', color=BLACK).to_corner(UR)\n",
        "        self.add(eq)"
      ],
      "id": "593da229",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Residuals and error\n",
        "\n",
        "The **residual** or **error** of a prediction is the difference between the prediction and the true output:\n",
        "\n",
        "$$\n",
        "e_i = y_i - f(\\mathbf{x}_i)\n",
        "$$\n"
      ],
      "id": "200abfaf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "%%manim -sqh -v CRITICAL --progress_bar none HP_MPG_LR\n",
        "from sklearn.linear_model import LinearRegression\n",
        "x = np.array(mpg_data['weight'])[:, np.newaxis]\n",
        "y = np.array(mpg_data['mpg'])\n",
        "model = LinearRegression().fit(x, y)\n",
        "other_lines = [(model.coef_.item() * 1.2, model.intercept_.item() * 0.95), (model.coef_.item() * 0.7, model.intercept_.item() * 0.9), (model.coef_.item() * 1.8, model.intercept_.item() * 1.4)]\n",
        "\n",
        "class HP_MPG_LR(Scene):\n",
        "    def construct(self):\n",
        "        self.camera.background_color = \"#ffffff\"\n",
        "        self.ax = Axes(\n",
        "            x_range=[1000, 5000, 500],\n",
        "            y_range=[10, 40, 10],\n",
        "            x_length=12,\n",
        "            y_length=8,\n",
        "            axis_config={\"color\": GREY},\n",
        "        )\n",
        "        labels = self.ax.get_axis_labels(x_label=\"Weight\\ (lbs)\", y_label=\"MPG\")\n",
        "        labels.set_color(GREY)\n",
        "        self.add(self.ax, labels)\n",
        "        \n",
        "\n",
        "        # Add data\n",
        "        coords = [[c1, c2] for (c1, c2) in zip(data['weight'], data['mpg'])]\n",
        "        all_dots = [Dot(color=BLACK).move_to(self.ax.c2p(coord[0],coord[1])) for coord in coords]\n",
        "        dots = VGroup(*all_dots)\n",
        "        self.add(dots)\n",
        "        \n",
        "        # Add highlighted points\n",
        "        for coord, name in list(zip(coords, data['car name'])):\n",
        "            if name in ['honda accord cvcc', 'dodge aspen']:\n",
        "                d = Dot(color=RED).move_to(self.ax.c2p(coord[0], coord[1]))\n",
        "                label = Text('  '.join([n.capitalize() for n in name.split()[:2]]), color=RED, font_size=24).next_to(d, UR)\n",
        "                self.add(d, label)\n",
        "\n",
        "        # Add Regression line\n",
        "        plot = self.ax.plot(lambda x: model.predict(np.atleast_2d(x)).item(), x_range=[1300, 4300], color=BLACK)\n",
        "        self.add(plot)\n",
        "\n",
        "        eq = Tex(r'$f(x)=wx+b$', color=BLACK).to_corner(UR)\n",
        "        self.add(eq)\n",
        "\n",
        "        # Add residuals\n",
        "        brace_num = 2\n",
        "        for i, coord in enumerate(coords):\n",
        "            x, y = coord\n",
        "            yline = model.predict(np.atleast_2d(x)).item()\n",
        "            line = Line(self.ax.c2p(x, y), self.ax.c2p(x, yline), color=GREY)\n",
        "            self.add(line)\n",
        "            if i == brace_num:\n",
        "                resid_brace = Brace(line, direction=LEFT, color=GREY)\n",
        "                resid_tex = resid_brace.get_tex(r\"e_i=y_i-f(x_i)\")\n",
        "                resid_tex.set_color(BLACK)\n",
        "                resid_tex.scale(0.8)\n",
        "                self.add(resid_brace, resid_tex)\n",
        "\n",
        "        model.predict(np.atleast_2d(x)).item()"
      ],
      "id": "9322c6f9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mean squared error\n",
        "\n",
        "We need a measure of error for the *entire dataset*. The **mean squared error** is the average of the residual squared for each observation in our dataset:\n",
        "\n",
        "$$\n",
        "MSE = \\frac{1}{N}\\sum_{i=1}^N (f(\\mathbf{x}_i) - y_i)^2 = \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2\n",
        "$$It follows that the best choice of linear function $f^*$ is the one that *minimizes* the mean squared error for our dataset. $$\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmin}} \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2 $$\n",
        "\n",
        "## Loss functions\n",
        "\n",
        "Mean squared error depends on the data inputs $(\\mathbf{x}_1,…,\\mathbf{x}_N)$, the data targets $(y_1,…,y_N)$ *and* the parameters $(\\mathbf{w})$. So we can express the MSE as a *function* of all three:\n",
        "\n",
        "$$\n",
        "MSE(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2 \n",
        "$$\n",
        "\n",
        "Here we have used $\\mathbf{X}$ and $\\mathbf{y}$ to refer to the entire collection of inputs and outputs from our dataset $( \\mathbf{D})$ respectively, so:\n",
        "\n",
        "$$\n",
        "\\mathbf{X} = \\begin{bmatrix}\\mathbf{x}_1 \\\\ \\mathbf{x}_2 \\\\ \\vdots \\\\ \\mathbf{x}_N \\end{bmatrix} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1n} \\\\ x_{21} & x_{22} & \\dots & x_{2n}\\\\ \\vdots & \\vdots  & \\ddots & \\vdots \\\\ x_{N1} & x_{N2} & \\dots & x_{Nn} \\end{bmatrix}, \\quad \\mathbf{y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "## Loss functions\n",
        "\n",
        "This is an example of **loss function**. We can drop the explicit dependence on $\\mathbf{X}$ and $\\mathbf{y}$, looking at the loss as purely a function of our choice of parameters:\n",
        "\n",
        "$$\n",
        "\\textbf{Loss}(\\mathbf{w})= MSE(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2 \n",
        "$$\n",
        "\n",
        "Again, if our goal is to minimize error, we want to choose the parameters $\\mathbf{w}^*$ that *minimize* this loss:\n",
        "\n",
        "$$\n",
        "\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmin}}\\ \\textbf{Loss}(\\mathbf{w})= \\underset{\\mathbf{w}}{\\text{argmin}} \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2\n",
        "$$\n",
        "\n",
        "## Visualizing loss\n",
        "\n",
        "If we consider the case where our inputs are 1-dimensional, as in the weight example above, then our parameter vector $\\mathbf{w}$ only has 2 entries: $w_1$ and $b$.\n"
      ],
      "id": "c0948186"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "%%manim -sqh -v CRITICAL --progress_bar none HP_MPG\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "mpg_data = pd.read_csv('data/auto-mpg.csv')\n",
        "data = mpg_data.sort_values(by=['weight'])[::50]\n",
        "\n",
        "x = np.array(data['weight'])[:, np.newaxis]\n",
        "y = np.array(data['mpg'])\n",
        "model = LinearRegression().fit(x, y)\n",
        "\n",
        "other_lines = [(model.coef_.item() * 1.2, model.intercept_.item() * 0.95), (model.coef_.item() * 0.7, model.intercept_.item() * 0.9), (model.coef_.item() * 1.8, model.intercept_.item() * 1.4)]\n",
        "\n",
        "class HP_MPG(Scene):\n",
        "    def construct(self):\n",
        "        self.camera.background_color = \"#ffffff\"\n",
        "        self.ax = Axes(\n",
        "            x_range=[1000, 5000, 500],\n",
        "            y_range=[10, 40, 10],\n",
        "            x_length=12,\n",
        "            y_length=8,\n",
        "            axis_config={\"color\": GREY},\n",
        "        )\n",
        "        self.add(self.ax)\n",
        "        \n",
        "        \n",
        "        coords = [[c1, c2] for (c1, c2) in zip(data['weight'], data['mpg'])]\n",
        "        dots = VGroup(*[Dot(color=BLACK).move_to(self.ax.c2p(coord[0],coord[1])) for coord in coords])\n",
        "        labels = self.ax.get_axis_labels(x_label=\"Weight\\ (lbs)\", y_label=\"MPG\")\n",
        "        labels.set_color(GREY)\n",
        "        self.add(dots,  labels)\n",
        "        for coord, name in list(zip(coords, data['car name'])):\n",
        "            if name in ['honda accord cvcc', 'dodge aspen']:\n",
        "                d = Dot(color=RED).move_to(self.ax.c2p(coord[0], coord[1]))\n",
        "                label = Text('  '.join([n.capitalize() for n in name.split()[:2]]), color=RED, font_size=24).next_to(d, UR)\n",
        "                self.add(d, label)\n",
        "\n",
        "        # Add Regression line\n",
        "        plot = self.ax.plot(lambda x: model.predict(np.atleast_2d(x)).item(), x_range=[1300, 4300], color=BLACK)\n",
        "        self.add(plot)\n",
        "\n",
        "        # Add other candidate lines\n",
        "        for param, color in zip(other_lines, [BLUE, ORANGE, GREEN, PURPLE]):\n",
        "            w, b = param\n",
        "            plot = self.ax.plot(lambda x: x * w + b, x_range=[1300, 4300], color=color)\n",
        "            self.add(plot)\n",
        "        \n",
        "\n",
        "        eq = Tex(r'$f(x)=wx+b$', color=BLACK).to_corner(UR)\n",
        "        self.add(eq)"
      ],
      "id": "477ff85d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "import matplotlib.pyplot as plt\n",
        "x = (x - x.mean()) / x.std()\n",
        "y = (y - y.mean()) / y.std()\n",
        "\n",
        "model = LinearRegression().fit(x, y)\n",
        "\n",
        "other_lines = [(model.coef_.item(), model.intercept_.item()), (model.coef_.item() * 1.2, model.intercept_.item() - 0.95), (model.coef_.item() * 0.7, model.intercept_.item() + 0.9), (model.coef_.item() * 1.8, model.intercept_.item() + 1.4)]\n",
        "\n",
        "W = np.arange(-2, 2, 0.1)\n",
        "B = np.arange(-2, 2, 0.1)\n",
        "W, B = np.meshgrid(W, B)\n",
        "resid = np.expand_dims(W, 0) * x.reshape((-1, 1, 1)) + np.expand_dims(B, 0) - y.reshape((-1, 1, 1)) # N x W x B\n",
        "loss = np.mean(resid ** 2, axis=0)\n",
        "\n",
        "\n",
        "CS = plt.contour(W, B, loss, cmap='coolwarm', levels=0.5 * np.arange(25))\n",
        "ax = plt.gca()\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "\n",
        "\n",
        "for param, color in zip(other_lines, [BLACK, BLUE, ORANGE, GREEN, PURPLE]):\n",
        "            w, b = param\n",
        "            ax.scatter(w, b, c=color, s=75)\n",
        "plt.xlabel('$w_1$')\n",
        "plt.ylabel('$b$')\n",
        "plt.title('$\\mathbf{Loss}(\\mathbf{w})$')\n",
        "ax.clabel(CS, inline=True, fontsize=10)\n",
        "pass"
      ],
      "id": "f1b753a6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that point where the loss is lowest, corresponds to the line that best fits our data!\n",
        "\n",
        "## Gradient descent\n",
        "\n",
        "We have a function $f(\\mathbf{\\cdot})$ and we would like find the input $\\mathbf{w}^*$ that minimizes the output of the function:\n",
        "\n",
        "$$\n",
        "\\text{Find: } \\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmin}}\\ f(\\mathbf{w})\n",
        "$$\n",
        "\n",
        "We don't know how to find $\\mathbf{w}^*$ directly, but if we have an initial guess $\\mathbf{w}^{(0)}$, we can try to update our guess to improve it.\n",
        "\n",
        "$$\n",
        "\\mathbf{w}^{(1)} \\leftarrow \\mathbf{w}^{(0)} + \\mathbf{g}\n",
        "$$\n"
      ],
      "id": "99c16b8c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "import matplotlib.pyplot as plt\n",
        "from autograd import grad\n",
        "x = (x - x.mean()) / x.std()\n",
        "y = (y - y.mean()) / y.std()\n",
        "\n",
        "model = LinearRegression().fit(x, y)\n",
        "\n",
        "W = np.arange(-2, 2, 0.1)\n",
        "B = np.arange(-2, 2, 0.1)\n",
        "W, B = np.meshgrid(W, B)\n",
        "\n",
        "\n",
        "resid = np.expand_dims(W, 0) * x.reshape((-1, 1, 1)) + np.expand_dims(B, 0) - y.reshape((-1, 1, 1)) # N x W x B\n",
        "loss = np.mean(resid ** 2, axis=0)\n",
        "\n",
        "def get_loss(w):\n",
        "    w, b = w[0], w[1]\n",
        "    resid = x * w + b - y\n",
        "    return 0.2 * np.mean(resid ** 2)\n",
        "\n",
        "gloss = grad(get_loss)\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "\n",
        "print(gloss(np.array([1.25, -1.5])))\n",
        "\n",
        "other_lines = [(1.25, -1.5)]\n",
        "\n",
        "for param, color in zip(other_lines, [BLACK, BLUE, ORANGE, GREEN, PURPLE]):\n",
        "            w, b = param\n",
        "            ax.scatter(w, b, c=color, s=75, zorder=4)\n",
        "            dw, db = -gloss(np.array([w, b]))\n",
        "            prop = dict(arrowstyle=\"-|>,head_width=0.4,head_length=0.8\",\n",
        "            shrinkA=0,shrinkB=0)\n",
        "            ax.annotate(\"\", xy=(dw + w,db + b), xytext=(w,b), arrowprops=prop)\n",
        "            #ax.arrow(w, b, dw, db, head_width=0.1, zorder=4)\n",
        "\n",
        "   \n",
        "plt.xlabel('$w_1$')\n",
        "plt.ylabel('$b$')\n",
        "plt.title('$\\mathbf{Loss}(\\mathbf{w})$')\n",
        "CS = ax.contour(W, B, loss, cmap='coolwarm', levels=0.5 * np.arange(25))\n",
        "ax.clabel(CS, inline=True, fontsize=10)\n",
        "pass"
      ],
      "id": "4904e6d0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradient descent\n",
        "\n",
        "The *gradient* of a function at point $\\mathbf{x}$ corresponds to the *slope* of $f$ at $\\mathbf{w}$, or equivalently the direction of maximum change. This gives us a natural choice for the update to our guess.\n",
        "\n",
        "$$\n",
        "\\mathbf{w}^{(1)} \\leftarrow \\mathbf{w}^{(0)} - \\nabla f(\\mathbf{w}^{(0)})\n",
        "$$\n"
      ],
      "id": "dd5f0cd1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "import matplotlib.pyplot as plt\n",
        "from autograd import grad\n",
        "x = (x - x.mean()) / x.std()\n",
        "y = (y - y.mean()) / y.std()\n",
        "\n",
        "model = LinearRegression().fit(x, y)\n",
        "\n",
        "W = np.arange(-2, 2, 0.1)\n",
        "B = np.arange(-2, 2, 0.1)\n",
        "W, B = np.meshgrid(W, B)\n",
        "\n",
        "\n",
        "resid = np.expand_dims(W, 0) * x.reshape((-1, 1, 1)) + np.expand_dims(B, 0) - y.reshape((-1, 1, 1)) # N x W x B\n",
        "loss = np.mean(resid ** 2, axis=0)\n",
        "\n",
        "def get_loss(w):\n",
        "    w, b = w[0], w[1]\n",
        "    resid = x * w + b - y\n",
        "    return 0.2 * np.mean(resid ** 2)\n",
        "\n",
        "gloss = grad(get_loss)\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "\n",
        "\n",
        "other_lines = [(1.25, -1.5)]\n",
        "\n",
        "for param, color in zip(other_lines, [BLACK, BLUE, ORANGE, GREEN, PURPLE]):\n",
        "            w, b = param\n",
        "            ax.scatter(w, b, c=color, s=75, zorder=4)\n",
        "            dw, db = -gloss(np.array([w, b]))\n",
        "            prop = dict(arrowstyle=\"-|>,head_width=0.4,head_length=0.8\",\n",
        "            shrinkA=0,shrinkB=0)\n",
        "            ax.annotate(\"\", xy=(dw + w,db + b), xytext=(w,b), arrowprops=prop)\n",
        "            #ax.arrow(w, b, dw, db, head_width=0.1, zorder=4)\n",
        "\n",
        "   \n",
        "plt.xlabel('$w_1$')\n",
        "plt.ylabel('$b$')\n",
        "plt.title('$\\mathbf{Loss}(\\mathbf{w})$')\n",
        "CS = ax.contour(W, B, loss, cmap='coolwarm', levels=0.5 * np.arange(25))\n",
        "ax.clabel(CS, inline=True, fontsize=10)\n",
        "pass"
      ],
      "id": "9321a744",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradient descent\n",
        "\n",
        "We can repeat this process many times, continuously updating our estimate.\n",
        "\n",
        "$$\n",
        "\\text{For }i \\text{ in 1,...,T}\\text{ :} \n",
        "\\quad \\mathbf{w}^{(i+1)} \\leftarrow \\mathbf{w}^{(i)} - \\nabla f(\\mathbf{w}^{(i)})\n",
        "$$\n"
      ],
      "id": "755d0367"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "import matplotlib.pyplot as plt\n",
        "from autograd import grad\n",
        "x = (x - x.mean()) / x.std()\n",
        "y = (y - y.mean()) / y.std()\n",
        "\n",
        "model = LinearRegression().fit(x, y)\n",
        "\n",
        "W = np.arange(-2, 2, 0.1)\n",
        "B = np.arange(-2, 2, 0.1)\n",
        "W, B = np.meshgrid(W, B)\n",
        "\n",
        "\n",
        "resid = np.expand_dims(W, 0) * x.reshape((-1, 1, 1)) + np.expand_dims(B, 0) - y.reshape((-1, 1, 1)) # N x W x B\n",
        "loss = np.mean(resid ** 2, axis=0)\n",
        "\n",
        "def get_loss(w, x, y):\n",
        "    w, b = w[0], w[1]\n",
        "    resid = x.flatten() * w + b - y\n",
        "    return 0.15 * np.mean(resid ** 2)\n",
        "\n",
        "gloss = grad(get_loss)\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "\n",
        "\n",
        "w, b = (1.25, -1.5)\n",
        "\n",
        "for i in range(6):\n",
        "    ax.scatter(w, b, c=color, s=75, zorder=4)\n",
        "    dw, db = -gloss(np.array([w, b]), x, y)\n",
        "    db = db + max(0, 0.05 * (5- i))\n",
        "    prop = dict(arrowstyle=\"-|>,head_width=0.4,head_length=0.8\",\n",
        "    shrinkA=0,shrinkB=0)\n",
        "    if i < 5:\n",
        "        ax.annotate(\"\", xy=(dw + w,db + b), xytext=(w,b), arrowprops=prop)\n",
        "    w, b = w + dw, b + db\n",
        "    #ax.arrow(w, b, dw, db, head_width=0.1, zorder=4)\n",
        "\n",
        "   \n",
        "plt.xlabel('$w_1$')\n",
        "plt.ylabel('$b$')\n",
        "plt.title('$\\mathbf{Loss}(\\mathbf{w})$')\n",
        "CS = ax.contour(W, B, loss, cmap='coolwarm', levels=0.5 * np.arange(25))\n",
        "ax.clabel(CS, inline=True, fontsize=10)\n",
        "pass"
      ],
      "id": "40dcac8a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradient descent convergence\n",
        "\n",
        "At it's minimum value $\\mathbf{w}^*$, a function $f$ *must* have a gradient of $\\mathbf{0}$.\n",
        "\n",
        "$$\n",
        "\\nabla f(\\mathbf{w}^*) = \\mathbf{0}\n",
        "$$\n",
        "\n",
        "It follows that:\n",
        "\n",
        "$$\n",
        "\\mathbf{w}^{*} = \\mathbf{w}^{*} - \\nabla f(\\mathbf{w}^{*})\n",
        "$$\n",
        "\n",
        "We could write our algorithm to account for this:\n",
        "\n",
        "$$\n",
        "\\text{While } \\nabla f(\\mathbf{w}^{(i)}) \\neq \\mathbf{0} \\text{ :}\n",
        "\\quad \\mathbf{w}^{(i+1)} \\leftarrow \\mathbf{w}^{(i)} - \\nabla f(\\mathbf{w}^{(i)})\n",
        "$$\n",
        "\n",
        "## Gradient descent convergence\n",
        "\n",
        "Stops the iteration when the gradient magnitude is sufficiently small:\n",
        "\n",
        "$$\n",
        "\\text{While } ||\\nabla f(\\mathbf{w}^{(i)})||_2 > \\epsilon \\text{ :} \n",
        "\\quad \\mathbf{w}^{(i+1)} \\leftarrow \\mathbf{w}^{(i)} - \\nabla f(\\mathbf{w}^{(i)})\n",
        "$$\n"
      ],
      "id": "1b70ead0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "import matplotlib.pyplot as plt\n",
        "from autograd import grad\n",
        "x = (x - x.mean()) / x.std()\n",
        "y = (y - y.mean()) / y.std()\n",
        "\n",
        "model = LinearRegression().fit(x, y)\n",
        "\n",
        "W = np.arange(-2, 2, 0.1)\n",
        "B = np.arange(-2, 2, 0.1)\n",
        "W, B = np.meshgrid(W, B)\n",
        "\n",
        "\n",
        "resid = np.expand_dims(W, 0) * x.reshape((-1, 1, 1)) + np.expand_dims(B, 0) - y.reshape((-1, 1, 1)) # N x W x B\n",
        "loss = np.mean(resid ** 2, axis=0)\n",
        "\n",
        "def get_loss(w, x, y):\n",
        "    w, b = w[0], w[1]\n",
        "    resid = x.flatten() * w + b - y\n",
        "    return 0.15 * np.mean(resid ** 2)\n",
        "\n",
        "gloss = grad(get_loss)\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "\n",
        "\n",
        "w, b = (1.25, -1.5)\n",
        "\n",
        "for i in range(12):\n",
        "    ax.scatter(w, b, c=color, s=75, zorder=4)\n",
        "    dw, db = -gloss(np.array([w, b]), x, y)\n",
        "    db = db + max(0, 0.05 * (5- i))\n",
        "    prop = dict(arrowstyle=\"-|>,head_width=0.4,head_length=0.8\",\n",
        "    shrinkA=0,shrinkB=0)\n",
        "    if i < 11:\n",
        "        ax.annotate(\"\", xy=(dw + w,db + b), xytext=(w,b), arrowprops=prop)\n",
        "    w, b = w + dw, b + db\n",
        "    #ax.arrow(w, b, dw, db, head_width=0.1, zorder=4)\n",
        "\n",
        "   \n",
        "plt.xlabel('$w_1$')\n",
        "plt.ylabel('$b$')\n",
        "plt.title('$\\mathbf{Loss}(\\mathbf{w})$')\n",
        "CS = ax.contour(W, B, loss, cmap='coolwarm', levels=0.5 * np.arange(25))\n",
        "ax.clabel(CS, inline=True, fontsize=10)\n",
        "pass"
      ],
      "id": "0c7e21da",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step sizes\n",
        "\n",
        "This approach says that when the magnitude of the gradient is large, we should take a large step, and vice-versa.\n"
      ],
      "id": "c9ae2878"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "import matplotlib.pyplot as plt\n",
        "from autograd import grad\n",
        "x = (x - x.mean()) / x.std()\n",
        "y = (y - y.mean()) / y.std()\n",
        "\n",
        "model = LinearRegression().fit(x, y)\n",
        "\n",
        "W = np.arange(-2, 2, 0.1)\n",
        "B = np.arange(-2, 2, 0.1)\n",
        "W, B = np.meshgrid(W, B)\n",
        "\n",
        "\n",
        "resid = np.expand_dims(W, 0) * x.reshape((-1, 1, 1)) + np.expand_dims(B, 0) - y.reshape((-1, 1, 1)) # N x W x B\n",
        "loss = np.mean(resid ** 2, axis=0)\n",
        "\n",
        "def get_loss(w, x, y):\n",
        "    w, b = w[0], w[1]\n",
        "    resid = x.flatten() * w + b - y\n",
        "    return 0.15 * np.mean(resid ** 2)\n",
        "\n",
        "gloss = grad(get_loss)\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "\n",
        "\n",
        "w, b = (1.25, -1.5)\n",
        "\n",
        "for i in range(12):\n",
        "    ax.scatter(w, b, c=color, s=75, zorder=4)\n",
        "    dw, db = -gloss(np.array([w, b]), x, y)\n",
        "    db = db + max(0, 0.05 * (5- i))\n",
        "    prop = dict(arrowstyle=\"-|>,head_width=0.4,head_length=0.8\",\n",
        "    shrinkA=0,shrinkB=0)\n",
        "    if i < 11:\n",
        "        ax.annotate(\"\", xy=(dw + w,db + b), xytext=(w,b), arrowprops=prop)\n",
        "    w, b = w + dw, b + db\n",
        "    #ax.arrow(w, b, dw, db, head_width=0.1, zorder=4)\n",
        "\n",
        "   \n",
        "plt.xlabel('$w_1$')\n",
        "plt.ylabel('$b$')\n",
        "plt.title('$\\mathbf{Loss}(\\mathbf{w})$')\n",
        "CS = ax.contour(W, B, loss, cmap='coolwarm', levels=0.5 * np.arange(25))\n",
        "ax.clabel(CS, inline=True, fontsize=10)\n",
        "pass"
      ],
      "id": "3ace4a48",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step sizes\n",
        "\n",
        "However, if we take too large a step, we can overshoot the minimum entirely! In the worst case, this can lead to *divergence,* where gradient descent overshoots the minimum more and more at each step.\n"
      ],
      "id": "e7aa2a24"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "import matplotlib.pyplot as plt\n",
        "from autograd import grad\n",
        "x = (x - x.mean()) / x.std()\n",
        "y = (y - y.mean()) / y.std()\n",
        "\n",
        "model = LinearRegression().fit(x, y)\n",
        "\n",
        "W = np.arange(-15, 15, 0.4)\n",
        "B = np.arange(-15, 15, 0.4)\n",
        "W, B = np.meshgrid(W, B)\n",
        "\n",
        "\n",
        "resid = np.expand_dims(W, 0) * x.reshape((-1, 1, 1)) + np.expand_dims(B, 0) - y.reshape((-1, 1, 1)) # N x W x B\n",
        "loss = np.mean(resid ** 2, axis=0)\n",
        "\n",
        "def get_loss(w, x, y):\n",
        "    w, b = w[0], w[1]\n",
        "    resid = x.flatten() * w + b - y\n",
        "    return 1.08 * np.mean(resid ** 2)\n",
        "\n",
        "gloss = grad(get_loss)\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "\n",
        "\n",
        "w, b = (1.25, -1.5)\n",
        "\n",
        "for i in range(12):\n",
        "    ax.scatter(w, b, c=color, s=75, zorder=4)\n",
        "    dw, db = -gloss(np.array([w, b]), x, y)\n",
        "    db = db + max(0, 0.05 * (5- i))\n",
        "    prop = dict(arrowstyle=\"-|>,head_width=0.4,head_length=0.8\",\n",
        "    shrinkA=0,shrinkB=0)\n",
        "    if i < 11:\n",
        "        ax.annotate(\"\", xy=(dw + w,db + b), xytext=(w,b), arrowprops=prop)\n",
        "    w, b = w + dw, b + db\n",
        "    #ax.arrow(w, b, dw, db, head_width=0.1, zorder=4)\n",
        "\n",
        "   \n",
        "plt.xlabel('$w_1$')\n",
        "plt.ylabel('$b$')\n",
        "plt.title('$\\mathbf{Loss}(\\mathbf{w})$')\n",
        "CS = ax.contour(W, B, loss, cmap='coolwarm', levels=15 * np.arange(25))\n",
        "ax.clabel(CS, inline=True, fontsize=10)\n",
        "pass"
      ],
      "id": "96faa56a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step sizes\n",
        "\n",
        "The gradient is making a *linear approximation* to the function. A strait line has no minimum, so the gradient has no information about where along the approximation the true minimum will be.\n"
      ],
      "id": "3d615370"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "%%manim -sqh -v CRITICAL --progress_bar none BasicFunction\n",
        "\n",
        "class BasicFunction(PositiveVectorScene):\n",
        "    def construct(self):\n",
        "        f = lambda x: (0.5 * x - 3) **  2\n",
        "        fx = self.ax.plot(f, color=RED)\n",
        "        df = lambda x: (0.5 * x - 3)\n",
        "        dfx = self.ax.plot(lambda x: df(11) * (x - 8.55) , color=BLACK)\n",
        "        eq = MathTex(r'f(\\mathbf{x})=2(\\frac{x_1}{5})^3 -5 (\\frac{x_1}{6})^2 + 4', color=BLACK, tex_template=self.template).to_edge(UP + 2 * UP)\n",
        "        #eq.move_to(*self.ax.c2p([8, 7, 0]))\n",
        "        labels = self.ax.get_axis_labels(x_label=\"x_1\", y_label=\"y = f(\\mathbf{x})\")\n",
        "        labels.set_color(GREY)\n",
        "        labels.set_tex_template(self.template)\n",
        "        p = Dot(color=BLACK).move_to(self.ax.c2p(11, f(11)))\n",
        "        self.add(dfx, fx,  p, labels)"
      ],
      "id": "ccbe9561",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step sizes\n",
        "\n",
        "The gradient gives us the direction of maximum change of the function, but this is only true in the *limit* of a very small step.\n",
        "\n",
        "$$\n",
        "\\frac{df}{d\\mathbf{w}}= \\underset{\\gamma \\rightarrow 0}{\\lim}\\ \\underset{\\|\\mathbf{\\epsilon}\\|_2 < \\gamma}{\\max} \\frac{f(\\mathbf{w} + \\mathbf{\\epsilon}) - f(\\mathbf{w})}{\\|\\mathbf{\\epsilon}\\|_2}\n",
        "$$\n",
        "\n",
        "In higher dimensions, the gradient may not point directly to the minimum.\n"
      ],
      "id": "3c3c4618"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "import matplotlib.pyplot as plt\n",
        "from autograd import grad\n",
        "x = (x - x.mean()) / x.std()\n",
        "y = (y - y.mean()) / y.std()\n",
        "\n",
        "model = LinearRegression().fit(x, y)\n",
        "\n",
        "W = np.arange(-2, 2, 0.1)\n",
        "B = np.arange(-2, 2, 0.1)\n",
        "W, B = np.meshgrid(W, B)\n",
        "\n",
        "\n",
        "resid = np.expand_dims(W, 0) * x.reshape((-1, 1, 1)) + np.expand_dims(B, 0) - y.reshape((-1, 1, 1)) # N x W x B\n",
        "loss = np.mean(resid ** 2, axis=0)\n",
        "\n",
        "def get_loss(w):\n",
        "    w, b = w[0], w[1]\n",
        "    resid = x * w + b - y\n",
        "    return 0.2 * np.mean(resid ** 2)\n",
        "\n",
        "gloss = grad(get_loss)\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "\n",
        "other_lines = [(1.25, -1.5)]\n",
        "\n",
        "for param, color in zip(other_lines, [BLACK, BLUE, ORANGE, GREEN, PURPLE]):\n",
        "            w, b = param\n",
        "            ax.scatter(w, b, c=color, s=75, zorder=4)\n",
        "            dw, db = -gloss(np.array([w, b]))\n",
        "            prop = dict(arrowstyle=\"-|>,head_width=0.4,head_length=0.8\", color='r',\n",
        "            shrinkA=0,shrinkB=0)\n",
        "            ax.annotate(\"\", xy=(dw + w,db + b), xytext=(w,b), arrowprops=prop, c='g')\n",
        "            prop = dict(arrowstyle=\"-|>,head_width=0.4,head_length=0.8\", color='b',\n",
        "            shrinkA=0,shrinkB=0)\n",
        "            ax.annotate(\"\", xy=(dw * 1.5 + w,db * 0.8 + b), xytext=(w,b), arrowprops=prop, c='k')\n",
        "            #ax.arrow(w, b, dw, db, head_width=0.1, zorder=4)\n",
        "\n",
        "   \n",
        "plt.xlabel('$w_1$')\n",
        "plt.ylabel('$b$')\n",
        "plt.title('$\\mathbf{Loss}(\\mathbf{w})$')\n",
        "CS = ax.contour(W, B, loss, cmap='coolwarm', levels=0.5 * np.arange(25))\n",
        "ax.clabel(CS, inline=True, fontsize=10)\n",
        "pass"
      ],
      "id": "802b94db",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step sizes\n",
        "\n",
        "We can introduce an additional control to our algorithm: a **step size** or **learning rate**. This is a small constant $\\alpha$, that we will multiply the gradient by in each of our updates.\n",
        "\n",
        "$$\n",
        "\\mathbf{w}^{(i+1)} \\leftarrow \\mathbf{w}^{(i)} - \\alpha \\nabla f(\\mathbf{w}^{(i)})\n",
        "$$\n",
        "\n",
        "Using a small learning rate $(\\alpha << 1)$ will make gradient descent slower, but *much* more reliable.\n"
      ],
      "id": "9b63bfb4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "import matplotlib.pyplot as plt\n",
        "from autograd import grad\n",
        "x = (x - x.mean()) / x.std()\n",
        "y = (y - y.mean()) / y.std()\n",
        "\n",
        "model = LinearRegression().fit(x, y)\n",
        "\n",
        "W = np.arange(-2, 2, 0.1)\n",
        "B = np.arange(-2, 2, 0.1)\n",
        "W, B = np.meshgrid(W, B)\n",
        "\n",
        "\n",
        "resid = np.expand_dims(W, 0) * x.reshape((-1, 1, 1)) + np.expand_dims(B, 0) - y.reshape((-1, 1, 1)) # N x W x B\n",
        "loss = np.mean(resid ** 2, axis=0)\n",
        "\n",
        "def get_loss(w, x, y):\n",
        "    w, b = w[0], w[1]\n",
        "    resid = x.flatten() * w + b - y\n",
        "    return 0.05 * np.mean(resid ** 2)\n",
        "\n",
        "gloss = grad(get_loss)\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "\n",
        "\n",
        "w, b = (1.25, -1.5)\n",
        "\n",
        "for i in range(12):\n",
        "    ax.scatter(w, b, c=color, s=75, zorder=4)\n",
        "    dw, db = -gloss(np.array([w, b]), x, y)\n",
        "    db = db + max(0, 0.01 * (10 - i))\n",
        "    prop = dict(arrowstyle=\"-|>,head_width=0.4,head_length=0.8\",\n",
        "    shrinkA=0,shrinkB=0)\n",
        "    if i < 11:\n",
        "        ax.annotate(\"\", xy=(dw + w,db + b), xytext=(w,b), arrowprops=prop)\n",
        "    w, b = w + dw, b + db\n",
        "    #ax.arrow(w, b, dw, db, head_width=0.1, zorder=4)\n",
        "\n",
        "   \n",
        "plt.xlabel('$w_1$')\n",
        "plt.ylabel('$b$')\n",
        "plt.title('$\\\\mathbf{Small }\\\\ \\\\alpha$')\n",
        "CS = ax.contour(W, B, loss, cmap='coolwarm', levels=0.5 * np.arange(25))\n",
        "ax.clabel(CS, inline=True, fontsize=10)\n",
        "pass"
      ],
      "id": "337dfcc6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "import matplotlib.pyplot as plt\n",
        "from autograd import grad\n",
        "x = (x - x.mean()) / x.std()\n",
        "y = (y - y.mean()) / y.std()\n",
        "\n",
        "model = LinearRegression().fit(x, y)\n",
        "\n",
        "W = np.arange(-2, 2, 0.1)\n",
        "B = np.arange(-2, 2, 0.1)\n",
        "W, B = np.meshgrid(W, B)\n",
        "\n",
        "\n",
        "resid = np.expand_dims(W, 0) * x.reshape((-1, 1, 1)) + np.expand_dims(B, 0) - y.reshape((-1, 1, 1)) # N x W x B\n",
        "loss = np.mean(resid ** 2, axis=0)\n",
        "\n",
        "def get_loss(w, x, y):\n",
        "    w, b = w[0], w[1]\n",
        "    resid = x.flatten() * w + b - y\n",
        "    return 0.75 * np.mean(resid ** 2)\n",
        "\n",
        "gloss = grad(get_loss)\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "\n",
        "\n",
        "w, b = (1.25, -1.5)\n",
        "\n",
        "for i in range(12):\n",
        "    ax.scatter(w, b, c=color, s=75, zorder=4)\n",
        "    dw, db = -gloss(np.array([w, b]), x, y)\n",
        "    db = db + max(0, 0.01 * (10 - i))\n",
        "    prop = dict(arrowstyle=\"-|>,head_width=0.4,head_length=0.8\",\n",
        "    shrinkA=0,shrinkB=0)\n",
        "    if i < 11:\n",
        "        ax.annotate(\"\", xy=(dw + w,db + b), xytext=(w,b), arrowprops=prop)\n",
        "    w, b = w + dw, b + db\n",
        "    #ax.arrow(w, b, dw, db, head_width=0.1, zorder=4)\n",
        "\n",
        "   \n",
        "plt.xlabel('$w_1$')\n",
        "plt.ylabel('$b$')\n",
        "plt.title('$\\\\mathbf{Large }\\\\ \\\\alpha$')\n",
        "CS = ax.contour(W, B, loss, cmap='coolwarm', levels=0.5 * np.arange(25))\n",
        "ax.clabel(CS, inline=True, fontsize=10)\n",
        "pass"
      ],
      "id": "d70ac9b2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimizing linear regression\n",
        "\n",
        "We can apply gradient descent to linear regression in order to find the parameters that minimize the mean squared error loss.\n",
        "\n",
        "$$\n",
        "\\nabla_{\\mathbf{w}} \\textbf{MSE}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = \n",
        "\\frac{d}{d\\mathbf{w}}\\bigg( \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2 \\bigg)\n",
        "$$\n",
        "\n",
        "$$\n",
        "= \\frac{2}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)\\mathbf{x}_i\n",
        "$$\n",
        "\n",
        "With this gradient our gradient descent update becomes:\n",
        "\n",
        "$$\n",
        "\\mathbf{w}^{(i+1)} \\longleftarrow \\mathbf{w}^{(i)} - \\alpha\\bigg(\\frac{2 }{N}\\bigg)\\sum_{i=1}^N  (\\mathbf{x}_i^T\\mathbf{w}^{(i)} - y_i)\\mathbf{x}_i\n",
        "$$\n",
        "\n",
        "## Optimizing linear regression directly\n",
        "\n",
        "We know that at the minimum, the gradient must be $\\mathbf{0}$, so the following condition must hold:\n",
        "\n",
        "$$\n",
        "\\mathbf{0} = \\bigg( \\frac{2}{N}\\bigg)\\sum_{i=1}^N  (\\mathbf{x}_i^T\\mathbf{w} - y_i)\\mathbf{x}_i\n",
        "$$\n",
        "\n",
        "We can solve for a $\\mathbf{w}$ that satisfied this condition by first dropping the constant $\\frac{2}{N}$.\n",
        "\n",
        "$$\n",
        "\\mathbf{0} = \\sum_{i=1}^N  (\\mathbf{x}_i^T\\mathbf{w} - y_i)\\mathbf{x}_i\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{0} = \\sum_{i=1}^N \\big( \\mathbf{x}_i\\mathbf{x}_i^T\\mathbf{w} - y_i \\mathbf{x}_i \\big)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^N  y_i \\mathbf{x}_i  =\\bigg(\\sum_{i=1}^N  \\mathbf{x}_i\\mathbf{x}_i^T \\bigg)  \\mathbf{w} \n",
        "$$\n",
        "\n",
        "## Optimizing linear regression directly\n",
        "\n",
        "Note that $\\mathbf{x}_i \\mathbf{x}_i^T$ is a vector *outer product*:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_i \\mathbf{x}_i^T = \\begin{bmatrix} x_{i1} \\\\ x_{i2} \\\\ \\vdots \\\\  x_{in}\\end{bmatrix} \\begin{bmatrix} x_{i1} & x_{i2} & \\dots &  x_{in}\\end{bmatrix} = \n",
        "\\begin{bmatrix} x_{i1} x_{i1} & x_{i1} x_{i2} & \\dots & x_{i1} x_{in} \\\\\n",
        "x_{i2} x_{i1} & x_{i2} x_{i2} & \\dots & x_{i2} x_{in} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "x_{in} x_{i1} & x_{in} x_{i2} & \\dots & x_{in} x_{in} \\\\\n",
        "\\end{bmatrix} \n",
        "$$\n",
        "\n",
        "Thus $\\bigg(\\sum_{i=1}^N \\mathbf{x}_i\\mathbf{x}_i^T \\bigg)$ is a matrix.\n",
        "\n",
        "## Optimizing linear regression directly\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^N  y_i \\mathbf{x}_i  =\\bigg(\\sum_{i=1}^N  \\mathbf{x}_i\\mathbf{x}_i^T \\bigg)  \\mathbf{w} \n",
        "$$\n",
        "\n",
        "Multiplying both sides by the inverse $\\bigg(\\sum_{i=1}^N \\mathbf{x}_i\\mathbf{x}_i^T \\bigg)^{-1}$ we get:\n",
        "\n",
        "$$\n",
        "\\bigg(\\sum_{i=1}^N  \\mathbf{x}_i\\mathbf{x}_i^T \\bigg)^{-1} \\bigg(\\sum_{i=1}^N  y_i \\mathbf{x}_i\\bigg)  =  \\mathbf{w}^* \n",
        "$$\n",
        "\n",
        "## Optimizing linear regression directly\n",
        "\n",
        "$$\n",
        "\\bigg(\\sum_{i=1}^N  \\mathbf{x}_i\\mathbf{x}_i^T \\bigg)^{-1} \\bigg(\\sum_{i=1}^N  y_i \\mathbf{x}_i\\bigg)  =  \\mathbf{w}^* \n",
        "$$\n",
        "\n",
        "We can write this more compactly using the stacked input matrix and label vector notation we saw in homework 1.\n",
        "\n",
        "$$\n",
        "\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_{1} \\\\ \\mathbf{x}_{2} \\\\ \\vdots \\\\  \\mathbf{x}_{N} \\end{bmatrix},\\quad \\mathbf{y} = \\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\  y_{N} \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "In this case, the expression becomes:\n",
        "\n",
        "$$\\mathbf{w}^* = \\big( \\mathbf{X}^T \\mathbf{X} \\big)^{-1} \\big(\\mathbf{y}\\mathbf{X}\\big)$$\n",
        "\n",
        "# Maximum likelihood estimation\n",
        "\n",
        "## Normal distributions\n",
        "\n",
        "The **Normal** distribution (also known as the **Gaussian** distribution) is a continuous probability distribution with the following probability density function:\n",
        "\n",
        "$$\n",
        "p(y) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\mathbf{exp}\\bigg(-\\frac{1}{2\\sigma^2} (y -\\mu)^2\\bigg)\n",
        "$$\n",
        "\n",
        "![](pictures/Normal_Distribution_PDF.svg){fig-align=\"center\"}\n",
        "\n",
        "## Linear regression as a probabilistic model\n",
        "\n",
        "The probabilistic model for linear regression will make the assumption that the output is *normally distributed* conditioned on the input:\n",
        "\n",
        "$$\n",
        "y_i \\sim N\\big(\\mathbf{x}_i^T \\mathbf{w},\\ \\sigma^2\\big)\n",
        "$$\n",
        "\n",
        "We can write the conditional probability or **likelihood** of an output as:\n",
        "\n",
        "$$\n",
        "p(y_i\\mid\\mathbf{x}_i, \\mathbf{w}) =  \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\mathbf{exp}\\bigg(-\\frac{1}{2\\sigma^2} (y_i - \\mathbf{x}_i^T\\mathbf{w})^2\\bigg)\n",
        "$$\n",
        "\n",
        "![Image credit: Lily Chen *Towards Data Science*](pictures/prob_lr.webp){fig-align=\"center\"}\n",
        "\n",
        "## Maximum likelihood estimation\n",
        "\n",
        "How do we find the optimal value for $\\mathbf{w}$? Choose the $\\mathbf{w}$ that **maximizes the likelihood** (conditional probability) of all of the outputs in our dataset:\n",
        "\n",
        "$$\n",
        "\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmax}} \\ p(\\mathbf{y} \\mid \\mathbf{X}, \\mathbf{w}) =\\underset{\\mathbf{w}}{\\text{argmax}} \\ p(y_1,...,y_N \\mid \\mathbf{x}_1, ...,\\mathbf{x}_N, \\mathbf{w}) $$\n",
        "\n",
        "Generally our model also assumes *conditional independence* across observations so:\n",
        "\n",
        "$$\n",
        "p(y_1,...,y_N \\mid \\mathbf{x}_1, ...,\\mathbf{x}_N, \\mathbf{w}) = \\prod_{i=1}^N p(y_i\\mid \\mathbf{x}_i, \\mathbf{w})\n",
        "$$\n",
        "\n",
        "## Maximum likelihood estimation\n",
        "\n",
        "Equivalently frame the optimal value in terms of the *negative log-likelihood* rather than the likelihood.\n",
        "\n",
        "$$\n",
        "\\underset{\\mathbf{w}}{\\text{argmax}} \\prod_{i=1}^N p(y_i\\mid \\mathbf{x}_i, \\mathbf{w}) = \\underset{\\mathbf{w}}{\\text{argmin}} - \\sum_{i=1}^N \\log p(y_i \\mid \\mathbf{x}_i, \\mathbf{w}) = \\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})\n",
        "$$\n",
        "\n",
        "We see that the negative log-likelihood is a natural *loss function* to optimize to find $\\mathbf{w}^*$.\n",
        "\n",
        "$$\n",
        "\\textbf{Loss}(\\mathbf{w}) =\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})=- \\sum_{i=1}^N \\log p(y_i \\mid \\mathbf{x}_i, \\mathbf{w}) \n",
        "$$\n",
        "\n",
        "## Maximum likelihood estimation\n",
        "\n",
        "We can write out the negative log-likelihood explicitly using the normal PDF:\n",
        "\n",
        "$$\n",
        "\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = -\\sum_{i=1}^N\\log\\bigg[\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\text{exp}\\bigg(-\\frac{1}{2\\sigma^2} (y_i - \\mathbf{x}_i^T\\mathbf{w})^2\\bigg)\\bigg]\n",
        "$$\n",
        "\n",
        "$$\n",
        " = \\frac{1}{2\\sigma^2} \\sum_{i=1}^N(y_i - \\mathbf{x}_i^T\\mathbf{w})^2 - \\frac{N}{\\sigma \\sqrt{2 \\pi}}\n",
        "$$\n",
        "\n",
        "## Maximum likelihood estimation\n",
        "\n",
        "$$\n",
        "\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^N(y_i - \\mathbf{x}_i^T\\mathbf{w})^2 - \\frac{N}{\\sigma \\sqrt{2 \\pi}}\n",
        "$$\n",
        "\n",
        "We see that this loss is very similar to the MSE loss. Taking the gradient this becomes even more clear.\n",
        "\n",
        "$$\n",
        "\\nabla_{\\mathbf{w}}\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = \n",
        "\\frac{d}{d\\mathbf{w}}\\bigg( \\frac{1}{2\\sigma^2} \\sum_{i=1}^N(y_i - \\mathbf{x}_i^T\\mathbf{w})^2 - \\frac{N}{\\sigma \\sqrt{2 \\pi}} \\bigg)\n",
        "$$\n",
        "\n",
        "$$\n",
        "= \\frac{1}{2\\sigma^2}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)\\mathbf{x}_i\n",
        "$$\n",
        "\n",
        "The optimal value for $\\mathbf{w}$ is the same for both MSE and negative log-likelihood and the optimal value does not depend on $\\sigma^2$!\n",
        "\n",
        "$$\n",
        "\\underset{\\mathbf{w}}{\\text{argmin}}\\  MSE(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = \\underset{\\mathbf{w}}{\\text{argmin}}\\ \\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})\n",
        "$$"
      ],
      "id": "4a5a2be2"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}