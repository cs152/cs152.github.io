<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Lecture 9: Optimization – CS 152: Neural Networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">CS 152: Neural Networks</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../calendar/calendar.html"> 
<span class="menu-text">Calendar</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../assignments/homeworks/homeworks.html"> 
<span class="menu-text">Homeworks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../assignments/final-project/outline.html"> 
<span class="menu-text">Project</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-resources" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Resources</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-resources">    
        <li>
    <a class="dropdown-item" href="https://probml.github.io/pml-book/book1.html">
 <span class="dropdown-text">Textbook</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://blank-app-ufu2uvdeosc.streamlit.app/">
 <span class="dropdown-text">Notebook conversion</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://www.gradescope.com/courses/960105">
 <span class="dropdown-text">Gradescope</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-solutions" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Solutions</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-solutions">    
        <li class="dropdown-header">Homework 1 Solutions</li>
        <li class="dropdown-header">Homework 2 Solutions</li>
        <li class="dropdown-header">Homework 3 Solutions</li>
        <li class="dropdown-header">Homework 4 Solutions</li>
        <li class="dropdown-header">Homework 5 Solutions</li>
        <li class="dropdown-header">Homework 6 Solutions</li>
        <li class="dropdown-header">Homework 7 Solutions</li>
        <li class="dropdown-header">Homework 8 Solutions</li>
        <li class="dropdown-header">Homework 9 Solutions</li>
        <li class="dropdown-header">Homework 10 Solutions</li>
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#playground" id="toc-playground" class="nav-link active" data-scroll-target="#playground">Playground</a></li>
  <li><a href="#initialization" id="toc-initialization" class="nav-link" data-scroll-target="#initialization">Initialization</a>
  <ul class="collapse">
  <li><a href="#symmetry-breaking" id="toc-symmetry-breaking" class="nav-link" data-scroll-target="#symmetry-breaking">Symmetry-breaking</a></li>
  <li><a href="#visualizing-learning-rates" id="toc-visualizing-learning-rates" class="nav-link" data-scroll-target="#visualizing-learning-rates">Visualizing learning rates</a></li>
  <li><a href="#scaled-initialization" id="toc-scaled-initialization" class="nav-link" data-scroll-target="#scaled-initialization">Scaled initialization</a></li>
  </ul></li>
  <li><a href="#stochastic-gradient-descent" id="toc-stochastic-gradient-descent" class="nav-link" data-scroll-target="#stochastic-gradient-descent">Stochastic Gradient Descent</a>
  <ul class="collapse">
  <li><a href="#computational-complexity" id="toc-computational-complexity" class="nav-link" data-scroll-target="#computational-complexity">Computational complexity</a></li>
  <li><a href="#computing-a-single-neuron" id="toc-computing-a-single-neuron" class="nav-link" data-scroll-target="#computing-a-single-neuron">Computing a single neuron</a></li>
  <li><a href="#computing-the-full-loss" id="toc-computing-the-full-loss" class="nav-link" data-scroll-target="#computing-the-full-loss">Computing the full loss</a></li>
  <li><a href="#real-world-costs" id="toc-real-world-costs" class="nav-link" data-scroll-target="#real-world-costs">Real world costs</a></li>
  <li><a href="#estimating-loss" id="toc-estimating-loss" class="nav-link" data-scroll-target="#estimating-loss">Estimating loss</a></li>
  <li><a href="#estimating-gradients" id="toc-estimating-gradients" class="nav-link" data-scroll-target="#estimating-gradients">Estimating gradients</a></li>
  <li><a href="#minibatch-sgd" id="toc-minibatch-sgd" class="nav-link" data-scroll-target="#minibatch-sgd">Minibatch SGD</a></li>
  </ul></li>
  <li><a href="#gradient-descent-extensions" id="toc-gradient-descent-extensions" class="nav-link" data-scroll-target="#gradient-descent-extensions">Gradient Descent Extensions</a>
  <ul class="collapse">
  <li><a href="#momentum" id="toc-momentum" class="nav-link" data-scroll-target="#momentum">Momentum</a></li>
  <li><a href="#sgd-momentum" id="toc-sgd-momentum" class="nav-link" data-scroll-target="#sgd-momentum">SGD + Momentum</a></li>
  <li><a href="#adaptive-gradients-rmsprop" id="toc-adaptive-gradients-rmsprop" class="nav-link" data-scroll-target="#adaptive-gradients-rmsprop">Adaptive gradients (RMSProp)</a></li>
  <li><a href="#adam" id="toc-adam" class="nav-link" data-scroll-target="#adam">Adam</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Lecture 9: Optimization</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="playground" class="level1">
<h1>Playground</h1>
<p>Try out the concepts from this lecture in the <a href="https://cs152-neural-networks-fall-2023.github.io/playground">Neural Network Playground!</a></p>
</section>
<section id="initialization" class="level1">
<h1>Initialization</h1>
<p>So far we’ve seen how train neural-networks with gradient descent. Recall that the gradient descent update for a weight <span class="math inline">\(\mathbf{w}\)</span> at step <span class="math inline">\(k\)</span> is: <span class="math display">\[ \mathbf{w}^{(k+1)} \longleftarrow \mathbf{w}^{(k)} - \alpha \nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y})\]</span></p>
<p>We subtract the gradient of the loss with respect to <span class="math inline">\(\mathbf{w}\)</span> from the current estimate of <span class="math inline">\(\mathbf{w}\)</span>. An important consideration for this algorithm is how to set the initial guess <span class="math inline">\(\mathbf{w}^{(0)}\)</span>. We call this process <strong>initialization</strong>.</p>
<section id="symmetry-breaking" class="level2">
<h2 class="anchored" data-anchor-id="symmetry-breaking">Symmetry-breaking</h2>
<p>In neural networks, we typically initialize parameters <em>randomly</em>. One important reason for random initialization is to make sure that different parameters have different starting values. To see why this is needed, let’s consider the prediction function for a simple neural network that takes in 1-dimensional inputs:</p>
<p><span class="math display">\[
f(\mathbf{x}) = \sigma(\mathbf{x}^T\mathbf{W}_1)^T\mathbf{w}_0=\sigma(x_1 w_{11}) w_{01} +\sigma (x_1 w_{12})w_{02}
\]</span></p>
<p>In this case we have 4 parameters: <span class="math inline">\(w_{01}, w_{02}, w_{11}, w_{12}\)</span>. If we initialize all to the same value, say <span class="math inline">\(w_{**} = a\)</span>, let’s see what happens to the derivatives we compute:</p>
<p><span class="math display">\[
\frac{d}{dw_{01}} f(\mathbf{x}) = \sigma(x_1 w_{11}) = \sigma(x_1 a)
\]</span></p>
<p><span class="math display">\[
\frac{d}{dw_{02}} f(\mathbf{x}) = \sigma(x_1 w_{12}) = \sigma(x_1 a)
\]</span></p>
<p>We see that <span class="math inline">\(\frac{d}{dw_{01}} = \frac{d}{dw_{02}}\)</span>! Our gradient descent update will set:</p>
<p><span class="math display">\[
w_{01}^{(1)} \longleftarrow w_{01}^{(0)} - \alpha \frac{d}{dw_{01}} = a - \alpha \sigma(x_1 a)
\]</span></p>
<p><span class="math display">\[
w_{02}^{(1)} \longleftarrow w_{02}^{(0)} - \alpha \frac{d}{dw_{02}} = a - \alpha \sigma(x_1 a)
\]</span></p>
<p>So after each gradient descent update the two values will continue to be the same! The gradient decent algorithm has no way to distinguish between these two weights and so it is stuck finding solutions where <span class="math inline">\(w_{01} = w_{02}\)</span> and <span class="math inline">\(w_{11}=w_{12}\)</span>. We call this the symmetry problem, and it means we no longer get any benefit from having multiple neurons.</p>
<p>We can see this in practice with a simple network:</p>
<div class="columns">
<div class="column" style="width:45%;">
<p><img src="images/paste-1.png" class="img-fluid"><br>
When the network is initialized with symmetry, the two neurons will always have the same output and our solution is poor.</p>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:45%;">
<p><img src="images/paste-2.png" class="img-fluid"></p>
<p>When initialized randomly, the two neurons can create different transforms and a much better solution is found.</p>
</div>
</div>
<p>If we plot the loss as a function of two <span class="math inline">\(w_{01}\)</span> and <span class="math inline">\(w_{02}\)</span> we can see what is happening graphically.</p>
<div class="columns">
<div class="column" style="width:45%;">
<p><img src="images/paste-3.png" class="img-fluid"></p>
<p>Initializing the two parameters equal corresponds to sitting on a ridge of the loss surface, there are equally valid solutions on either side, but gradient descent gives us no way to chose between them.</p>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:45%;">
<p><img src="images/paste-6.png" class="img-fluid"></p>
<p>If we plot the (negative) gradient of the loss we see that the gradient of any point on the ridge always points along the ridge. Gradient descent corresponds to following these arrows to find a minimum.</p>
</div>
</div>
</section>
<section id="visualizing-learning-rates" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-learning-rates">Visualizing learning rates</h2>
<p>As an aside, plotting the gradient as a vector field also gives us an convenient way to visualize the effects of different learning rates. Recall that the learning rate corresponds to how much we <em>scale</em> the gradient each time we take a step.</p>
<div class="columns">
<div class="column" style="width:30%;">
<p><img src="images/paste-11.png" class="img-fluid"></p>
<p>A small learning rate means we will move slowly, so It may take a long time to find the minimum.</p>
</div><div class="column" style="width:5%;">

</div><div class="column" style="width:30%;">
<p><img src="images/paste-10.png" class="img-fluid"></p>
<p>A well-chosen learning rate lets us find a minimum quickly.</p>
</div><div class="column" style="width:5%;">

</div><div class="column" style="width:30%;">
<p><img src="images/paste-13.png" class="img-fluid"></p>
<p>A too-large learning rate means that steps may take us flying past the minimum!</p>
</div>
</div>
</section>
<section id="scaled-initialization" class="level2">
<h2 class="anchored" data-anchor-id="scaled-initialization">Scaled initialization</h2>
<p>Now that we’ve seen the benefits of initializing randomly, we need to consider what distribution to initialize from. An obvious choice might be a standard normal distribution, with mean <span class="math inline">\(0\)</span> and standard deviation <span class="math inline">\(1\)</span>.</p>
<p><span class="math display">\[w_{i} \sim \mathcal{N}(0, 1) \quad \forall\ w_{i} \in \mathbf{w}\]</span>This has a subtle issue though. To see why let’s consider a linear function defined by randomly initialized weights:</p>
<p><span class="math display">\[
f(\mathbf{x}) = \sum_{i=1}^d x_i w_i
\]</span></p>
<p>Let’s consider the mean and variance of this output with respect to <span class="math inline">\(\mathbf{w}\)</span>:</p>
<p><span class="math display">\[
\mathbb{E} \big[f(\mathbf{x})\big] = \mathbb{E} \bigg[  \sum_{i=1}^d x_i w_i \bigg] =   \sum_{i=1}^d x_i \mathbb{E} \big[w_i \big] = 0, \quad w_i \sim \mathcal{N}(0, 1)
\]</span></p>
<p><span class="math display">\[
\text{Var} \big[f(\mathbf{x})\big] = \text{Var}  \bigg[  \sum_{i=1}^d x_i w_i \bigg] =   \sum_{i=1}^d \text{Var} \big[ x_i w_i \big] = \sum_{i=1}^d x_i^2 \text{Var} [w_i] = \sum_{i=1}^d x_i^2
\]</span></p>
<p>We see a few things here, the mean is <span class="math inline">\(0\)</span> and the variance depends on <span class="math inline">\(x_i\)</span>, which is reasonable. However we see that the variance also depends on <span class="math inline">\(d\)</span>, the dimensionality of the input. In particular it’s <span class="math inline">\(\mathcal{O}(d)\)</span>. Why is this important? Because it means that if we increase the number of neurons at each layer in our network, the variance of the network’s predictions will also increase!</p>
<p>If our network has many neurons in each layer (large networks can have 1000’s!) the variance of outputs can be extreme, leading to poor initializations that correspond to extremely steep prediction functions. Here we can compare a few intializations from a network with just 8 neurons per layer to a network with 2.</p>
<div class="columns">
<div class="column" style="width:45%;">
<p><img src="images/paste-14.png" class="img-fluid"></p>
<p><img src="images/paste-15.png" class="img-fluid"></p>
<p><img src="images/paste-16.png" class="img-fluid"></p>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:45%;">
<p><img src="images/paste-17.png" class="img-fluid"></p>
<p><img src="images/paste-18.png" class="img-fluid"></p>
<p><img src="images/paste-19.png" class="img-fluid"></p>
</div>
</div>
<p>In practice this can make gradient descent difficult as these initialization are often very far from the minimum and the gradients are typically large, meaning small learning rates are needed to prevent divergence.</p>
<p>A better approach would keep the variance consistent no matter how many inputs there are. We can reduce the variance by dividing our initial weights by some scale factor <span class="math inline">\(s\)</span>.</p>
<p><span class="math display">\[
f(\mathbf{x}) = \sum_{i=1}^d x_i w_i\bigg(\frac{1}{s}\bigg)
\]</span></p>
<p>If we want the variance to be independent of <span class="math inline">\(d\)</span>, then we want:</p>
<p><span class="math display">\[
s = \sqrt{d}
\]</span></p>
<p>We can verify this by computing the variance:</p>
<p><span class="math display">\[
\text{Var}  \bigg[  \sum_{i=1}^d x_i w_i \bigg(\frac{1}{\sqrt{d}}\bigg) \bigg] =   \sum_{i=1}^d \text{Var} \bigg[ x_i w_i \bigg(\frac{1}{\sqrt{d}}\bigg) \bigg] = \sum_{i=1}^d x_i^2 \bigg(\frac{1}{\sqrt{d}}\bigg)^2 \text{Var} [w_i] = \frac{1}{d}\sum_{i=1}^d x_i^2
\]</span></p>
<p>This is equivalent to drawing our initial weights for each layer from a normal distribution with standard deviation equal to 1 over the square root of the number of inputs:</p>
<p><span class="math display">\[w_{i} \sim \mathcal{N}\bigg(0, \frac{1}{\sqrt{d}}\bigg) \quad \forall\ w_{i} \in \mathbf{w},\ \mathbf{w}\in \mathbb{R}^{d}\]</span></p>
<p>This is known as <strong>Kaiming normal initialization</strong> (sometimes also called <strong>He initialization</strong>, after the inventor Kaiming He).</p>
<p>For neural network layers where the weights are a matrix <span class="math inline">\(\mathbf{W} \in \mathbb{R}^{d \times e}\)</span>, this works the same way:</p>
<p><span class="math display">\[w_{ij} \sim \mathcal{N}\bigg(0, \frac{1}{\sqrt{d}}\bigg) \quad \forall\ w_{ij} \in \mathbf{W},\ \mathbf{w}\in \mathbb{R}^{d \times e}\]</span></p>
<p>A popular alternative scales the distribution according to both the number of inputs and outputs of the layer:</p>
<p><span class="math display">\[w_{ij} \sim \mathcal{N}\bigg(0, \sqrt{\frac{2}{d + e}}\bigg) \quad \forall\ w_{ij} \in \mathbf{W},\ \mathbf{w}\in \mathbb{R}^{d \times e}\]</span></p>
<p>This is known as <strong>Xavier initialization</strong> (or <strong>Glorot initialization</strong> after the inventor Xavier Glorot).</p>
<p>We can compare initializations from a standard normal with initializations from a Kaiming normal.</p>
<div class="columns">
<div class="column" style="width:45%;">
<p><strong>Standard normal</strong> <span class="math inline">\(w_{i} \sim \mathcal{N}\bigg(0, 1\bigg)\)</span></p>
<p><img src="images/paste-14.png" class="img-fluid"></p>
<p><img src="images/paste-15.png" class="img-fluid"></p>
<p><img src="images/paste-16.png" class="img-fluid"></p>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:45%;">
<p><strong>Kaiming normal</strong> <span class="math inline">\(w_{i} \sim \mathcal{N}\bigg(0, \frac{1}{\sqrt{d}}\bigg)\)</span></p>
<p><img src="images/paste-24.png" class="img-fluid"></p>
<p><img src="images/paste-25.png" class="img-fluid"></p>
<p><img src="images/paste-26.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="stochastic-gradient-descent" class="level1">
<h1>Stochastic Gradient Descent</h1>
<p><a href="../lecture9-optimization/viz.html">Visualizer</a></p>
<section id="computational-complexity" class="level2">
<h2 class="anchored" data-anchor-id="computational-complexity">Computational complexity</h2>
<p>An important consideration for any algorithm is the computational complexity. So far we’ve ignored the computational cost of training a neural network, but in practice it can be quite significant! It’s worthwhile to think about what this complexity is and if there’s any way to improve it.</p>
<p>Let’s start by reviewing the factors that will determine the cost of running gradient descent to train a network and define some notation for each:</p>
<ul>
<li><p><strong>Dataset size (</strong> <span class="math inline">\(N\)</span> <strong>):</strong> As we have previously, we’ll use <span class="math inline">\(N\)</span> to denote the number of observations in the dataset that we’ll use to train our model (the <em>training set</em>).</p></li>
<li><p><strong>Dimensionality (</strong> <span class="math inline">\(d\)</span> <strong>):</strong> We’ll use the notation <span class="math inline">\(d\)</span> to refer to <em>both</em> the <strong>number of input features</strong> <em>and</em> the <strong>number of neurons per layer</strong>. In general this will let us conveniently refer to the number of inputs and outputs for any given layer with a single number. In practice the number of input dimensions may not match the number of neurons per layer (and the number of neurons per layer may not actually be constant!), but since we’re mostly concerned with an asymptotic upper bound on complexity, we’ll just assume <span class="math inline">\(d\)</span> is the size of the largest layer.</p></li>
<li><p><strong>Number of layers (</strong> <span class="math inline">\(L\)</span> <strong>):</strong> The other factor of our network architecture is of course the number of layers, which we’ll call <span class="math inline">\(L\)</span>.</p></li>
<li><p><strong>Number of steps (</strong> <span class="math inline">\(S\)</span> <strong>):</strong> Finally how steps of gradient descent we take will of course also have an input on the running time. We’ll use <span class="math inline">\(S\)</span> to denote this number.</p></li>
</ul>
<p>We can consider one of the networks shown above as a specific example:</p>
<p><img src="images/paste-24.png" class="img-fluid"></p>
<p>Here we see that this network has <span class="math inline">\(8\)</span> neurons per layer, so <span class="math inline">\(d=8\)</span>, a total of <span class="math inline">\(6\)</span> hidden layers, so <span class="math inline">\(L=6\)</span> and the training set shown has roughly <span class="math inline">\(100\)</span> observations, so <span class="math inline">\(N=100\)</span>. We won’t worry about the number of steps ( <span class="math inline">\(S\)</span> ) for now.</p>
<p>In this example our neural network is being used on a regression task, so at each step of gradient descent we’ll to compute the mean squared error loss:</p>
<p><span class="math display">\[\underset{\text{MSE}}{\textbf{Loss}} (\mathbf{w}, \mathbf{X}, \mathbf{y}) = \frac{1}{N}\sum_{i=1}^N (f(\mathbf{x}_i, \mathbf{w}) - y_i)^2\]</span>And then it’s gradient with respect to the parameters, <span class="math inline">\(\mathbf{w}\)</span>.</p>
<p><span class="math display">\[\nabla_{\mathbf{w}}\underset{\text{MSE}}{\textbf{Loss}} (\mathbf{w}, \mathbf{X}, \mathbf{y}) = \nabla_{\mathbf{w}}\frac{1}{N}\sum_{i=1}^N (f(\mathbf{x}_i, \mathbf{w}) - y_i)^2\]</span></p>
<p>Our analysis would be equivalent for classification problems.</p>
</section>
<section id="computing-a-single-neuron" class="level2">
<h2 class="anchored" data-anchor-id="computing-a-single-neuron">Computing a single neuron</h2>
<p>Let’s start simple, what is the cost computing a <em>single</em> neuron in our network? We know that the calculation that a single neuron performs is (we’ll ignore a bias term because it could be analyzed as a single extra term of the summation) :</p>
<p><span class="math display">\[\phi(\mathbf{x})=\sigma\big(\mathbf{x}^T\mathbf{w} \big) = \sigma\bigg(\sum_{i=1}^d x_i w_i \bigg)\]</span></p>
<p>We see that computing activation function <span class="math inline">\(\sigma(\cdot)\)</span> is constant time, but the cost of the summation: <span class="math inline">\(\sum_{i=1}^d x_i w_i\)</span> will scale linearly with the dimension <span class="math inline">\(d\)</span>, so we can write the running time generally as <span class="math inline">\(\mathcal{O}(d)\)</span>.</p>
<p>That was straightforward, but we’re not quite done with a single neuron yet! Remember that we’re also going to be computing gradients, so we’ll need to make sure that the cost of our backward pass update for this neuron won’t increase our asymptotic running time.</p>
<p>Recall that in our backward pass update we’re provided <span class="math inline">\(\frac{dLoss}{d\phi}\)</span> and we need to compute<span class="math inline">\(\frac{dLoss}{dx_i}\)</span> <span class="math inline">\(\frac{dLoss}{dw_i}\)</span>. Once again we see that the activation <span class="math inline">\(\sigma\)</span> only acts on a single value, so the update will simply be: <span class="math display">\[\frac{dLoss}{d(\mathbf{x}^T\mathbf{w})}=\frac{dLoss}{d\phi}\frac{d\phi}{d(\mathbf{x}^T\mathbf{w})}\]</span> As all of these terms are scalars, the cost is once again constant.</p>
<p>Now given the scalar <span class="math inline">\(\frac{dLoss}{d(\mathbf{x}^T\mathbf{w})}\)</span>, we compute the derivative with respect to <span class="math inline">\(x_i\)</span> or <span class="math inline">\(w_i\)</span> as:</p>
<p><span class="math display">\[
\frac{dLoss}{dx_i}=\frac{dLoss}{d(\mathbf{x}^T\mathbf{w})}\frac{d(\mathbf{x}^T\mathbf{w})}{dx_i}= \frac{dLoss}{d(\mathbf{x}^T\mathbf{w})} w_i\]</span> <span class="math display">\[
\frac{dLoss}{dw_i}=\frac{dLoss}{d(\mathbf{x}^T\mathbf{w})}\frac{d(\mathbf{x}^T\mathbf{w})}{dw_i}= \frac{dLoss}{d(\mathbf{x}^T\mathbf{w})} x_i
\]</span></p>
<p>A constant ( <span class="math inline">\(\mathcal{O}(1)\)</span> ) operation per entry. Since <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{w}\)</span> both have <span class="math inline">\(d\)</span> entries, the cost of the backward pass update will still be <span class="math inline">\(\mathcal{O}(d)\)</span>.</p>
</section>
<section id="computing-the-full-loss" class="level2">
<h2 class="anchored" data-anchor-id="computing-the-full-loss">Computing the full loss</h2>
<p>Things get a little easier from here. We know that each layer of our network has <span class="math inline">\(d\)</span> neurons, each of which takes <span class="math inline">\(\mathcal{O}(d)\)</span> time to compute. Therefore the total cost for a layer will be <span class="math inline">\(\mathcal{O}(d^2)\)</span>.</p>
<p>Our full network has <span class="math inline">\(L\)</span> layers, therefore the full network will take <span class="math inline">\(\mathcal{O}(Ld^2)\)</span> time to compute.</p>
<p>Looking back at our loss:</p>
<p><span class="math display">\[\underset{\text{MSE}}{\textbf{Loss}} (\mathbf{w}, \mathbf{X}, \mathbf{y}) = \frac{1}{N}\sum_{i=1}^N (f(\mathbf{x}_i, \mathbf{w}) - y_i)^2\]</span></p>
<p>We see that so far we’ve bounded the time it takes to compute <em>one</em> of our predictions: <span class="math inline">\(f(\mathbf{x}_i, \mathbf{w})\)</span>. In order to compute the full loss (and therefore gradient), we need to do this for every term in our summation, a total of <span class="math inline">\(N\)</span> times. This give us a total cost to compute the loss of <span class="math inline">\(\mathcal{O}(NLd^2)\)</span>. We already saw that our backward pass updates don’t increase our asymptotic cost, so in total the cost of a single gradient descent update:</p>
<p><span class="math display">\[ \mathbf{w}^{(k+1)} \longleftarrow \mathbf{w}^{(k)} - \alpha \nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y})\]</span></p>
<p>Is <span class="math inline">\(\mathcal{O}(NLd^2)\)</span>.</p>
<p>Accounting for the number of update steps ( <span class="math inline">\(S\)</span> ) we perform we see that the <strong>total cost of gradient descent is:</strong> <span class="math inline">\(\mathcal{O}(SNLd^2)\)</span>.</p>
</section>
<section id="real-world-costs" class="level2">
<h2 class="anchored" data-anchor-id="real-world-costs">Real world costs</h2>
<p>Let’s asses how bad this running time actually is. To do so we’ll need to consider how large each of these factors actually is in practice. We’ll start by looking at a state-of-the-art (for 2012) neural network for classifying images: AlexNet.</p>
<p><img src="images/paste-29.png" class="img-fluid" width="960"></p>
</section>
<section id="estimating-loss" class="level2">
<h2 class="anchored" data-anchor-id="estimating-loss">Estimating loss</h2>
<p>Neural network MSE loss:</p>
<p><span class="math display">\[\underset{\text{MSE}}{\textbf{Loss}} (\mathbf{w}, \mathbf{X}, \mathbf{y}) = \frac{1}{N}\sum_{i=1}^N (f(\mathbf{x}_i, \mathbf{w}) - y_i)^2\]</span></p>
<p>Estimate by sampling:</p>
<p><span class="math display">\[\underset{\text{MSE}}{\textbf{Loss}} (\mathbf{w}, \mathbf{X}, \mathbf{y}) \approx (f(\mathbf{x}_i, \mathbf{w}) - y_i)^2, \quad i \sim \text{Uniform}(1, N)\]</span></p>
<p>Expectation of sampled loss is the true loss!</p>
<p><span class="math display">\[\mathbb{E}_i[(f(\mathbf{x}_i, \mathbf{w}) - y_i)^2] = \sum_{i=1}^N p(i)(f(\mathbf{x}_i, \mathbf{w}) - y_i)^2 =\frac{1}{N} \sum_{i=1}^N (f(\mathbf{x}_i, \mathbf{w}) - y_i)^2\]</span></p>
<p>In general any loss that can be written as a mean of individual losses can be estimated in this way:</p>
<p><span class="math display">\[\textbf{Loss} (\mathbf{w}, \mathbf{X}, \mathbf{y}) = \frac{1}{N} \sum_{i=1}^N \textbf{Loss} (\mathbf{w}, \mathbf{x}_i, y_i)\]</span></p>
<p><span class="math display">\[\textbf{Loss} (\mathbf{w}, \mathbf{X}, \mathbf{y}) = \mathbb{E}[\textbf{Loss} (\mathbf{w}, \mathbf{x}_i, y_i)], \quad i\sim \text{Uniform}(1,N)\]</span></p>
</section>
<section id="estimating-gradients" class="level2">
<h2 class="anchored" data-anchor-id="estimating-gradients">Estimating gradients</h2>
<p>Gradient descent update:</p>
<p><span class="math display">\[ \mathbf{w}^{(k+1)} \longleftarrow \mathbf{w}^{(k)} - \alpha \nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y})\]</span></p>
<p>Gradient can be composed into a sum of gradients and estimated the same way!</p>
<p><span class="math display">\[\nabla_{\mathbf{w}} \textbf{Loss} (\mathbf{w}, \mathbf{X}, \mathbf{y}) = \nabla_{\mathbf{w}} \bigg( \frac{1}{N} \sum_{i=1}^N \textbf{Loss} (\mathbf{w}, \mathbf{x}_i, y_i)\bigg)\]</span></p>
<p><span class="math display">\[=\frac{1}{N} \sum_{i=1}^N  \nabla_{\mathbf{w}} \textbf{Loss} (\mathbf{w}, \mathbf{x}_i, y_i) = \mathbb{E}[\nabla_{\mathbf{w}} \textbf{Loss} (\mathbf{w}, \mathbf{x}_i, y_i)], \quad i\sim \text{Uniform}(1, N)\]</span></p>
<p><em>Stochastic gradient descent update:</em></p>
<p><span class="math display">\[ \mathbf{w}^{(k+1)} \longleftarrow \mathbf{w}^{(k)} - \alpha \nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{x}_i, y_i), \quad i\sim \text{Uniform}(1, N)\]</span></p>
</section>
<section id="minibatch-sgd" class="level2">
<h2 class="anchored" data-anchor-id="minibatch-sgd">Minibatch SGD</h2>
<p>Can estimate gradients with a <em>minibatch</em> of <span class="math inline">\(B\)</span> observations:</p>
<p><span class="math display">\[\text{Batch:}\ \{(\mathbf{x}_{b_1}, y_{b_1}), (\mathbf{x}_{b_2}, y_{b_2}), ...,  (\mathbf{x}_{b_B}, y_{b_B})\}, \quad \{b_1, b_2, ...,b_B\} \sim \text{Uniform}(1, N)\]</span></p>
<p><span class="math display">\[\nabla_{\mathbf{w}} \textbf{Loss} (\mathbf{w}, \mathbf{X}, \mathbf{y}) \approx \frac{1}{B} \sum_{i=1}^B \nabla_{\mathbf{w}} \textbf{Loss} (\mathbf{w}, \mathbf{x}_{b_i}, y_{b_i}), \quad \{b_1, b_2, ...,b_B\} \sim \text{Uniform}(1, N)\]</span></p>
<p>This still gives the correct expectation</p>
<p><span class="math display">\[\mathbb{E}\bigg[\frac{1}{B} \sum_{i=1}^B \nabla_{\mathbf{w}} \textbf{Loss} (\mathbf{w}, \mathbf{x}_{b_i}, y_{b_i})\bigg] = \bigg(\frac{1}{B}\bigg) \sum_{i=1}^B\mathbb{E}\bigg[ \nabla_{\mathbf{w}} \textbf{Loss} (\mathbf{w}, \mathbf{x}_{b_i}, y_{b_i})\bigg]\]</span> <span class="math display">\[ = \nabla_{\mathbf{w}} \textbf{Loss} (\mathbf{w}, \mathbf{X}, \mathbf{y})\]</span></p>
<p>The variance decreases with the size of the batch!</p>
<p><span class="math display">\[\text{Var}\bigg[\frac{1}{B} \sum_{i=1}^B \nabla_{\mathbf{w}} \textbf{Loss} (\mathbf{w}, \mathbf{x}_{b_i}, y_{b_i})\bigg] =  \bigg(\frac{1}{B^2}\bigg) \sum_{i=1}^B\text{Var}\bigg[ \nabla_{\mathbf{w}} \textbf{Loss} (\mathbf{w}, \mathbf{x}_{b_i}, y_{b_i})\bigg]\]</span> <span class="math display">\[= \bigg(\frac{1}{B}\bigg)\text{Var}\bigg[ \nabla_{\mathbf{w}} \textbf{Loss} (\mathbf{w}, \mathbf{x}_{b_i}, y_{b_i})\bigg]\]</span></p>
</section>
</section>
<section id="gradient-descent-extensions" class="level1">
<h1>Gradient Descent Extensions</h1>
<section id="momentum" class="level2">
<h2 class="anchored" data-anchor-id="momentum">Momentum</h2>
<p>Gradient descent with momentum updates the <em>average gradient</em> then uses the running average to take descent steps.</p>
<p><span class="math display">\[ \mathbf{v}^{(k+1)} \longleftarrow \beta \mathbf{v}^{(k)} + (1-\beta) \nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y})\]</span></p>
<p><span class="math display">\[ \mathbf{w}^{(k+1)} \longleftarrow \mathbf{w}^{(k)} - \alpha v^{(k+1)}\]</span></p>
</section>
<section id="sgd-momentum" class="level2">
<h2 class="anchored" data-anchor-id="sgd-momentum">SGD + Momentum</h2>
<p>We can apply momentum for stochastic gradient descent as well</p>
<p><span class="math display">\[ \mathbf{v}^{(k+1)} \longleftarrow \beta \mathbf{v}^{(k)} + (1-\beta) \nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{x}_i, y_i), \quad i\sim \text{Uniform}(1,N)\]</span></p>
<p><span class="math display">\[ \mathbf{w}^{(k+1)} \longleftarrow \mathbf{w}^{(k)} - \alpha v^{(k+1)}\]</span></p>
<p><span class="math display">\[\nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y}) \approx \sum_{j=1}^k \beta^{k-j}(1-\beta) \nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(j)}, \mathbf{x}_{i^{(j)}}, y_{i^{(j)}})\]</span></p>
</section>
<section id="adaptive-gradients-rmsprop" class="level2">
<h2 class="anchored" data-anchor-id="adaptive-gradients-rmsprop">Adaptive gradients (RMSProp)</h2>
<p><span class="math display">\[\nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y}) = \begin{bmatrix} \frac{dL}{dw^{(k)}_1} \\ \frac{dL}{dw^{(k)}_2} \\ \vdots \end{bmatrix}\]</span></p>
<p><span class="math display">\[\begin{bmatrix} 3.1\\ 2.2 \\ \vdots \end{bmatrix} \leftarrow
\begin{bmatrix} 5.0 \\ 1.8 \\ \vdots \end{bmatrix}\leftarrow
\begin{bmatrix} 1.5 \\ 4.4 \\ \vdots \end{bmatrix}...\]</span></p>
<p><span class="math display">\[\begin{bmatrix} 10.1\\ 0.04 \\ \vdots \end{bmatrix} \leftarrow
\begin{bmatrix} 8.6 \\ 0.02 \\ \vdots \end{bmatrix}\leftarrow
\begin{bmatrix} 9.4 \\ 0.009 \\ \vdots \end{bmatrix}...\]</span></p>
<p><span class="math display">\[ \mathbf{s}^{(k+1)} \longleftarrow \beta \mathbf{s}^{(k)} + (1-\beta) (\nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y}))^2\]</span></p>
<p><span class="math display">\[ \mathbf{w}^{(k+1)} \longleftarrow \mathbf{w}^{(k)} - \alpha \frac{\nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y})}{\sqrt{\mathbf{s}^{(k+1)} + \epsilon}}\]</span> <span class="math display">\[\epsilon &lt;&lt; 1, \quad \text{e.g. } \epsilon = 1e^{-7}\]</span></p>
<p><span class="math display">\[\frac{\nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y})}{\sqrt{\mathbf{s}^{(k+1)}}} =
\begin{bmatrix} \frac{\frac{dL}{dw_1}}{\sqrt{\big(\frac{dL}{dw_1}}\big)^2} \\ \frac{\frac{dL}{dw_2}}{\sqrt{\big(\frac{dL}{dw_2}}\big)^2} \\ \vdots \end{bmatrix}  =
\begin{bmatrix} \text{sign}\big(\frac{dL}{dw_1} \big) \\ \text{sign}\big(\frac{dL}{dw_2} \big) \\ \vdots \end{bmatrix} = \begin{bmatrix} +1 \\ -1 \\ \vdots \end{bmatrix} \]</span></p>
</section>
<section id="adam" class="level2">
<h2 class="anchored" data-anchor-id="adam">Adam</h2>
<p><span class="math display">\[ \mathbf{v}^{(k+1)} \longleftarrow \beta_1 \mathbf{v}^{(k)} + (1-\beta_1) \nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y})\]</span> <span class="math display">\[ \mathbf{s}^{(k+1)} \longleftarrow \beta_2 \mathbf{s}^{(k)} + (1-\beta_2) (\nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y}))^2\]</span> <span class="math display">\[ \mathbf{w}^{(k+1)} \longleftarrow \mathbf{w}^{(k)} - \alpha \frac{\mathbf{v}^{(k+1)}
}{\sqrt{\mathbf{s}^{(k+1)} + \epsilon}}\]</span> <span class="math display">\[ \mathbf{w}^{(k+1)} \longleftarrow \mathbf{w}^{(k)} - \alpha \frac{\frac{\mathbf{v}^{(k+1)}}{(1-\beta_1^k)}
}{\sqrt{\frac{\mathbf{s}^{(k+1)}}{(1-\beta_2^k)} + \epsilon}}\]</span> <span class="math display">\[\mathbf{v}^{(0)} = \mathbf{0}, \quad \mathbf{s}^{(0)} = \mathbf{0}\]</span> <span class="math display">\[\frac{\mathbf{v}^{(k+1)}}{(1-\beta_1^k)} = \frac{\beta_1 \mathbf{0} + (1-\beta_1)\nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y})}{(1-\beta_1^1)} = \nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y})\]</span></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>