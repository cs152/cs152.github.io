<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.319">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>CS 152: Neural Networks - Lecture 2: Linear regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script type="module" src="../site_libs/quarto-ojs/quarto-ojs-runtime.js"></script>
<link href="../site_libs/quarto-ojs/quarto-ojs.css" rel="stylesheet">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">CS 152: Neural Networks</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../calendar/calendar.html" rel="" target="">
 <span class="menu-text">Calendar</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://drive.google.com/drive/folders/1JFhwmcFBTHiRbfhQ0VRDj9xIPxFpHuWj?usp=drive_link" rel="" target="">
 <span class="menu-text">Homeworks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../assignments/final-project/outline.html" rel="" target="">
 <span class="menu-text">Project</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/orgs/CS152/" rel="" target="">
 <span class="menu-text">Github</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://www.gradescope.com/courses/710173" rel="" target="">
 <span class="menu-text">Gradescope</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#linear-regression" id="toc-linear-regression" class="nav-link active" data-scroll-target="#linear-regression">Linear regression</a>
  <ul class="collapse">
  <li><a href="#functions-revisited" id="toc-functions-revisited" class="nav-link" data-scroll-target="#functions-revisited">Functions revisited</a></li>
  <li><a href="#linear-functions" id="toc-linear-functions" class="nav-link" data-scroll-target="#linear-functions">Linear Functions</a></li>
  <li><a href="#parameterized-functions" id="toc-parameterized-functions" class="nav-link" data-scroll-target="#parameterized-functions">Parameterized Functions</a></li>
  <li><a href="#handling-bias-compactly" id="toc-handling-bias-compactly" class="nav-link" data-scroll-target="#handling-bias-compactly">Handling bias compactly</a></li>
  <li><a href="#datasets-and-observations" id="toc-datasets-and-observations" class="nav-link" data-scroll-target="#datasets-and-observations">Datasets and observations</a></li>
  <li><a href="#prediction-functions" id="toc-prediction-functions" class="nav-link" data-scroll-target="#prediction-functions">Prediction functions</a></li>
  <li><a href="#linear-interpolation" id="toc-linear-interpolation" class="nav-link" data-scroll-target="#linear-interpolation">Linear interpolation</a></li>
  <li><a href="#linear-regression-1" id="toc-linear-regression-1" class="nav-link" data-scroll-target="#linear-regression-1">Linear regression</a></li>
  <li><a href="#residuals-and-error" id="toc-residuals-and-error" class="nav-link" data-scroll-target="#residuals-and-error">Residuals and error</a></li>
  <li><a href="#mean-squared-error" id="toc-mean-squared-error" class="nav-link" data-scroll-target="#mean-squared-error">Mean squared error</a></li>
  <li><a href="#loss-functions" id="toc-loss-functions" class="nav-link" data-scroll-target="#loss-functions">Loss functions</a></li>
  <li><a href="#visualizing-loss" id="toc-visualizing-loss" class="nav-link" data-scroll-target="#visualizing-loss">Visualizing loss</a></li>
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link" data-scroll-target="#gradient-descent">Gradient descent</a></li>
  <li><a href="#gradient-descent-convergence" id="toc-gradient-descent-convergence" class="nav-link" data-scroll-target="#gradient-descent-convergence">Gradient descent convergence</a></li>
  <li><a href="#step-sizes" id="toc-step-sizes" class="nav-link" data-scroll-target="#step-sizes">Step sizes</a></li>
  <li><a href="#optimizing-linear-regression" id="toc-optimizing-linear-regression" class="nav-link" data-scroll-target="#optimizing-linear-regression">Optimizing linear regression</a></li>
  <li><a href="#optimizing-linear-regression-directly" id="toc-optimizing-linear-regression-directly" class="nav-link" data-scroll-target="#optimizing-linear-regression-directly">Optimizing linear regression directly</a></li>
  </ul></li>
  <li><a href="#maximum-likelihood-estimation" id="toc-maximum-likelihood-estimation" class="nav-link" data-scroll-target="#maximum-likelihood-estimation">Maximum likelihood estimation</a>
  <ul class="collapse">
  <li><a href="#normal-distributions" id="toc-normal-distributions" class="nav-link" data-scroll-target="#normal-distributions">Normal distributions</a></li>
  <li><a href="#linear-regression-as-a-probabilistic-model" id="toc-linear-regression-as-a-probabilistic-model" class="nav-link" data-scroll-target="#linear-regression-as-a-probabilistic-model">Linear regression as a probabilistic model</a></li>
  <li><a href="#maximum-likelihood-estimation-1" id="toc-maximum-likelihood-estimation-1" class="nav-link" data-scroll-target="#maximum-likelihood-estimation-1">Maximum likelihood estimation</a></li>
  <li><a href="#section" id="toc-section" class="nav-link" data-scroll-target="#section"></a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Lecture 2: Linear regression</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="linear-regression" class="level1">
<h1>Linear regression</h1>
<section id="functions-revisited" class="level2">
<h2 class="anchored" data-anchor-id="functions-revisited">Functions revisited</h2>
<p>In the previous lecture we reviewed the concept of a <em>function</em>, which is a mapping from a set of possible inputs to a corresponding set of outputs. Here we’ll consider functions with vector inputs and scalar outputs.</p>
<p><span class="math display">\[
y=f(\mathbf{x}), \quad \text{Input: } \mathbf{x} \in\mathbb{R}^n \longrightarrow \text{ Output: }y \in\mathbb{R}
\]</span></p>
<p>Mathematically, we can easily definite a function using a sequence of basic operations.</p>
<div class="cell" data-execution_count="2">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-3-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>This function gives us the relationship between <em>inputs</em> <span class="math inline">\(\mathbf{x}\)</span> and outputs <span class="math inline">\(f(\mathbf{x})\)</span>. That is, for any given input <span class="math inline">\(x\)</span>, we can find the corresponding output <span class="math inline">\(y\)</span> by applying our function <span class="math inline">\(f(\mathbf{x})\)</span>.</p>
</section>
<section id="linear-functions" class="level2">
<h2 class="anchored" data-anchor-id="linear-functions">Linear Functions</h2>
<p>A <em>linear function</em> is any function <span class="math inline">\(f\)</span> where the following conditions always hold: <span class="math display">\[ f(\mathbf{x} + \mathbf{y}) =f(\mathbf{x}) + f(\mathbf{y})\]</span> and <span class="math display">\[ f(a\mathbf{x}) = a f(\mathbf{x})\]</span>For a linear function, the output can be defined as a weighted sum of the inputs. In other words a linear function of a vector can always be written as:</p>
<p><span class="math display">\[
f(\mathbf{x}) = \sum_{i=1}^n x_iw_i
\]</span></p>
<p>We can add an offset <span class="math inline">\(b\)</span> to create an <em>affine</em> function:</p>
<p><span class="math display">\[
f(\mathbf{x}) = \sum_{i=1}^n x_iw_i +b
\]</span></p>
<p>We can also write this using a dot-product between our input <span class="math inline">\(\mathbf{x}\)</span> and parameter vector <span class="math inline">\(\mathbf{w}\)</span> as:</p>
<p><span class="math display">\[
f(\mathbf{x}) = \mathbf{x} \cdot \mathbf{w} + b \quad \text{or} \quad f(\mathbf{x}) = \mathbf{x}^T  \mathbf{w} + b
\]</span></p>
<p>In one dimension, a linear (or affine) function is always a line, for example:</p>
<div class="cell" data-execution_count="3">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-4-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>In higher dimensions, it is a plane or hyperplane:</p>
<div class="cell" data-execution_count="4">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-5-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>In numpy we can easily write a function of this form:</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> np.array([<span class="op">-</span><span class="fl">0.6</span>, <span class="op">-</span><span class="fl">0.2</span>])</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.dot(x, w) <span class="op">+</span> b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="parameterized-functions" class="level2">
<h2 class="anchored" data-anchor-id="parameterized-functions">Parameterized Functions</h2>
<p>Linear and affine functions are examples of <strong>classes of functions</strong>, they define a general form for many different functions. Using the affine example, we see that we can define a particular function by choosing values for <span class="math inline">\(w_i\)</span> and <span class="math inline">\(b\)</span>.</p>
<p><span class="math display">\[
f(\mathbf{x}) = \sum_{i=1}^n x_iw_i +b
\]</span></p>
<p>We will refer to the values that define a function within our class (e.g.&nbsp;<span class="math inline">\(w_i\)</span> and <span class="math inline">\(b\)</span>) as the <strong>parameters</strong> of the function, by changing these values, we can change the function.</p>
<p>We typically refer to <span class="math inline">\(\mathbf{w}\)</span> specifically as the <strong>weight vector</strong> (or weights) and <span class="math inline">\(b\)</span> as the <strong>bias</strong>. To summarize:</p>
<p><span class="math display">\[
\textbf{Affine function:  }f(\mathbf{x}) = \mathbf{x}^T\mathbf{w}+b,\quad \textbf{Parameters:}\quad \big(\text{Weights:  } \mathbf{w},\ \text{Bias:  } b \big)
\]</span></p>
</section>
<section id="handling-bias-compactly" class="level2">
<h2 class="anchored" data-anchor-id="handling-bias-compactly">Handling bias compactly</h2>
<p>Notationally, it can be tedious to always write the bias term. A common approach to compactly describing linear or affine functions is to use <em>augmented</em> inputs and weights, such that for <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{w}\in \mathbb{R}^n\)</span>, we add <span class="math inline">\(x_{n+1}=1\)</span> and <span class="math inline">\(w_{n+1}=b\)</span>. So:</p>
<p><span class="math display">\[
\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} \longrightarrow \mathbf{x}_{aug}= \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \\ 1 \end{bmatrix} \quad \text{and} \quad \mathbf{w} = \begin{bmatrix} w_1 \\ w_2 \\ \vdots \\ w_n \end{bmatrix} \longrightarrow \mathbf{w}_{aug}=  \begin{bmatrix} w_1 \\ w_2 \\ \vdots \\ w_n \\ b \end{bmatrix}
\]</span></p>
<p>We can easily see then that using this notation:</p>
<p><span class="math display">\[
f(\mathbf{x}) = \mathbf{x}^T \mathbf{w} +b = \mathbf{x}_{aug}^T \mathbf{w}_{aug}
\]</span></p>
<p>This approach is common enough that we typically won’t bother with the <span class="math inline">\(aug\)</span> notation and just assume that any function defined as <span class="math inline">\(f(\mathbf{x})=\mathbf{x}^T\mathbf{w}\)</span> can be defined to include a bias implicitly. Note that in this case the function is a <em>linear function</em> of the augmented input, thus we will still typically refer to functions of this form as linear functions.</p>
<p>In numpy this is similarly straightforward:</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> np.array([<span class="op">-</span><span class="fl">0.6</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.pad(x, ((<span class="dv">0</span>,<span class="dv">1</span>),), constant_values<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.dot(x, w)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="datasets-and-observations" class="level2">
<h2 class="anchored" data-anchor-id="datasets-and-observations">Datasets and observations</h2>
<p>In the real-world we often have access to inputs and outputs in the form of <em>data</em>, but not to an actual function that we can evaluate.</p>
<p>Specifically we will say that we have access to a <strong>dataset</strong> <span class="math inline">\(\mathcal{D}\)</span> made up of <span class="math inline">\(N\)</span> pairs of inputs ( <span class="math inline">\(\mathbf{x}\)</span> ) and outputs ( <span class="math inline">\(y\)</span> ):</p>
<p><span class="math display">\[
\mathcal{D} = \{ (\mathbf{x}_1, y_1),\ (\mathbf{x}_2, y_2),\ ...\ (\mathbf{x}_N, y_N)\}
\]</span></p>
<p>We call each of these pairs an <strong>observation</strong>. Let’s take a look at a real world example of a dataset.</p>
<section id="fuel-efficiency" class="level4">
<h4 class="anchored" data-anchor-id="fuel-efficiency">Fuel efficiency</h4>
<p>Let’s imagine we’re designing a car and we would like to know what the fuel efficiency of the car we’re designing will be in <em>miles per gallon</em> (MPG). We know some properties of our current design, such as the weight and horsepower, that we know should affect the efficiency. Ideally we would have access to a function that would give us the MPG rating if we provide these <strong>features</strong>.</p>
<p><span class="math display">\[
\text{mpg} = f(\text{weight},\ \text{horsepower}...)
\]</span></p>
<p>Unfortunately we don’t know the exact relationship between a car’s features and fuel efficiency. However, we can look at other cars on the market and see what the corresponding inputs and outputs would be:</p>
<p><span class="math display">\[
\text{Honda Accord: } \begin{bmatrix} \text{Weight:} &amp; \text{2500 lbs} \\ \text{Horsepower:} &amp; \text{ 123 HP} \\ \text{Displacement:} &amp; \text{ 2.4 L} \\ \text{0-60mph:} &amp; \text{ 7.8 Sec} \end{bmatrix} \longrightarrow \text{   MPG: 33mpg}
\]</span></p>
<p><span class="math display">\[
\text{Dodge Aspen: } \begin{bmatrix} \text{Weight:} &amp; \text{3800 lbs} \\ \text{Horsepower:} &amp; \text{ 155 HP} \\ \text{Displacement:} &amp; \text{ 3.2 L} \\ \text{0-60mph:} &amp; \text{ 6.8 Sec} \end{bmatrix} \longrightarrow \text{   MPG: 21mpg}
\]</span></p>
<p><span class="math display">\[
\vdots \quad \vdots
\]</span></p>
<p>Our dataset will be this collection of data that we have for all other cars. In general, each <em>observation</em> in this dataset will correspond to a car.</p>
<p><span class="math display">\[
\text{Dataset: } \mathcal{D}=\{(\mathbf{x}_i,\ y_i) \text{  for  } i\in 1...N\}
\]</span></p>
<p><span class="math display">\[
\text{Input: } \mathbf{x}_i= \begin{bmatrix} \text{Weight} \\ \text{Horsepower} \\ \text{Displacement} \\ \text{0-60mph}  \end{bmatrix}, \quad \text{Output: } y_i = MPG
\]</span></p>
<p>Just as with a known function, we can plot the inputs vs the outputs, however in this case, we only know the outputs for the inputs we’ve seen in our dataset. Let’s take a look at a single feature: <em>the weight of a car</em>.</p>
<div class="cell" data-execution_count="7">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-8-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="prediction-functions" class="level2">
<h2 class="anchored" data-anchor-id="prediction-functions">Prediction functions</h2>
<p>Our dataset gives us a set of known inputs and outputs for our unknown functions. The central question we will address in this course is then:</p>
<section id="how-do-we-predict-the-output-for-an-input-that-we-havent-seen-before" class="level4">
<h4 class="anchored" data-anchor-id="how-do-we-predict-the-output-for-an-input-that-we-havent-seen-before"><em>How do we <strong>predict</strong> the output for an input that we haven’t seen before?</em></h4>
<p>For example, in our car scenario, we might know that the car that we’re designing will weigh 3100 lbs. In our dataset we’ve haven’t seen a car that weighs exactly 3100 lbs, so we need a way to <em>predict</em> the output of the function at input 3100 lbs.</p>
<div class="cell" data-execution_count="8">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-9-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>In general, our approach to this problem will be to <strong>model</strong> our unknown function with a known function that we can evaluate at any input. We want to chose a function <span class="math inline">\(f\)</span> such that for any observation our dataset, the output of this function <em>approximates</em> the true <strong>target</strong> output that we observed.</p>
<p><span class="math display">\[
f(\mathbf{x}_i) \approx y_i, \quad \forall (\mathbf{x}_i, y_i) \in \mathcal{D}
\]</span></p>
</section>
</section>
<section id="linear-interpolation" class="level2">
<h2 class="anchored" data-anchor-id="linear-interpolation">Linear interpolation</h2>
<p>One reasonable approach we might consider is <em>linear interpolation</em>. In this approach, we simply connect all the neighboring points with straight lines:</p>
<div class="cell" data-execution_count="9">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-10-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>In some cases this can be a reasonable approach! In fact it’s how the <code>plt.plot</code> function works. Real data however tends to be <em>messy</em>. The measurements in our dataset might not be 100% accurate or they might even conflict! What do we do if we have two observations with the same input and different outputs?</p>
<p><span class="math display">\[(\text{Weight: }3100, \text{MPG: } 34), \quad (\text{Weight: }3100, \text{MPG: } 23) \longrightarrow f(3100) = ?\]</span></p>
<p>As the size and number of features in our inputs gets larger, this become even more complex. We can see this if we try to apply interpolation to a much larger MPG dataset:</p>
<div class="cell" data-execution_count="10">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-11-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="linear-regression-1" class="level2">
<h2 class="anchored" data-anchor-id="linear-regression-1">Linear regression</h2>
<p><strong>Linear regression</strong> is the approach of modeling an unknown function with a linear function. From our discussion of linear functions, we know that this means that we will make predictions using a function of the form:</p>
<p><span class="math display">\[
f(\mathbf{x}) = \mathbf{x}^T\mathbf{w} = \sum_{i=1}^n x_i w_i
\]</span></p>
<p>Meaning that the output will be a weighted sum of the <em>features</em> of the input. In the case of our car example, we will make predictions as:</p>
<p><span class="math display">\[
\text{Predicted MPG} = f(\mathbf{x})=
\]</span></p>
<p><span class="math display">\[
(\text{weight})w_1 + (\text{horsepower})w_2 + (\text{displacement})w_3 + (\text{0-60mph})w_4 + b
\]</span></p>
<p>Or in matrix notation:</p>
<p><span class="math display">\[
f(\mathbf{x})= \begin{bmatrix} \text{Weight} \\ \text{Horsepower} \\ \text{Displacement} \\ \text{0-60mph} \\ 1 \end{bmatrix} \cdot \begin{bmatrix} w_1 \\ w_2\\ w_3 \\ w_4\\ b\end{bmatrix}
\]</span></p>
<p>We see that under this approach each <strong>weight</strong> <span class="math inline">\((w_1, w_2…)\)</span> tells us how much our prediction changes as we change the corresponding feature. For example, if we were to increase the weight of our car by 1 lb, the predicted MPG would change by <span class="math inline">\(w_1\)</span>.</p>
<p>The set of weights defines the particular linear regression function. In numpy we can define a generic class for linear regression:</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Regression:</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, weights):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> weights</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.dot(x, <span class="va">self</span>.weights)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Regression(np.array([<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>]))</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>model.predict(np.array([<span class="dv">5</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">1</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="77">
<pre><code>14</code></pre>
</div>
</div>
<p>If we again look at our plot of weight vs.&nbsp;MPG, we see we could chose many different linear functions to make predictions:</p>
<div class="cell" data-execution_count="12">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-13-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="residuals-and-error" class="level2">
<h2 class="anchored" data-anchor-id="residuals-and-error">Residuals and error</h2>
<p>The <strong>residual</strong> or <strong>error</strong> of a prediction is the difference between the prediction and the true output:</p>
<p><span class="math display">\[
e_i = y_i - f(\mathbf{x}_i)
\]</span></p>
<div class="cell" data-execution_count="13">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-14-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="mean-squared-error" class="level2">
<h2 class="anchored" data-anchor-id="mean-squared-error">Mean squared error</h2>
<p>In deciding what linear function to use, we need a measure of error for the <em>entire dataset</em>. A computationally convenient measure is <strong>mean squared error (MSE)</strong>. The mean squared error is the averaged of the error squared for each observation in our dataset:</p>
<p><span class="math display">\[
MSE = \frac{1}{N}\sum_{i=1}^N (f(\mathbf{x}_i) - y_i)^2 = \frac{1}{N}\sum_{i=1}^N (\mathbf{x}_i^T\mathbf{w} - y_i)^2
\]</span>It follows that the best choice of linear function <span class="math inline">\(f^*\)</span> is the one that <em>minimizes</em> the mean squared error for our dataset. Since each linear function is defined by a parameter vector <span class="math inline">\(\mathbf{w}\)</span>, this is equivalent to finding <span class="math inline">\(\mathbf{w}^*\)</span>, the parameters vector that minimizes the mean squared error. <span class="math display">\[\mathbf{w}^* = \underset{\mathbf{w}}{\text{argmin}} \frac{1}{N}\sum_{i=1}^N (\mathbf{x}_i^T\mathbf{w} - y_i)^2 \]</span></p>
</section>
<section id="loss-functions" class="level2">
<h2 class="anchored" data-anchor-id="loss-functions">Loss functions</h2>
<p>Note that the mean squared error depends on the data inputs <span class="math inline">\((\mathbf{x}_1,…,\mathbf{x}_N)\)</span>, the data targets <span class="math inline">\((y_1,…,y_N)\)</span> <em>and</em> the parameters <span class="math inline">\((\mathbf{w})\)</span>. So we can express the MSE as a <em>function</em> of all three:</p>
<p><span class="math display">\[
MSE(\mathbf{w}, \mathbf{X}, \mathbf{y}) = \frac{1}{N}\sum_{i=1}^N (\mathbf{x}_i^T\mathbf{w} - y_i)^2
\]</span></p>
<p>Here we have used <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> to refer to the entire collection of inputs and outputs from our dataset <span class="math inline">\((\mathcal{D})\)</span> respectively, so:</p>
<p><span class="math display">\[
\mathbf{X} = \begin{bmatrix}\mathbf{x}_1 \\ \mathbf{x}_2 \\ \vdots \\ \mathbf{x}_N \end{bmatrix} = \begin{bmatrix}x_{11} &amp; x_{12} &amp; \dots &amp; x_{1n} \\ x_{21} &amp; x_{22} &amp; \dots &amp; x_{2n}\\ \vdots &amp; \vdots  &amp; \ddots &amp; \vdots \\ x_{N1} &amp; x_{N2} &amp; \dots &amp; x_{Nn} \end{bmatrix}, \quad \mathbf{y} = \begin{bmatrix}y_1 \\ y_2 \\ \vdots \\ y_N \end{bmatrix}
\]</span></p>
<p>This is an example of <strong>loss function</strong>, for our given dataset this function tells us how much error (loss) we are incurring for a given choice of <span class="math inline">\(\mathbf{w}\)</span>. If we assume our dataset is fixed we can drop the explicit dependence on <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>, looking at the loss as purely a function of our choice of parameters:</p>
<p><span class="math display">\[
\textbf{Loss}(\mathbf{w})= MSE(\mathbf{w}, \mathbf{X}, \mathbf{y}) = \frac{1}{N}\sum_{i=1}^N (\mathbf{x}_i^T\mathbf{w} - y_i)^2
\]</span></p>
<p>Again, if our goal is to minimize error, we want to choose the parameters <span class="math inline">\(\mathbf{w}^*\)</span> that <em>minimize</em> this loss:</p>
<p><span class="math display">\[
\mathbf{w}^* = \underset{\mathbf{w}}{\text{argmin}}\ \textbf{Loss}(\mathbf{w})= \underset{\mathbf{w}}{\text{argmin}} \frac{1}{N}\sum_{i=1}^N (\mathbf{x}_i^T\mathbf{w} - y_i)^2
\]</span></p>
</section>
<section id="visualizing-loss" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-loss">Visualizing loss</h2>
<p>If we consider the case where our inputs are 1-dimensional, as in the weight example above, then our parameter vector <span class="math inline">\(\mathbf{w}\)</span> only has 2 entries: <span class="math inline">\(w_1\)</span> and <span class="math inline">\(b\)</span>. In this case, we can actually plot our loss function directly!</p>
<div class="cell" data-execution_count="14">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-15-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="15">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-16-output-1.png" width="558" height="428"></p>
</div>
</div>
<p>We see that point where the loss is lowest, corresponds to the line that best fits our data!</p>
</section>
<section id="gradient-descent" class="level2">
<h2 class="anchored" data-anchor-id="gradient-descent">Gradient descent</h2>
<p>Now that we have a way to determine the quality of a choice of parameters <span class="math inline">\(\mathbf{w}\)</span>, using our <em>loss</em> function, we need a way to actually find the <span class="math inline">\(\mathbf{w}^*\)</span> that minimizes our loss. To do this we will turn to an algorithm called <strong>gradient descent</strong>. In this lecture we will introduce gradient descent, but we will go into much more depth in a future lecture.</p>
<p>We’ll introduce gradient descent as a method to find the minimum of a generic function. We have some function <span class="math inline">\(f(\mathbf{\cdot})\)</span> and we would like find the input <span class="math inline">\(\mathbf{w}^*\)</span> that minimizes the output of the function:</p>
<p><span class="math display">\[
\text{Find: } \mathbf{w}^* = \underset{\mathbf{w}}{\text{argmin}}\ f(\mathbf{w})
\]</span></p>
<p>We don’t know how to find <span class="math inline">\(\mathbf{w}^*\)</span> directly, but if we have an initial guess <span class="math inline">\(\mathbf{w}^{(0)}\)</span>, we can try to update our guess to improve it.</p>
<p><span class="math display">\[
\mathbf{w}^{(1)} \leftarrow \mathbf{w}^{(0)} + \mathbf{g}
\]</span></p>
<div class="cell" data-execution_count="16">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-17-output-1.png" width="558" height="428"></p>
</div>
</div>
<p>Here we are changing <span class="math inline">\(\mathbf{w}^{(0)}\)</span> by moving in the direction of <span class="math inline">\(\mathbf{g}\)</span>. If we recall that the <em>gradient</em> of a function at point <span class="math inline">\(\mathbf{x}\)</span> corresponds to the <em>slope</em> of <span class="math inline">\(f\)</span> at <span class="math inline">\(\mathbf{w}\)</span>, or equivalently the direction of maximum change. This gives us a natural choice for the update to our guess.</p>
<p><span class="math display">\[
\mathbf{w}^{(1)} \leftarrow \mathbf{w}^{(0)} - \nabla f(\mathbf{w}^{(0)})
\]</span></p>
<p>Note that because the gradient corresponds to the direction that maximally <em>increases</em> <span class="math inline">\(f(\mathbf{w})\)</span>, we actually need to subtract the gradient in order to minimize our function. We can repeat this process many times, continuously updating our estimate.</p>
<p><span class="math display">\[
\text{For }i \text{ in 1,...,T}\text{ :} \\
\quad \mathbf{w}^{(i+1)} \leftarrow \mathbf{w}^{(i)} - \nabla f(\mathbf{w}^{(i)})
\]</span></p>
<div class="cell" data-execution_count="17">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-18-output-1.png" width="558" height="428"></p>
</div>
</div>
</section>
<section id="gradient-descent-convergence" class="level2">
<h2 class="anchored" data-anchor-id="gradient-descent-convergence">Gradient descent convergence</h2>
<p>Recall that it’s minimum value <span class="math inline">\(\mathbf{w}^*\)</span>, a function <span class="math inline">\(f\)</span> <em>must</em> have a gradient of <span class="math inline">\(\mathbf{0}\)</span>.</p>
<p><span class="math display">\[
\nabla f(\mathbf{w}^*) = \mathbf{0}
\]</span></p>
<p>It follows that:</p>
<p><span class="math display">\[
\mathbf{w}^{*} = \mathbf{w}^{*} - \nabla f(\mathbf{w}^{*})
\]</span></p>
<p>This means that if our gradient descent reaches the minimum, it will stop updating the guess and we know that we can stop our iteration. So we could write our algorithm to account for this:</p>
<p><span class="math display">\[
\text{While } \nabla f(\mathbf{w}^{(i)}) \neq \mathbf{0} \text{ :} \\
\quad \mathbf{w}^{(i+1)} \leftarrow \mathbf{w}^{(i)} - \nabla f(\mathbf{w}^{(i)})
\]</span></p>
<p>In practice though, it could take infinitely many updates to find the <em>exact</em> minimum. A more common approach is to define a convergence criteria that stops the iteration when the gradient magnitude is sufficiently small:</p>
<p><span class="math display">\[
\text{While } ||\nabla f(\mathbf{w}^{(i)})||_2 &gt; \epsilon \text{ :} \\
\quad \mathbf{w}^{(i+1)} \leftarrow \mathbf{w}^{(i)} - \nabla f(\mathbf{w}^{(i)})
\]</span></p>
</section>
<section id="step-sizes" class="level2">
<h2 class="anchored" data-anchor-id="step-sizes">Step sizes</h2>
<p>Notice that the gradient descent algorithm we’ve defined so far not only says that we want to update our guess in the <em>direction</em> of the gradient, it also say that we want to move in that direction a distance equal to the <em>magnitude</em> of the gradient. It turns out this is often a very bad idea!</p>
<p>This approach says that when the magnitude of the gradient is large, we should take a large step, and vice-versa. This is desirable for many functions as it means when we’re far from the minimum we take large steps, moving toward the minimum more quickly. While when we’re close to the minimum we take small steps to refine our guess more precisely.</p>
<div class="cell" data-execution_count="18">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-19-output-1.png" width="558" height="428"></p>
</div>
</div>
<p>However, if we take too large a step, we can overshoot the minimum entirely! In the worst case, this can lead to <em>divergence,</em> where gradient descent overshoots the minimum more and more at each step.</p>
<div class="cell" data-execution_count="19">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-20-output-1.png" width="558" height="428"></p>
</div>
</div>
<p>Remember that the gradient is making a <em>linear approximation</em> to the function. A strait line has no minimum, so the gradient has no information about where along the approximation the true minimum will be.</p>
<div class="cell" data-execution_count="20">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-21-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Also remember that the gradient gives us the direction of maximum change of the function, but this is only true in the <em>limit</em> of a very small step.</p>
<p><span class="math display">\[
\frac{df}{d\mathbf{w}}= \underset{\gamma \rightarrow 0}{\lim}\ \underset{\|\mathbf{\epsilon}\|_2 &lt; \gamma}{\max} \frac{f(\mathbf{w} + \mathbf{\epsilon}) - f(\mathbf{w})}{\|\mathbf{\epsilon}\|_2}
\]</span></p>
<p>So in higher dimensions, the gradient may not point directly to the minimum.</p>
<p>All of these issues motivate the need to control the size of our updates. We will typically do this by introducing an additional control to our algorithm: a <strong>step size</strong> or <strong>learning rate</strong>. This is a small constant <span class="math inline">\(\alpha\)</span>, that we will multiply the gradient by in each of our updates.</p>
<p><span class="math display">\[
\mathbf{w}^{(i+1)} \leftarrow \mathbf{w}^{(i)} - \alpha \nabla f(\mathbf{w}^{(i)})
\]</span></p>
<p>Using a small learning rate <span class="math inline">\((\alpha &lt;&lt; 1)\)</span> will make gradient descent slower, but <em>much</em> more reliable. Later on in the semester we will explore how to choose <span class="math inline">\(\alpha\)</span> (and even update it during optimization).</p>
<div class="cell" data-execution_count="21">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-22-output-1.png" width="558" height="427"></p>
</div>
</div>
<div class="cell" data-execution_count="22">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-23-output-1.png" width="558" height="428"></p>
</div>
</div>
</section>
<section id="optimizing-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="optimizing-linear-regression">Optimizing linear regression</h2>
<p>We can apply gradient descent to linear regression in order to find the parameters that minimize the mean squared error loss. To do this we need to find the gradient of the mean squared error with respect to the parameters:</p>
<p><span class="math display">\[
\nabla_{\mathbf{w}} \textbf{MSE}(\mathbf{w}, \mathbf{X}, \mathbf{y}) =
\frac{d}{d\mathbf{w}}\bigg( \frac{1}{N}\sum_{i=1}^N (\mathbf{x}_i^T\mathbf{w} - y_i)^2 \bigg)
\]</span></p>
<p><span class="math display">\[
= \frac{2}{N}\sum_{i=1}^N (\mathbf{x}_i^T\mathbf{w} - y_i)\mathbf{x}_i
\]</span></p>
<p>With this gradient our gradient descent update becomes:</p>
<p><span class="math display">\[
\mathbf{w}^{(i+1)} \longleftarrow \mathbf{w}^{(i)} - \alpha\bigg(\frac{2 }{N}\bigg)\sum_{i=1}^N  (\mathbf{x}_i^T\mathbf{w}^{(i)} - y_i)\mathbf{x}_i
\]</span></p>
<p>We can see that this update is a sum of all the inputs weighted by their corresponding residual given the current value of the parameters.</p>
<p>We can see how the the parameters of our regression model change as we run gradient descent.</p>
</section>
<section id="optimizing-linear-regression-directly" class="level2">
<h2 class="anchored" data-anchor-id="optimizing-linear-regression-directly">Optimizing linear regression directly</h2>
<p>Gradient descent works well for finding the optimal parameters for a linear regression model, but in fact we can actually find the optimal set of parameters directly, without needing to run an iterative algorithm.</p>
<p>We know that at the minimum, the gradient must be <span class="math inline">\(\mathbf{0}\)</span>, so the following condition must hold:</p>
<p><span class="math display">\[
\mathbf{0} = \bigg( \frac{2}{N}\bigg)\sum_{i=1}^N  (\mathbf{x}_i^T\mathbf{w} - y_i)\mathbf{x}_i
\]</span></p>
<p>We can solve for a <span class="math inline">\(\mathbf{w}\)</span> that satisfied this condition by first dropping the constant <span class="math inline">\(\frac{2}{N}\)</span>.</p>
<p><span class="math display">\[
\mathbf{0} = \sum_{i=1}^N  (\mathbf{x}_i^T\mathbf{w} - y_i)\mathbf{x}_i
\]</span></p>
<p><span class="math display">\[
\mathbf{0} = \sum_{i=1}^N \big( \mathbf{x}_i\mathbf{x}_i^T\mathbf{w} - y_i \mathbf{x}_i \big)
\]</span></p>
<p><span class="math display">\[
\sum_{i=1}^N  y_i \mathbf{x}_i  =\bigg(\sum_{i=1}^N  \mathbf{x}_i\mathbf{x}_i^T \bigg)  \mathbf{w}
\]</span></p>
<p>Note that <span class="math inline">\(\mathbf{x}_i \mathbf{x}_i^T\)</span> is a vector <em>outer product</em>:</p>
<p><span class="math display">\[
\mathbf{x}_i \mathbf{x}_i^T = \begin{bmatrix} x_{i1} \\ x_{i2} \\ \vdots \\  x_{in}\end{bmatrix} \begin{bmatrix} x_{i1} &amp; x_{i2} &amp; \dots &amp;  x_{in}\end{bmatrix} =
\begin{bmatrix} x_{i1} x_{i1} &amp; x_{i1} x_{i2} &amp; \dots &amp; x_{i1} x_{in} \\
x_{i2} x_{i1} &amp; x_{i2} x_{i2} &amp; \dots &amp; x_{i2} x_{in} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{in} x_{i1} &amp; x_{in} x_{i2} &amp; \dots &amp; x_{in} x_{in} \\
\end{bmatrix}
\]</span></p>
<p>Thus <span class="math inline">\(\bigg(\sum_{i=1}^N \mathbf{x}_i\mathbf{x}_i^T \bigg)\)</span> is a matrix. Multiplying both sides by the inverse <span class="math inline">\(\bigg(\sum_{i=1}^N \mathbf{x}_i\mathbf{x}_i^T \bigg)^{-1}\)</span> we get:</p>
<p><span class="math display">\[
\bigg(\sum_{i=1}^N  \mathbf{x}_i\mathbf{x}_i^T \bigg)^{-1} \bigg(\sum_{i=1}^N  y_i \mathbf{x}_i\bigg)  =  \mathbf{w}^*
\]</span></p>
<p>We can write this more compactly using the stacked input matrix and label vector notation we saw in homework 1.</p>
<p><span class="math display">\[
\mathbf{X} = \begin{bmatrix} \mathbf{x}_{1} \\ \mathbf{x}_{2} \\ \vdots \\  \mathbf{x}_{N} \end{bmatrix},\quad \mathbf{y} = \begin{bmatrix} y_{1} \\ y_{2} \\ \vdots \\  y_{N} \end{bmatrix}
\]</span></p>
<p>In this case, the expression becomes:</p>
<p><span class="math display">\[\mathbf{w}^* = \big( \mathbf{X}^T \mathbf{X} \big)^{-1} \big(\mathbf{y}\mathbf{X}\big)\]</span></p>
</section>
</section>
<section id="maximum-likelihood-estimation" class="level1 page-columns page-full">
<h1>Maximum likelihood estimation</h1>
<p>Let’s now take a look a linear regression from a slightly different perspective: the probabilistic view. We’ll get to the exact same approach, but with a motivation guided by statistics.</p>
<section id="normal-distributions" class="level2">
<h2 class="anchored" data-anchor-id="normal-distributions">Normal distributions</h2>
<p>The <strong>Normal</strong> distribution (also known as the <strong>Gaussian</strong> distribution) is a continuous probability distribution with the following probability density function:</p>
<p><span class="math display">\[
p(y) = \frac{1}{\sigma \sqrt{2 \pi}} \text{exp}\bigg(-\frac{1}{2\sigma^2} (y -\mu)^2\bigg)
\]</span></p>
<p>The normal distribution shows up almost everywhere in probability and statistics. Most notably, the <em>central limit theorem</em> tells us that the mean of many independent and identically distributed random outcomes tends towards a normal distribution.</p>
</section>
<section id="linear-regression-as-a-probabilistic-model" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="linear-regression-as-a-probabilistic-model">Linear regression as a probabilistic model</h2>
<p>Our function approximation view of linear regression says that we can <em>approximate</em> an unknown function with a linear function. An alternate approach is to define a <em>distribution</em> over the output <span class="math inline">\(y_i\)</span> of our unknown function given an input <span class="math inline">\(\mathbf{x}_i\)</span>. In particular, the probabilistic model for linear regression will make the assumption that the output is <em>normally distributed</em> conditioned on the input:</p>
<p><span class="math display">\[
y_i \sim \mathcal{N}\big(\mathbf{x}_i^T \mathbf{w},\ \sigma^2\big)
\]</span></p>
<p>Here we see the assumption we’re making is that the <em>mean</em> of the distribution is a linear function of the input, while the variance is fixed. Under this model, we can write the conditional probability or <strong>likelihood</strong> of an output as:</p>
<p><span class="math display">\[
p(y_i\mid\mathbf{x}_i, \mathbf{w}) =  \frac{1}{\sigma \sqrt{2 \pi}} \text{exp}\bigg(-\frac{1}{2\sigma^2} (y_i - \mathbf{x}_i^T\mathbf{w})^2\bigg)
\]</span></p>
<p>Why view linear regression as a probabilistic model? Well, generally for real data we can’t know if there actually <em>is</em> a function that perfectly maps inputs to outputs. It could be that there are variables we’re not accounting for, that there errors in our measurements for the data we collected or simply that there is some inherent randomness in the outputs. This view of linear regression makes the uncertainty in our predictions explicit.</p>
<div class="cell">
<div class="sourceCode cell-code hidden" id="cb5" data-startfrom="2" data-source-offset="-0"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 1;"><span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>MathJax <span class="op">=</span> {</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>  <span class="kw">const</span> MathJax <span class="op">=</span> <span class="cf">await</span> <span class="pp">require</span>(<span class="st">'mathjax@2.7.5/MathJax.js?config=TeX-MML-AM_CHTML'</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="op">.</span><span class="fu">catch</span>(() <span class="kw">=&gt;</span> <span class="bu">window</span><span class="op">.</span><span class="at">MathJax</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>  <span class="co">// configure MathJax</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>  MathJax<span class="op">.</span><span class="at">Hub</span><span class="op">.</span><span class="fu">Config</span>({</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="dt">tex2jax</span><span class="op">:</span> {<span class="dt">inlineMath</span><span class="op">:</span> [[<span class="st">'$'</span><span class="op">,</span><span class="st">'$'</span>]<span class="op">,</span> [<span class="st">'</span><span class="sc">\\</span><span class="st">('</span><span class="op">,</span><span class="st">'</span><span class="sc">\\</span><span class="st">)'</span>]]}<span class="op">,</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="dt">displayMath</span><span class="op">:</span> [ [<span class="st">'$$'</span><span class="op">,</span><span class="st">'$$'</span>]<span class="op">,</span> [<span class="st">"</span><span class="sc">\\</span><span class="st">["</span><span class="op">,</span><span class="st">"</span><span class="sc">\\</span><span class="st">]"</span>] ]<span class="op">,</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="dt">processEscapes</span><span class="op">:</span> <span class="kw">true</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>  })  </span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> MathJax</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>Plotly <span class="op">=</span> <span class="pp">require</span>(<span class="st">"https://cdn.plot.ly/plotly-latest.min.js"</span>)<span class="op">;</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>tfbase <span class="op">=</span> <span class="pp">require</span>(<span class="st">'@tensorflow/tfjs@4.11.0'</span>)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>pyodide <span class="op">=</span> {</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>  <span class="kw">const</span> p <span class="op">=</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">await</span> <span class="pp">require</span>(<span class="st">"https://cdn.jsdelivr.net/pyodide/v0.22.1/full/pyodide.js"</span>)<span class="op">;</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>  <span class="bu">console</span><span class="op">.</span><span class="fu">log</span>(p)<span class="op">;</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> p<span class="op">.</span><span class="fu">loadPyodide</span>()<span class="op">;</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>PyScope <span class="op">=</span> <span class="kw">function</span>() {</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> scope <span class="op">=</span> pyodide<span class="op">.</span><span class="fu">toPy</span>({})<span class="op">;</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> py <span class="op">=</span> <span class="kw">async</span> (strings<span class="op">,</span> <span class="op">...</span>expressions) <span class="kw">=&gt;</span> {</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> globals <span class="op">=</span> {}<span class="op">;</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">const</span> code <span class="op">=</span> strings<span class="op">.</span><span class="fu">reduce</span>((result<span class="op">,</span> string<span class="op">,</span> index) <span class="kw">=&gt;</span> {</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> (expressions[index]) {</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>        <span class="kw">const</span> name <span class="op">=</span> <span class="vs">`x</span><span class="sc">${</span>index<span class="sc">}</span><span class="vs">`</span><span class="op">;</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>        globals[name] <span class="op">=</span> expressions[index]<span class="op">;</span></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> result <span class="op">+</span> string <span class="op">+</span> name<span class="op">;</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>      }</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>      <span class="cf">return</span> result <span class="op">+</span> string<span class="op">;</span></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>    }<span class="op">,</span> <span class="st">''</span>)<span class="op">;</span></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">await</span> pyodide<span class="op">.</span><span class="fu">loadPackagesFromImports</span>(code)<span class="op">;</span></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>    scope<span class="op">.</span><span class="fu">update</span>(pyodide<span class="op">.</span><span class="at">globals</span>)<span class="op">;</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>    <span class="kw">const</span> result <span class="op">=</span> <span class="cf">await</span> pyodide<span class="op">.</span><span class="fu">runPythonAsync</span>(</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>      code<span class="op">,</span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>      {</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>        <span class="dt">globals</span><span class="op">:</span> scope</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>      }</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>    )<span class="op">;</span></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (result<span class="op">?.</span><span class="at">t2Js</span>) <span class="cf">return</span> result<span class="op">.</span><span class="fu">t2Js</span>()<span class="op">;</span></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (result<span class="op">?.</span><span class="at">toJs</span>) <span class="cf">return</span> result<span class="op">.</span><span class="fu">toJs</span>()<span class="op">;</span></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> result<span class="op">;</span></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>  }<span class="op">;</span></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> py<span class="op">;</span></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>py <span class="op">=</span> {</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> testscope <span class="op">=</span> <span class="fu">PyScope</span>()<span class="op">;</span></span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> py <span class="op">=</span> <span class="kw">async</span> (strings<span class="op">,</span> <span class="op">...</span>expressions) <span class="kw">=&gt;</span> {</span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> globals <span class="op">=</span> {}<span class="op">;</span></span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>    <span class="kw">const</span> code <span class="op">=</span> strings<span class="op">.</span><span class="fu">reduce</span>((result<span class="op">,</span> string<span class="op">,</span> index) <span class="kw">=&gt;</span> {</span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> (expressions[index]) {</span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a>        <span class="kw">const</span> name <span class="op">=</span> <span class="vs">`x</span><span class="sc">${</span>index<span class="sc">}</span><span class="vs">`</span><span class="op">;</span></span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>        globals[name] <span class="op">=</span> expressions[index]<span class="op">;</span></span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> result <span class="op">+</span> string <span class="op">+</span> name<span class="op">;</span></span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a>      }</span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a>      <span class="cf">return</span> result <span class="op">+</span> string<span class="op">;</span></span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a>    }<span class="op">,</span> <span class="st">''</span>)<span class="op">;</span></span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a>    <span class="cf">await</span> pyodide<span class="op">.</span><span class="fu">loadPackagesFromImports</span>(code)<span class="op">;</span></span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a>    pyodide<span class="op">.</span><span class="at">globals</span><span class="op">.</span><span class="fu">update</span>(pyodide<span class="op">.</span><span class="fu">toPy</span>(globals))</span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a>    <span class="kw">const</span> result <span class="op">=</span> <span class="cf">await</span> pyodide<span class="op">.</span><span class="fu">runPythonAsync</span>(</span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a>      code<span class="op">,</span></span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a>      {<span class="dt">globals</span><span class="op">:</span> pyodide<span class="op">.</span><span class="at">globals</span>}</span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a>    )<span class="op">;</span></span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (result<span class="op">?.</span><span class="at">t2Js</span>) <span class="cf">return</span> result<span class="op">.</span><span class="fu">t2Js</span>()<span class="op">;</span></span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (result<span class="op">?.</span><span class="at">toJs</span>) <span class="cf">return</span> result<span class="op">.</span><span class="fu">toJs</span>()<span class="op">;</span></span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> result<span class="op">;</span></span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a>  }<span class="op">;</span></span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a>  <span class="kw">const</span> sigmoidGradConfig  <span class="op">=</span> {</span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a>    <span class="dt">kernelName</span><span class="op">:</span> tfbase<span class="op">.</span><span class="at">Sigmoid</span><span class="op">,</span></span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a>    <span class="dt">inputsToSave</span><span class="op">:</span> [<span class="st">'x'</span>]<span class="op">,</span></span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a>    <span class="dt">gradFunc</span><span class="op">:</span> (dy<span class="op">,</span> saved) <span class="kw">=&gt;</span> {</span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a>      <span class="kw">const</span> [x] <span class="op">=</span> saved<span class="op">;</span></span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a>      <span class="kw">const</span> y <span class="op">=</span> tfbase<span class="op">.</span><span class="fu">sigmoid</span>(x)<span class="op">;</span></span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a>      <span class="cf">return</span> {<span class="dt">x</span><span class="op">:</span> () <span class="kw">=&gt;</span> tfbase<span class="op">.</span><span class="fu">mul</span>(dy<span class="op">,</span> tfbase<span class="op">.</span><span class="fu">mul</span>(y<span class="op">,</span> tfbase<span class="op">.</span><span class="fu">sub</span>(tfbase<span class="op">.</span><span class="fu">scalar</span>(<span class="dv">1</span>)<span class="op">,</span> y)))}<span class="op">;</span></span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a>  }<span class="op">;</span></span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a>  tfbase<span class="op">.</span><span class="fu">registerGradient</span>(sigmoidGradConfig)<span class="op">;</span></span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a>  <span class="kw">const</span> tanhGradConfig <span class="op">=</span> {</span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a>    <span class="dt">kernelName</span><span class="op">:</span> tfbase<span class="op">.</span><span class="at">Tanh</span><span class="op">,</span></span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true" tabindex="-1"></a>    <span class="dt">inputsToSave</span><span class="op">:</span> [<span class="st">'x'</span>]<span class="op">,</span></span>
<span id="cb5-90"><a href="#cb5-90" aria-hidden="true" tabindex="-1"></a>    <span class="dt">gradFunc</span><span class="op">:</span> (dy<span class="op">,</span> saved) <span class="kw">=&gt;</span> {</span>
<span id="cb5-91"><a href="#cb5-91" aria-hidden="true" tabindex="-1"></a>      <span class="kw">const</span> [x] <span class="op">=</span> saved<span class="op">;</span></span>
<span id="cb5-92"><a href="#cb5-92" aria-hidden="true" tabindex="-1"></a>      <span class="kw">const</span> y <span class="op">=</span> tfbase<span class="op">.</span><span class="fu">tanh</span>(x)<span class="op">;</span></span>
<span id="cb5-93"><a href="#cb5-93" aria-hidden="true" tabindex="-1"></a>      <span class="cf">return</span> {<span class="dt">x</span><span class="op">:</span> () <span class="kw">=&gt;</span> tfbase<span class="op">.</span><span class="fu">mul</span>(tfbase<span class="op">.</span><span class="fu">sub</span>(tfbase<span class="op">.</span><span class="fu">scalar</span>(<span class="dv">1</span>)<span class="op">,</span> tfbase<span class="op">.</span><span class="fu">square</span>(y))<span class="op">,</span> dy)}<span class="op">;</span></span>
<span id="cb5-94"><a href="#cb5-94" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb5-95"><a href="#cb5-95" aria-hidden="true" tabindex="-1"></a>  }<span class="op">;</span></span>
<span id="cb5-96"><a href="#cb5-96" aria-hidden="true" tabindex="-1"></a>  tfbase<span class="op">.</span><span class="fu">registerGradient</span>(tanhGradConfig)<span class="op">;</span></span>
<span id="cb5-97"><a href="#cb5-97" aria-hidden="true" tabindex="-1"></a>   <span class="kw">const</span> expGradConfig <span class="op">=</span> {</span>
<span id="cb5-98"><a href="#cb5-98" aria-hidden="true" tabindex="-1"></a>    <span class="dt">kernelName</span><span class="op">:</span> tfbase<span class="op">.</span><span class="at">Exp</span><span class="op">,</span></span>
<span id="cb5-99"><a href="#cb5-99" aria-hidden="true" tabindex="-1"></a>    <span class="dt">inputsToSave</span><span class="op">:</span> [<span class="st">'x'</span>]<span class="op">,</span></span>
<span id="cb5-100"><a href="#cb5-100" aria-hidden="true" tabindex="-1"></a>    <span class="dt">gradFunc</span><span class="op">:</span> (dy<span class="op">,</span> saved) <span class="kw">=&gt;</span> {</span>
<span id="cb5-101"><a href="#cb5-101" aria-hidden="true" tabindex="-1"></a>      <span class="kw">const</span> [x] <span class="op">=</span> saved<span class="op">;</span></span>
<span id="cb5-102"><a href="#cb5-102" aria-hidden="true" tabindex="-1"></a>      <span class="kw">const</span> y <span class="op">=</span> tfbase<span class="op">.</span><span class="fu">exp</span>(x)<span class="op">;</span></span>
<span id="cb5-103"><a href="#cb5-103" aria-hidden="true" tabindex="-1"></a>      <span class="cf">return</span> {<span class="dt">x</span><span class="op">:</span> () <span class="kw">=&gt;</span> tfbase<span class="op">.</span><span class="fu">mul</span>(dy<span class="op">,</span> y)}<span class="op">;</span></span>
<span id="cb5-104"><a href="#cb5-104" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb5-105"><a href="#cb5-105" aria-hidden="true" tabindex="-1"></a>  }<span class="op">;</span> </span>
<span id="cb5-106"><a href="#cb5-106" aria-hidden="true" tabindex="-1"></a>  tfbase<span class="op">.</span><span class="fu">registerGradient</span>(expGradConfig)<span class="op">;</span></span>
<span id="cb5-107"><a href="#cb5-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-108"><a href="#cb5-108" aria-hidden="true" tabindex="-1"></a>  <span class="kw">function</span> <span class="fu">dispatchEvent</span>(element){</span>
<span id="cb5-109"><a href="#cb5-109" aria-hidden="true" tabindex="-1"></a>    element<span class="op">.</span><span class="fu">dispatchEvent</span>(<span class="kw">new</span> <span class="bu">Event</span>(<span class="st">"input"</span><span class="op">,</span> {<span class="dt">bubbles</span><span class="op">:</span> <span class="kw">true</span>}))<span class="op">;</span></span>
<span id="cb5-110"><a href="#cb5-110" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb5-111"><a href="#cb5-111" aria-hidden="true" tabindex="-1"></a>  pyodide<span class="op">.</span><span class="at">globals</span><span class="op">.</span><span class="fu">update</span>(pyodide<span class="op">.</span><span class="fu">toPy</span>({<span class="dt">Plotbase</span><span class="op">:</span> Plot<span class="op">,</span> <span class="dt">tfbase</span><span class="op">:</span> tfbase<span class="op">,</span> <span class="dt">Plotlybase</span><span class="op">:</span> Plotly<span class="op">,</span> <span class="dt">dispatchEvent</span><span class="op">:</span> dispatchEvent<span class="op">,</span> <span class="dt">d3base</span><span class="op">:</span> d3}))</span>
<span id="cb5-112"><a href="#cb5-112" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb5-113"><a href="#cb5-113" aria-hidden="true" tabindex="-1"></a><span class="cf">await</span> <span class="fu">py</span><span class="vs">`</span></span>
<span id="cb5-114"><a href="#cb5-114" aria-hidden="true" tabindex="-1"></a><span class="vs">from pyodide.ffi import create_once_callable</span></span>
<span id="cb5-115"><a href="#cb5-115" aria-hidden="true" tabindex="-1"></a><span class="vs">from types import SimpleNamespace</span></span>
<span id="cb5-116"><a href="#cb5-116" aria-hidden="true" tabindex="-1"></a><span class="vs">from pyodide.ffi import to_js</span></span>
<span id="cb5-117"><a href="#cb5-117" aria-hidden="true" tabindex="-1"></a><span class="vs">from js import Object, document</span></span>
<span id="cb5-118"><a href="#cb5-118" aria-hidden="true" tabindex="-1"></a><span class="vs">import pandas</span></span>
<span id="cb5-119"><a href="#cb5-119" aria-hidden="true" tabindex="-1"></a><span class="vs">import numpy as np</span></span>
<span id="cb5-120"><a href="#cb5-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-121"><a href="#cb5-121" aria-hidden="true" tabindex="-1"></a><span class="vs">tfbase = SimpleNamespace(**tfbase)</span></span>
<span id="cb5-122"><a href="#cb5-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-123"><a href="#cb5-123" aria-hidden="true" tabindex="-1"></a><span class="vs">def convert_tensor(a, *args):</span></span>
<span id="cb5-124"><a href="#cb5-124" aria-hidden="true" tabindex="-1"></a><span class="vs">  if isinstance(a, Parameter):</span></span>
<span id="cb5-125"><a href="#cb5-125" aria-hidden="true" tabindex="-1"></a><span class="vs">    a = a.value.value</span></span>
<span id="cb5-126"><a href="#cb5-126" aria-hidden="true" tabindex="-1"></a><span class="vs">  if isinstance(a, Tensor):</span></span>
<span id="cb5-127"><a href="#cb5-127" aria-hidden="true" tabindex="-1"></a><span class="vs">    a = a.value</span></span>
<span id="cb5-128"><a href="#cb5-128" aria-hidden="true" tabindex="-1"></a><span class="vs">  if isinstance(a, np.ndarray):</span></span>
<span id="cb5-129"><a href="#cb5-129" aria-hidden="true" tabindex="-1"></a><span class="vs">    a = a.tolist()</span></span>
<span id="cb5-130"><a href="#cb5-130" aria-hidden="true" tabindex="-1"></a><span class="vs">  return to_js(a)</span></span>
<span id="cb5-131"><a href="#cb5-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-132"><a href="#cb5-132" aria-hidden="true" tabindex="-1"></a><span class="vs">def convert(a):</span></span>
<span id="cb5-133"><a href="#cb5-133" aria-hidden="true" tabindex="-1"></a><span class="vs">  if isinstance(a, Parameter):</span></span>
<span id="cb5-134"><a href="#cb5-134" aria-hidden="true" tabindex="-1"></a><span class="vs">    a = a.value.value</span></span>
<span id="cb5-135"><a href="#cb5-135" aria-hidden="true" tabindex="-1"></a><span class="vs">  if isinstance(a, Tensor):</span></span>
<span id="cb5-136"><a href="#cb5-136" aria-hidden="true" tabindex="-1"></a><span class="vs">    a = a.value</span></span>
<span id="cb5-137"><a href="#cb5-137" aria-hidden="true" tabindex="-1"></a><span class="vs">  if isinstance(a, np.ndarray):</span></span>
<span id="cb5-138"><a href="#cb5-138" aria-hidden="true" tabindex="-1"></a><span class="vs">    a = a.tolist()</span></span>
<span id="cb5-139"><a href="#cb5-139" aria-hidden="true" tabindex="-1"></a><span class="vs">  return to_js(a, dict_converter=Object.fromEntries, default_converter=convert_tensor)</span></span>
<span id="cb5-140"><a href="#cb5-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-141"><a href="#cb5-141" aria-hidden="true" tabindex="-1"></a><span class="vs">def convert_start(shape, start):</span></span>
<span id="cb5-142"><a href="#cb5-142" aria-hidden="true" tabindex="-1"></a><span class="vs">  start = start or 0</span></span>
<span id="cb5-143"><a href="#cb5-143" aria-hidden="true" tabindex="-1"></a><span class="vs">  if start &lt; 0:</span></span>
<span id="cb5-144"><a href="#cb5-144" aria-hidden="true" tabindex="-1"></a><span class="vs">    start = shape + start</span></span>
<span id="cb5-145"><a href="#cb5-145" aria-hidden="true" tabindex="-1"></a><span class="vs">  start = min(start, shape - 1)</span></span>
<span id="cb5-146"><a href="#cb5-146" aria-hidden="true" tabindex="-1"></a><span class="vs">  return start</span></span>
<span id="cb5-147"><a href="#cb5-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-148"><a href="#cb5-148" aria-hidden="true" tabindex="-1"></a><span class="vs">def convert_end(shape, start, end): </span></span>
<span id="cb5-149"><a href="#cb5-149" aria-hidden="true" tabindex="-1"></a><span class="vs">  start = convert_start(shape, start)</span></span>
<span id="cb5-150"><a href="#cb5-150" aria-hidden="true" tabindex="-1"></a><span class="vs">  if end is None:</span></span>
<span id="cb5-151"><a href="#cb5-151" aria-hidden="true" tabindex="-1"></a><span class="vs">    end = shape</span></span>
<span id="cb5-152"><a href="#cb5-152" aria-hidden="true" tabindex="-1"></a><span class="vs">  else:</span></span>
<span id="cb5-153"><a href="#cb5-153" aria-hidden="true" tabindex="-1"></a><span class="vs">    end = convert_start(shape, end)</span></span>
<span id="cb5-154"><a href="#cb5-154" aria-hidden="true" tabindex="-1"></a><span class="vs">  return end - start</span></span>
<span id="cb5-155"><a href="#cb5-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-156"><a href="#cb5-156" aria-hidden="true" tabindex="-1"></a><span class="vs">class Tensor:</span></span>
<span id="cb5-157"><a href="#cb5-157" aria-hidden="true" tabindex="-1"></a><span class="vs">  keepall = False</span></span>
<span id="cb5-158"><a href="#cb5-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-159"><a href="#cb5-159" aria-hidden="true" tabindex="-1"></a><span class="vs">  class Keep:</span></span>
<span id="cb5-160"><a href="#cb5-160" aria-hidden="true" tabindex="-1"></a><span class="vs">    def __enter__(self):</span></span>
<span id="cb5-161"><a href="#cb5-161" aria-hidden="true" tabindex="-1"></a><span class="vs">        self.value = Tensor.keepall</span></span>
<span id="cb5-162"><a href="#cb5-162" aria-hidden="true" tabindex="-1"></a><span class="vs">        Tensor.keepall = True</span></span>
<span id="cb5-163"><a href="#cb5-163" aria-hidden="true" tabindex="-1"></a><span class="vs">    </span></span>
<span id="cb5-164"><a href="#cb5-164" aria-hidden="true" tabindex="-1"></a><span class="vs">    def __exit__(self, *args):</span></span>
<span id="cb5-165"><a href="#cb5-165" aria-hidden="true" tabindex="-1"></a><span class="vs">        Tensor.keepall = self.value</span></span>
<span id="cb5-166"><a href="#cb5-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-167"><a href="#cb5-167" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __init__(self, *args, value=None, keep=None, **kwargs):</span></span>
<span id="cb5-168"><a href="#cb5-168" aria-hidden="true" tabindex="-1"></a><span class="vs">    if keep is None:</span></span>
<span id="cb5-169"><a href="#cb5-169" aria-hidden="true" tabindex="-1"></a><span class="vs">      self.keep = Tensor.keepall</span></span>
<span id="cb5-170"><a href="#cb5-170" aria-hidden="true" tabindex="-1"></a><span class="vs">    else:</span></span>
<span id="cb5-171"><a href="#cb5-171" aria-hidden="true" tabindex="-1"></a><span class="vs">      self.keep = keep</span></span>
<span id="cb5-172"><a href="#cb5-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-173"><a href="#cb5-173" aria-hidden="true" tabindex="-1"></a><span class="vs">    if not (value is None):</span></span>
<span id="cb5-174"><a href="#cb5-174" aria-hidden="true" tabindex="-1"></a><span class="vs">      self.value = value</span></span>
<span id="cb5-175"><a href="#cb5-175" aria-hidden="true" tabindex="-1"></a><span class="vs">    elif len(args) and isinstance(args[0], Tensor):</span></span>
<span id="cb5-176"><a href="#cb5-176" aria-hidden="true" tabindex="-1"></a><span class="vs">      self.value = tfbase.add(args[0].value, 0)</span></span>
<span id="cb5-177"><a href="#cb5-177" aria-hidden="true" tabindex="-1"></a><span class="vs">    elif len(args) and args[0] is None:</span></span>
<span id="cb5-178"><a href="#cb5-178" aria-hidden="true" tabindex="-1"></a><span class="vs">      self.value = tfbase.tensor(0.)</span></span>
<span id="cb5-179"><a href="#cb5-179" aria-hidden="true" tabindex="-1"></a><span class="vs">    else:</span></span>
<span id="cb5-180"><a href="#cb5-180" aria-hidden="true" tabindex="-1"></a><span class="vs">      args = [convert(a) for a in args]</span></span>
<span id="cb5-181"><a href="#cb5-181" aria-hidden="true" tabindex="-1"></a><span class="vs">      kwargs = {k: convert(a) for (k, a) in kwargs.items()}</span></span>
<span id="cb5-182"><a href="#cb5-182" aria-hidden="true" tabindex="-1"></a><span class="vs">      self.value = tfbase.tensor(*args, **kwargs)</span></span>
<span id="cb5-183"><a href="#cb5-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-184"><a href="#cb5-184" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __getattr__(self, name):</span></span>
<span id="cb5-185"><a href="#cb5-185" aria-hidden="true" tabindex="-1"></a><span class="vs">    if name == 'T':</span></span>
<span id="cb5-186"><a href="#cb5-186" aria-hidden="true" tabindex="-1"></a><span class="vs">      return self.transpose()</span></span>
<span id="cb5-187"><a href="#cb5-187" aria-hidden="true" tabindex="-1"></a><span class="vs">    attr = getattr(self.value, name)</span></span>
<span id="cb5-188"><a href="#cb5-188" aria-hidden="true" tabindex="-1"></a><span class="vs">    if callable(attr):</span></span>
<span id="cb5-189"><a href="#cb5-189" aria-hidden="true" tabindex="-1"></a><span class="vs">      def run(*args, **kwargs):</span></span>
<span id="cb5-190"><a href="#cb5-190" aria-hidden="true" tabindex="-1"></a><span class="vs">        args = [convert(a) for a in args]</span></span>
<span id="cb5-191"><a href="#cb5-191" aria-hidden="true" tabindex="-1"></a><span class="vs">        kwargs = {k: convert(a) for (k, a) in kwargs.items()}</span></span>
<span id="cb5-192"><a href="#cb5-192" aria-hidden="true" tabindex="-1"></a><span class="vs">        output = attr(*args, **kwargs)</span></span>
<span id="cb5-193"><a href="#cb5-193" aria-hidden="true" tabindex="-1"></a><span class="vs">        return Tensor(value=output)</span></span>
<span id="cb5-194"><a href="#cb5-194" aria-hidden="true" tabindex="-1"></a><span class="vs">      # Prevent premature garbage collection</span></span>
<span id="cb5-195"><a href="#cb5-195" aria-hidden="true" tabindex="-1"></a><span class="vs">      run._ref = self</span></span>
<span id="cb5-196"><a href="#cb5-196" aria-hidden="true" tabindex="-1"></a><span class="vs">      return run</span></span>
<span id="cb5-197"><a href="#cb5-197" aria-hidden="true" tabindex="-1"></a><span class="vs">    return attr</span></span>
<span id="cb5-198"><a href="#cb5-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-199"><a href="#cb5-199" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __add__(a, b):</span></span>
<span id="cb5-200"><a href="#cb5-200" aria-hidden="true" tabindex="-1"></a><span class="vs">    return Tensor(value=tfbase.add(convert(a), convert(b)))</span></span>
<span id="cb5-201"><a href="#cb5-201" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __radd__(a, b):</span></span>
<span id="cb5-202"><a href="#cb5-202" aria-hidden="true" tabindex="-1"></a><span class="vs">    return Tensor(value=tfbase.add(convert(b), convert(a)))</span></span>
<span id="cb5-203"><a href="#cb5-203" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __sub__(a, b):</span></span>
<span id="cb5-204"><a href="#cb5-204" aria-hidden="true" tabindex="-1"></a><span class="vs">    return Tensor(value=tfbase.sub(convert(a), convert(b)))</span></span>
<span id="cb5-205"><a href="#cb5-205" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __rsub__(a, b):</span></span>
<span id="cb5-206"><a href="#cb5-206" aria-hidden="true" tabindex="-1"></a><span class="vs">    return Tensor(value=tfbase.sub(convert(b), convert(a)))</span></span>
<span id="cb5-207"><a href="#cb5-207" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __mul__(a, b):</span></span>
<span id="cb5-208"><a href="#cb5-208" aria-hidden="true" tabindex="-1"></a><span class="vs">    return Tensor(value=tfbase.mul(convert(a), convert(b)))</span></span>
<span id="cb5-209"><a href="#cb5-209" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __rmul__(a, b):</span></span>
<span id="cb5-210"><a href="#cb5-210" aria-hidden="true" tabindex="-1"></a><span class="vs">    return Tensor(value=tfbase.mul(convert(b), convert(a)))</span></span>
<span id="cb5-211"><a href="#cb5-211" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __truediv__(a, b):</span></span>
<span id="cb5-212"><a href="#cb5-212" aria-hidden="true" tabindex="-1"></a><span class="vs">    return Tensor(value=tfbase.div(convert(a), convert(b)))</span></span>
<span id="cb5-213"><a href="#cb5-213" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __rtruediv__(a, b):</span></span>
<span id="cb5-214"><a href="#cb5-214" aria-hidden="true" tabindex="-1"></a><span class="vs">    return Tensor(value=tfbase.div(convert(b), convert(a)))</span></span>
<span id="cb5-215"><a href="#cb5-215" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __floordiv__(a, b):</span></span>
<span id="cb5-216"><a href="#cb5-216" aria-hidden="true" tabindex="-1"></a><span class="vs">    return Tensor(value=tfbase.floorDiv(convert(a), convert(b)))</span></span>
<span id="cb5-217"><a href="#cb5-217" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __rfloordiv__(a, b):</span></span>
<span id="cb5-218"><a href="#cb5-218" aria-hidden="true" tabindex="-1"></a><span class="vs">    return Tensor(value=tfbase.floorDiv(convert(b), convert(a)))</span></span>
<span id="cb5-219"><a href="#cb5-219" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __pow__(a, b):</span></span>
<span id="cb5-220"><a href="#cb5-220" aria-hidden="true" tabindex="-1"></a><span class="vs">    return Tensor(value=tfbase.pow(convert(a), convert(b)))</span></span>
<span id="cb5-221"><a href="#cb5-221" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __rpow__(a, b):</span></span>
<span id="cb5-222"><a href="#cb5-222" aria-hidden="true" tabindex="-1"></a><span class="vs">    return Tensor(value=tfbase.pow(convert(b), convert(a)))</span></span>
<span id="cb5-223"><a href="#cb5-223" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __neg__(a):</span></span>
<span id="cb5-224"><a href="#cb5-224" aria-hidden="true" tabindex="-1"></a><span class="vs">    return Tensor(value=tfbase.neg(convert(a)))</span></span>
<span id="cb5-225"><a href="#cb5-225" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __eq__(a, b):</span></span>
<span id="cb5-226"><a href="#cb5-226" aria-hidden="true" tabindex="-1"></a><span class="vs">    return Tensor(value=tfbase.equal(convert(a), convert(b)))</span></span>
<span id="cb5-227"><a href="#cb5-227" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __neq__(a, b):</span></span>
<span id="cb5-228"><a href="#cb5-228" aria-hidden="true" tabindex="-1"></a><span class="vs">    return Tensor(value=tfbase.notEqual(convert(a), convert(b)))</span></span>
<span id="cb5-229"><a href="#cb5-229" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __lt__(a, b):</span></span>
<span id="cb5-230"><a href="#cb5-230" aria-hidden="true" tabindex="-1"></a><span class="vs">    return Tensor(value=tfbase.less(convert(a), convert(b)))</span></span>
<span id="cb5-231"><a href="#cb5-231" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __gt__(a, b):</span></span>
<span id="cb5-232"><a href="#cb5-232" aria-hidden="true" tabindex="-1"></a><span class="vs">    return Tensor(value=tfbase.greater(convert(a), convert(b)))</span></span>
<span id="cb5-233"><a href="#cb5-233" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __leq__(a, b):</span></span>
<span id="cb5-234"><a href="#cb5-234" aria-hidden="true" tabindex="-1"></a><span class="vs">    return Tensor(value=tfbase.lessEqual(convert(a), convert(b)))</span></span>
<span id="cb5-235"><a href="#cb5-235" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __geq__(a, b):</span></span>
<span id="cb5-236"><a href="#cb5-236" aria-hidden="true" tabindex="-1"></a><span class="vs">    return Tensor(value=tfbase.greaterEqual(convert(a), convert(b)))</span></span>
<span id="cb5-237"><a href="#cb5-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-238"><a href="#cb5-238" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __del__(self):</span></span>
<span id="cb5-239"><a href="#cb5-239" aria-hidden="true" tabindex="-1"></a><span class="vs">    if hasattr(self.value, 'dispose') and not self.keep:</span></span>
<span id="cb5-240"><a href="#cb5-240" aria-hidden="true" tabindex="-1"></a><span class="vs">      self.value.dispose()</span></span>
<span id="cb5-241"><a href="#cb5-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-242"><a href="#cb5-242" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __iter__(self):</span></span>
<span id="cb5-243"><a href="#cb5-243" aria-hidden="true" tabindex="-1"></a><span class="vs">    for x in self.value.arraySync():</span></span>
<span id="cb5-244"><a href="#cb5-244" aria-hidden="true" tabindex="-1"></a><span class="vs">        yield Tensor(x)</span></span>
<span id="cb5-245"><a href="#cb5-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-246"><a href="#cb5-246" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __getitem__(self, args):</span></span>
<span id="cb5-247"><a href="#cb5-247" aria-hidden="true" tabindex="-1"></a><span class="vs">    tosqueeze = []</span></span>
<span id="cb5-248"><a href="#cb5-248" aria-hidden="true" tabindex="-1"></a><span class="vs">    starts, ends, steps = [], [], []</span></span>
<span id="cb5-249"><a href="#cb5-249" aria-hidden="true" tabindex="-1"></a><span class="vs">    value = self</span></span>
<span id="cb5-250"><a href="#cb5-250" aria-hidden="true" tabindex="-1"></a><span class="vs">    </span></span>
<span id="cb5-251"><a href="#cb5-251" aria-hidden="true" tabindex="-1"></a><span class="vs">    if not (type(args) is tuple):</span></span>
<span id="cb5-252"><a href="#cb5-252" aria-hidden="true" tabindex="-1"></a><span class="vs">      args = (args,)</span></span>
<span id="cb5-253"><a href="#cb5-253" aria-hidden="true" tabindex="-1"></a><span class="vs">    </span></span>
<span id="cb5-254"><a href="#cb5-254" aria-hidden="true" tabindex="-1"></a><span class="vs">    for ind in range(len(args)):</span></span>
<span id="cb5-255"><a href="#cb5-255" aria-hidden="true" tabindex="-1"></a><span class="vs">      if args[ind] is Ellipsis:</span></span>
<span id="cb5-256"><a href="#cb5-256" aria-hidden="true" tabindex="-1"></a><span class="vs">        start = args[:ind]</span></span>
<span id="cb5-257"><a href="#cb5-257" aria-hidden="true" tabindex="-1"></a><span class="vs">        rest = args[(ind + 1):]</span></span>
<span id="cb5-258"><a href="#cb5-258" aria-hidden="true" tabindex="-1"></a><span class="vs">        args = start + tuple([slice(None)] * (len(self.value.shape) - (len(start) + len(rest)))) + rest</span></span>
<span id="cb5-259"><a href="#cb5-259" aria-hidden="true" tabindex="-1"></a><span class="vs">        break</span></span>
<span id="cb5-260"><a href="#cb5-260" aria-hidden="true" tabindex="-1"></a><span class="vs">    </span></span>
<span id="cb5-261"><a href="#cb5-261" aria-hidden="true" tabindex="-1"></a><span class="vs">    for i, (shape, dim) in enumerate(zip(self.value.shape, args)):</span></span>
<span id="cb5-262"><a href="#cb5-262" aria-hidden="true" tabindex="-1"></a><span class="vs">      if isinstance(dim, slice):</span></span>
<span id="cb5-263"><a href="#cb5-263" aria-hidden="true" tabindex="-1"></a><span class="vs">        starts.append(dim.start or 0)</span></span>
<span id="cb5-264"><a href="#cb5-264" aria-hidden="true" tabindex="-1"></a><span class="vs">        ends.append(dim.stop or shape)</span></span>
<span id="cb5-265"><a href="#cb5-265" aria-hidden="true" tabindex="-1"></a><span class="vs">        steps.append(dim.step or 1)</span></span>
<span id="cb5-266"><a href="#cb5-266" aria-hidden="true" tabindex="-1"></a><span class="vs">      elif Tensor(dim).shape:</span></span>
<span id="cb5-267"><a href="#cb5-267" aria-hidden="true" tabindex="-1"></a><span class="vs">        t = Tensor(dim)</span></span>
<span id="cb5-268"><a href="#cb5-268" aria-hidden="true" tabindex="-1"></a><span class="vs">        if t.value.dtype == 'bool':</span></span>
<span id="cb5-269"><a href="#cb5-269" aria-hidden="true" tabindex="-1"></a><span class="vs">          inds = [ind for (ind, e) in enumerate(t.value.arraySync()) if e]</span></span>
<span id="cb5-270"><a href="#cb5-270" aria-hidden="true" tabindex="-1"></a><span class="vs">          inds = tf.cast(tf.reshape(Tensor(inds), [-1]), 'int32')</span></span>
<span id="cb5-271"><a href="#cb5-271" aria-hidden="true" tabindex="-1"></a><span class="vs">          value = tf.gather(value, inds, i)</span></span>
<span id="cb5-272"><a href="#cb5-272" aria-hidden="true" tabindex="-1"></a><span class="vs">        else:</span></span>
<span id="cb5-273"><a href="#cb5-273" aria-hidden="true" tabindex="-1"></a><span class="vs">          inds = tf.cast(tf.reshape(t, [-1]), 'int32')</span></span>
<span id="cb5-274"><a href="#cb5-274" aria-hidden="true" tabindex="-1"></a><span class="vs">          value = tf.gather(value, inds, i)</span></span>
<span id="cb5-275"><a href="#cb5-275" aria-hidden="true" tabindex="-1"></a><span class="vs">      else:</span></span>
<span id="cb5-276"><a href="#cb5-276" aria-hidden="true" tabindex="-1"></a><span class="vs">        starts.append(dim)</span></span>
<span id="cb5-277"><a href="#cb5-277" aria-hidden="true" tabindex="-1"></a><span class="vs">        ends.append(dim + 1)</span></span>
<span id="cb5-278"><a href="#cb5-278" aria-hidden="true" tabindex="-1"></a><span class="vs">        steps.append(1)</span></span>
<span id="cb5-279"><a href="#cb5-279" aria-hidden="true" tabindex="-1"></a><span class="vs">        tosqueeze.append(i)</span></span>
<span id="cb5-280"><a href="#cb5-280" aria-hidden="true" tabindex="-1"></a><span class="vs">    value = tf.stridedSlice(value, convert(starts), convert(ends), convert(steps))</span></span>
<span id="cb5-281"><a href="#cb5-281" aria-hidden="true" tabindex="-1"></a><span class="vs">    if len(tosqueeze) &gt; 0:</span></span>
<span id="cb5-282"><a href="#cb5-282" aria-hidden="true" tabindex="-1"></a><span class="vs">      value = tf.squeeze(value, tosqueeze)</span></span>
<span id="cb5-283"><a href="#cb5-283" aria-hidden="true" tabindex="-1"></a><span class="vs">    return value</span></span>
<span id="cb5-284"><a href="#cb5-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-285"><a href="#cb5-285" aria-hidden="true" tabindex="-1"></a><span class="vs">  def t2Js(self):</span></span>
<span id="cb5-286"><a href="#cb5-286" aria-hidden="true" tabindex="-1"></a><span class="vs">    return to_js(self.value.arraySync())</span></span>
<span id="cb5-287"><a href="#cb5-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-288"><a href="#cb5-288" aria-hidden="true" tabindex="-1"></a><span class="vs">class wrapper:</span></span>
<span id="cb5-289"><a href="#cb5-289" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __init__(self, f):</span></span>
<span id="cb5-290"><a href="#cb5-290" aria-hidden="true" tabindex="-1"></a><span class="vs">    self.f = f</span></span>
<span id="cb5-291"><a href="#cb5-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-292"><a href="#cb5-292" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __call__(self, x, *args, **kwargs):</span></span>
<span id="cb5-293"><a href="#cb5-293" aria-hidden="true" tabindex="-1"></a><span class="vs">    with Tensor.Keep():</span></span>
<span id="cb5-294"><a href="#cb5-294" aria-hidden="true" tabindex="-1"></a><span class="vs">      return convert(self.f(Tensor(value=x), *args, **kwargs))</span></span>
<span id="cb5-295"><a href="#cb5-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-296"><a href="#cb5-296" aria-hidden="true" tabindex="-1"></a><span class="vs">class grad:</span></span>
<span id="cb5-297"><a href="#cb5-297" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __init__(self, f):</span></span>
<span id="cb5-298"><a href="#cb5-298" aria-hidden="true" tabindex="-1"></a><span class="vs">    self.f = f</span></span>
<span id="cb5-299"><a href="#cb5-299" aria-hidden="true" tabindex="-1"></a><span class="vs">    self.wrapper = wrapper(f)</span></span>
<span id="cb5-300"><a href="#cb5-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-301"><a href="#cb5-301" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __call__(self, x, *args, **kwargs):</span></span>
<span id="cb5-302"><a href="#cb5-302" aria-hidden="true" tabindex="-1"></a><span class="vs">    output = tfbase.grad(create_once_callable(self.wrapper))(x.value, *args, **kwargs)</span></span>
<span id="cb5-303"><a href="#cb5-303" aria-hidden="true" tabindex="-1"></a><span class="vs">    return Tensor(value=output)</span></span>
<span id="cb5-304"><a href="#cb5-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-305"><a href="#cb5-305" aria-hidden="true" tabindex="-1"></a><span class="vs">class wrappers:</span></span>
<span id="cb5-306"><a href="#cb5-306" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __init__(self, f):</span></span>
<span id="cb5-307"><a href="#cb5-307" aria-hidden="true" tabindex="-1"></a><span class="vs">    self.f = f</span></span>
<span id="cb5-308"><a href="#cb5-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-309"><a href="#cb5-309" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __call__(self, *args):</span></span>
<span id="cb5-310"><a href="#cb5-310" aria-hidden="true" tabindex="-1"></a><span class="vs">    with Tensor.Keep():</span></span>
<span id="cb5-311"><a href="#cb5-311" aria-hidden="true" tabindex="-1"></a><span class="vs">      wrapped_args = [Tensor(value=x) for x in args]</span></span>
<span id="cb5-312"><a href="#cb5-312" aria-hidden="true" tabindex="-1"></a><span class="vs">      return convert(self.f(*wrapped_args))</span></span>
<span id="cb5-313"><a href="#cb5-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-314"><a href="#cb5-314" aria-hidden="true" tabindex="-1"></a><span class="vs">class grads:</span></span>
<span id="cb5-315"><a href="#cb5-315" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __init__(self, f):</span></span>
<span id="cb5-316"><a href="#cb5-316" aria-hidden="true" tabindex="-1"></a><span class="vs">    self.f = f</span></span>
<span id="cb5-317"><a href="#cb5-317" aria-hidden="true" tabindex="-1"></a><span class="vs">    self.wrapper = wrappers(f)</span></span>
<span id="cb5-318"><a href="#cb5-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-319"><a href="#cb5-319" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __call__(self, *args):</span></span>
<span id="cb5-320"><a href="#cb5-320" aria-hidden="true" tabindex="-1"></a><span class="vs">    output = tfbase.grads(create_once_callable(self.wrapper))(to_js([arg.value for arg in args]))</span></span>
<span id="cb5-321"><a href="#cb5-321" aria-hidden="true" tabindex="-1"></a><span class="vs">    return [Tensor(value=x) for x in output]</span></span>
<span id="cb5-322"><a href="#cb5-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-323"><a href="#cb5-323" aria-hidden="true" tabindex="-1"></a><span class="vs">tf = Tensor(value=tfbase)</span></span>
<span id="cb5-324"><a href="#cb5-324" aria-hidden="true" tabindex="-1"></a><span class="vs">Plotbase = SimpleNamespace(**Plotbase)</span></span>
<span id="cb5-325"><a href="#cb5-325" aria-hidden="true" tabindex="-1"></a><span class="vs">Plotlybase = SimpleNamespace(**Plotlybase)</span></span>
<span id="cb5-326"><a href="#cb5-326" aria-hidden="true" tabindex="-1"></a><span class="vs">d3base = SimpleNamespace(**d3base)</span></span>
<span id="cb5-327"><a href="#cb5-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-328"><a href="#cb5-328" aria-hidden="true" tabindex="-1"></a><span class="vs">def meshgrid(*args):</span></span>
<span id="cb5-329"><a href="#cb5-329" aria-hidden="true" tabindex="-1"></a><span class="vs">  return tuple([Tensor(value=a) for a in tfbase.meshgrid(*[convert(arg) for arg in args])])</span></span>
<span id="cb5-330"><a href="#cb5-330" aria-hidden="true" tabindex="-1"></a><span class="vs">tf.meshgrid = meshgrid</span></span>
<span id="cb5-331"><a href="#cb5-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-332"><a href="#cb5-332" aria-hidden="true" tabindex="-1"></a><span class="vs">def default_convert(obj, default_f, other):</span></span>
<span id="cb5-333"><a href="#cb5-333" aria-hidden="true" tabindex="-1"></a><span class="vs">  if isinstance(obj, Tensor):</span></span>
<span id="cb5-334"><a href="#cb5-334" aria-hidden="true" tabindex="-1"></a><span class="vs">    obj = obj.t2Js()</span></span>
<span id="cb5-335"><a href="#cb5-335" aria-hidden="true" tabindex="-1"></a><span class="vs">  if isinstance(obj, pandas.DataFrame):</span></span>
<span id="cb5-336"><a href="#cb5-336" aria-hidden="true" tabindex="-1"></a><span class="vs">    obj = obj.to_dict('records') </span></span>
<span id="cb5-337"><a href="#cb5-337" aria-hidden="true" tabindex="-1"></a><span class="vs">  return default_f(obj)</span></span>
<span id="cb5-338"><a href="#cb5-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-339"><a href="#cb5-339" aria-hidden="true" tabindex="-1"></a><span class="vs">def plotconvert(a):</span></span>
<span id="cb5-340"><a href="#cb5-340" aria-hidden="true" tabindex="-1"></a><span class="vs">  return to_js(a, dict_converter=Object.fromEntries, default_converter=default_convert)</span></span>
<span id="cb5-341"><a href="#cb5-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-342"><a href="#cb5-342" aria-hidden="true" tabindex="-1"></a><span class="vs">class PlotWrapper:</span></span>
<span id="cb5-343"><a href="#cb5-343" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __init__(self, base=None):</span></span>
<span id="cb5-344"><a href="#cb5-344" aria-hidden="true" tabindex="-1"></a><span class="vs">    self.base = base</span></span>
<span id="cb5-345"><a href="#cb5-345" aria-hidden="true" tabindex="-1"></a><span class="vs">    </span></span>
<span id="cb5-346"><a href="#cb5-346" aria-hidden="true" tabindex="-1"></a><span class="vs">  def __getattr__(self, name):</span></span>
<span id="cb5-347"><a href="#cb5-347" aria-hidden="true" tabindex="-1"></a><span class="vs">    attr = getattr(self.base, name)</span></span>
<span id="cb5-348"><a href="#cb5-348" aria-hidden="true" tabindex="-1"></a><span class="vs">    if callable(attr):</span></span>
<span id="cb5-349"><a href="#cb5-349" aria-hidden="true" tabindex="-1"></a><span class="vs">      def run(*args, **kwargs):</span></span>
<span id="cb5-350"><a href="#cb5-350" aria-hidden="true" tabindex="-1"></a><span class="vs">        args = [plotconvert(a) for a in args]</span></span>
<span id="cb5-351"><a href="#cb5-351" aria-hidden="true" tabindex="-1"></a><span class="vs">        kwargs = {k: plotconvert(a) for (k, a) in kwargs.items()}</span></span>
<span id="cb5-352"><a href="#cb5-352" aria-hidden="true" tabindex="-1"></a><span class="vs">        return attr(*args, **kwargs)</span></span>
<span id="cb5-353"><a href="#cb5-353" aria-hidden="true" tabindex="-1"></a><span class="vs">      return run</span></span>
<span id="cb5-354"><a href="#cb5-354" aria-hidden="true" tabindex="-1"></a><span class="vs">    return attr</span></span>
<span id="cb5-355"><a href="#cb5-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-356"><a href="#cb5-356" aria-hidden="true" tabindex="-1"></a><span class="vs">Plot = PlotWrapper(Plotbase)</span></span>
<span id="cb5-357"><a href="#cb5-357" aria-hidden="true" tabindex="-1"></a><span class="vs">Plotly = PlotWrapper(Plotlybase)</span></span>
<span id="cb5-358"><a href="#cb5-358" aria-hidden="true" tabindex="-1"></a><span class="vs">d3 = PlotWrapper(d3base)</span></span>
<span id="cb5-359"><a href="#cb5-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-360"><a href="#cb5-360" aria-hidden="true" tabindex="-1"></a><span class="vs">def PlotlyFigure(width=800, height=None, hide_toolbar=True, overlay=True):</span></span>
<span id="cb5-361"><a href="#cb5-361" aria-hidden="true" tabindex="-1"></a><span class="vs">  if height is None:</span></span>
<span id="cb5-362"><a href="#cb5-362" aria-hidden="true" tabindex="-1"></a><span class="vs">    height = 0.75 * width</span></span>
<span id="cb5-363"><a href="#cb5-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-364"><a href="#cb5-364" aria-hidden="true" tabindex="-1"></a><span class="vs">  width, height = int(width), int(height)</span></span>
<span id="cb5-365"><a href="#cb5-365" aria-hidden="true" tabindex="-1"></a><span class="vs">  container = document.createElement('div')</span></span>
<span id="cb5-366"><a href="#cb5-366" aria-hidden="true" tabindex="-1"></a><span class="vs">  container.style.width = str(width) + 'px'</span></span>
<span id="cb5-367"><a href="#cb5-367" aria-hidden="true" tabindex="-1"></a><span class="vs">  container.style.height = str(height) + 'px'</span></span>
<span id="cb5-368"><a href="#cb5-368" aria-hidden="true" tabindex="-1"></a><span class="vs">  </span></span>
<span id="cb5-369"><a href="#cb5-369" aria-hidden="true" tabindex="-1"></a><span class="vs">  lineplot = document.createElement('div')</span></span>
<span id="cb5-370"><a href="#cb5-370" aria-hidden="true" tabindex="-1"></a><span class="vs">  lineplot.classList.add("plotlydiv")</span></span>
<span id="cb5-371"><a href="#cb5-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-372"><a href="#cb5-372" aria-hidden="true" tabindex="-1"></a><span class="vs">  if hide_toolbar:</span></span>
<span id="cb5-373"><a href="#cb5-373" aria-hidden="true" tabindex="-1"></a><span class="vs">    container.classList.add("hidetoolbar")</span></span>
<span id="cb5-374"><a href="#cb5-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-375"><a href="#cb5-375" aria-hidden="true" tabindex="-1"></a><span class="vs">  container.append(lineplot)</span></span>
<span id="cb5-376"><a href="#cb5-376" aria-hidden="true" tabindex="-1"></a><span class="vs">  if overlay:</span></span>
<span id="cb5-377"><a href="#cb5-377" aria-hidden="true" tabindex="-1"></a><span class="vs">    overlay = document.createElement('div')</span></span>
<span id="cb5-378"><a href="#cb5-378" aria-hidden="true" tabindex="-1"></a><span class="vs">    overlay.classList.add("plotlyoverlay")</span></span>
<span id="cb5-379"><a href="#cb5-379" aria-hidden="true" tabindex="-1"></a><span class="vs">    </span></span>
<span id="cb5-380"><a href="#cb5-380" aria-hidden="true" tabindex="-1"></a><span class="vs">    container.append(overlay)</span></span>
<span id="cb5-381"><a href="#cb5-381" aria-hidden="true" tabindex="-1"></a><span class="vs">    </span></span>
<span id="cb5-382"><a href="#cb5-382" aria-hidden="true" tabindex="-1"></a><span class="vs">    container.style.position = 'relative'</span></span>
<span id="cb5-383"><a href="#cb5-383" aria-hidden="true" tabindex="-1"></a><span class="vs">    overlay.style.top = '0'</span></span>
<span id="cb5-384"><a href="#cb5-384" aria-hidden="true" tabindex="-1"></a><span class="vs">    overlay.style.bottom = '0'</span></span>
<span id="cb5-385"><a href="#cb5-385" aria-hidden="true" tabindex="-1"></a><span class="vs">    overlay.style.width = '100%'</span></span>
<span id="cb5-386"><a href="#cb5-386" aria-hidden="true" tabindex="-1"></a><span class="vs">    overlay.style.position = 'absolute'</span></span>
<span id="cb5-387"><a href="#cb5-387" aria-hidden="true" tabindex="-1"></a><span class="vs">  return container</span></span>
<span id="cb5-388"><a href="#cb5-388" aria-hidden="true" tabindex="-1"></a><span class="vs">  </span></span>
<span id="cb5-389"><a href="#cb5-389" aria-hidden="true" tabindex="-1"></a><span class="vs">def PlotlyInput(width=800, height=None, hide_toolbar=True, sync=None):</span></span>
<span id="cb5-390"><a href="#cb5-390" aria-hidden="true" tabindex="-1"></a><span class="vs">  container = PlotlyFigure(width, height, hide_toolbar)</span></span>
<span id="cb5-391"><a href="#cb5-391" aria-hidden="true" tabindex="-1"></a><span class="vs">  lineplot, overlay = container.childNodes[0], container.childNodes[1]</span></span>
<span id="cb5-392"><a href="#cb5-392" aria-hidden="true" tabindex="-1"></a><span class="vs">  if sync is None:</span></span>
<span id="cb5-393"><a href="#cb5-393" aria-hidden="true" tabindex="-1"></a><span class="vs">    sync = container</span></span>
<span id="cb5-394"><a href="#cb5-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-395"><a href="#cb5-395" aria-hidden="true" tabindex="-1"></a><span class="vs">  class mover:</span></span>
<span id="cb5-396"><a href="#cb5-396" aria-hidden="true" tabindex="-1"></a><span class="vs">    def __init__(self):</span></span>
<span id="cb5-397"><a href="#cb5-397" aria-hidden="true" tabindex="-1"></a><span class="vs">      self.mousedown = False</span></span>
<span id="cb5-398"><a href="#cb5-398" aria-hidden="true" tabindex="-1"></a><span class="vs">    </span></span>
<span id="cb5-399"><a href="#cb5-399" aria-hidden="true" tabindex="-1"></a><span class="vs">    def __call__(self, event):</span></span>
<span id="cb5-400"><a href="#cb5-400" aria-hidden="true" tabindex="-1"></a><span class="vs">      if event.type == 'mousedown':</span></span>
<span id="cb5-401"><a href="#cb5-401" aria-hidden="true" tabindex="-1"></a><span class="vs">        self.mousedown = True</span></span>
<span id="cb5-402"><a href="#cb5-402" aria-hidden="true" tabindex="-1"></a><span class="vs">      if event.type == 'mouseleave':</span></span>
<span id="cb5-403"><a href="#cb5-403" aria-hidden="true" tabindex="-1"></a><span class="vs">        self.mousedown = False</span></span>
<span id="cb5-404"><a href="#cb5-404" aria-hidden="true" tabindex="-1"></a><span class="vs">      if event.type == 'mouseup':</span></span>
<span id="cb5-405"><a href="#cb5-405" aria-hidden="true" tabindex="-1"></a><span class="vs">        self.mousedown = False</span></span>
<span id="cb5-406"><a href="#cb5-406" aria-hidden="true" tabindex="-1"></a><span class="vs">  </span></span>
<span id="cb5-407"><a href="#cb5-407" aria-hidden="true" tabindex="-1"></a><span class="vs">      if self.mousedown:</span></span>
<span id="cb5-408"><a href="#cb5-408" aria-hidden="true" tabindex="-1"></a><span class="vs">        x = float(lineplot._fullLayout.xaxis.p2c(event.layerX - lineplot._fullLayout.margin.l))</span></span>
<span id="cb5-409"><a href="#cb5-409" aria-hidden="true" tabindex="-1"></a><span class="vs">        y = float(lineplot._fullLayout.yaxis.p2c(event.layerY - lineplot._fullLayout.margin.t))</span></span>
<span id="cb5-410"><a href="#cb5-410" aria-hidden="true" tabindex="-1"></a><span class="vs">        sync.value = to_js([x, y])</span></span>
<span id="cb5-411"><a href="#cb5-411" aria-hidden="true" tabindex="-1"></a><span class="vs">        dispatchEvent(sync)</span></span>
<span id="cb5-412"><a href="#cb5-412" aria-hidden="true" tabindex="-1"></a><span class="vs">        </span></span>
<span id="cb5-413"><a href="#cb5-413" aria-hidden="true" tabindex="-1"></a><span class="vs">  </span></span>
<span id="cb5-414"><a href="#cb5-414" aria-hidden="true" tabindex="-1"></a><span class="vs">  e = mover()</span></span>
<span id="cb5-415"><a href="#cb5-415" aria-hidden="true" tabindex="-1"></a><span class="vs">  overlay.addEventListener('mousemove', to_js(e))</span></span>
<span id="cb5-416"><a href="#cb5-416" aria-hidden="true" tabindex="-1"></a><span class="vs">  overlay.addEventListener('mousedown', to_js(e))</span></span>
<span id="cb5-417"><a href="#cb5-417" aria-hidden="true" tabindex="-1"></a><span class="vs">  overlay.addEventListener('mouseup', to_js(e))</span></span>
<span id="cb5-418"><a href="#cb5-418" aria-hidden="true" tabindex="-1"></a><span class="vs">  overlay.addEventListener('mouseleave', to_js(e))</span></span>
<span id="cb5-419"><a href="#cb5-419" aria-hidden="true" tabindex="-1"></a><span class="vs">  container.value = to_js([0., 0.])</span></span>
<span id="cb5-420"><a href="#cb5-420" aria-hidden="true" tabindex="-1"></a><span class="vs">  return container</span></span>
<span id="cb5-421"><a href="#cb5-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-422"><a href="#cb5-422" aria-hidden="true" tabindex="-1"></a><span class="vs">def PlotlyReactive(container, traces=[], layout={}, options={}):</span></span>
<span id="cb5-423"><a href="#cb5-423" aria-hidden="true" tabindex="-1"></a><span class="vs">  full_layout = dict(width=int(container.style.width.replace('px', '')), height=int(container.style.height.replace('px', '')))</span></span>
<span id="cb5-424"><a href="#cb5-424" aria-hidden="true" tabindex="-1"></a><span class="vs">  full_layout.update(layout)</span></span>
<span id="cb5-425"><a href="#cb5-425" aria-hidden="true" tabindex="-1"></a><span class="vs">  full_options = {'displayModeBar' : not container.classList.contains('hidetoolbar')}</span></span>
<span id="cb5-426"><a href="#cb5-426" aria-hidden="true" tabindex="-1"></a><span class="vs">  full_options.update(options)</span></span>
<span id="cb5-427"><a href="#cb5-427" aria-hidden="true" tabindex="-1"></a><span class="vs">  plot = container.childNodes[0]</span></span>
<span id="cb5-428"><a href="#cb5-428" aria-hidden="true" tabindex="-1"></a><span class="vs">  Plotly.react(plot, traces, full_layout, full_options)</span></span>
<span id="cb5-429"><a href="#cb5-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-430"><a href="#cb5-430" aria-hidden="true" tabindex="-1"></a><span class="vs">def colorMap(t, cmap='inferno', cmin=None, cmax=None, scale='linear', res=100):</span></span>
<span id="cb5-431"><a href="#cb5-431" aria-hidden="true" tabindex="-1"></a><span class="vs">  import matplotlib.cm as cm</span></span>
<span id="cb5-432"><a href="#cb5-432" aria-hidden="true" tabindex="-1"></a><span class="vs">  if cmin is None:</span></span>
<span id="cb5-433"><a href="#cb5-433" aria-hidden="true" tabindex="-1"></a><span class="vs">    cmin = tf.min(t)</span></span>
<span id="cb5-434"><a href="#cb5-434" aria-hidden="true" tabindex="-1"></a><span class="vs">  if cmax is None:</span></span>
<span id="cb5-435"><a href="#cb5-435" aria-hidden="true" tabindex="-1"></a><span class="vs">    cmax = tf.max(t)</span></span>
<span id="cb5-436"><a href="#cb5-436" aria-hidden="true" tabindex="-1"></a><span class="vs">  </span></span>
<span id="cb5-437"><a href="#cb5-437" aria-hidden="true" tabindex="-1"></a><span class="vs">  t = (t - cmin) / (cmax - cmin)</span></span>
<span id="cb5-438"><a href="#cb5-438" aria-hidden="true" tabindex="-1"></a><span class="vs">  if scale == 'log':</span></span>
<span id="cb5-439"><a href="#cb5-439" aria-hidden="true" tabindex="-1"></a><span class="vs">    e = tf.exp(1)</span></span>
<span id="cb5-440"><a href="#cb5-440" aria-hidden="true" tabindex="-1"></a><span class="vs">    t = t * (e - 1) + 1</span></span>
<span id="cb5-441"><a href="#cb5-441" aria-hidden="true" tabindex="-1"></a><span class="vs">    t = tf.log(t)</span></span>
<span id="cb5-442"><a href="#cb5-442" aria-hidden="true" tabindex="-1"></a><span class="vs">  cmap = Tensor(cm.get_cmap(cmap, res + 1)(range(res + 1)))</span></span>
<span id="cb5-443"><a href="#cb5-443" aria-hidden="true" tabindex="-1"></a><span class="vs">  t = t * res</span></span>
<span id="cb5-444"><a href="#cb5-444" aria-hidden="true" tabindex="-1"></a><span class="vs">  shape = t.shape</span></span>
<span id="cb5-445"><a href="#cb5-445" aria-hidden="true" tabindex="-1"></a><span class="vs">  tflat = tf.reshape(t, [-1])</span></span>
<span id="cb5-446"><a href="#cb5-446" aria-hidden="true" tabindex="-1"></a><span class="vs">  tfloor = tf.gather(cmap, tf.floor(tflat).cast('int32'))</span></span>
<span id="cb5-447"><a href="#cb5-447" aria-hidden="true" tabindex="-1"></a><span class="vs">  tceil = tf.gather(cmap, tf.ceil(tflat).cast('int32'))</span></span>
<span id="cb5-448"><a href="#cb5-448" aria-hidden="true" tabindex="-1"></a><span class="vs">  tfrac = tf.reshape(tflat - tf.floor(tflat), [-1, 1])</span></span>
<span id="cb5-449"><a href="#cb5-449" aria-hidden="true" tabindex="-1"></a><span class="vs">  tflat = tfrac * tceil + (1. - tfrac) * tfloor</span></span>
<span id="cb5-450"><a href="#cb5-450" aria-hidden="true" tabindex="-1"></a><span class="vs">  t = tf.reshape(tflat, list(shape) + [4])</span></span>
<span id="cb5-451"><a href="#cb5-451" aria-hidden="true" tabindex="-1"></a><span class="vs">  return t</span></span>
<span id="cb5-452"><a href="#cb5-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-453"><a href="#cb5-453" aria-hidden="true" tabindex="-1"></a><span class="vs">def plotTensor(t, canvas, size=None, cmap=None, interpolation='nearest', **kwargs):</span></span>
<span id="cb5-454"><a href="#cb5-454" aria-hidden="true" tabindex="-1"></a><span class="vs">  if not (cmap is None):</span></span>
<span id="cb5-455"><a href="#cb5-455" aria-hidden="true" tabindex="-1"></a><span class="vs">    t = colorMap(t, cmap, **kwargs)</span></span>
<span id="cb5-456"><a href="#cb5-456" aria-hidden="true" tabindex="-1"></a><span class="vs">  if size is None:</span></span>
<span id="cb5-457"><a href="#cb5-457" aria-hidden="true" tabindex="-1"></a><span class="vs">    size = (canvas.height, canvas.width)</span></span>
<span id="cb5-458"><a href="#cb5-458" aria-hidden="true" tabindex="-1"></a><span class="vs">  if interpolation == 'bilinear':</span></span>
<span id="cb5-459"><a href="#cb5-459" aria-hidden="true" tabindex="-1"></a><span class="vs">    t = tfbase.image['resizeBilinear'](t.value, list(size))</span></span>
<span id="cb5-460"><a href="#cb5-460" aria-hidden="true" tabindex="-1"></a><span class="vs">  else:</span></span>
<span id="cb5-461"><a href="#cb5-461" aria-hidden="true" tabindex="-1"></a><span class="vs">    t = tfbase.image['resizeNearestNeighbor'](t.value, list(size))</span></span>
<span id="cb5-462"><a href="#cb5-462" aria-hidden="true" tabindex="-1"></a><span class="vs">  tfbase.browser['toPixels'](t, canvas)</span></span>
<span id="cb5-463"><a href="#cb5-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-464"><a href="#cb5-464" aria-hidden="true" tabindex="-1"></a><span class="vs">from itertools import chain</span></span>
<span id="cb5-465"><a href="#cb5-465" aria-hidden="true" tabindex="-1"></a><span class="vs">import math</span></span>
<span id="cb5-466"><a href="#cb5-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-467"><a href="#cb5-467" aria-hidden="true" tabindex="-1"></a><span class="vs">class Module:</span></span>
<span id="cb5-468"><a href="#cb5-468" aria-hidden="true" tabindex="-1"></a><span class="vs">    def __init__(self):</span></span>
<span id="cb5-469"><a href="#cb5-469" aria-hidden="true" tabindex="-1"></a><span class="vs">        self._submodules = dict()</span></span>
<span id="cb5-470"><a href="#cb5-470" aria-hidden="true" tabindex="-1"></a><span class="vs">        self.eval = False</span></span>
<span id="cb5-471"><a href="#cb5-471" aria-hidden="true" tabindex="-1"></a><span class="vs">        self._store = False</span></span>
<span id="cb5-472"><a href="#cb5-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-473"><a href="#cb5-473" aria-hidden="true" tabindex="-1"></a><span class="vs">    def parameters(self):</span></span>
<span id="cb5-474"><a href="#cb5-474" aria-hidden="true" tabindex="-1"></a><span class="vs">        return chain.from_iterable(map(lambda x: x.parameters(), self._submodules.values()))</span></span>
<span id="cb5-475"><a href="#cb5-475" aria-hidden="true" tabindex="-1"></a><span class="vs">    </span></span>
<span id="cb5-476"><a href="#cb5-476" aria-hidden="true" tabindex="-1"></a><span class="vs">    def __setattr__(self, name, value):</span></span>
<span id="cb5-477"><a href="#cb5-477" aria-hidden="true" tabindex="-1"></a><span class="vs">        if isinstance(value, Module):</span></span>
<span id="cb5-478"><a href="#cb5-478" aria-hidden="true" tabindex="-1"></a><span class="vs">            self._submodules[name] = value</span></span>
<span id="cb5-479"><a href="#cb5-479" aria-hidden="true" tabindex="-1"></a><span class="vs">        super().__setattr__(name, value)</span></span>
<span id="cb5-480"><a href="#cb5-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-481"><a href="#cb5-481" aria-hidden="true" tabindex="-1"></a><span class="vs">    def __call__(self, *args, **kwargs):</span></span>
<span id="cb5-482"><a href="#cb5-482" aria-hidden="true" tabindex="-1"></a><span class="vs">        value = self.forward(*args, **kwargs)</span></span>
<span id="cb5-483"><a href="#cb5-483" aria-hidden="true" tabindex="-1"></a><span class="vs">        self._store = False</span></span>
<span id="cb5-484"><a href="#cb5-484" aria-hidden="true" tabindex="-1"></a><span class="vs">        return value</span></span>
<span id="cb5-485"><a href="#cb5-485" aria-hidden="true" tabindex="-1"></a><span class="vs">    </span></span>
<span id="cb5-486"><a href="#cb5-486" aria-hidden="true" tabindex="-1"></a><span class="vs">    def forward(self):</span></span>
<span id="cb5-487"><a href="#cb5-487" aria-hidden="true" tabindex="-1"></a><span class="vs">        raise NotImplementedError()</span></span>
<span id="cb5-488"><a href="#cb5-488" aria-hidden="true" tabindex="-1"></a><span class="vs">    </span></span>
<span id="cb5-489"><a href="#cb5-489" aria-hidden="true" tabindex="-1"></a><span class="vs">    def train(self):</span></span>
<span id="cb5-490"><a href="#cb5-490" aria-hidden="true" tabindex="-1"></a><span class="vs">        self.eval = False</span></span>
<span id="cb5-491"><a href="#cb5-491" aria-hidden="true" tabindex="-1"></a><span class="vs">        for sm in self._submodules.values():</span></span>
<span id="cb5-492"><a href="#cb5-492" aria-hidden="true" tabindex="-1"></a><span class="vs">            sm.train()</span></span>
<span id="cb5-493"><a href="#cb5-493" aria-hidden="true" tabindex="-1"></a><span class="vs">    </span></span>
<span id="cb5-494"><a href="#cb5-494" aria-hidden="true" tabindex="-1"></a><span class="vs">    def eval(self):</span></span>
<span id="cb5-495"><a href="#cb5-495" aria-hidden="true" tabindex="-1"></a><span class="vs">        self.eval = True</span></span>
<span id="cb5-496"><a href="#cb5-496" aria-hidden="true" tabindex="-1"></a><span class="vs">        for sm in self._submodules.values():</span></span>
<span id="cb5-497"><a href="#cb5-497" aria-hidden="true" tabindex="-1"></a><span class="vs">            sm.eval()</span></span>
<span id="cb5-498"><a href="#cb5-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-499"><a href="#cb5-499" aria-hidden="true" tabindex="-1"></a><span class="vs">    def store(self):</span></span>
<span id="cb5-500"><a href="#cb5-500" aria-hidden="true" tabindex="-1"></a><span class="vs">        self.store = True</span></span>
<span id="cb5-501"><a href="#cb5-501" aria-hidden="true" tabindex="-1"></a><span class="vs">        for sm in self._submodules.values():</span></span>
<span id="cb5-502"><a href="#cb5-502" aria-hidden="true" tabindex="-1"></a><span class="vs">            sm.eval()</span></span>
<span id="cb5-503"><a href="#cb5-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-504"><a href="#cb5-504" aria-hidden="true" tabindex="-1"></a><span class="vs">class Parameter(Module):</span></span>
<span id="cb5-505"><a href="#cb5-505" aria-hidden="true" tabindex="-1"></a><span class="vs">    def __init__(self, value):</span></span>
<span id="cb5-506"><a href="#cb5-506" aria-hidden="true" tabindex="-1"></a><span class="vs">        super().__init__()</span></span>
<span id="cb5-507"><a href="#cb5-507" aria-hidden="true" tabindex="-1"></a><span class="vs">        self.value = value</span></span>
<span id="cb5-508"><a href="#cb5-508" aria-hidden="true" tabindex="-1"></a><span class="vs">        self.temp = None</span></span>
<span id="cb5-509"><a href="#cb5-509" aria-hidden="true" tabindex="-1"></a><span class="vs">        self.grad = None</span></span>
<span id="cb5-510"><a href="#cb5-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-511"><a href="#cb5-511" aria-hidden="true" tabindex="-1"></a><span class="vs">    def parameters(self):</span></span>
<span id="cb5-512"><a href="#cb5-512" aria-hidden="true" tabindex="-1"></a><span class="vs">        return [self]</span></span>
<span id="cb5-513"><a href="#cb5-513" aria-hidden="true" tabindex="-1"></a><span class="vs">    </span></span>
<span id="cb5-514"><a href="#cb5-514" aria-hidden="true" tabindex="-1"></a><span class="vs">class Sequential(Module):</span></span>
<span id="cb5-515"><a href="#cb5-515" aria-hidden="true" tabindex="-1"></a><span class="vs">    def __init__(self, *args):</span></span>
<span id="cb5-516"><a href="#cb5-516" aria-hidden="true" tabindex="-1"></a><span class="vs">        super().__init__()</span></span>
<span id="cb5-517"><a href="#cb5-517" aria-hidden="true" tabindex="-1"></a><span class="vs">        self.sequence = []</span></span>
<span id="cb5-518"><a href="#cb5-518" aria-hidden="true" tabindex="-1"></a><span class="vs">        for arg in args:</span></span>
<span id="cb5-519"><a href="#cb5-519" aria-hidden="true" tabindex="-1"></a><span class="vs">            if isinstance(arg, Module):</span></span>
<span id="cb5-520"><a href="#cb5-520" aria-hidden="true" tabindex="-1"></a><span class="vs">                self.sequence.append(arg)</span></span>
<span id="cb5-521"><a href="#cb5-521" aria-hidden="true" tabindex="-1"></a><span class="vs">            else:</span></span>
<span id="cb5-522"><a href="#cb5-522" aria-hidden="true" tabindex="-1"></a><span class="vs">                self.sequence.extend(arg)</span></span>
<span id="cb5-523"><a href="#cb5-523" aria-hidden="true" tabindex="-1"></a><span class="vs">        </span></span>
<span id="cb5-524"><a href="#cb5-524" aria-hidden="true" tabindex="-1"></a><span class="vs">        self._submodules = {k: v for (k,v) in enumerate(self.sequence)}</span></span>
<span id="cb5-525"><a href="#cb5-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-526"><a href="#cb5-526" aria-hidden="true" tabindex="-1"></a><span class="vs">    def __getitem__(self, index):</span></span>
<span id="cb5-527"><a href="#cb5-527" aria-hidden="true" tabindex="-1"></a><span class="vs">        return self.sequence[index]</span></span>
<span id="cb5-528"><a href="#cb5-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-529"><a href="#cb5-529" aria-hidden="true" tabindex="-1"></a><span class="vs">    def forward(self, X):</span></span>
<span id="cb5-530"><a href="#cb5-530" aria-hidden="true" tabindex="-1"></a><span class="vs">        for m in self.sequence:</span></span>
<span id="cb5-531"><a href="#cb5-531" aria-hidden="true" tabindex="-1"></a><span class="vs">            X = m(X)</span></span>
<span id="cb5-532"><a href="#cb5-532" aria-hidden="true" tabindex="-1"></a><span class="vs">        return X</span></span>
<span id="cb5-533"><a href="#cb5-533" aria-hidden="true" tabindex="-1"></a><span class="vs">    </span></span>
<span id="cb5-534"><a href="#cb5-534" aria-hidden="true" tabindex="-1"></a><span class="vs">ModuleList = Sequential</span></span>
<span id="cb5-535"><a href="#cb5-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-536"><a href="#cb5-536" aria-hidden="true" tabindex="-1"></a><span class="vs">class Sigmoid(Module):</span></span>
<span id="cb5-537"><a href="#cb5-537" aria-hidden="true" tabindex="-1"></a><span class="vs">    def forward(self, X):</span></span>
<span id="cb5-538"><a href="#cb5-538" aria-hidden="true" tabindex="-1"></a><span class="vs">        return tf.sigmoid(X)</span></span>
<span id="cb5-539"><a href="#cb5-539" aria-hidden="true" tabindex="-1"></a><span class="vs">    </span></span>
<span id="cb5-540"><a href="#cb5-540" aria-hidden="true" tabindex="-1"></a><span class="vs">class ReLU(Module):</span></span>
<span id="cb5-541"><a href="#cb5-541" aria-hidden="true" tabindex="-1"></a><span class="vs">    def forward(self, X):</span></span>
<span id="cb5-542"><a href="#cb5-542" aria-hidden="true" tabindex="-1"></a><span class="vs">        return tf.relu(X)</span></span>
<span id="cb5-543"><a href="#cb5-543" aria-hidden="true" tabindex="-1"></a><span class="vs">    </span></span>
<span id="cb5-544"><a href="#cb5-544" aria-hidden="true" tabindex="-1"></a><span class="vs">class Tanh(Module):</span></span>
<span id="cb5-545"><a href="#cb5-545" aria-hidden="true" tabindex="-1"></a><span class="vs">    def forward(self, X):</span></span>
<span id="cb5-546"><a href="#cb5-546" aria-hidden="true" tabindex="-1"></a><span class="vs">        return tf.tanh(X)</span></span>
<span id="cb5-547"><a href="#cb5-547" aria-hidden="true" tabindex="-1"></a><span class="vs">    </span></span>
<span id="cb5-548"><a href="#cb5-548" aria-hidden="true" tabindex="-1"></a><span class="vs">class Linear(Module):</span></span>
<span id="cb5-549"><a href="#cb5-549" aria-hidden="true" tabindex="-1"></a><span class="vs">    def __init__(self, in_features, out_features):</span></span>
<span id="cb5-550"><a href="#cb5-550" aria-hidden="true" tabindex="-1"></a><span class="vs">        super().__init__()</span></span>
<span id="cb5-551"><a href="#cb5-551" aria-hidden="true" tabindex="-1"></a><span class="vs">        # Kaiming He initialization</span></span>
<span id="cb5-552"><a href="#cb5-552" aria-hidden="true" tabindex="-1"></a><span class="vs">        self.W = Parameter(tf.randomNormal([in_features, out_features]) * math.sqrt((2 / out_features) / 3))</span></span>
<span id="cb5-553"><a href="#cb5-553" aria-hidden="true" tabindex="-1"></a><span class="vs">        self.b = Parameter(tf.randomNormal([out_features]) * math.sqrt((2 / out_features) / 3))</span></span>
<span id="cb5-554"><a href="#cb5-554" aria-hidden="true" tabindex="-1"></a><span class="vs">        self.input = None</span></span>
<span id="cb5-555"><a href="#cb5-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-556"><a href="#cb5-556" aria-hidden="true" tabindex="-1"></a><span class="vs">    def forward(self, x):</span></span>
<span id="cb5-557"><a href="#cb5-557" aria-hidden="true" tabindex="-1"></a><span class="vs">        # Returns a new Matrix</span></span>
<span id="cb5-558"><a href="#cb5-558" aria-hidden="true" tabindex="-1"></a><span class="vs">        self.input = None</span></span>
<span id="cb5-559"><a href="#cb5-559" aria-hidden="true" tabindex="-1"></a><span class="vs">        return tf.dot(x, self.W) + self.b</span></span>
<span id="cb5-560"><a href="#cb5-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-561"><a href="#cb5-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-562"><a href="#cb5-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-563"><a href="#cb5-563" aria-hidden="true" tabindex="-1"></a><span class="vs">        </span></span>
<span id="cb5-564"><a href="#cb5-564" aria-hidden="true" tabindex="-1"></a><span class="vs">class Optimizer:</span></span>
<span id="cb5-565"><a href="#cb5-565" aria-hidden="true" tabindex="-1"></a><span class="vs">    def __init__(self, model, loss=None, store=False):</span></span>
<span id="cb5-566"><a href="#cb5-566" aria-hidden="true" tabindex="-1"></a><span class="vs">        self.parameters = list(model.parameters())</span></span>
<span id="cb5-567"><a href="#cb5-567" aria-hidden="true" tabindex="-1"></a><span class="vs">        self.model = model</span></span>
<span id="cb5-568"><a href="#cb5-568" aria-hidden="true" tabindex="-1"></a><span class="vs">        self.loss = loss</span></span>
<span id="cb5-569"><a href="#cb5-569" aria-hidden="true" tabindex="-1"></a><span class="vs">        self.store = store</span></span>
<span id="cb5-570"><a href="#cb5-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-571"><a href="#cb5-571" aria-hidden="true" tabindex="-1"></a><span class="vs">    def _grads(self, loss, *args, **kwargs):</span></span>
<span id="cb5-572"><a href="#cb5-572" aria-hidden="true" tabindex="-1"></a><span class="vs">        def loss_internal(*params):</span></span>
<span id="cb5-573"><a href="#cb5-573" aria-hidden="true" tabindex="-1"></a><span class="vs">            for val, param in zip(params, self.parameters):</span></span>
<span id="cb5-574"><a href="#cb5-574" aria-hidden="true" tabindex="-1"></a><span class="vs">                param.temp = param.value</span></span>
<span id="cb5-575"><a href="#cb5-575" aria-hidden="true" tabindex="-1"></a><span class="vs">                param.value = val</span></span>
<span id="cb5-576"><a href="#cb5-576" aria-hidden="true" tabindex="-1"></a><span class="vs">            try:</span></span>
<span id="cb5-577"><a href="#cb5-577" aria-hidden="true" tabindex="-1"></a><span class="vs">                l = loss(self.model, *args, **kwargs)</span></span>
<span id="cb5-578"><a href="#cb5-578" aria-hidden="true" tabindex="-1"></a><span class="vs">            finally:</span></span>
<span id="cb5-579"><a href="#cb5-579" aria-hidden="true" tabindex="-1"></a><span class="vs">                for param in self.parameters:</span></span>
<span id="cb5-580"><a href="#cb5-580" aria-hidden="true" tabindex="-1"></a><span class="vs">                    param.value = param.temp</span></span>
<span id="cb5-581"><a href="#cb5-581" aria-hidden="true" tabindex="-1"></a><span class="vs">                    param.temp = None</span></span>
<span id="cb5-582"><a href="#cb5-582" aria-hidden="true" tabindex="-1"></a><span class="vs">            return l</span></span>
<span id="cb5-583"><a href="#cb5-583" aria-hidden="true" tabindex="-1"></a><span class="vs">        </span></span>
<span id="cb5-584"><a href="#cb5-584" aria-hidden="true" tabindex="-1"></a><span class="vs">        return grads(loss_internal)(*map(lambda p: p.value, self.parameters))</span></span>
<span id="cb5-585"><a href="#cb5-585" aria-hidden="true" tabindex="-1"></a><span class="vs">    </span></span>
<span id="cb5-586"><a href="#cb5-586" aria-hidden="true" tabindex="-1"></a><span class="vs">    def _step(self, grads):</span></span>
<span id="cb5-587"><a href="#cb5-587" aria-hidden="true" tabindex="-1"></a><span class="vs">        raise NotImplementedError()</span></span>
<span id="cb5-588"><a href="#cb5-588" aria-hidden="true" tabindex="-1"></a><span class="vs">    </span></span>
<span id="cb5-589"><a href="#cb5-589" aria-hidden="true" tabindex="-1"></a><span class="vs">    def step(self, *args, **kwargs):</span></span>
<span id="cb5-590"><a href="#cb5-590" aria-hidden="true" tabindex="-1"></a><span class="vs">        grads = self._grads(self.loss, *args, **kwargs)</span></span>
<span id="cb5-591"><a href="#cb5-591" aria-hidden="true" tabindex="-1"></a><span class="vs">        if self.store:</span></span>
<span id="cb5-592"><a href="#cb5-592" aria-hidden="true" tabindex="-1"></a><span class="vs">          for grad, param in zip(grads, self.parameters):</span></span>
<span id="cb5-593"><a href="#cb5-593" aria-hidden="true" tabindex="-1"></a><span class="vs">            param.grad = grad</span></span>
<span id="cb5-594"><a href="#cb5-594" aria-hidden="true" tabindex="-1"></a><span class="vs">        return self._step(grads)</span></span>
<span id="cb5-595"><a href="#cb5-595" aria-hidden="true" tabindex="-1"></a><span class="vs">    </span></span>
<span id="cb5-596"><a href="#cb5-596" aria-hidden="true" tabindex="-1"></a><span class="vs">    def stepWithLoss(self, loss, *args, **kwargs):</span></span>
<span id="cb5-597"><a href="#cb5-597" aria-hidden="true" tabindex="-1"></a><span class="vs">        grads = self._grads(loss, *args, **kwargs)</span></span>
<span id="cb5-598"><a href="#cb5-598" aria-hidden="true" tabindex="-1"></a><span class="vs">        return self._step(grads)</span></span>
<span id="cb5-599"><a href="#cb5-599" aria-hidden="true" tabindex="-1"></a><span class="vs">    </span></span>
<span id="cb5-600"><a href="#cb5-600" aria-hidden="true" tabindex="-1"></a><span class="vs">class SGD(Optimizer):</span></span>
<span id="cb5-601"><a href="#cb5-601" aria-hidden="true" tabindex="-1"></a><span class="vs">    def __init__(self, model, loss, lr=0.001, store=False):</span></span>
<span id="cb5-602"><a href="#cb5-602" aria-hidden="true" tabindex="-1"></a><span class="vs">        super().__init__(model, loss, store)</span></span>
<span id="cb5-603"><a href="#cb5-603" aria-hidden="true" tabindex="-1"></a><span class="vs">        self.lr = lr</span></span>
<span id="cb5-604"><a href="#cb5-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-605"><a href="#cb5-605" aria-hidden="true" tabindex="-1"></a><span class="vs">    def _step(self, grads):</span></span>
<span id="cb5-606"><a href="#cb5-606" aria-hidden="true" tabindex="-1"></a><span class="vs">        for grad, param in zip(grads, self.parameters):</span></span>
<span id="cb5-607"><a href="#cb5-607" aria-hidden="true" tabindex="-1"></a><span class="vs">            param.value = param.value - self.lr * grad</span></span>
<span id="cb5-608"><a href="#cb5-608" aria-hidden="true" tabindex="-1"></a><span class="vs">`</span></span>
<span id="cb5-609"><a href="#cb5-609" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb5-610"><a href="#cb5-610" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> py<span class="op">;</span></span>
<span id="cb5-611"><a href="#cb5-611" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-2" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-3" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-4" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-5" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-6" data-nodetype="declaration">

</div>
</div>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code hidden" id="cb6" data-startfrom="0" data-source-offset="0"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line -1;"><span id="cb6-0"><a href="#cb6-0" aria-hidden="true" tabindex="-1"></a>mpg <span class="op">=</span> <span class="fu">FileAttachment</span>(<span class="st">"auto-mpg.csv"</span>)<span class="op">.</span><span class="fu">csv</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="ojs-cell-2" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code hidden" id="cb7" data-startfrom="2" data-source-offset="-0"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 1;"><span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> <span class="fu">py</span><span class="vs">`</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="vs"># </span><span class="sc">${</span>plots<span class="sc">}</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="vs"># Setup the data and prediction functions</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="vs">import pandas as pd</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="vs">df = pd.DataFrame(</span><span class="sc">${</span>mpg<span class="sc">}</span><span class="vs">)[['weight', 'mpg']]</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="vs">df = df.astype(float).dropna().values</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="vs">x, y = df[:, :1], df[:, 1:]</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="vs">x = Tensor((x - x.mean()) / x.std())</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="vs">y = Tensor((y - y.mean()) / y.std())</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="vs">def get_batch(batchsize, x, y):</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="vs">  return x, y</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="vs">scale = Tensor([[1., 1.]])</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="vs">def predict(w, x):</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="vs">  w = w.reshape((-1, 2)) * scale</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="vs">  x = x.reshape((-1, 1))</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="vs">  x = tf.concat([x, tf.onesLike(x)], 1)</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="vs">  return tf.dot(x, w.T)</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="vs">wrange = tf.linspace(-3, 3, 25)</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a><span class="vs">brange = tf.linspace(-3, 3, 25)</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="vs">ww, bb = tf.meshgrid(wrange, brange)</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a><span class="vs">paramgrid = tf.stack([ww.flatten(), bb.flatten()]).T</span></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a><span class="vs">eyetheta = 0</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a><span class="vs">(x, y)</span></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a><span class="vs">`</span></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>surfaces <span class="op">=</span> <span class="fu">py</span><span class="vs">`</span></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a><span class="vs"># </span><span class="sc">${</span>batch<span class="sc">}</span><span class="vs"> </span><span class="sc">${</span>plots<span class="sc">}</span></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a><span class="vs"># Plot the loss surface</span></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a><span class="vs">l1weight = 0.</span></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a><span class="vs">l2weight = 0.</span></span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a><span class="vs">def loss(w, x, y):</span></span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a><span class="vs">  w = w.reshape((-1, 2))</span></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a><span class="vs">  return (tf.mean((predict(w, x) - y) ** 2, 0)) + l1weight * tf.abs(w).sum(1) + l2weight * (w ** 2).sum(1) </span></span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a><span class="vs">lossgrid = loss(paramgrid, x, y).reshape(ww.shape)</span></span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a><span class="vs">losscontour = plotconvert(dict(x=wrange, y=brange, z=lossgrid, type='contour', ncontours=25, showscale=False, ))</span></span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a><span class="vs">losssurface = plotconvert(dict(x=wrange, y=brange, z=lossgrid, type='surface', showlegend=False, showscale=False, opacity=0.8,  contours=dict(x=dict(show=True), y=dict(show=True))))</span></span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a><span class="vs">`</span></span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a><span class="fu">py</span><span class="vs">`</span></span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a><span class="vs"># </span><span class="sc">${</span>surfaces<span class="sc">}</span></span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a><span class="vs">cweights = </span><span class="sc">${</span>weights<span class="sc">}</span></span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a><span class="vs">startpoint = dict(x=[cweights[0]], y=[cweights[1]], mode='markers', showlegend=False, marker=dict(color='firebrick', size=10, line= {'color': 'black', 'width': 3}))</span></span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a><span class="vs">fullweightlist = [Tensor(cweights)]</span></span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a><span class="vs">batchweightlist = [Tensor(cweights)]</span></span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a><span class="vs">steps = int(</span><span class="sc">${</span>steps<span class="sc">}</span><span class="vs">)</span></span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a><span class="vs">lr = float(</span><span class="sc">${</span>learningrate<span class="sc">}</span><span class="vs">) if steps &gt; 0 else 0.</span></span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a><span class="vs">momentum = 0.</span></span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a><span class="vs">nxbatch, nybatch = batches[0]</span></span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a><span class="vs">batchgrad = grad(lambda t: loss(t, nxbatch, nybatch))(batchweightlist[-1])</span></span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a><span class="vs">beta = 0.</span></span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a><span class="vs">velocity = batchgrad</span></span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a><span class="vs">magnitude = batchgrad ** 2</span></span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a><span class="vs">if beta &gt; 0:</span></span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a><span class="vs">  batchgrad = batchgrad / tf.sqrt(magnitude + 1e-8)</span></span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a><span class="vs">for i, (nxbatch, nybatch) in zip(range(max(1, steps)), batches):</span></span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a><span class="vs">  fullgrad = lr * grad(lambda t: loss(t, x, y))(fullweightlist[-1])</span></span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a><span class="vs">  bgrad = grad(lambda t: loss(t, nxbatch, nybatch))(batchweightlist[-1])</span></span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a><span class="vs">  velocity = momentum * velocity + (1 - momentum) * bgrad</span></span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a><span class="vs">  magnitude = beta * magnitude + (1. - beta) * (bgrad ** 2)</span></span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a><span class="vs">  batchgrad = velocity</span></span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a><span class="vs">  if beta &gt; 0:</span></span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a><span class="vs">    batchgrad = velocity / tf.sqrt(magnitude + 1e-8)</span></span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a><span class="vs">  </span></span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a><span class="vs">  fullweightlist.append((fullweightlist[-1] - fullgrad).flatten())</span></span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a><span class="vs">  batchweightlist.append((batchweightlist[-1] - lr * batchgrad).flatten())</span></span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a><span class="vs">  </span></span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true" tabindex="-1"></a><span class="vs">fullweights = tf.stack(fullweightlist)</span></span>
<span id="cb7-87"><a href="#cb7-87" aria-hidden="true" tabindex="-1"></a><span class="vs">batchweights = tf.stack(batchweightlist)</span></span>
<span id="cb7-88"><a href="#cb7-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-89"><a href="#cb7-89" aria-hidden="true" tabindex="-1"></a><span class="vs">gradplot = dict(x=fullweights[:, 0], y=fullweights[:, 1], showlegend=False, line=dict(color='black'))</span></span>
<span id="cb7-90"><a href="#cb7-90" aria-hidden="true" tabindex="-1"></a><span class="vs">batchgradplot = dict(x=batchweights[:, 0], y=batchweights[:, 1], showlegend=False, line=dict(color='orange'))</span></span>
<span id="cb7-91"><a href="#cb7-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-92"><a href="#cb7-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-93"><a href="#cb7-93" aria-hidden="true" tabindex="-1"></a><span class="vs">zloss = loss(fullweights, x, y)</span></span>
<span id="cb7-94"><a href="#cb7-94" aria-hidden="true" tabindex="-1"></a><span class="vs">batchzloss = loss(batchweights, x, y)</span></span>
<span id="cb7-95"><a href="#cb7-95" aria-hidden="true" tabindex="-1"></a><span class="vs">threedgradplot = dict(x=fullweights[:, 0], y=fullweights[:, 1], z=zloss, showlegend=False, marker=dict(size=4), line=dict(color='black', width=4), type='scatter3d')</span></span>
<span id="cb7-96"><a href="#cb7-96" aria-hidden="true" tabindex="-1"></a><span class="vs">threedbatchgradplot = dict(x=batchweights[:, 0], y=batchweights[:, 1], z=batchzloss, showlegend=False, marker=dict(size=4), line=dict(color='orange', width=4), type='scatter3d')</span></span>
<span id="cb7-97"><a href="#cb7-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-98"><a href="#cb7-98" aria-hidden="true" tabindex="-1"></a><span class="vs">finalloss = zloss[0].t2Js()</span></span>
<span id="cb7-99"><a href="#cb7-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-100"><a href="#cb7-100" aria-hidden="true" tabindex="-1"></a><span class="vs">PlotlyReactive(lossplot, [losscontour, startpoint, gradplot, batchgradplot], {'title': 'Loss = %.3f' % finalloss, 'showlegend': False, 'xaxis': {'range': [-3, 3], 'title': 'Slope (w)'}, 'yaxis': {'range': [-3, 3], 'title': {</span></span>
<span id="cb7-101"><a href="#cb7-101" aria-hidden="true" tabindex="-1"></a><span class="vs">      'text': 'Bias (b)'</span></span>
<span id="cb7-102"><a href="#cb7-102" aria-hidden="true" tabindex="-1"></a><span class="vs">    }}})</span></span>
<span id="cb7-103"><a href="#cb7-103" aria-hidden="true" tabindex="-1"></a><span class="vs">`</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display hidden">
<div>
<div id="ojs-cell-3-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display hidden">
<div>
<div id="ojs-cell-3-2" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display hidden">
<div>
<div id="ojs-cell-3-3" data-nodetype="expression">

</div>
</div>
</div>
<div class="sourceCode cell-code hidden" id="cb8" data-startfrom="105" data-source-offset="-3420"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 104;"><span id="cb8-105"><a href="#cb8-105" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> <span class="fu">py</span><span class="vs">`</span></span>
<span id="cb8-106"><a href="#cb8-106" aria-hidden="true" tabindex="-1"></a><span class="vs"># </span><span class="sc">${</span>batch<span class="sc">}</span></span>
<span id="cb8-107"><a href="#cb8-107" aria-hidden="true" tabindex="-1"></a><span class="vs"># Plot the data scatterplot and prediction function</span></span>
<span id="cb8-108"><a href="#cb8-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-109"><a href="#cb8-109" aria-hidden="true" tabindex="-1"></a><span class="vs">inweights = </span><span class="sc">${</span>weights<span class="sc">}</span></span>
<span id="cb8-110"><a href="#cb8-110" aria-hidden="true" tabindex="-1"></a><span class="vs">cweights = Tensor(inweights)</span></span>
<span id="cb8-111"><a href="#cb8-111" aria-hidden="true" tabindex="-1"></a><span class="vs">errors = ybatch.reshape((-1,)) - predict(cweights, xbatch.reshape((-1,)),).flatten()</span></span>
<span id="cb8-112"><a href="#cb8-112" aria-hidden="true" tabindex="-1"></a><span class="vs">losses = (errors) ** 2</span></span>
<span id="cb8-113"><a href="#cb8-113" aria-hidden="true" tabindex="-1"></a><span class="vs">batchdata = dict(x=xbatch.reshape((-1,)), y=ybatch.reshape((-1,)), mode='markers', marker=dict(color=tf.sqrt(losses), colorscale='RdBu', cmin=0, cmax=2))</span></span>
<span id="cb8-114"><a href="#cb8-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-115"><a href="#cb8-115" aria-hidden="true" tabindex="-1"></a><span class="vs">xrange = tf.linspace(-2, 3, 50)</span></span>
<span id="cb8-116"><a href="#cb8-116" aria-hidden="true" tabindex="-1"></a><span class="vs">pfunction = dict(x=xrange.flatten(), y=predict(cweights, xrange).flatten(), line=dict(color='black'))</span></span>
<span id="cb8-117"><a href="#cb8-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-118"><a href="#cb8-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-119"><a href="#cb8-119" aria-hidden="true" tabindex="-1"></a><span class="vs">PlotlyReactive(scatterfig, [ batchdata, pfunction], {'title': 'Prediction function: y = %.2f x + %.2f' % (inweights[0], inweights[1]), 'showlegend': False, 'xaxis': {'range': [-2, 3], 'title': {'text': 'x (weight)'}}, 'yaxis': {'range': [-2, 3], 'title': {'text': 'y (MPG)'} }})</span></span>
<span id="cb8-120"><a href="#cb8-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-121"><a href="#cb8-121" aria-hidden="true" tabindex="-1"></a><span class="vs">histdata = dict(x=errors, type='histogram', xbins= dict(start=-3, end=3, size=0.15))</span></span>
<span id="cb8-122"><a href="#cb8-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-123"><a href="#cb8-123" aria-hidden="true" tabindex="-1"></a><span class="vs">nx = tf.linspace(-3, 3, 100)</span></span>
<span id="cb8-124"><a href="#cb8-124" aria-hidden="true" tabindex="-1"></a><span class="vs">ny = tf.exp(-nx ** 2)</span></span>
<span id="cb8-125"><a href="#cb8-125" aria-hidden="true" tabindex="-1"></a><span class="vs">normdata = dict(x=nx, y=ny, line=dict(color='red'), yaxis='y2',)</span></span>
<span id="cb8-126"><a href="#cb8-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-127"><a href="#cb8-127" aria-hidden="true" tabindex="-1"></a><span class="vs">PlotlyReactive(histfig, [ histdata, normdata], {'showlegend': False, 'title': 'Distribution of residuals', 'yaxis': {'title': {'text': 'Frequency'}},  'yaxis2': {'overlaying': 'y', 'range': [0, 1], 'visible': False}, 'xaxis': {'range': [-3, 3], 'title': {'text': 'Error (residual)'} }})</span></span>
<span id="cb8-128"><a href="#cb8-128" aria-hidden="true" tabindex="-1"></a><span class="vs">losses.mean()</span></span>
<span id="cb8-129"><a href="#cb8-129" aria-hidden="true" tabindex="-1"></a><span class="vs">`</span></span>
<span id="cb8-130"><a href="#cb8-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-131"><a href="#cb8-131" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> <span class="fu">py</span><span class="vs">`</span></span>
<span id="cb8-132"><a href="#cb8-132" aria-hidden="true" tabindex="-1"></a><span class="vs"># </span><span class="sc">${</span>data<span class="sc">}</span></span>
<span id="cb8-133"><a href="#cb8-133" aria-hidden="true" tabindex="-1"></a><span class="vs">batchsize = 0</span></span>
<span id="cb8-134"><a href="#cb8-134" aria-hidden="true" tabindex="-1"></a><span class="vs">batches = [get_batch(batchsize, x, y) for i in range(max(1, int(</span><span class="sc">${</span>steps<span class="sc">}</span><span class="vs">)))]</span></span>
<span id="cb8-135"><a href="#cb8-135" aria-hidden="true" tabindex="-1"></a><span class="vs">xbatch, ybatch = batches[0]</span></span>
<span id="cb8-136"><a href="#cb8-136" aria-hidden="true" tabindex="-1"></a><span class="vs">`</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display hidden">
<div>
<div id="ojs-cell-3-4" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display hidden">
<div>
<div id="ojs-cell-3-5" data-nodetype="declaration">

</div>
</div>
</div>
</div>
<div class="column-screen columns">
<div class="column" style="width:33%;">
<div class="cell">
<div class="sourceCode cell-code hidden" id="cb9" data-startfrom="0" data-source-offset="0"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line -1;"><span id="cb9-0"><a href="#cb9-0" aria-hidden="true" tabindex="-1"></a>scatter <span class="op">=</span> <span class="fu">py</span><span class="vs">`</span></span>
<span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="vs"># Scatterplot figure</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="vs">scatterfig = PlotlyFigure(width=500, height=500)</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="vs">scatterfig</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="vs">`</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="ojs-cell-4" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code hidden" id="cb10" data-startfrom="0" data-source-offset="0"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line -1;"><span id="cb10-0"><a href="#cb10-0" aria-hidden="true" tabindex="-1"></a><span class="co">//viewof learningrate = Inputs.range([0, 3], {value: 1, step: 0.01, label: " Learning rate"})</span></span>
<span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>learningrate <span class="op">=</span> <span class="dv">0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="ojs-cell-5" data-nodetype="declaration">

</div>
</div>
</div>
</div><div class="column" style="width:33%;">
<div class="cell">
<div class="sourceCode cell-code hidden" id="cb11" data-startfrom="2" data-source-offset="-1"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 1;"><span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>plots <span class="op">=</span> <span class="fu">py</span><span class="vs">`</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="vs">lossplot = PlotlyInput(width=500, height=500)</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="vs">`</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>viewof weights <span class="op">=</span> <span class="fu">py</span><span class="vs">`</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="vs"># </span><span class="sc">${</span>plots<span class="sc">}</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="vs">lossplot</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="vs">`</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-6-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-6-2" data-nodetype="declaration">

</div>
</div>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code hidden" id="cb12" data-startfrom="0" data-source-offset="0"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line -1;"><span id="cb12-0"><a href="#cb12-0" aria-hidden="true" tabindex="-1"></a><span class="co">//viewof steps = Inputs.range([0, 10], {value: 0, step: 1, label: "  Steps"})</span></span>
<span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>steps <span class="op">=</span> <span class="dv">0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="ojs-cell-7" data-nodetype="declaration">

</div>
</div>
</div>
</div><div class="column" style="width:33%;">
<div class="cell">
<div class="sourceCode cell-code hidden" id="cb13" data-startfrom="0" data-source-offset="0"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line -1;"><span id="cb13-0"><a href="#cb13-0" aria-hidden="true" tabindex="-1"></a>hist <span class="op">=</span> <span class="fu">py</span><span class="vs">`</span></span>
<span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="vs"># Scatterplot figure</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="vs">histfig = PlotlyFigure(width=500, height=500)</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="vs">histfig</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="vs">`</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="ojs-cell-8" data-nodetype="declaration">

</div>
</div>
</div>
</div>
</div>
</section>
<section id="maximum-likelihood-estimation-1" class="level2">
<h2 class="anchored" data-anchor-id="maximum-likelihood-estimation-1">Maximum likelihood estimation</h2>
<p>With this view of linear regression in mind, let’s ask again how we find the optimal value for <span class="math inline">\(\mathbf{w}\)</span>. Possibly the most widely used approach to this is to simply choose the <span class="math inline">\(\mathbf{w}\)</span> that <em>maximizes the likelihood</em> (conditional probability) of all of the outputs in our dataset:</p>
<p><span class="math display">\[
\mathbf{w}^* = \underset{\mathbf{w}}{\text{argmax}} \ p(\mathbf{y} \mid \mathbf{X}, \mathbf{w}) =\underset{\mathbf{w}}{\text{argmax}} \ p(y_1,...,y_N \mid \mathbf{x}_1, ...,\mathbf{x}_N, \mathbf{w}) \]</span></p>
<p>Generally our model also assumes <em>conditional independence</em> across observations so:</p>
<p><span class="math display">\[
p(y_1,...,y_N \mid \mathbf{x}_1, ...,\mathbf{x}_N, \mathbf{w}) = \prod_{i=1}^N p(y_i\mid \mathbf{x}_i, \mathbf{w})
\]</span></p>
<p>For convenience, it is typical to frame the optimal value in terms of the <em>negative log-likelihood</em> rather than the likelihood, but the two are equivalent.</p>
<p><span class="math display">\[
\underset{\mathbf{w}}{\text{argmax}} \prod_{i=1}^N p(y_i\mid \mathbf{x}_i, \mathbf{w}) = \underset{\mathbf{w}}{\text{argmin}} - \sum_{i=1}^N \log p(y_i \mid \mathbf{x}_i, \mathbf{w}) = \textbf{NLL}(\mathbf{w}, \mathbf{X}, \mathbf{y})
\]</span></p>
<p>We see that the negative log-likelihood is a natural <em>loss function</em> to optimize to find <span class="math inline">\(\mathbf{w}^*\)</span>.</p>
<p><span class="math display">\[
\textbf{Loss}(\mathbf{w}) =\textbf{NLL}(\mathbf{w}, \mathbf{X}, \mathbf{y})=- \sum_{i=1}^N \log p(y_i \mid \mathbf{x}_i, \mathbf{w})
\]</span></p>
<p>We can write out the negative log-likelihood explicitly using the normal PDF:</p>
<p><span class="math display">\[
\textbf{NLL}(\mathbf{w}, \mathbf{X}, \mathbf{y}) = -\sum_{i=1}^N\log\bigg[\frac{1}{\sigma \sqrt{2 \pi}} \text{exp}\bigg(-\frac{1}{2\sigma^2} (y_i - \mathbf{x}_i^T\mathbf{w})^2\bigg)\bigg]
\]</span></p>
<p><span class="math display">\[
= \frac{1}{2\sigma^2} \sum_{i=1}^N(y_i - \mathbf{x}_i^T\mathbf{w})^2 + N \log \sigma \sqrt{2 \pi}
\]</span></p>
<p>We see that this loss is very similar to the MSE loss. Taking the gradient this becomes even more clear.</p>
<p><span class="math display">\[
\nabla_{\mathbf{w}}\textbf{NLL}(\mathbf{w}, \mathbf{X}, \mathbf{y}) =
\frac{d}{d\mathbf{w}}\bigg( \frac{1}{2\sigma^2} \sum_{i=1}^N(y_i - \mathbf{x}_i^T\mathbf{w})^2 + N \log \sigma \sqrt{2 \pi} \bigg)
\]</span></p>
<p><span class="math display">\[
= \frac{1}{2\sigma^2}\sum_{i=1}^N (\mathbf{x}_i^T\mathbf{w} - y_i)\mathbf{x}_i
\]</span></p>
<p>As we saw in the MSE case, the optimal value <span class="math inline">\(\mathbf{w}^*\)</span> does not depend on the constant value outside the summation. This means that the optimal value for <span class="math inline">\(\mathbf{w}\)</span> is the same for both MSE and negative log-likelihood and the optimal value does not depend on <span class="math inline">\(\sigma^2\)</span>!</p>
<p><span class="math display">\[
\underset{\mathbf{w}}{\text{argmin}}\  MSE(\mathbf{w}, \mathbf{X}, \mathbf{y}) = \underset{\mathbf{w}}{\text{argmin}}\ \textbf{NLL}(\mathbf{w}, \mathbf{X}, \mathbf{y})
\]</span></p>
<pre><code></code></pre>
</section>
<section id="section" class="level2">
<h2 class="anchored" data-anchor-id="section"></h2>
<pre><code></code></pre>


</section>
</section>

</main> <!-- /main -->
<script type="ojs-module-contents">
{"contents":[{"methodName":"interpret","cellName":"ojs-cell-1","inline":false,"source":"MathJax = {\n  const MathJax = await require('mathjax@2.7.5/MathJax.js?config=TeX-MML-AM_CHTML')\n    .catch(() => window.MathJax)\n  \n  // configure MathJax\n  MathJax.Hub.Config({\n    tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]},\n    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ],\n    processEscapes: true\n  })  \n  return MathJax\n}\n\nPlotly = require(\"https://cdn.plot.ly/plotly-latest.min.js\");\ntfbase = require('@tensorflow/tfjs@4.11.0')\npyodide = {\n  const p =\n    await require(\"https://cdn.jsdelivr.net/pyodide/v0.22.1/full/pyodide.js\");\n  console.log(p);\n  return p.loadPyodide();\n}\n\nPyScope = function() {\n  let scope = pyodide.toPy({});\n  \n  let py = async (strings, ...expressions) => {\n    let globals = {};\n    const code = strings.reduce((result, string, index) => {\n      if (expressions[index]) {\n        const name = `x${index}`;\n        globals[name] = expressions[index];\n        return result + string + name;\n      }\n      return result + string;\n    }, '');\n    await pyodide.loadPackagesFromImports(code);\n    scope.update(pyodide.globals);\n    const result = await pyodide.runPythonAsync(\n      code,\n      {\n        globals: scope\n      }\n    );\n    if (result?.t2Js) return result.t2Js();\n    if (result?.toJs) return result.toJs();\n    return result;\n  };\n  \n  return py;\n}\n\npy = {\n  let testscope = PyScope();\n  let py = async (strings, ...expressions) => {\n    let globals = {};\n    const code = strings.reduce((result, string, index) => {\n      if (expressions[index]) {\n        const name = `x${index}`;\n        globals[name] = expressions[index];\n        return result + string + name;\n      }\n      return result + string;\n    }, '');\n    await pyodide.loadPackagesFromImports(code);\n    pyodide.globals.update(pyodide.toPy(globals))\n    const result = await pyodide.runPythonAsync(\n      code,\n      {globals: pyodide.globals}\n    );\n    if (result?.t2Js) return result.t2Js();\n    if (result?.toJs) return result.toJs();\n    return result;\n  };\n\n  const sigmoidGradConfig  = {\n    kernelName: tfbase.Sigmoid,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n      const [x] = saved;\n      const y = tfbase.sigmoid(x);\n      return {x: () => tfbase.mul(dy, tfbase.mul(y, tfbase.sub(tfbase.scalar(1), y)))};\n    }\n  };\n  tfbase.registerGradient(sigmoidGradConfig);\n\n  const tanhGradConfig = {\n    kernelName: tfbase.Tanh,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n      const [x] = saved;\n      const y = tfbase.tanh(x);\n      return {x: () => tfbase.mul(tfbase.sub(tfbase.scalar(1), tfbase.square(y)), dy)};\n    }\n  };\n  tfbase.registerGradient(tanhGradConfig);\n   const expGradConfig = {\n    kernelName: tfbase.Exp,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n      const [x] = saved;\n      const y = tfbase.exp(x);\n      return {x: () => tfbase.mul(dy, y)};\n    }\n  }; \n  tfbase.registerGradient(expGradConfig);\n\n  function dispatchEvent(element){\n    element.dispatchEvent(new Event(\"input\", {bubbles: true}));\n  }\n  pyodide.globals.update(pyodide.toPy({Plotbase: Plot, tfbase: tfbase, Plotlybase: Plotly, dispatchEvent: dispatchEvent, d3base: d3}))\n  \nawait py`\nfrom pyodide.ffi import create_once_callable\nfrom types import SimpleNamespace\nfrom pyodide.ffi import to_js\nfrom js import Object, document\nimport pandas\nimport numpy as np\n\ntfbase = SimpleNamespace(**tfbase)\n\ndef convert_tensor(a, *args):\n  if isinstance(a, Parameter):\n    a = a.value.value\n  if isinstance(a, Tensor):\n    a = a.value\n  if isinstance(a, np.ndarray):\n    a = a.tolist()\n  return to_js(a)\n\ndef convert(a):\n  if isinstance(a, Parameter):\n    a = a.value.value\n  if isinstance(a, Tensor):\n    a = a.value\n  if isinstance(a, np.ndarray):\n    a = a.tolist()\n  return to_js(a, dict_converter=Object.fromEntries, default_converter=convert_tensor)\n\ndef convert_start(shape, start):\n  start = start or 0\n  if start < 0:\n    start = shape + start\n  start = min(start, shape - 1)\n  return start\n\ndef convert_end(shape, start, end): \n  start = convert_start(shape, start)\n  if end is None:\n    end = shape\n  else:\n    end = convert_start(shape, end)\n  return end - start\n\nclass Tensor:\n  keepall = False\n\n  class Keep:\n    def __enter__(self):\n        self.value = Tensor.keepall\n        Tensor.keepall = True\n    \n    def __exit__(self, *args):\n        Tensor.keepall = self.value\n\n  def __init__(self, *args, value=None, keep=None, **kwargs):\n    if keep is None:\n      self.keep = Tensor.keepall\n    else:\n      self.keep = keep\n\n    if not (value is None):\n      self.value = value\n    elif len(args) and isinstance(args[0], Tensor):\n      self.value = tfbase.add(args[0].value, 0)\n    elif len(args) and args[0] is None:\n      self.value = tfbase.tensor(0.)\n    else:\n      args = [convert(a) for a in args]\n      kwargs = {k: convert(a) for (k, a) in kwargs.items()}\n      self.value = tfbase.tensor(*args, **kwargs)\n\n  def __getattr__(self, name):\n    if name == 'T':\n      return self.transpose()\n    attr = getattr(self.value, name)\n    if callable(attr):\n      def run(*args, **kwargs):\n        args = [convert(a) for a in args]\n        kwargs = {k: convert(a) for (k, a) in kwargs.items()}\n        output = attr(*args, **kwargs)\n        return Tensor(value=output)\n      # Prevent premature garbage collection\n      run._ref = self\n      return run\n    return attr\n\n  def __add__(a, b):\n    return Tensor(value=tfbase.add(convert(a), convert(b)))\n  def __radd__(a, b):\n    return Tensor(value=tfbase.add(convert(b), convert(a)))\n  def __sub__(a, b):\n    return Tensor(value=tfbase.sub(convert(a), convert(b)))\n  def __rsub__(a, b):\n    return Tensor(value=tfbase.sub(convert(b), convert(a)))\n  def __mul__(a, b):\n    return Tensor(value=tfbase.mul(convert(a), convert(b)))\n  def __rmul__(a, b):\n    return Tensor(value=tfbase.mul(convert(b), convert(a)))\n  def __truediv__(a, b):\n    return Tensor(value=tfbase.div(convert(a), convert(b)))\n  def __rtruediv__(a, b):\n    return Tensor(value=tfbase.div(convert(b), convert(a)))\n  def __floordiv__(a, b):\n    return Tensor(value=tfbase.floorDiv(convert(a), convert(b)))\n  def __rfloordiv__(a, b):\n    return Tensor(value=tfbase.floorDiv(convert(b), convert(a)))\n  def __pow__(a, b):\n    return Tensor(value=tfbase.pow(convert(a), convert(b)))\n  def __rpow__(a, b):\n    return Tensor(value=tfbase.pow(convert(b), convert(a)))\n  def __neg__(a):\n    return Tensor(value=tfbase.neg(convert(a)))\n  def __eq__(a, b):\n    return Tensor(value=tfbase.equal(convert(a), convert(b)))\n  def __neq__(a, b):\n    return Tensor(value=tfbase.notEqual(convert(a), convert(b)))\n  def __lt__(a, b):\n    return Tensor(value=tfbase.less(convert(a), convert(b)))\n  def __gt__(a, b):\n    return Tensor(value=tfbase.greater(convert(a), convert(b)))\n  def __leq__(a, b):\n    return Tensor(value=tfbase.lessEqual(convert(a), convert(b)))\n  def __geq__(a, b):\n    return Tensor(value=tfbase.greaterEqual(convert(a), convert(b)))\n\n  def __del__(self):\n    if hasattr(self.value, 'dispose') and not self.keep:\n      self.value.dispose()\n\n  def __iter__(self):\n    for x in self.value.arraySync():\n        yield Tensor(x)\n\n  def __getitem__(self, args):\n    tosqueeze = []\n    starts, ends, steps = [], [], []\n    value = self\n    \n    if not (type(args) is tuple):\n      args = (args,)\n    \n    for ind in range(len(args)):\n      if args[ind] is Ellipsis:\n        start = args[:ind]\n        rest = args[(ind + 1):]\n        args = start + tuple([slice(None)] * (len(self.value.shape) - (len(start) + len(rest)))) + rest\n        break\n    \n    for i, (shape, dim) in enumerate(zip(self.value.shape, args)):\n      if isinstance(dim, slice):\n        starts.append(dim.start or 0)\n        ends.append(dim.stop or shape)\n        steps.append(dim.step or 1)\n      elif Tensor(dim).shape:\n        t = Tensor(dim)\n        if t.value.dtype == 'bool':\n          inds = [ind for (ind, e) in enumerate(t.value.arraySync()) if e]\n          inds = tf.cast(tf.reshape(Tensor(inds), [-1]), 'int32')\n          value = tf.gather(value, inds, i)\n        else:\n          inds = tf.cast(tf.reshape(t, [-1]), 'int32')\n          value = tf.gather(value, inds, i)\n      else:\n        starts.append(dim)\n        ends.append(dim + 1)\n        steps.append(1)\n        tosqueeze.append(i)\n    value = tf.stridedSlice(value, convert(starts), convert(ends), convert(steps))\n    if len(tosqueeze) > 0:\n      value = tf.squeeze(value, tosqueeze)\n    return value\n\n  def t2Js(self):\n    return to_js(self.value.arraySync())\n\nclass wrapper:\n  def __init__(self, f):\n    self.f = f\n\n  def __call__(self, x, *args, **kwargs):\n    with Tensor.Keep():\n      return convert(self.f(Tensor(value=x), *args, **kwargs))\n\nclass grad:\n  def __init__(self, f):\n    self.f = f\n    self.wrapper = wrapper(f)\n\n  def __call__(self, x, *args, **kwargs):\n    output = tfbase.grad(create_once_callable(self.wrapper))(x.value, *args, **kwargs)\n    return Tensor(value=output)\n\nclass wrappers:\n  def __init__(self, f):\n    self.f = f\n\n  def __call__(self, *args):\n    with Tensor.Keep():\n      wrapped_args = [Tensor(value=x) for x in args]\n      return convert(self.f(*wrapped_args))\n\nclass grads:\n  def __init__(self, f):\n    self.f = f\n    self.wrapper = wrappers(f)\n\n  def __call__(self, *args):\n    output = tfbase.grads(create_once_callable(self.wrapper))(to_js([arg.value for arg in args]))\n    return [Tensor(value=x) for x in output]\n\ntf = Tensor(value=tfbase)\nPlotbase = SimpleNamespace(**Plotbase)\nPlotlybase = SimpleNamespace(**Plotlybase)\nd3base = SimpleNamespace(**d3base)\n\ndef meshgrid(*args):\n  return tuple([Tensor(value=a) for a in tfbase.meshgrid(*[convert(arg) for arg in args])])\ntf.meshgrid = meshgrid\n\ndef default_convert(obj, default_f, other):\n  if isinstance(obj, Tensor):\n    obj = obj.t2Js()\n  if isinstance(obj, pandas.DataFrame):\n    obj = obj.to_dict('records') \n  return default_f(obj)\n\ndef plotconvert(a):\n  return to_js(a, dict_converter=Object.fromEntries, default_converter=default_convert)\n\nclass PlotWrapper:\n  def __init__(self, base=None):\n    self.base = base\n    \n  def __getattr__(self, name):\n    attr = getattr(self.base, name)\n    if callable(attr):\n      def run(*args, **kwargs):\n        args = [plotconvert(a) for a in args]\n        kwargs = {k: plotconvert(a) for (k, a) in kwargs.items()}\n        return attr(*args, **kwargs)\n      return run\n    return attr\n\nPlot = PlotWrapper(Plotbase)\nPlotly = PlotWrapper(Plotlybase)\nd3 = PlotWrapper(d3base)\n\ndef PlotlyFigure(width=800, height=None, hide_toolbar=True, overlay=True):\n  if height is None:\n    height = 0.75 * width\n\n  width, height = int(width), int(height)\n  container = document.createElement('div')\n  container.style.width = str(width) + 'px'\n  container.style.height = str(height) + 'px'\n  \n  lineplot = document.createElement('div')\n  lineplot.classList.add(\"plotlydiv\")\n\n  if hide_toolbar:\n    container.classList.add(\"hidetoolbar\")\n\n  container.append(lineplot)\n  if overlay:\n    overlay = document.createElement('div')\n    overlay.classList.add(\"plotlyoverlay\")\n    \n    container.append(overlay)\n    \n    container.style.position = 'relative'\n    overlay.style.top = '0'\n    overlay.style.bottom = '0'\n    overlay.style.width = '100%'\n    overlay.style.position = 'absolute'\n  return container\n  \ndef PlotlyInput(width=800, height=None, hide_toolbar=True, sync=None):\n  container = PlotlyFigure(width, height, hide_toolbar)\n  lineplot, overlay = container.childNodes[0], container.childNodes[1]\n  if sync is None:\n    sync = container\n\n  class mover:\n    def __init__(self):\n      self.mousedown = False\n    \n    def __call__(self, event):\n      if event.type == 'mousedown':\n        self.mousedown = True\n      if event.type == 'mouseleave':\n        self.mousedown = False\n      if event.type == 'mouseup':\n        self.mousedown = False\n  \n      if self.mousedown:\n        x = float(lineplot._fullLayout.xaxis.p2c(event.layerX - lineplot._fullLayout.margin.l))\n        y = float(lineplot._fullLayout.yaxis.p2c(event.layerY - lineplot._fullLayout.margin.t))\n        sync.value = to_js([x, y])\n        dispatchEvent(sync)\n        \n  \n  e = mover()\n  overlay.addEventListener('mousemove', to_js(e))\n  overlay.addEventListener('mousedown', to_js(e))\n  overlay.addEventListener('mouseup', to_js(e))\n  overlay.addEventListener('mouseleave', to_js(e))\n  container.value = to_js([0., 0.])\n  return container\n\ndef PlotlyReactive(container, traces=[], layout={}, options={}):\n  full_layout = dict(width=int(container.style.width.replace('px', '')), height=int(container.style.height.replace('px', '')))\n  full_layout.update(layout)\n  full_options = {'displayModeBar' : not container.classList.contains('hidetoolbar')}\n  full_options.update(options)\n  plot = container.childNodes[0]\n  Plotly.react(plot, traces, full_layout, full_options)\n\ndef colorMap(t, cmap='inferno', cmin=None, cmax=None, scale='linear', res=100):\n  import matplotlib.cm as cm\n  if cmin is None:\n    cmin = tf.min(t)\n  if cmax is None:\n    cmax = tf.max(t)\n  \n  t = (t - cmin) / (cmax - cmin)\n  if scale == 'log':\n    e = tf.exp(1)\n    t = t * (e - 1) + 1\n    t = tf.log(t)\n  cmap = Tensor(cm.get_cmap(cmap, res + 1)(range(res + 1)))\n  t = t * res\n  shape = t.shape\n  tflat = tf.reshape(t, [-1])\n  tfloor = tf.gather(cmap, tf.floor(tflat).cast('int32'))\n  tceil = tf.gather(cmap, tf.ceil(tflat).cast('int32'))\n  tfrac = tf.reshape(tflat - tf.floor(tflat), [-1, 1])\n  tflat = tfrac * tceil + (1. - tfrac) * tfloor\n  t = tf.reshape(tflat, list(shape) + [4])\n  return t\n\ndef plotTensor(t, canvas, size=None, cmap=None, interpolation='nearest', **kwargs):\n  if not (cmap is None):\n    t = colorMap(t, cmap, **kwargs)\n  if size is None:\n    size = (canvas.height, canvas.width)\n  if interpolation == 'bilinear':\n    t = tfbase.image['resizeBilinear'](t.value, list(size))\n  else:\n    t = tfbase.image['resizeNearestNeighbor'](t.value, list(size))\n  tfbase.browser['toPixels'](t, canvas)\n\nfrom itertools import chain\nimport math\n\nclass Module:\n    def __init__(self):\n        self._submodules = dict()\n        self.eval = False\n        self._store = False\n\n    def parameters(self):\n        return chain.from_iterable(map(lambda x: x.parameters(), self._submodules.values()))\n    \n    def __setattr__(self, name, value):\n        if isinstance(value, Module):\n            self._submodules[name] = value\n        super().__setattr__(name, value)\n\n    def __call__(self, *args, **kwargs):\n        value = self.forward(*args, **kwargs)\n        self._store = False\n        return value\n    \n    def forward(self):\n        raise NotImplementedError()\n    \n    def train(self):\n        self.eval = False\n        for sm in self._submodules.values():\n            sm.train()\n    \n    def eval(self):\n        self.eval = True\n        for sm in self._submodules.values():\n            sm.eval()\n\n    def store(self):\n        self.store = True\n        for sm in self._submodules.values():\n            sm.eval()\n\nclass Parameter(Module):\n    def __init__(self, value):\n        super().__init__()\n        self.value = value\n        self.temp = None\n        self.grad = None\n\n    def parameters(self):\n        return [self]\n    \nclass Sequential(Module):\n    def __init__(self, *args):\n        super().__init__()\n        self.sequence = []\n        for arg in args:\n            if isinstance(arg, Module):\n                self.sequence.append(arg)\n            else:\n                self.sequence.extend(arg)\n        \n        self._submodules = {k: v for (k,v) in enumerate(self.sequence)}\n\n    def __getitem__(self, index):\n        return self.sequence[index]\n\n    def forward(self, X):\n        for m in self.sequence:\n            X = m(X)\n        return X\n    \nModuleList = Sequential\n\nclass Sigmoid(Module):\n    def forward(self, X):\n        return tf.sigmoid(X)\n    \nclass ReLU(Module):\n    def forward(self, X):\n        return tf.relu(X)\n    \nclass Tanh(Module):\n    def forward(self, X):\n        return tf.tanh(X)\n    \nclass Linear(Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        # Kaiming He initialization\n        self.W = Parameter(tf.randomNormal([in_features, out_features]) * math.sqrt((2 / out_features) / 3))\n        self.b = Parameter(tf.randomNormal([out_features]) * math.sqrt((2 / out_features) / 3))\n        self.input = None\n\n    def forward(self, x):\n        # Returns a new Matrix\n        self.input = None\n        return tf.dot(x, self.W) + self.b\n\n\n\n        \nclass Optimizer:\n    def __init__(self, model, loss=None, store=False):\n        self.parameters = list(model.parameters())\n        self.model = model\n        self.loss = loss\n        self.store = store\n\n    def _grads(self, loss, *args, **kwargs):\n        def loss_internal(*params):\n            for val, param in zip(params, self.parameters):\n                param.temp = param.value\n                param.value = val\n            try:\n                l = loss(self.model, *args, **kwargs)\n            finally:\n                for param in self.parameters:\n                    param.value = param.temp\n                    param.temp = None\n            return l\n        \n        return grads(loss_internal)(*map(lambda p: p.value, self.parameters))\n    \n    def _step(self, grads):\n        raise NotImplementedError()\n    \n    def step(self, *args, **kwargs):\n        grads = self._grads(self.loss, *args, **kwargs)\n        if self.store:\n          for grad, param in zip(grads, self.parameters):\n            param.grad = grad\n        return self._step(grads)\n    \n    def stepWithLoss(self, loss, *args, **kwargs):\n        grads = self._grads(loss, *args, **kwargs)\n        return self._step(grads)\n    \nclass SGD(Optimizer):\n    def __init__(self, model, loss, lr=0.001, store=False):\n        super().__init__(model, loss, store)\n        self.lr = lr\n\n    def _step(self, grads):\n        for grad, param in zip(grads, self.parameters):\n            param.value = param.value - self.lr * grad\n`\n  \n  return py;\n}\n"},{"methodName":"interpret","cellName":"ojs-cell-2","inline":false,"source":"mpg = FileAttachment(\"auto-mpg.csv\").csv()\n"},{"methodName":"interpret","cellName":"ojs-cell-3","inline":false,"source":"data = py`\n# ${plots}\n# Setup the data and prediction functions\nimport pandas as pd\ndf = pd.DataFrame(${mpg})[['weight', 'mpg']]\ndf = df.astype(float).dropna().values\n\nx, y = df[:, :1], df[:, 1:]\nx = Tensor((x - x.mean()) / x.std())\ny = Tensor((y - y.mean()) / y.std())\n\ndef get_batch(batchsize, x, y):\n  return x, y\n\nscale = Tensor([[1., 1.]])\n\ndef predict(w, x):\n  w = w.reshape((-1, 2)) * scale\n  x = x.reshape((-1, 1))\n  x = tf.concat([x, tf.onesLike(x)], 1)\n  return tf.dot(x, w.T)\n\nwrange = tf.linspace(-3, 3, 25)\nbrange = tf.linspace(-3, 3, 25)\nww, bb = tf.meshgrid(wrange, brange)\nparamgrid = tf.stack([ww.flatten(), bb.flatten()]).T\neyetheta = 0\n\n(x, y)\n`\n\nsurfaces = py`\n# ${batch} ${plots}\n# Plot the loss surface\n\n\nl1weight = 0.\nl2weight = 0.\n\n\ndef loss(w, x, y):\n  w = w.reshape((-1, 2))\n  return (tf.mean((predict(w, x) - y) ** 2, 0)) + l1weight * tf.abs(w).sum(1) + l2weight * (w ** 2).sum(1) \n\nlossgrid = loss(paramgrid, x, y).reshape(ww.shape)\nlosscontour = plotconvert(dict(x=wrange, y=brange, z=lossgrid, type='contour', ncontours=25, showscale=False, ))\nlosssurface = plotconvert(dict(x=wrange, y=brange, z=lossgrid, type='surface', showlegend=False, showscale=False, opacity=0.8,  contours=dict(x=dict(show=True), y=dict(show=True))))\n`\n\npy`\n# ${surfaces}\n\ncweights = ${weights}\nstartpoint = dict(x=[cweights[0]], y=[cweights[1]], mode='markers', showlegend=False, marker=dict(color='firebrick', size=10, line= {'color': 'black', 'width': 3}))\n\n\nfullweightlist = [Tensor(cweights)]\nbatchweightlist = [Tensor(cweights)]\nsteps = int(${steps})\nlr = float(${learningrate}) if steps > 0 else 0.\n\nmomentum = 0.\nnxbatch, nybatch = batches[0]\nbatchgrad = grad(lambda t: loss(t, nxbatch, nybatch))(batchweightlist[-1])\nbeta = 0.\nvelocity = batchgrad\nmagnitude = batchgrad ** 2\nif beta > 0:\n  batchgrad = batchgrad / tf.sqrt(magnitude + 1e-8)\n\nfor i, (nxbatch, nybatch) in zip(range(max(1, steps)), batches):\n  fullgrad = lr * grad(lambda t: loss(t, x, y))(fullweightlist[-1])\n\n  bgrad = grad(lambda t: loss(t, nxbatch, nybatch))(batchweightlist[-1])\n  velocity = momentum * velocity + (1 - momentum) * bgrad\n  magnitude = beta * magnitude + (1. - beta) * (bgrad ** 2)\n  batchgrad = velocity\n  if beta > 0:\n    batchgrad = velocity / tf.sqrt(magnitude + 1e-8)\n  \n  fullweightlist.append((fullweightlist[-1] - fullgrad).flatten())\n  batchweightlist.append((batchweightlist[-1] - lr * batchgrad).flatten())\n  \n\nfullweights = tf.stack(fullweightlist)\nbatchweights = tf.stack(batchweightlist)\n\ngradplot = dict(x=fullweights[:, 0], y=fullweights[:, 1], showlegend=False, line=dict(color='black'))\nbatchgradplot = dict(x=batchweights[:, 0], y=batchweights[:, 1], showlegend=False, line=dict(color='orange'))\n\n\nzloss = loss(fullweights, x, y)\nbatchzloss = loss(batchweights, x, y)\nthreedgradplot = dict(x=fullweights[:, 0], y=fullweights[:, 1], z=zloss, showlegend=False, marker=dict(size=4), line=dict(color='black', width=4), type='scatter3d')\nthreedbatchgradplot = dict(x=batchweights[:, 0], y=batchweights[:, 1], z=batchzloss, showlegend=False, marker=dict(size=4), line=dict(color='orange', width=4), type='scatter3d')\n\nfinalloss = zloss[0].t2Js()\n\nPlotlyReactive(lossplot, [losscontour, startpoint, gradplot, batchgradplot], {'title': 'Loss = %.3f' % finalloss, 'showlegend': False, 'xaxis': {'range': [-3, 3], 'title': 'Slope (w)'}, 'yaxis': {'range': [-3, 3], 'title': {\n      'text': 'Bias (b)'\n    }}})\n`\n\nloss = py`\n# ${batch}\n# Plot the data scatterplot and prediction function\n\ninweights = ${weights}\ncweights = Tensor(inweights)\nerrors = ybatch.reshape((-1,)) - predict(cweights, xbatch.reshape((-1,)),).flatten()\nlosses = (errors) ** 2\nbatchdata = dict(x=xbatch.reshape((-1,)), y=ybatch.reshape((-1,)), mode='markers', marker=dict(color=tf.sqrt(losses), colorscale='RdBu', cmin=0, cmax=2))\n\nxrange = tf.linspace(-2, 3, 50)\npfunction = dict(x=xrange.flatten(), y=predict(cweights, xrange).flatten(), line=dict(color='black'))\n\n\nPlotlyReactive(scatterfig, [ batchdata, pfunction], {'title': 'Prediction function: y = %.2f x + %.2f' % (inweights[0], inweights[1]), 'showlegend': False, 'xaxis': {'range': [-2, 3], 'title': {'text': 'x (weight)'}}, 'yaxis': {'range': [-2, 3], 'title': {'text': 'y (MPG)'} }})\n\nhistdata = dict(x=errors, type='histogram', xbins= dict(start=-3, end=3, size=0.15))\n\nnx = tf.linspace(-3, 3, 100)\nny = tf.exp(-nx ** 2)\nnormdata = dict(x=nx, y=ny, line=dict(color='red'), yaxis='y2',)\n\nPlotlyReactive(histfig, [ histdata, normdata], {'showlegend': False, 'title': 'Distribution of residuals', 'yaxis': {'title': {'text': 'Frequency'}},  'yaxis2': {'overlaying': 'y', 'range': [0, 1], 'visible': False}, 'xaxis': {'range': [-3, 3], 'title': {'text': 'Error (residual)'} }})\nlosses.mean()\n`\n\nbatch = py`\n# ${data}\nbatchsize = 0\nbatches = [get_batch(batchsize, x, y) for i in range(max(1, int(${steps})))]\nxbatch, ybatch = batches[0]\n`\n"},{"methodName":"interpret","cellName":"ojs-cell-4","inline":false,"source":"scatter = py`\n# Scatterplot figure\nscatterfig = PlotlyFigure(width=500, height=500)\nscatterfig\n`\n"},{"methodName":"interpret","cellName":"ojs-cell-5","inline":false,"source":"//viewof learningrate = Inputs.range([0, 3], {value: 1, step: 0.01, label: \" Learning rate\"})\nlearningrate = 0\n"},{"methodName":"interpret","cellName":"ojs-cell-6","inline":false,"source":"\nplots = py`\nlossplot = PlotlyInput(width=500, height=500)\n`\n\nviewof weights = py`\n# ${plots}\nlossplot\n`\n"},{"methodName":"interpret","cellName":"ojs-cell-7","inline":false,"source":"//viewof steps = Inputs.range([0, 10], {value: 0, step: 1, label: \"  Steps\"})\nsteps = 0\n"},{"methodName":"interpret","cellName":"ojs-cell-8","inline":false,"source":"hist = py`\n# Scatterplot figure\nhistfig = PlotlyFigure(width=500, height=500)\nhistfig\n`\n"},{"methodName":"interpretQuiet","source":"shinyInput('weights')"}]}
</script>
<script type="module">
if (window.location.protocol === "file:") { alert("The OJS runtime does not work with file:// URLs. Please use a web server to view this document."); }
window._ojs.paths.runtimeToDoc = "../../lecture2-linear-regression";
window._ojs.paths.runtimeToRoot = "../..";
window._ojs.paths.docToRoot = "..";
window._ojs.selfContained = false;
window._ojs.runtime.interpretFromScriptTags();
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>