{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 4:** Automatic Differentiation and Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this homework we will build a tiny automatic differentiation, matrix and neural network library from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run me first!\n",
    "from hw4_support import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python features\n",
    "This homework makes use of a few fancy features in Python that are worth knowing about if you are unfamiliar.\n",
    "- [Variable length arguments](https://book.pythontips.com/en/latest/args_and_kwargs.html) (e.g. `*args`)\n",
    "- [List comprehensions](https://book.pythontips.com/en/latest/comprehensions.html#list-comprehensions) (e.g. `[a**2 for a in range(5)]`)\n",
    "- [Magic methods](https://rszalski.github.io/magicmethods/) (e.g. `__add__`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Reverse-mode automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by developing an automatic differentiation class that uses *reverse-mode automatic differentiation*, as this is what will be most useful for neural networks. \n",
    "\n",
    "Recall that for reverse-mode AD to work, everytime we perform an operation on one or more numbers we need to store the result of that operation as well as the *parent values* (the inputs to the operation). We also need to be able to compute the derivative of that operation. Since for every operation we need to store several pieces of data and several functions, it makes sense to define a *class* to represent the result of an operation. \n",
    "\n",
    "For example, if we want to make a class that represents the operation `c=a+b` our class needs several properties:\n",
    "- `value`: The value of the operation (`c`)\n",
    "- `parents`: The parent operations (`a` and `b`)\n",
    "- `grad`: The derivative of the final loss with respect to `c` ($\\frac{dL}{dc}$)\n",
    "- `func`: A function that computes the operation (`a+b`)\n",
    "- `grads`: A function that computes the derivatives of the operation ($\\frac{dc}{da}$ and $\\frac{dc}{db}$)\n",
    "\n",
    "For this homework, we've provided the outline of such a class, called `AutogradValue`. This will be the base class for all of our possible operations and represents declaring a variable with a value (`a = 5`). This is useful because it lets us define values that we might want to find derivatives with respect to.\n",
    "\n",
    "Let's see how this will work in practice. If we want to take derivatives we will first define the inputs using `AutogradValue`.\n",
    "\n",
    "```{python}\n",
    "a = AutogradValue(5)\n",
    "b = AutogradValue(2)\n",
    "```\n",
    "Then we can perform whatever operations we want on these inputs:\n",
    "```{python}\n",
    "c = a + b\n",
    "L = log(c)\n",
    "```\n",
    "Each of these operations will produce a new `AutogradValue` object representing the result of that operation.\n",
    "\n",
    "Finally we can run the backward pass by running a method `backward()` (that we will write) on the outout `L`. This will compute the gradients of  `L` with respect to each input that we defined ($\\frac{dL}{da}$ and $\\frac{dL}{db}$). Rather than returning these derivatives, the `backward()` method will *update* the `grad` property of `a` and `b`, making it easy to access the correct derivative.\n",
    "\n",
    "```{python}\n",
    "L.backward()\n",
    "dL_da = a.grad\n",
    "```\n",
    "\n",
    "We'll also be able to compute operations with non-AutogradValue numbers, but obviously won't be able to compute derivaitives with respect to these values.\n",
    "```{python}\n",
    "s = 4\n",
    "L = s * a\n",
    "dL_da = a.grad # Will work because a is an AutogradValue\n",
    "dL_ds = s.grad # Will give an error because s is not an AutogradValue\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen what our final produce will look like, let's define our `AutogradValue` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutogradValue:\n",
    "    '''\n",
    "    Base class for automatic differentiation operations. Represents variable delcaration.\n",
    "    Subclasses will overwrite func and grads to define new operations.\n",
    "\n",
    "    Properties:\n",
    "        parents (list): A list of the inputs to the operation, may be AutogradValue or float\n",
    "        args    (list): A list of raw values of each input (as floats)\n",
    "        grad    (float): The derivative of the final loss with respect to this value (dL/da)\n",
    "        value   (float): The value of the result of this operation\n",
    "    '''\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        self.parents = list(args)\n",
    "        self.args = [arg.value if isinstance(arg, AutogradValue) else arg for arg in self.parents]\n",
    "        self.grad = 0.\n",
    "        self.value = self.forward_pass()\n",
    "\n",
    "    def func(self, input):\n",
    "        '''\n",
    "        Compute the value of the operation given the inputs.\n",
    "        For declaring a variable, this is just the identity function (return the input).\n",
    "\n",
    "        Args:\n",
    "            input (float): The input to the operation\n",
    "        Returns:\n",
    "            value (float): The result of the operation\n",
    "        '''\n",
    "        return input\n",
    "\n",
    "    def grads(self, *args):\n",
    "        '''\n",
    "        Compute the derivative of the operation with respect to each input.\n",
    "        In the base case the derivative of the identity function is just 1. (da/da = 1).\n",
    "\n",
    "        Args:\n",
    "            input (float): The input to the operation\n",
    "        Returns:\n",
    "            grads (tuple): The derivative of the operation with respect to each input\n",
    "                            Here there is only a single input, so we return a length-1 tuple.\n",
    "        '''\n",
    "        return (1,)\n",
    "    \n",
    "    def forward_pass(self):\n",
    "        # Calls func to compute the value of this operation \n",
    "        return self.func(*self.args)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        # Python magic function for string representation.\n",
    "        return str(self.value)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined the framework for an operation that can be used in automatic differentiation, we need to define some actual useful operations by subclassing `AutogradValue`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1\n",
    "Fill out the `func` and `grads` methods of each subclass below. Recall that `func` should always return the result of the operation and `grads` should always return a `tuple` of the derivative with respect to each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (3924302752.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 22\u001b[0;36m\u001b[0m\n\u001b[0;31m    def grads(self, a, b):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class _add(AutogradValue):\n",
    "    def func(self, a, b):\n",
    "        return a + b\n",
    "    \n",
    "    def grads(self, a, b):\n",
    "        return 1., 1.\n",
    "\n",
    "class _sub(AutogradValue):\n",
    "    def func(self, a, b):\n",
    "        return a - b\n",
    "    \n",
    "    def grads(self, a, b):\n",
    "        return 1., -1.\n",
    "\n",
    "class _neg(AutogradValue):\n",
    "    def func(self, a):\n",
    "        return -a\n",
    "    \n",
    "    def grads(self, a):\n",
    "        return (-1.,)\n",
    "    \n",
    "class _mul(AutogradValue):\n",
    "    def func(self, a, b):\n",
    "        return a * b\n",
    "    \n",
    "    def grads(self, a, b):\n",
    "        return b, a\n",
    "\n",
    "class _div(AutogradValue):\n",
    "    def func(self, a, b):\n",
    "        return a / b\n",
    "    \n",
    "    def grads(self, a, b):\n",
    "        return 1 / b, -a / (b * b)\n",
    "    \n",
    "class _exp(AutogradValue):\n",
    "    def func(self, a):\n",
    "        return math.exp(a)\n",
    "    \n",
    "    def grads(self, a):\n",
    "        return (math.exp(a),)\n",
    "\n",
    "class _log(AutogradValue):\n",
    "    def func(self, a):\n",
    "        return math.log(a)\n",
    "    \n",
    "    def grads(self, a):\n",
    "        return (1 / a,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we'll define our basic functions and operators in terms of the operator classes we just wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp(a):\n",
    "    return _exp(a) if isinstance(a, AutogradValue) else math.exp(a)\n",
    "def log(a):\n",
    "    return _log(a) if isinstance(a, AutogradValue) else math.log(a)\n",
    "\n",
    "# Note: Remember that above we defined a class for each type of operation\n",
    "# so in this code we are overriding the basic operators for AutogradValue\n",
    "# such that they construct a new object of the class corresponding to the\n",
    "# given operation and return it. \n",
    "# (You don't need to everything that's happening here to do the HW)\n",
    "AutogradValue.exp = lambda a: _exp(a)\n",
    "AutogradValue.log = lambda a: _log(a)\n",
    "AutogradValue.__add__ = lambda a, b: _add(a, b)\n",
    "AutogradValue.__radd__ = lambda a, b: _add(b, a)\n",
    "AutogradValue.__sub__ = lambda a, b: _sub(a, b)\n",
    "AutogradValue.__rsub__ = lambda a, b: _sub(b, a)\n",
    "AutogradValue.__neg__ = lambda a: _neg(a)\n",
    "AutogradValue.__mul__ = lambda a, b: _mul(a, b)\n",
    "AutogradValue.__rmul__ = lambda a, b: _mul(b, a)\n",
    "AutogradValue.__truediv__ = lambda a, b: _div(a, b)\n",
    "AutogradValue.__rtruediv__ = lambda a, b: _div(b, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now be able to use our `AutogradValue` objects as if they are numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = AutogradValue(5)\n",
    "b = AutogradValue(2)\n",
    "\n",
    "print((a + 5) * b)\n",
    "print(log(b))\n",
    "\n",
    "test_operators(AutogradValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let confirm that we do keep the entire compuational graph for operations defined in this way. \n",
    "\n",
    "#### Q2\n",
    "Write a function `graph_print` that takes a single argument. If the argument is an `AutogradValue` (or one of its subclasses), print its `value` property and then call `graph_print` on each of its parents. If the argument is not an `AutogradValue`, just print it. The format of printing is not important.\n",
    "\n",
    "***Hint:** You can use the built-in Python function `isinstance` to determine if something is an `AutogradValue` or one of its subclasses. e.g. `isinstance(a, AutogradValue)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_print(a):\n",
    "    # Check if we're an AutogradValue\n",
    "    if isinstance(a, AutogradValue):\n",
    "        # Recursively call on each parent\n",
    "        for p in a.parents:\n",
    "            graph_print(p)\n",
    "        print(a.value)\n",
    "    else:\n",
    "        print(a)\n",
    "\n",
    "a = AutogradValue(5.)\n",
    "b = AutogradValue(2.)\n",
    "c = log((a + 5) * b)\n",
    "graph_print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function should print (it's ok if the numbers or order aren't exact):\n",
    "```\n",
    "2.995732273553991\n",
    "20.0\n",
    "10.0\n",
    "5.0\n",
    "5.0\n",
    "5\n",
    "2.0\n",
    "2.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in order to do automatic differentiation, we need to define how to do the backward pass. We'll start with the backward_step for a single operation.\n",
    "\n",
    "#### Q3\n",
    "Fill in the method `backward_pass` which computes a single step of the reverse pass through the computational graph (assume `self` is an `AutogradValue` instance). If `backward_pass` is called on a value `c`, the method should:\n",
    "- Assume that `self.grad` contains the derivaive of the final loss with respect to `c` ($\\frac{dL}{dc}$).\n",
    "- Check if each parent of `c`  is an `AutogradValue`. If it is, update that parent's `grad` property to account for `c` (e.g. for parent `a`, update the value of $\\frac{dL}{da}$)\n",
    "\n",
    "**For example:** if `c` represents the result of an addition so `c = a + b`,\n",
    "calling `backward_pass` on `c` will update the `grad` property of both `a` and `b`. (`a.grad` represents $\\frac{dL}{da}$ and is initialized to `0`).\n",
    "\n",
    "***Hint:** `grads` will be one of the methods we wrote in Q1. Recall that if `c` has parents `a` and `b` then `grads` method will give $\\frac{dc}{da}$ and $\\frac{dc}{db}$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(self):\n",
    "    local_grads = self.grads(*self.args)\n",
    "\n",
    "    # Loop through pairs of parents and their corresponding grads\n",
    "    for node, grad in zip(self.parents, local_grads):\n",
    "        # Update the gradient of each AutogradValue parent\n",
    "        if isinstance(node, AutogradValue):\n",
    "            node.grad += self.grad * grad\n",
    "\n",
    "\n",
    "AutogradValue.backward_pass = backward_pass\n",
    "\n",
    "# Test our implementation\n",
    "test_backward_pass(AutogradValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need to define the backward method itself. We will call this on the loss value to find the derivatives of the loss with respect to each input. This means working our way backward through the sequence of operations. Remember that if `c=a+b`, then if `c.grad` is $\\frac{dL}{dc}$, calling `backward_pass` on `c` will update $\\frac{dL}{da}$ (`a.grad`) and $\\frac{dL}{db}$ (`b.grad`).\n",
    " \n",
    "The complication is that `c` may be used in multiple operations, so we can't call `backward_pass` on `c` until we've called `backward_pass` on each child operation of `c` otherwise `c.grad` won't have the correct value of $\\frac{dL}{dc}$, as in this example:\n",
    "\n",
    "```{python}\n",
    "c = a + b\n",
    "g = c * 2\n",
    "h = c + 4\n",
    "L = g * h\n",
    "\n",
    "L.backward_pass() # Updates dL/dg and dL/dh\n",
    "h.backward_pass() # Updates dL/dc\n",
    "\n",
    "##WRONG ORDER\n",
    "c.backward_pass() # Incorrect because dL/dc hasn't accounted for dL/dg\n",
    "g.backward_pass()\n",
    "\n",
    "## CORRECT ORDER\n",
    "g.backward_pass() # Updates dL/dc\n",
    "c.backward_pass() # Updates dL/da and dL/db\n",
    "```\n",
    "\n",
    "#### Q4\n",
    "Fill in the `backward` method for `AutogradValue`. Your backward method should call `backward_pass` on each operation used to compute the loss (`self` is the loss value). Some important things to keep in mind:\n",
    "- `backward_pass` should only be called **once** on each operation\n",
    "- `backward_pass` must be called on **every child** of an operation before it can be called on the operation.\n",
    "- You should not try to call `backward_pass` on values that aren't instances of `AutogradValue`, even though this might be stored in `operation.parents`\n",
    "\n",
    "***Hint:** The efficiency of this method will have a large impact on the running time of later problems! We won't score efficiency in grading, but it still might be worth optimizing this function a bit.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple, but slow implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self):\n",
    "    # We call backward on the loss, so dL/dL = 1\n",
    "    self.grad = 1.\n",
    "\n",
    "    # Setup a queue of nodes to visit, starting with self (the final loss)\n",
    "    queue = [self]\n",
    "    # Setup a list keep track of the order to call backward_pass()\n",
    "    order = []\n",
    "\n",
    "    # Visit each AutogradValue in the queue\n",
    "    while len(queue) > 0:\n",
    "        node = queue.pop()\n",
    "        if isinstance(node, AutogradValue):\n",
    "\n",
    "            # We only want to keep the last instance of each node in the\n",
    "            # order, so if we visit a node already in the order, remove it\n",
    "            if node in order:\n",
    "                order.remove(node)\n",
    "\n",
    "            # Add the node to the end of the order and its paraent to the queue\n",
    "            order.append(node)\n",
    "            queue.extend(node.parents)\n",
    "    \n",
    "    # Once we have the order call backward pass on every node\n",
    "    for node in order:\n",
    "        node.backward_pass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Faster implementation by keeping track of visit counts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self):\n",
    "    # We call backward on the loss, so dL/dL = 1\n",
    "    self.grad = 1.\n",
    "    queue = [self]\n",
    "    order = []\n",
    "\n",
    "    # Additionally keep track of the visit counts for each node\n",
    "    counts = {}\n",
    "    while len(queue) > 0:\n",
    "        node = queue.pop()\n",
    "        \n",
    "        # Rather than removing nodes from the order [slow, O(N)], \n",
    "        # just mark that it has been visited again [O(1)]\n",
    "        if isinstance(node, AutogradValue):\n",
    "            if node in counts:\n",
    "                counts[node] += 1\n",
    "            else:\n",
    "                counts[node] = 1\n",
    "\n",
    "            order.append(node)\n",
    "            queue.extend(node.parents)\n",
    "    \n",
    "    # Go through the order, but only call backward pass once we're at\n",
    "    # the last vist for a given node\n",
    "    for node in order:\n",
    "        counts[node] -= 1\n",
    "        if counts[node] == 0:\n",
    "            node.backward_pass()\n",
    "\n",
    "AutogradValue.backward = backward\n",
    "# Test our implementation\n",
    "test_backward(AutogradValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our `AutogradValue` class to compute derivatives!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = AutogradValue(5)\n",
    "b = AutogradValue(2)\n",
    "\n",
    "L = -log(5 *b + a)\n",
    "L.backward()\n",
    "print(a.grad, b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to train a neural network using our automatic differentiation implementation, we're going to want to be able to use numpy to do matrix operations. Fortunately, the our `AutogradValue` class is (mostly) compatible with numpy!\n",
    "\n",
    "We can create arrays of `AutogradValue` and take derivatives as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([AutogradValue(5), AutogradValue(2)])\n",
    "L = np.dot(a, a)\n",
    "L.backward()\n",
    "print('Gradient for a', a[0].grad, a[1].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be a bit tedious to define every AutogradValue array in this way, so let's write some convinience functions to make doing automatic differentiation with numpy easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5\n",
    "Complete the following two functions `wrap_array` and `unwrap_gradient`. \n",
    "\n",
    "`wrap_array` should take a numpy array of floats and return a new array where every element has been made into an `AutogradValue`. \n",
    "\n",
    "`unwrap_gradient` should take a numpy array of `AutogradValue` and return a new array of floats, where every element is the extracted `grad` property of the corresponding element from the original array.\n",
    "\n",
    "Both of these functions should work on 2-D arrays (matrices) at a minimum (but more general solutions that support 1 and/or >2 dimensional arrays are also possible).\n",
    "\n",
    "***Hint:** You can create an array from nested lists as shown above.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We'll start by creating a function that applys a function to each element of an array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def element_map(f, a):\n",
    "    '''\n",
    "    Creates a new array the same shape as a, with a function f applied to each element.\n",
    "\n",
    "    Args:\n",
    "        a (function): The function to apply\n",
    "        a (array): The array to map\n",
    "    Returns:\n",
    "        g (array): An array g, such that g[i,j] = f(a[i,j])\n",
    "    '''\n",
    "\n",
    "    # Store the original shape\n",
    "    shape = a.shape\n",
    "    \n",
    "    # Create a 1-d array with the same elements using flatten()\n",
    "    # then iterate through applying f to each element\n",
    "    flat_wrapped = np.array([f(ai) for ai in a.flatten()])\n",
    "\n",
    "    # Reshape back to the original shape\n",
    "    return flat_wrapped.reshape(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can use our `element_map` function to implement both wrapping and unwrapping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def wrap_array(a):\n",
    "    '''\n",
    "    Wraps the elements of an array with AutogradValue\n",
    "\n",
    "    Args:\n",
    "        a (array of float): The array to wrap\n",
    "    Returns:\n",
    "        g (array of AutogradValue): An array g, such that g[i,j] = AutogradValue(a[i,j])\n",
    "    '''\n",
    "    return element_map(AutogradValue, a)\n",
    "    \n",
    "\n",
    "def unwrap_gradient(a):\n",
    "    '''\n",
    "    Unwraps the gradient of an array with AutogradValues\n",
    "\n",
    "    Args:\n",
    "        a (array of AutogradValue): The array to unwrap\n",
    "    Returns:\n",
    "        g (array of float): An array g, such that g[i,j] = a[i,j].grad\n",
    "    '''\n",
    "    return element_map(lambda ai: ai.grad, a)\n",
    "\n",
    "\n",
    "test_wrap_unwrap(wrap_array, unwrap_gradient, AutogradValue)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementing a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have everything we need to apply our automatic differentiation to train a neural network!\n",
    "\n",
    "Before we do that though, let's try out our automatic differentiation for logistic regression. Below is a slight modification of LogisticRegression implementation we saw in the last homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x):\n",
    "    # Computes the sigmoid function\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, dims):\n",
    "        '''\n",
    "        Args:\n",
    "            dims (int): d, the dimension of each input\n",
    "        '''\n",
    "        self.weights = np.zeros((dims + 1, 1))\n",
    "\n",
    "    def prediction_function(self, X, w):\n",
    "        '''\n",
    "        Get the result of our base function for prediction (i.e. x^t w)\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "            w (array): A (d+1) x 1 vector of weights.\n",
    "        Returns:\n",
    "            pred (array): A length N vector of f(X).\n",
    "        '''\n",
    "        X = np.pad(X, ((0,0), (0,1)), constant_values=1.)\n",
    "        return np.dot(X, w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict labels given a set of inputs.\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "        Returns:\n",
    "            pred (array): An N x 1 column vector of predictions in {0, 1}\n",
    "        '''\n",
    "        return (self.prediction_function(X, self.weights) > 0)\n",
    "    \n",
    "    def predict_probability(self, X):\n",
    "        '''\n",
    "        Predict the probability of each class given a set of inputs\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "        Returns:\n",
    "            probs (array): An N x 1 column vector of predicted class probabilities\n",
    "        '''\n",
    "        return sigmoid(self.prediction_function(X, self.weights))\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        '''\n",
    "        Compute the accuracy of the model's predictions on a dataset\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "            y (array): A length N vector of labels.\n",
    "        Returns:\n",
    "            acc (float): The accuracy of the classifier\n",
    "        '''\n",
    "        y = y.reshape((-1, 1))\n",
    "        return (self.predict(X) == y).mean()\n",
    "\n",
    "    def nll(self, X, y, w=None):\n",
    "        '''\n",
    "        Compute the negative log-likelihood loss.\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "            y (array): A length N vector of labels.\n",
    "            w (array, optional): A (d+1) x 1 matrix of weights.\n",
    "        Returns:\n",
    "            nll (float): The NLL loss\n",
    "        '''\n",
    "        if w is None:\n",
    "            w = self.weights\n",
    "\n",
    "        y = y.reshape((-1, 1))\n",
    "        xw = self.prediction_function(X, w)\n",
    "        py = sigmoid((2 * y - 1) * xw)\n",
    "        return -(np.log(py)).sum()\n",
    "    \n",
    "    def nll_gradient(self, X, y):\n",
    "        '''\n",
    "        Compute the gradient of the negative log-likelihood loss.\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "            y (array): A length N vector of labels.\n",
    "        Returns:\n",
    "            grad (array): A length (d + 1) vector with the gradient\n",
    "        '''\n",
    "        y = y.reshape((-1, 1))\n",
    "        xw = self.prediction_function(X, self.weights)\n",
    "        py = sigmoid((2 * y - 1) * xw)\n",
    "        grad = ((1 - py) * (2 * y - 1)).reshape((-1, 1)) * np.pad(X, [(0,0), (0,1)], constant_values=1.)\n",
    "        return -np.sum(grad, axis=0)\n",
    "    \n",
    "    def nll_and_grad_no_autodiff(self, X, y):\n",
    "        # Compute nll_and_grad without automatic diferentiation\n",
    "        return self.nll(X, y), self.nll_gradient(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6 \n",
    "Write the method `nll_and_grad` for the LogisticRegression class using the automatic differentiation tools we built above. Verify that it gives a similar answer to `nll_and_grad_no_autodiff`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_and_grad(self, X, y):\n",
    "    # Wrap the array we want to differentiate with respect to (weights)\n",
    "    w = wrap_array(self.weights)\n",
    "\n",
    "    # Run the NLL functoin and call backward to populate the gradients\n",
    "    nll = self.nll(X, y, w)\n",
    "    nll.backward()\n",
    "\n",
    "    # Get both the nll value and graident\n",
    "    return nll.value, unwrap_gradient(w)\n",
    "LogisticRegression.nll_and_grad = nll_and_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our automatic differentiation is very inefficient (we'll fix this in the next homework!), so we'll test our model on a very small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(100, noise=0.1)\n",
    "model = LogisticRegression(2)\n",
    "gradient_descent(model, X, y, lr=3e-2, steps=250)\n",
    "\n",
    "print('Model accuracy: %.3f' % model.accuracy(X, y))\n",
    "plot_boundary(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extend our model to be a neural network! We'll create a neural network class that extends our logistic regression class. First we'll setup the needed weight matrices.\n",
    "\n",
    "#### Q7\n",
    "Fill in the Neural Network `__init__` method below. The method should take in the input data dimension and a list of integers specifying the size of each hidden layer (the number of neurons in each layer). The function should create a list of numpy arrays of the appropriate shapes for the weight matrices. \n",
    "\n",
    "For example if `dims` is `2` and `hidden_sizes` is `[4, 4]`, then `self.weights` should have 3 entries of shapes `[(4x2), (4x4), (1x4)]`. This network is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div scale=0.5><svg xmlns=\"http://www.w3.org/2000/svg\" style=\"cursor: move;\" viewbox=\"100 100 1660 899\" width=\"600\" height=\"400\"><g transform=\"translate(-1450.305465592915,-694.5417988897334) scale(2.441893025338307)\"><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,469.5, 846.6666666666666,429.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,469.5, 846.6666666666666,469.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,469.5, 846.6666666666666,509.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,469.5, 846.6666666666666,549.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,509.5, 846.6666666666666,429.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,509.5, 846.6666666666666,469.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,509.5, 846.6666666666666,509.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M666.6666666666666,509.5, 846.6666666666666,549.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,429.5, 1026.6666666666667,429.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,429.5, 1026.6666666666667,469.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,429.5, 1026.6666666666667,509.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,469.5, 1026.6666666666667,429.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,469.5, 1026.6666666666667,469.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,469.5, 1026.6666666666667,509.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,509.5, 1026.6666666666667,429.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,509.5, 1026.6666666666667,469.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,509.5, 1026.6666666666667,509.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,549.5, 1026.6666666666667,429.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,549.5, 1026.6666666666667,469.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,549.5, 1026.6666666666667,509.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M1026.6666666666667,429.5, 1206.6666666666667,489.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M1026.6666666666667,469.5, 1206.6666666666667,489.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M1026.6666666666667,509.5, 1206.6666666666667,489.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,429.5, 1026.6666666666667,549.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,469.5, 1026.6666666666667,549.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,509.5, 1026.6666666666667,549.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M846.6666666666666,549.5, 1026.6666666666667,549.5\"></path><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;\" marker-end=\"\" d=\"M1026.6666666666667,549.5, 1206.6666666666667,489.5\"></path><circle r=\"10\" class=\"node\" id=\"0_0\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"666.6666666666666\" cy=\"469.5\"></circle><circle r=\"10\" class=\"node\" id=\"0_1\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"666.6666666666666\" cy=\"509.5\"></circle><circle r=\"10\" class=\"node\" id=\"1_0\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"846.6666666666666\" cy=\"429.5\"></circle><circle r=\"10\" class=\"node\" id=\"1_1\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"846.6666666666666\" cy=\"469.5\"></circle><circle r=\"10\" class=\"node\" id=\"1_2\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"846.6666666666666\" cy=\"509.5\"></circle><circle r=\"10\" class=\"node\" id=\"1_3\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"846.6666666666666\" cy=\"549.5\"></circle><circle r=\"10\" class=\"node\" id=\"2_0\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1026.6666666666667\" cy=\"429.5\"></circle><circle r=\"10\" class=\"node\" id=\"2_1\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1026.6666666666667\" cy=\"469.5\"></circle><circle r=\"10\" class=\"node\" id=\"2_2\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1026.6666666666667\" cy=\"509.5\"></circle><text class=\"text\" dy=\".35em\" style=\"font-size: 12px;\" x=\"631.6666666666666\" y=\"589.5\">Input Layer ∈ ℝ²</text><text class=\"text\" dy=\".35em\" style=\"font-size: 12px;\" x=\"811.6666666666666\" y=\"589.5\">Hidden Layer ∈ ℝ⁴</text><text class=\"text\" dy=\".35em\" style=\"font-size: 12px;\" x=\"991.6666666666667\" y=\"589.5\">Hidden Layer ∈ ℝ⁴</text><circle r=\"10\" class=\"node\" id=\"2_3\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1026.6666666666667\" cy=\"549.5\"></circle><circle r=\"10\" class=\"node\" id=\"3_0\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1206.6666666666667\" cy=\"489.5\"></circle><text class=\"text\" dy=\".35em\" style=\"font-size: 12px;\" x=\"1171.6666666666667\" y=\"589.5\">Output Layer ∈ ℝ¹</text></g><defs><marker id=\"arrow\" viewBox=\"0 -5 10 10\" markerWidth=\"7\" markerHeight=\"7\" orient=\"auto\" refX=\"40\"><path d=\"M0,-5L10,0L0,5\" style=\"stroke: rgb(80, 80, 80); fill: none;\"></path></marker></defs></svg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you find it easier you could also define the weights in terms of $W^T$ instead, in which case the shapes would be: `[(2x4), (4x4), (4x1)]`. You could also consider how to add a bias term at each layer as in logistic regression (but this isn't nessecary for full credit).\n",
    "\n",
    "The values in each array should be drawn from a normal distribution with standard deviation 1. You can create such a matrix in numpy using:\n",
    "\n",
    "```\n",
    "np.random.normal(scale=1., size=shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(LogisticRegression):\n",
    "    def __init__(self, dims, hidden_sizes=[]):\n",
    "        # Create a list of all layer dimensions (including input and output)\n",
    "        sizes = [dims] + hidden_sizes + [1]\n",
    "        # Create each layer weight matrix (including bias dimension)\n",
    "        self.weights = [np.random.normal(scale=1., size=(i + 1, o)) \n",
    "                        for (i, o) in zip(sizes[:-1], sizes[1:])]\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that for logistic regression the prediction function (before threholding or sigmoid) was $\\mathbf{X}\\mathbf{w}$. We now want to implement the prediction function for our neural network class. This function should perform the appropriate feature transforms and multiply by the regression weights. For a neural network with a single hidden layer this will look like:\n",
    "\n",
    "$f(\\mathbf{X}) = \\sigma(\\mathbf{X}\\mathbf{W}_1^T)\\mathbf{w}_0$\n",
    "\n",
    "Use the **sigmoid** activation function for this problem.\n",
    "\n",
    "For multiple layers we can also think of this a a **chain** of feature transforms:\n",
    "$$\\Phi_1 = \\sigma(\\mathbf{X}\\mathbf{W}_1^T)$$\n",
    "$$\\Phi_2 = \\sigma(\\Phi_1 \\mathbf{W}_2^T)$$\n",
    "$$...$$\n",
    "$$\\Phi_l = \\sigma(\\Phi_{l-1} \\mathbf{W}_l^T)$$\n",
    "$$f(\\mathbf{X}) = \\Phi_l\\mathbf{w}_0$$\n",
    "Where $\\Phi_i$ is just the variable that represents the neurons at layer $i$ (the result of the first $i$ transforms applied to $\\mathbf{X}$).\n",
    "\n",
    "#### Q8 \n",
    "Implement the prediction function as described above. Note that the prediction function should use the weights passed into the `w` argument rather than `self.weights`, this will make it easier to implement the next question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_function(self, X, w):\n",
    "    '''\n",
    "    Get the result of our base function for prediction (i.e. x^t w)\n",
    "\n",
    "    Args:\n",
    "        X (array): An N x d matrix of observations.\n",
    "        w (list of arrays): A list of weight matrices\n",
    "    Returns:\n",
    "        pred (array): An N x 1 matrix of f(X).\n",
    "    '''\n",
    "    # Iterate through the weights of each layer and apply the linear function and activation\n",
    "    for wi in w[:-1]:\n",
    "        X = np.pad(X, ((0,0), (0,1)), constant_values=1.) # Only if we're using bias\n",
    "        X = sigmoid(np.dot(X, wi))\n",
    "\n",
    "    # For the output layer, we don't apply the activation\n",
    "    X = np.pad(X, ((0,0), (0,1)), constant_values=1.)\n",
    "    return np.dot(X, wi).reshape((-1, 1))\n",
    "\n",
    "NeuralNetwork.prediction_function = prediction_function\n",
    "test_nn_prediction_function(NeuralNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q9\n",
    "Implement an `nll_and_grad` method for the `NeuralNetwork` class using your automatic differentiation implmentation to compute the gradient with respect to each weight matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_and_grad(self, X, y):\n",
    "    '''\n",
    "    Get the negative log-likelihood loss and its gradient\n",
    "\n",
    "    Args:\n",
    "        X (array): An N x d matrix of observations.\n",
    "        y (array): A length N vector of labels\n",
    "    Returns:\n",
    "        nll (float): The negative log-likelihood\n",
    "        grads (list of arrays): A list of the gradient of the nll with respect\n",
    "                                to each value in self.weights.\n",
    "    '''\n",
    "    # Same approach as Q6, but this time we need to wrap and unwrap lists of weights\n",
    "    w = [wrap_array(wi) for wi in self.weights]\n",
    "    nll = self.nll(X, y, w)\n",
    "    nll.backward()\n",
    "    return nll.value, [unwrap_gradient(wi) for wi in w]\n",
    "\n",
    "NeuralNetwork.nll_and_grad = nll_and_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything in place to train a neural network from scratch! Let's try it on our tiny dataset. Feel free to change the inputs.\n",
    "\n",
    "***Hint**: If this give very poor results and/or runs very slowly, make sure to carefully check the shape of each operation in your code to make sure it matches your expectation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(100, noise=0.1)\n",
    "model = NeuralNetwork(2, [5, 5])\n",
    "gradient_descent(model, X, y, lr=3e-2, steps=250)\n",
    "\n",
    "print('Model accuracy: %.3f' % model.accuracy(X, y))\n",
    "plot_boundary(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Forward-mode automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try out the other type of automatic differentiation that we learned about: *forward-mode*. To do this we'll create a subclass of our `AutogradValue` class called `ForwardValue`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that for this version of automatic differentiaion each operation needs to keep track of the derivative of it's value with respect *each* original input. For example, in the code below the `g` object needs to store both $\\frac{dg}{da}$ *and* $\\frac{dg}{db}$.\n",
    "\n",
    "```\n",
    "a = ForwardValue(5)\n",
    "b = ForwardValue(2)\n",
    "c = a + b\n",
    "g = c * 2\n",
    "```\n",
    "\n",
    "Our `ForwardValue` class will maintain a `dict` called `forward_grads` that maps each original input object to the derivative of the current value with respect to that input (so `g` will have a `dict` with keys that are the `a` and `b` **objects**). \n",
    "\n",
    "In the base case, when we declare a variable the derivative with respect to itself is just 1 ($\\frac{da}{da}=1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardValue(AutogradValue):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__(*args)\n",
    "        self.forward_grads = {self: 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see now that our `forward_pass` method needs to update `forward_grads` (e.g. to compute $\\frac{dg}{da}$ and $\\frac{dg}{db}$) using the `forward_grads` values of its parents (e.g. $\\frac{dc}{da}$ and $\\frac{dc}{db}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q10\n",
    "Implement the `forward_pass` method below for forward-mode automatic differentiation. This method should update the `foward_grads` property of the operation such that:\n",
    "- `foward_grads` has an entry for every input that appears in `foward_grads` of *any* parent operation.\n",
    "- If an input appears in more than 1 parent, make sure to *add* the gradients appropritately (if `g` has parents `b` and `c` then $\\frac{dg}{da} = \\frac{dg}{db}\\frac{db}{da} + \\frac{dg}{dc}\\frac{dc}{da}$  )\n",
    "- Parents that are not `AutogradValue` objects are ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our `forward_pass` method is working correctly, we should have the following behaivior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our inputs as ForwardValue objects\n",
    "a = ForwardValue(5)\n",
    "b = ForwardValue(2)\n",
    "\n",
    "# Perform operations\n",
    "c = a * b\n",
    "g = 3 * c + a\n",
    "\n",
    "\n",
    "# We should have the following in the forward_grads property of c and d (note that the keys are ForwardValue objects!)\n",
    "c.forward_grads = {a: 2, b: 5}  # dc/da and dc/db\n",
    "g.forward_grads = {a: 3 * 2 + 1, b: 3 * 5} # dg/da = dg/dc dc/da + dg/da, dg/db = dg/dc dc/db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the method below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(self):\n",
    "    self.forward_grads = {}\n",
    "    grads = self.grads(*self.args)\n",
    "                \n",
    "    # Again iterate through pairs of parent, local derivative\n",
    "    for node, grad  in zip(self.parents, grads):\n",
    "        # Check if the parent has a forward_grads property\n",
    "        if hasattr(node, 'forward_grads'):\n",
    "            # Iterate through all inputs in the parents forward_grad dict. \n",
    "            # Add it to our forward_grads if we haven't yet and update it\n",
    "            for key, value in node.forward_grads.items():\n",
    "                if key not in self.forward_grads:\n",
    "                    self.forward_grads[key] = 0\n",
    "                self.forward_grads[key] += value * grad\n",
    "                \n",
    "    # Make sure to still return the operation's value\n",
    "    return self.func(*self.args)\n",
    "\n",
    "# Overwrite the AutogradValue method so that operators still work\n",
    "AutogradValue.forward_pass = forward_pass\n",
    "test_forward_mode(ForwardValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take derivates of functions much like with our reverse-mode implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ForwardValue(5)\n",
    "b = ForwardValue(2)\n",
    "L = -log(5 *b + a)\n",
    "\n",
    "dL_da = L.forward_grads[a]\n",
    "dL_db = L.forward_grads[b]\n",
    "print('dL/da = %.3f, dL/db = %.3f' % (dL_da, dL_db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q11\n",
    "\n",
    "Complete the `ForwardModeNeuralNetwork` class that inherits from `NeuralNetwork` by implementing `nll_and_grad` to use the forward-mode implementation you just wrote!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hint:** Just like with the `NeuralNetwork` class from before we need to first wrap the weight elements into objects, this time of class `ForwardValue`. After wrapping we should have:*\n",
    "`wrapped_w = [[ForwardValue(w[0,0]), ...], ...]` \n",
    "*Once we caluculate the loss, we should have a dictionary that maps `ForwardValue` objects to their respective derivatives: `L.forward_grads = {ForwardValue(w[0,0]): dL/dw00, ...}`. You will need to **unwrap** these derivatives as before to create a gradient array. This time the unwrap function will need to access derivative values from `L.forward_grads`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardModeNeuralNetwork(NeuralNetwork):\n",
    "    def nll_and_grad(self, X, y):\n",
    "        # Here we'll reuse the element_map function we wrote for Q5 to wrap the weights\n",
    "        w = [element_map(ForwardValue, wi) for wi in self.weights]\n",
    "        nll = self.nll(X, y, w)\n",
    "\n",
    "        # The derivative for each element is contained in nll.forward_grads, so to unwrap\n",
    "        # we need to extract each element from that map\n",
    "        grads = [element_map(lambda x: nll.forward_grads[x], wi) for wi in w]\n",
    "        return nll.value, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again test it on our tiny dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(100, noise=0.1)\n",
    "model = ForwardModeNeuralNetwork(2, [5, 5])\n",
    "gradient_descent(model, X, y, lr=3e-2, steps=250)\n",
    "\n",
    "print('Model accuracy: %.3f' % model.accuracy(X, y))\n",
    "plot_boundary(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q12\n",
    "Based on what we've learned and your experience here, why would we choose forward or reverse-mode automatic differentiation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed in class, reverse-mode is typically more efficient when the number of inupts in large. In this case the problem was small and both were comparable. We might also find forward-mode to be simpler to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
