{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckmBOk_nmdDl"
   },
   "source": [
    "# **Homework 6:** Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR NAME HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborators\n",
    "\n",
    "Please list anyone you discussed or collaborated on this assignment with below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIST COLLABORATORS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course feedback\n",
    "\n",
    "Please submit this week's course survey here: https://forms.gle/ELjvh2PK7iiAHbaC8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to PDF\n",
    "\n",
    "Please convert this notebook to PDF for submission to Gradescope using this tool: https://blank-app-ufu2uvdeosc.streamlit.app/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cE8XDumNmdDp"
   },
   "outputs": [],
   "source": [
    "# Uncomment and run if using Colab!\n",
    "\n",
    "#!wget https://cs152.github.io/assignments/homeworks/Homework%206/hw6_support.py\n",
    "\n",
    "# Run me first!\n",
    "from hw6_support import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Copy the corresponding answers from homework 5 here. You may use either your own answers or published solutions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _add(AutogradValue):\n",
    "    # Addition operator (a + b)\n",
    "    def func(self, a, b):\n",
    "        return a + b\n",
    "\n",
    "    def grads(self, a, b):\n",
    "        return 1., 1.\n",
    "\n",
    "class _neg(AutogradValue):\n",
    "    # Negation operator (-a)\n",
    "    def func(self, a):\n",
    "        return -a\n",
    "\n",
    "    def grads(self, a):\n",
    "        return (-1.,)\n",
    "\n",
    "class _sub(AutogradValue):\n",
    "    # Subtraction operator (a - b)\n",
    "    def func(self, a, b):\n",
    "        # Your code here\n",
    "\n",
    "    def grads(self, a, b):\n",
    "        # Your code here\n",
    "\n",
    "class _mul(AutogradValue):\n",
    "    # Multiplication operator (a * b)\n",
    "    def func(self, a, b):\n",
    "        # Your code here\n",
    "\n",
    "    def grads(self, a, b):\n",
    "        # Your code here\n",
    "\n",
    "class _div(AutogradValue):\n",
    "    # Division operator (a / b)\n",
    "    def func(self, a, b):\n",
    "        # Your code here\n",
    "\n",
    "    def grads(self, a, b):\n",
    "        # Your code here\n",
    "\n",
    "class _exp(AutogradValue):\n",
    "    # Exponent operator (e^a, or exp(a))\n",
    "    def func(self, a):\n",
    "        # Your code here\n",
    "\n",
    "    def grads(self, a):\n",
    "        # Your code here\n",
    "\n",
    "class _log(AutogradValue):\n",
    "    # (Natural) log operator (log(a))\n",
    "    def func(self, a):\n",
    "        # Your code here\n",
    "\n",
    "    def grads(self, a):\n",
    "        # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Copy the corresponding answers from homework 5 here. You may use either your own answers or published solutions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oub6-gpsmdDs"
   },
   "outputs": [],
   "source": [
    "def backward_pass(self):\n",
    "    ## COPY CODE FROM HOMEWORK 5 HERE\n",
    "    local_grads = self.grads(*self.args)\n",
    "\n",
    "AutogradValue.backward_pass = backward_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7ylfIvNmdDx"
   },
   "outputs": [],
   "source": [
    "def backward(self):\n",
    "    # We call backward on the loss, so dL/dL = 1\n",
    "    self.grad = 1.\n",
    "\n",
    "    ## COPY CODE FROM HOMEWORK 5 HERE\n",
    "\n",
    "AutogradValue.backward = backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp(a):\n",
    "    return _exp(a) if isinstance(a, AutogradValue) else math.exp(a)\n",
    "def log(a):\n",
    "    return _log(a) if isinstance(a, AutogradValue) else math.log(a)\n",
    "\n",
    "# Note: Remember that above we defined a class for each type of operation\n",
    "# so in this code we are overriding the basic operators for AutogradValue\n",
    "# such that they construct a new object of the class corresponding to the\n",
    "# given operation and return it.\n",
    "# (You don't need to everything that's happening here to do the HW)\n",
    "AutogradValue.exp = lambda a: _exp(a)\n",
    "AutogradValue.log = lambda a: _log(a)\n",
    "AutogradValue.__add__ = lambda a, b: _add(a, b)\n",
    "AutogradValue.__radd__ = lambda a, b: _add(b, a)\n",
    "AutogradValue.__sub__ = lambda a, b: _sub(a, b)\n",
    "AutogradValue.__rsub__ = lambda a, b: _sub(b, a)\n",
    "AutogradValue.__neg__ = lambda a: _neg(a)\n",
    "AutogradValue.__mul__ = lambda a, b: _mul(a, b)\n",
    "AutogradValue.__rmul__ = lambda a, b: _mul(b, a)\n",
    "AutogradValue.__truediv__ = lambda a, b: _div(a, b)\n",
    "AutogradValue.__rtruediv__ = lambda a, b: _div(b, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cgidu0mmdDx"
   },
   "source": [
    "Now we're ready to test out our `AutogradValue` implementation in the context it's designed for: neural networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgYBPXPXmdDx"
   },
   "outputs": [],
   "source": [
    "def pad(a):\n",
    "    # Pads an array with a column of 1s (for bias term)\n",
    "    return a.pad() if isinstance(a, AutogradValue) else np.pad(a, ((0, 0), (0, 1)), constant_values=1., mode='constant')\n",
    "\n",
    "def matmul(a, b):\n",
    "    # Multiplys two matrices\n",
    "    return _matmul(a, b) if isinstance(a, AutogradValue) or isinstance(b, AutogradValue) else np.matmul(a, b)\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Computes the sigmoid function\n",
    "    return 1. / (1. + (-x).exp()) if isinstance(x, AutogradValue) else 1. / (1. + np.exp(-x))\n",
    "\n",
    "def log(x):\n",
    "    # Computes the sigmoid function\n",
    "    return x.log() if isinstance(x, AutogradValue) else np.log(x)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, dims, hidden_sizes=[]):\n",
    "        # Create a list of all layer dimensions (including input and output)\n",
    "        sizes = [dims] + hidden_sizes + [1]\n",
    "        # Create each layer weight matrix (including bias dimension)\n",
    "        self.weights = [np.random.normal(scale=1., size=(i + 1, o))\n",
    "                        for (i, o) in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def prediction_function(self, X, w):\n",
    "        # Iterate through the weights of each layer and apply the linear function and activation\n",
    "        for wi in w[:-1]:\n",
    "            X = pad(X) # Only if we're using bias\n",
    "            X = sigmoid(matmul(X, wi))\n",
    "\n",
    "        # For the output layer, we don't apply the activation\n",
    "        X = pad(X)\n",
    "        return matmul(X, w[-1])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.prediction_function(X, self.weights) > 0)\n",
    "\n",
    "    def predict_probability(self, X):\n",
    "        return sigmoid(self.prediction_function(X, self.weights))\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y = y.reshape((-1, 1))\n",
    "        return (self.predict(X) == y).mean()\n",
    "\n",
    "    def nll(self, X, y, w=None):\n",
    "        if w is None:\n",
    "            w = self.weights\n",
    "\n",
    "        y = y.reshape((-1, 1))\n",
    "        xw = self.prediction_function(X, w)\n",
    "        py = sigmoid(xw * (2 * y - 1))\n",
    "        return -(log(py)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qS-UQZImdDx"
   },
   "source": [
    "#### Autograd for a neural network\n",
    "\n",
    "**Copy the corresponding answers from homework 5 here. You may use either your own answers or published solutions.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_array(a):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "\n",
    "def unwrap_gradient(a):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tjPd3UnXmdDx"
   },
   "outputs": [],
   "source": [
    "def nll_and_grad(self, X, y):\n",
    "    ## YOUR CODE HERE\n",
    "    loss =\n",
    "    grads =\n",
    "    return loss.value, grads\n",
    "\n",
    "NeuralNetwork.nll_and_grad = nll_and_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjQ3HlJmmdDx"
   },
   "source": [
    "We now have everything in place to train a neural network from scratch! Let's try it on our tiny dataset. Feel free to change the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2yR_dWYmdDx"
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(100, noise=0.1)\n",
    "model = NeuralNetwork(2, [5, 5])\n",
    "gradient_descent(model, X, y, lr=3e-2, steps=250)\n",
    "\n",
    "print('Model accuracy: %.3f' % model.accuracy(X, y))\n",
    "plot_boundary(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{dL}{d\\mathbf{a}} = ?$$\n",
    "$$\\frac{dL}{d\\mathbf{c}} = ?$$\n",
    "$$\\frac{d\\mathbf{c}}{d\\mathbf{a}} = ?$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{dL}{d\\mathbf{w}} = ?$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xJtKcodmdDx"
   },
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufSJD8NXmdDx"
   },
   "outputs": [],
   "source": [
    "class _add(AutogradValue):\n",
    "    def func(self, a, b):\n",
    "        return a + b\n",
    "\n",
    "    def vjp(self, grad, a, b):\n",
    "        return grad, grad\n",
    "\n",
    "class _pad(AutogradValue):\n",
    "    def func(self, a):\n",
    "        return np.pad(a, ((0, 0), (0, 1)), constant_values=1., mode='constant')\n",
    "\n",
    "    def vjp(self, grad, a):\n",
    "        return (grad[:, :-1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0ibN4zFmdDx"
   },
   "outputs": [],
   "source": [
    "class _sub(AutogradValue):\n",
    "    def func(self, a, b):\n",
    "        return a - b\n",
    "\n",
    "    def vjp(self, grad, a, b):\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "class _neg(AutogradValue):\n",
    "    def func(self, a):\n",
    "        return -a\n",
    "\n",
    "    def vjp(self, grad, a):\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "class _mul(AutogradValue):\n",
    "    def func(self, a, b):\n",
    "        return a * b\n",
    "\n",
    "    def vjp(self, grad, a, b):\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "class _div(AutogradValue):\n",
    "    def func(self, a, b):\n",
    "        return a / b\n",
    "\n",
    "    def vjp(self, grad, a, b):\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "class _exp(AutogradValue):\n",
    "    def func(self, a):\n",
    "        return np.exp(a)\n",
    "\n",
    "    def vjp(self,grad,  a):\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "class _log(AutogradValue):\n",
    "    def func(self, a):\n",
    "        return np.log(a)\n",
    "\n",
    "    def vjp(self, grad, a):\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "test_vjp(_neg, '_neg', )\n",
    "test_vjp(_exp, '_exp', true_func=anp.exp)\n",
    "test_vjp(_log, '_log', true_func=anp.log)\n",
    "test_vjp(_sub, '_sub', True)\n",
    "test_vjp(_mul, '_mul', True)\n",
    "test_vjp(_div, '_div', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWk2C_9qmdDy"
   },
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pd9s8O9XmdDz"
   },
   "outputs": [],
   "source": [
    "class _sum(AutogradValue):\n",
    "    def func(self, a):\n",
    "        return np.sum(a)\n",
    "\n",
    "    def vjp(self, grad, a):\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "test_vjp(_sum, '_sum', true_func=anp.sum, issum=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oN3_whtMmdDz"
   },
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q5**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zk8pS2l7mdDz"
   },
   "source": [
    "$$\\frac{dL}{dA_{ij}} = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ysOYspImdDz"
   },
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q6**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSC7UV-jmdDz"
   },
   "source": [
    "$$\\frac{dC_{il}}{dA_{ij}}= , \\quad\\frac{dL}{dA_{ij}} = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjFtJ_n_mdDz"
   },
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q7**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6tUzw42mdDz"
   },
   "source": [
    "$$\\frac{dL}{d\\mathbf{A}}= $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Po8gwXlmdDz"
   },
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q8**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUnVoFBhmdDz"
   },
   "source": [
    "$$\\frac{dL}{d\\mathbf{B}}=$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pae33higmdDz"
   },
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uacKzNi-mdDz"
   },
   "outputs": [],
   "source": [
    "class _matmul(AutogradValue):\n",
    "    def func(self, a, b):\n",
    "        return np.matmul(a, b)\n",
    "\n",
    "    def vjp(self, grad, a, b):\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "test_vjp(_matmul, '_matmul', binary=True, true_func=anp.matmul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhKJB5KwmdDz"
   },
   "source": [
    "Now that we've written the `vjp` versions of our operators, we'll update our `AutogradValue` class to use them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PY4Ma1OemdDz"
   },
   "outputs": [],
   "source": [
    "# Note: Remember that above we defined a class for each type of operation\n",
    "# so in this code we are overriding the basic operators for AutogradValue\n",
    "# such that they construct a new object of the class corresponding to the\n",
    "# given operation and return it.\n",
    "# (You don't need to everything that's happening here to do the HW)\n",
    "AutogradValue.vjp = lambda self, g, a: (1.,)\n",
    "AutogradValue.exp = lambda a: _exp(a)\n",
    "AutogradValue.log = lambda a: _log(a)\n",
    "AutogradValue.pad = lambda a: _pad(a)\n",
    "AutogradValue.sum = lambda a: _sum(a)\n",
    "AutogradValue.matmul = lambda a, b: _matmul(a, b)\n",
    "AutogradValue.__add__ = lambda a, b: _add(a, b)\n",
    "AutogradValue.__radd__ = lambda a, b: _add(b, a)\n",
    "AutogradValue.__sub__ = lambda a, b: _sub(a, b)\n",
    "AutogradValue.__rsub__ = lambda a, b: _sub(b, a)\n",
    "AutogradValue.__neg__ = lambda a: _neg(a)\n",
    "AutogradValue.__mul__ = lambda a, b: _mul(a, b)\n",
    "AutogradValue.__rmul__ = lambda a, b: _mul(b, a)\n",
    "AutogradValue.__truediv__ = lambda a, b: _div(a, b)\n",
    "AutogradValue.__rtruediv__ = lambda a, b: _div(b, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZu0tAMhmdDz"
   },
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msVRADbGmdDz"
   },
   "outputs": [],
   "source": [
    "def backward_pass(self):\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "AutogradValue.backward_pass = backward_pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUsNvAxomdDz"
   },
   "source": [
    "<div style=\"page-break-after: always; visibility: hidden\">  \\pagebreak  </div>\n",
    "\n",
    "#### **Q11**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UT66PPgUmdDz"
   },
   "outputs": [],
   "source": [
    "def nll_and_grad(self, X, y):\n",
    "    ## YOUR CODE HERE\n",
    "    loss =\n",
    "    grads =\n",
    "    return loss.value, grads\n",
    "\n",
    "NeuralNetwork.nll_and_grad = nll_and_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIxJnbXGmdDz"
   },
   "source": [
    "We should be able to run it with a much larger network now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uiyddgHAmdDz"
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(100, noise=0.1)\n",
    "model = NeuralNetwork(2, [25, 25])\n",
    "gradient_descent(model, X, y, lr=3e-2, steps=250)\n",
    "\n",
    "print('Model accuracy: %.3f' % model.accuracy(X, y))\n",
    "plot_boundary(model, X, y)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
