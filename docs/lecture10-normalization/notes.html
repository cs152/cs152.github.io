<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Lecture 10: Normalization â€“ CS 152: Neural Networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">CS 152: Neural Networks</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../calendar/calendar.html"> 
<span class="menu-text">Calendar</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../assignments/homeworks/homeworks.html"> 
<span class="menu-text">Homeworks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../assignments/final-project/outline.html"> 
<span class="menu-text">Project</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-resources" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Resources</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-resources">    
        <li>
    <a class="dropdown-item" href="https://probml.github.io/pml-book/book1.html">
 <span class="dropdown-text">Textbook</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://blank-app-ufu2uvdeosc.streamlit.app/">
 <span class="dropdown-text">Notebook conversion</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://www.gradescope.com/courses/710173">
 <span class="dropdown-text">Gradescope</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-solutions" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Solutions</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-solutions">    
        <li class="dropdown-header">Homework 1 Solutions</li>
        <li class="dropdown-header">Homework 2 Solutions</li>
        <li class="dropdown-header">Homework 3 Solutions</li>
        <li class="dropdown-header">Homework 4 Solutions</li>
        <li class="dropdown-header">Homework 5 Solutions</li>
        <li class="dropdown-header">Homework 6 Solutions</li>
        <li class="dropdown-header">Homework 7 Solutions</li>
        <li class="dropdown-header">Homework 8 Solutions</li>
        <li class="dropdown-header">Homework 9 Solutions</li>
        <li class="dropdown-header">Homework 10 Solutions</li>
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#gradient-descent-for-deep-networks" id="toc-gradient-descent-for-deep-networks" class="nav-link active" data-scroll-target="#gradient-descent-for-deep-networks">Gradient Descent for Deep Networks</a>
  <ul class="collapse">
  <li><a href="#vanishing-and-exploding-gradients" id="toc-vanishing-and-exploding-gradients" class="nav-link" data-scroll-target="#vanishing-and-exploding-gradients">Vanishing and exploding gradients</a></li>
  <li><a href="#gradient-clipping" id="toc-gradient-clipping" class="nav-link" data-scroll-target="#gradient-clipping">Gradient clipping</a></li>
  </ul></li>
  <li><a href="#normalization" id="toc-normalization" class="nav-link" data-scroll-target="#normalization">Normalization</a>
  <ul class="collapse">
  <li><a href="#input-scaling" id="toc-input-scaling" class="nav-link" data-scroll-target="#input-scaling">Input scaling</a></li>
  <li><a href="#input-centering" id="toc-input-centering" class="nav-link" data-scroll-target="#input-centering">Input centering</a></li>
  <li><a href="#input-normalization" id="toc-input-normalization" class="nav-link" data-scroll-target="#input-normalization">Input normalization</a></li>
  <li><a href="#estimating-data-statistics" id="toc-estimating-data-statistics" class="nav-link" data-scroll-target="#estimating-data-statistics">Estimating data statistics</a></li>
  <li><a href="#batch-normalization" id="toc-batch-normalization" class="nav-link" data-scroll-target="#batch-normalization">Batch normalization</a></li>
  <li><a href="#distribution-shift" id="toc-distribution-shift" class="nav-link" data-scroll-target="#distribution-shift">Distribution shift</a></li>
  <li><a href="#batch-normalization-in-multiple-layers" id="toc-batch-normalization-in-multiple-layers" class="nav-link" data-scroll-target="#batch-normalization-in-multiple-layers">Batch normalization in multiple layers</a></li>
  <li><a href="#batch-normalization-at-test-time" id="toc-batch-normalization-at-test-time" class="nav-link" data-scroll-target="#batch-normalization-at-test-time">Batch normalization at test time</a></li>
  <li><a href="#layer-normalization" id="toc-layer-normalization" class="nav-link" data-scroll-target="#layer-normalization">Layer normalization</a></li>
  <li><a href="#scaled-normalization" id="toc-scaled-normalization" class="nav-link" data-scroll-target="#scaled-normalization">Scaled normalization</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Lecture 10: Normalization</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="gradient-descent-for-deep-networks" class="level1">
<h1>Gradient Descent for Deep Networks</h1>
<section id="vanishing-and-exploding-gradients" class="level2">
<h2 class="anchored" data-anchor-id="vanishing-and-exploding-gradients">Vanishing and exploding gradients</h2>
<p>In previous lectures we discussed what can happen if we make a neural network too <em>wide</em>, meaning that there are many neurons a each layer. We saw that as long as we are careful in how we initialize the parameters, we can prevent any issues that might arrive. Now weâ€™ll consider what happens if we make our network too <em>deep</em>, that is weâ€™ll increase the number of layers. Modern neural networks can have up to 100â€™s of layers, so itâ€™s important to make sure that gradient descent will work well even in this extreme case.</p>
<p>Just like before weâ€™ll analyze the <em>scale</em> of the gradient to make sure that weâ€™re not going to take any extreme steps as we go that might cause our learning to stall or even diverge. This time weâ€™ll be a little less formal and only take a look at a high level view of what happens to the gradient as the number of layers increases, as the specifics can vary quite a bit from network to network.</p>
<p>Recall that our neural network feature transform can be written as a composition of feature transform functions, one for each of the <span class="math inline">\(\ell\)</span> layers.</p>
<p><span class="math display">\[\phi(\mathbf{x}) = \phi_\ell(\phi_{\ell-1}(\phi_{\ell-2}(...\phi_1(\mathbf{x})...)))\]</span></p>
<p>In practice each layer will be a linear transformation followed by an activation function like the <span class="math inline">\(\text{relu}(\cdot)\)</span> or <span class="math inline">\(\sigma (\cdot)\)</span> function.</p>
<p><span class="math display">\[\phi(\mathbf{x}) = \text{relu}(\text{relu}(\text{relu}(...\mathbf{x}^T\mathbf{W}_1 + \mathbf{b}_1...)^T\mathbf{W}_{\ell-1} + \mathbf{b}_{\ell-1})^T\mathbf{W}_\ell + \mathbf{b}_\ell)\]</span></p>
<p>Weâ€™ll then generally make a prediction using a linear function this output and compute a loss by comparing this prediction to a true label using a metric like mean squared error:</p>
<p><span class="math display">\[f(\mathbf{x}) = \phi(\mathbf{x})^T\mathbf{w}_0 + b_0\]</span></p>
<p><span class="math display">\[
\text{Loss}(\mathbf{x}, y) = (f(\mathbf{x}) - y)^2
\]</span></p>
<p>We can write our neural network loss as a series of operations:</p>
<p><span class="math display">\[
\Phi_1 = \sigma(\mathbf{x}^T\mathbf{W}_1 + \mathbf{b}_1)
\]</span></p>
<p><span class="math display">\[
\Phi_2 = \sigma(\Phi_1^T\mathbf{W}_2 + \mathbf{b}_2)
\]</span></p>
<p><span class="math display">\[
\vdots
\]</span></p>
<p><span class="math display">\[
\Phi_\ell = \sigma(\Phi_{\ell-1}^T\mathbf{W}_\ell + \mathbf{b}_\ell)
\]</span></p>
<p><span class="math display">\[
\mathbf{f} = \Phi_{\ell-1}^T\mathbf{W}_0 + \mathbf{b}_0
\]</span></p>
<p><span class="math display">\[
\mathbf{L} = \text{Loss}(\mathbf{f}, y)
\]</span> Now letâ€™s consider the gradient of the loss with respect to <span class="math inline">\(\mathbf{W}_1\)</span> , the first set of weights used in the network. We can write this gradient using the chain rule as:</p>
<p><span class="math display">\[\frac{d\mathbf{L}}{d\mathbf{W}_1}=\frac{d\mathbf{L}}{d\Phi_\ell}\frac{d\Phi_{\ell}}{d\Phi_{\ell-1}}...\frac{d\Phi_2}{d\Phi_1}\frac{d\Phi_1}{d\mathbf{W}_1}\]</span></p>
<p>For the sake of simplicity, weâ€™ll consider the 1-dimensional case, so all our variables are scalars rather than vectors/matrices (<span class="math inline">\(\mathbf{W}_1\)</span> <em>becomes</em> <span class="math inline">\(w_1\)</span>, <span class="math inline">\(\mathbf{x}\)</span> becomes <span class="math inline">\(x\)</span>). In this case we can rewrite this using a product and expanding <span class="math inline">\(\frac{d\Phi_1}{dw_1}\)</span>.</p>
<p><span class="math display">\[\frac{d\Phi_1}{dw_1}=x\sigma'(xw_1 + b_1)\]</span> <span class="math display">\[\frac{d\mathbf{L}}{dw_1}=x\sigma'(xw_1 + b_1)\frac{d\mathbf{L}}{d\Phi_\ell}\prod_{i=2}^{\ell}\frac{d\Phi_{i}}{d\Phi_{i-1}}\]</span></p>
<p><span class="math display">\[
w_1, x, \frac{d\Phi_0}{d\Phi_1},... \in \mathbb{R}
\]</span></p>
<p>Finally we can consider how this gradient grows/shrinks as we increase the number of layers. We donâ€™t know exactly what the gradient of each layer will be, but given our initialization itâ€™s reasonable to assume that theyâ€™re all relatively consistent. For example if we use a linear (no) activation the gradient we simply get:<span class="math display">\[
\frac{d\Phi_{i}}{d\Phi_{i-1}}= \frac{d}{d\Phi_{i-1}}(\Phi_{i-1}w_i + b_i)=  w_i, \quad
\]</span>Weâ€™ll use <span class="math inline">\(M\)</span> to denote the approximate magnitude of each term in the product:</p>
<p><span class="math display">\[\bigg|\frac{d\Phi_{i}}{d\Phi_{i-1}}\bigg| \approx M, \forall i\]</span></p>
<p>Now it becomes clear that the scale of the gradient grows/shrinks exponentially with the number of layers <span class="math inline">\((\ell)\)</span>!</p>
<p><span class="math display">\[\bigg|\frac{dL}{dw_1}\bigg| = |x| \prod_{i=2}^{\ell}\bigg| \frac{d\Phi_{i}}{d\Phi_{i-1}}\bigg|... \approx |x|\big(\textcolor{red}{M^L}\big)...\]</span></p>
<p>Therefore we have two concerning possibilities: if <span class="math inline">\(M\)</span> is larger than <span class="math inline">\(1\)</span>, our gradient could become extremely large. We call this an <strong>exploding gradient:</strong></p>
<p><span class="math display">\[\textbf{If: } M &gt; 1 \longrightarrow \frac{dL}{dw_1} &gt;&gt; 1\]</span></p>
<p>If <span class="math inline">\(M\)</span> is smaller than 1, our gradient could be very small. We call this a <strong>vanishing gradient:</strong></p>
<p><span class="math display">\[\textbf{If: } M &lt; 1 \longrightarrow \frac{df}{dw_L} \approx 0\]</span></p>
<p>Concretely if we have a 100 layer network and <span class="math inline">\(M=1.5\)</span> then <span class="math inline">\(\frac{d\mathbf{L}}{dw_1}\approx 4\times10^{17}\)</span>. If <span class="math inline">\(M=0.75\)</span>, then <span class="math inline">\(\frac{d\mathbf{L}}{dw_1}\approx 3\times10^{-13}\)</span>. Only in the case where <span class="math inline">\(M\approx 1\)</span> do we have a stable gradient scale.</p>
<p>Itâ€™s tempting to think we could just initialize our weights carefully such that <span class="math inline">\(M\approx 1\)</span> or change our learning rate to counteract this scale. Unfortunately, once we start updating our network weights with gradient descent, <span class="math inline">\(M\)</span> could change and we could easily move from one regime to another. Geometrically, this problem corresponds to a loss function that has <em>both</em> very steep slopes and very flat plateaus.</p>
</section>
<section id="gradient-clipping" class="level2">
<h2 class="anchored" data-anchor-id="gradient-clipping">Gradient clipping</h2>
<p>Letâ€™s start by looking at a very simple method to address the exploding gradient problem. Instead of scaling the gradient by a fixed amount, weâ€™ll set a cap <span class="math inline">\((\epsilon)\)</span> on the size of step that we can take. If gradient exceeds that maximum step, weâ€™ll simply try to re-scale it to the desired length. We call this approach <strong>gradient clipping.</strong> Weâ€™ll define two slightly different operations to <em>clip</em> a vector to a given length. If our gradient is actually a matrix or a collection of vectors/matrices, we could always <em>flatten</em> all the individual partial derivatives into one big, long vector to apply the clipping operation.</p>
<p>Weâ€™ll call our first approach <strong>clip-by-value</strong>. In this case, we will simply limit the value of any individual entry in our vector to be no more than <span class="math inline">\(\epsilon\)</span> and no less than <span class="math inline">\(-\epsilon\)</span>. We can write this mathematically as:</p>
<p><span class="math display">\[\textbf{clip}_{\text{value}}\big(\mathbf{x}, \epsilon\big) = \begin{bmatrix} \min(\max(x_1, -\epsilon), \epsilon) \\ \min(\max(x_2, - \epsilon), \epsilon) \\ \vdots \end{bmatrix}\]</span></p>
<p>Geometrically, this corresponds to restricting <span class="math inline">\(\mathbf{x}\)</span> to a box centered at the origin.</p>
<p>For our second approach, rather than considering each dimension individually, weâ€™ll restrict the overall <em>length</em> (magnitude) of the vector, while maintaining the direction. Remember that we define the length of a vector by its 2-norm: <span class="math inline">\(||\mathbf{x}||_2 = \sqrt{\sum_{i=1}^d x_i^2}\)</span>, therefore we call this approach <strong>clip-by-norm.</strong> If we want to re-scale our vector to have length <span class="math inline">\(\epsilon\)</span> we simply need to divide each entry by <span class="math inline">\(||\mathbf{x}||_2\)</span> and multiply by <span class="math inline">\(\epsilon\)</span>, therefore our clipping operation will look like:</p>
<p><span class="math display">\[\textbf{clip}_{\text{norm}}\big(\mathbf{x}, \epsilon\big) = \begin{cases}
\frac{\epsilon\mathbf{x} }{\| \mathbf{x} \|_2} \quad &amp; \textbf{if: } \|\mathbf{x}\|_2 &gt; \epsilon \\
\mathbf{x} \  \quad\quad &amp; \textbf{if: } \|\mathbf{x}\|_2 \leq \epsilon
\end{cases}\]</span></p>
<p>Applying either of these clipping operations within gradient descent would look like this:</p>
<p><span class="math display">\[ \mathbf{w}^{(k+1)} \longleftarrow \mathbf{w}^{(k)} - \alpha\ \textbf{clip}\big(\nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y})\big)\]</span></p>
<p>As a final note, we donâ€™t want to re-scale gradients smaller than length <span class="math inline">\(\epsilon\)</span> to be larger, because ultimately our gradient <em>should</em> have length 0 at the optimum. This means we may need an alternative way to handle vanishing gradients. Weâ€™ll come back to this!</p>
</section>
</section>
<section id="normalization" class="level1">
<h1>Normalization</h1>
<section id="input-scaling" class="level2">
<h2 class="anchored" data-anchor-id="input-scaling">Input scaling</h2>
<p>Returning to our analysis of the gradient magnitude above we see that thereâ€™s another term that affects the scale of our gradient, <span class="math inline">\(|x|\)</span>, the scale of our input features!</p>
<p><span class="math display">\[\bigg|\frac{dL}{dw_1}\bigg| = |x| \prod_{i=2}^{\ell}\bigg| \frac{d\Phi_{i}}{d\Phi_{i-1}}\bigg|... \approx |x|\big(\textcolor{red}{M^L}\big)...\]</span></p>
<p>Unlike the initial values of our parameters, we donâ€™t choose our data, so thereâ€™s nothing that prevents the scale of <span class="math inline">\(x\)</span> itself from being very large or very small. Ideally weâ€™d like a predictable scale for our data so that we can set things our learning rate more easily.</p>
<p>Moreover, as we saw in our discussion of RMSProp optimization <em>mismatch</em> in scale between dimensions can also cause optimization issues even if the difference is not exponentially large! This is quite common in practice; in our initial fuel economy example we saw that each car had weight measurements in the range of 1000-4000lbs and acceleration measurements in the range of 5-10sec. While RMSProp can help, it would be ideal if we could re-scale our data to eliminate these differences before we even start running gradient descent.</p>
</section>
<section id="input-centering" class="level2">
<h2 class="anchored" data-anchor-id="input-centering">Input centering</h2>
<p>Before we get to how to re-scale our data, letâ€™s consider one other way that an unexpected data distribution could break the assumptions that we used when designing a neural network. When we first introduced a neural network feature transform, we showed that in order for it to give us an improvement over a linear model, we needed to introduce non-linear <em>activation</em> functions into the network.</p>
<p><span class="math display">\[
\phi(x) = {\color{red}\sigma}(\mathbf{x}^T\mathbf{W}_1 + \mathbf{b}_1)
\]</span></p>
<p>In order for these activation to be useful however, our inputs need to be centered around <span class="math inline">\(0\)</span>. If we plot our common choices for activation functions we can see why.</p>
<p>Starting with the <span class="math inline">\(\text{relu}\)</span> function, we see that if <em>all</em> observed inputs are positive, then the function <em>is linear</em> over the entire range of inputs. Conversely, if all observed inputs are negative, weâ€™re even worse off; weâ€™ll never get outputs other than <span class="math inline">\(0\)</span>! Itâ€™s only when our data spans both positive and negative values that our prediction function will look non-linear.</p>
<p>For the sigmoid function, <span class="math inline">\(\sigma(\cdot)\)</span>, we have a similar story. In this case, if all observations are much larger than <span class="math inline">\(0\)</span>, the function will always output 1. That is: <span class="math inline">\(\sigma(x)\approx 1 \text{ if }x &gt;&gt; 0\ \forall x\)</span>, while if all the observations are far below <span class="math inline">\(0\)</span>, the function will output <span class="math inline">\(0\)</span>; <span class="math inline">\(\sigma(x)\approx 0 \text{ if }x &lt;&lt; 0\ \forall x\)</span>. In this case we also see that the <em>variance</em> of the data matters; if all the data is <em>too</em> close to <span class="math inline">\(0\)</span>, the function again looks linear; <span class="math inline">\(\sigma(x)\approx x \text{ if }|x| &lt;&lt; 1\ \forall x\)</span>.</p>
</section>
<section id="input-normalization" class="level2">
<h2 class="anchored" data-anchor-id="input-normalization">Input normalization</h2>
<p>Ultimately weâ€™ve seen that weâ€™d like our data to be centered around <span class="math inline">\(0\)</span> and to have a predictable scale. One way to say this more formally is that we want the mean (expectation) of our data to be <span class="math inline">\(0\)</span> and the variance of our data to be a known constant, usually <span class="math inline">\(1\)</span>. So weâ€™d like:</p>
<p><span class="math display">\[
\mathbb{E}[x]=0, \quad \text{Var}[x]=1
\]</span></p>
<p>A simple way to achieve this is to <strong>normalize</strong> our data. That is, for every observation weâ€™ll apply a transformation that subtracts the mean and divides by the square root of the variance.</p>
<p><span class="math display">\[
\text{Norm}(x) = \frac{x-\mathbb{E}[x]}{\sqrt{\text{Var}[x]}}
\]</span></p>
<p>By definition the expectation of this transformed value is <span class="math inline">\(0\)</span> and the variance is <span class="math inline">\(1\)</span>!</p>
<p><span class="math display">\[
\mathbb{E}[\text{Norm}(x)]= \mathbb{E}\bigg[ \frac{x-\mathbb{E}[x]}{\sqrt{\text{Var}[x]}} \bigg]=\frac{\mathbb{E}\big[ x-\mathbb{E}[x] \big]}{\sqrt{\text{Var}[x]}}=\frac{\mathbb{E}[x]-\mathbb{E}[x]}{\sqrt{\text{Var}[x]}}=0
\]</span></p>
<p><span class="math display">\[
\text{Var}[\text{Norm}(x)]= \text{Var}\bigg[ \frac{x-\mathbb{E}[x]}{\sqrt{\text{Var}[x]}} \bigg]=\frac{\text{Var}\big[ x-\mathbb{E}[x] \big]}{(\sqrt{\text{Var}[x]})^2}=\frac{\text{Var}[x]-0}{\text{Var}[x]}=1
\]</span></p>
<p>Note that in this case, weâ€™ve framed things in terms of scalar inputs <span class="math inline">\(x\)</span>. If our inputs are vectors, <span class="math inline">\(\mathbf{x}\)</span>, weâ€™ll just do the same thing for each dimension.</p>
<p><span class="math display">\[
\text{Norm}(\mathbf{x}) = \begin{bmatrix} \frac{x_1-\mathbb{E}[x_1]}{\sqrt{\text{Var}[x_1]}} \\ \frac{x_2-\mathbb{E}[x_2]}{\sqrt{\text{Var}[x_2]}} \\ \vdots \end{bmatrix} = \frac{\mathbf{x}-\mathbb{E}[\mathbf{x}]}{\sqrt{\text{Var}[\mathbf{x}]}}
\]</span></p>
<p>In this case weâ€™ll train <span class="math inline">\(\mathbb{E}[\mathbf{x}]\)</span> and <span class="math inline">\(\text{Var}[\mathbf{x}]\)</span> as the element-wise mean and variance.</p>
</section>
<section id="estimating-data-statistics" class="level2">
<h2 class="anchored" data-anchor-id="estimating-data-statistics">Estimating data statistics</h2>
<p>Unfortunately, we donâ€™t know the <em>true</em> mean and variance of the data, as our training data doesnâ€™t likely doesnâ€™t encompass all the data in the world. So weâ€™ll typically weâ€™ll <em>estimate</em> the mean and variance using what we have.</p>
<p>Recall that <strong>sample mean</strong>, <span class="math inline">\(\mathbf{\bar{x}}\)</span>, gives us the optimal estimate of the expectation for a given sample of values. In this case we can compute the sample mean over our dataset.</p>
<p><span class="math display">\[\text{Dataset: } \{\mathbf{x}_1, \mathbf{x}_2,...,\mathbf{x}_N\}\]</span></p>
<p><span class="math display">\[\mathbb{E}[\mathbf{x}] \approx \bar{\mathbf{x}} = \frac{1}{N}\sum_{i=1}^{N} \mathbf{x}_i\quad \text{(sample mean)}\]</span></p>
<p>Similarly, the <strong>sample variance</strong>, <span class="math inline">\(\mathbf{s}^2\)</span> can be used as a good estimate of the true variance. There are actually two common ways to compute the sample variance. The <strong>biased estimator</strong>:</p>
<p><span class="math display">\[\text{Var}[\mathbf{x}] \approx \mathbf{s}^2 = \frac{1}{N}\sum_{i=1}^{N} \bigg(\mathbf{x}_i - \bigg(\frac{1}{N}\sum_{i=1}^{N} \mathbf{x}_i\bigg)\bigg)^2\quad \text{(biased sample var.)}\]</span></p>
<p>and the <strong>unbiased estimator:</strong></p>
<p><span class="math display">\[\text{Var}[\mathbf{x}] \approx \mathbf{s}^2 = \frac{1}{N-1}\sum_{i=1}^{N} \bigg(\mathbf{x}_i - \bigg(\frac{1}{N}\sum_{i=1}^{N} \mathbf{x}_i\bigg)\bigg)^2\quad \text{(sample var.)}\]</span></p>
<p>The differences between these two versions arenâ€™t too important for our purposes, so weâ€™ll leave that discussion for a statistics course. Both are commonly used in neural network applications and usually perform basically identically in practice.</p>
<p>Now we can re-define our normalization operation to use this sample mean and variance:</p>
<p><span class="math display">\[\text{Norm}(x) = \frac{ x - \bar{x}}{\sqrt{s^2}}\]</span></p>
</section>
<section id="batch-normalization" class="level2">
<h2 class="anchored" data-anchor-id="batch-normalization">Batch normalization</h2>
<p>In some cases, if may not be practical to compute the estimates of the mean and variance over the entire dataset ahead of time (e.g.&nbsp;if weâ€™re streaming date from an external source). In this case we can apply <strong>batch normalization.</strong> In this case the operation we perform will look almost exactly like our normalization operation, but weâ€™ll compute the statistics over the current <em>batch</em> that weâ€™re using for stochastic gradient descent.</p>
<p><span class="math display">\[
\text{BatchNorm}(x) = \frac{ x - \bar{x}}{\sqrt{s^2 + \epsilon}}
\]</span> <span class="math display">\[\text{Batch: } \{\mathbf{x}_1, \mathbf{x}_2,...,\mathbf{x}_B\}\]</span> <span class="math display">\[\mathbb{E}[\mathbf{x}] \approx \bar{\mathbf{x}} = \frac{1}{B}\sum_{i=1}^{B} \mathbf{x}_i\quad \text{(sample mean)}\]</span> <span class="math display">\[\text{Var}[\mathbf{x}] \approx  \mathbf{s}^2 = \frac{1}{B-1}\sum_{i=1}^{B} \bigg(\mathbf{x}_i - \bigg(\frac{1}{B}\sum_{i=1}^{B} \mathbf{x}_i\bigg)\bigg)^2\quad \text{(sample var.)}\]</span></p>
<p>In this case weâ€™ll also in include a small constant <span class="math inline">\(\epsilon &lt;&lt; 1\)</span> in the denominator of the transform, just as we did in RMSProp to prevent division by <span class="math inline">\(0\)</span> if we happen to sample a batch with <span class="math inline">\(0\)</span> variance.</p>
</section>
<section id="distribution-shift" class="level2">
<h2 class="anchored" data-anchor-id="distribution-shift">Distribution shift</h2>
<p>You might notice that even if we transform our data to have <span class="math inline">\(\mathbb{E}[\mathbf{x}]=0\)</span> and <span class="math inline">\(\text{Var}[\mathbf{x}]=1\)</span>, once our data goes through several layers:</p>
<p><span class="math display">\[
\Phi_1 = \sigma(\mathbf{x}^T\mathbf{W}_1 + \mathbf{b}_1)
\]</span></p>
<p><span class="math display">\[
\Phi_2 = \sigma(\Phi_1^T\mathbf{W}_2 + \mathbf{b}_2)
\]</span></p>
<p><span class="math display">\[
\vdots
\]</span></p>
<p><span class="math display">\[
\Phi_i = \sigma(\Phi_{i-1}^T\mathbf{W}_i + \mathbf{b}_i)
\]</span></p>
<p>this may no longer hold. That is, we may find that <span class="math inline">\(\mathbb{E}[\Phi_{i}]\neq 0\)</span>, <span class="math inline">\(\text{Var}[\Phi_{i}]\neq 1\)</span>. Again, we could carefully tune our initialization to avoid this at first, but once we start changing the weights in gradient descent, we could quickly drift away, particularly if the number of layers is large. This means that layer <span class="math inline">\(i+1\)</span> may run into exactly the same issues we identified above.</p>
<p><span class="math display">\[
\Phi_{i+1} = \sigma(\Phi_{i}^T\mathbf{W}_{i+1} + \mathbf{b}_{i+1})
\]</span></p>
<p>Even worse, we might find that not only does the distribution of <span class="math inline">\(\Phi_i\)</span> not have our desired mean and variance, its distribution could change dramatically every time we update the weights!</p>
<p>Remember that at step <span class="math inline">\(k\)</span> weâ€™ll update the weights <span class="math inline">\(\mathbf{W}_{i+1}\)</span> according to the current input <span class="math inline">\(\Phi_i^{(k)}\)</span>:</p>
<p><span class="math display">\[
\mathbf{W}_{i+1}^{(k+1)} \longleftarrow \mathbf{W}_{i+1}^{(k)} - \alpha \Phi_{i}^{(k)}\sigma'(\Phi_{i}^{(k)}\mathbf{W}_{i+1} + \mathbf{b}_{i+1})
\]</span></p>
<p>But since weâ€™re also updating <span class="math inline">\(\{\mathbf{W}_{1},\mathbf{b}_1,...,\mathbf{W}_{i},\mathbf{b}_i \}\)</span> at the same time, when we go to make a prediction, may find that the distribution of <span class="math inline">\(\Phi_{i}\)</span> has changed and our gradient step for <span class="math inline">\(\mathbf{W}_{i+1}\)</span> looks bad in hindsight. We call this problem <strong>distribution shift.</strong> Updating the weights sequentially might help avoid this issue, but would be very <em>very</em> slow. Instead we can use the normalization tool we just discussed to <em>force</em> the distribution of <span class="math inline">\(\Phi_{i}\)</span> to have the properties we want.</p>
<p>In other words, we can apply normalization at every layer!</p>
</section>
<section id="batch-normalization-in-multiple-layers" class="level2">
<h2 class="anchored" data-anchor-id="batch-normalization-in-multiple-layers">Batch normalization in multiple layers</h2>
<p>Since the distribution of <span class="math inline">\(\Phi_{i}\)</span> will change as we update the weights <span class="math inline">\(\{\mathbf{W}_{1},\mathbf{b}_1,...,\mathbf{W}_{i},\mathbf{b}_i \}\)</span>, weâ€™ll need to continuously update our estimates of <span class="math inline">\(\mathbb{E}[\Phi_i]\)</span> and <span class="math inline">\(\text{Var}[\Phi_i]\)</span> as well. Meaning that if weâ€™re using mini-batch stochastic gradient descent, weâ€™ll also want to use batch-noramlization to avoid recomputing the mean and variance at each layer for the whole dataset at every step. With the addition of batch normalization operations (<span class="math inline">\(BN(\cdot)\)</span>), our network will now be computed as: <span class="math display">\[
\Phi_1 = \sigma(BN(\mathbf{X})^T\mathbf{W}_1 + \mathbf{b}_1)
\]</span></p>
<p><span class="math display">\[
\Phi_2 = \sigma(BN(\Phi_1)^T\mathbf{W}_2 + \mathbf{b}_2)
\]</span></p>
<p><span class="math display">\[
\vdots
\]</span></p>
<p><span class="math display">\[
\Phi_\ell = \sigma(BN(\Phi_{\ell-1})^T\mathbf{W}_\ell + \mathbf{b}_\ell)
\]</span></p>
<p><span class="math display">\[
\mathbf{f} = BN(\Phi_{\ell-1})^T\mathbf{W}_0 + \mathbf{b}_0
\]</span></p>
<p><span class="math display">\[
\mathbf{L} = \text{Loss}(\mathbf{f}, y)
\]</span></p>
</section>
<section id="batch-normalization-at-test-time" class="level2">
<h2 class="anchored" data-anchor-id="batch-normalization-at-test-time">Batch normalization at test time</h2>
<p>So far weâ€™ve made an implicit assumption for batch normalization that the size of our batch is larger than one: <span class="math inline">\(B&gt;1\)</span>. If <span class="math inline">\(B=1\)</span>, we run into some issues with out mean and variance estimates: <span class="math display">\[\bar{\mathbf{x}}= \frac{1}{1}\sum_{i=1}^1 \mathbf{x}_1 = \mathbf{x}_1 \longrightarrow \mathbf{x}_1 - \mathbf{\bar{x}} = 0\]</span> <span class="math display">\[\mathbf{s}^2= \frac{1}{0}\sum_{i=1}^1 (\mathbf{x}_1 - \mathbf{x}_1)^2= \frac{0}{0}=\mathbf{?}\]</span> <span class="math display">\[\text{BatchNorm}(x) = \frac{0}{\sqrt{\mathbf{?} + \epsilon}}\]</span></p>
<p>Even if we to use the biased variance estimator, weâ€™d still get a divide by 0 error in out batch norm calculation!</p>
<p>This isnâ€™t too big a deal when weâ€™re training our network; we can just always make sure out batch size is <span class="math inline">\(&gt;1\)</span> if weâ€™re using batch norm. The problem is that we want others to be able to use our network we canâ€™t enforce that they must use a batch size of more than one. After all, in practice if I want to use a neural network to, for example, identify a species of flower in a photo, I shouldnâ€™t need to give it 9 other photos of flowers just to make one prediction!</p>
<p>Even if we could force users to give our network multiple examples, it could be difficult to enforce that the sample batch they chose was truely random. If the selection of the batch is biased, it could throw off the mean and variance estimates we need!</p>
<p>The solution often used in practice is to define batch normalization differently depending on whether weâ€™re training the network or testing it on new data. At training time we can keep the same approarch from before.</p>
<p>However, weâ€™ll also keep track of a running average of the sample mean and sample variance that we observe at each step. Weâ€™ll use the same <em>exponential moving average</em> approach we</p>
<p><span class="math display">\[\underset{\text{train}}{\text{BatchNorm}}(x) = \frac{ x - \bar{x}}{\sqrt{s^2 + \epsilon}}\]</span></p>
<p>Running estimate: <span class="math display">\[\bar{\mu}^{(k+1)} \longleftarrow \beta \bar{\mu}^{(k)} + (1-\beta) \bar{x}^{(k)}\]</span> <span class="math display">\[\bar{\sigma}^{2(k+1)} \longleftarrow \beta \bar{\sigma}^{2(k)} + (1-\beta) s^{2(k)}\]</span></p>
<p><span class="math display">\[\underset{\text{test}}{\text{BatchNorm}}(x) = \frac{ x - \bar{\mu}}{\sqrt{\bar{\sigma}^2 + \epsilon}}\]</span></p>
</section>
<section id="layer-normalization" class="level2">
<h2 class="anchored" data-anchor-id="layer-normalization">Layer normalization</h2>
<p>Normalize over the layer:</p>
<p><span class="math display">\[\text{LayerNorm}(\mathbf{x}) = \frac{\mathbf{x} - \bar{x}}{\sqrt{s^2 + \epsilon}}, \quad \mathbf{x} = \begin{bmatrix} x_1 \\ \vdots \\ x_d\end{bmatrix}\]</span></p>
<p>Training &amp; test time: <span class="math display">\[\bar{x} = \frac{1}{d}\sum_{i=1}^{d} x_i\quad \text{(output mean)}\]</span> Biased estimator: <span class="math display">\[s^2 = \frac{1}{d}\sum_{i=1}^{d} \bigg(x_i - \bigg(\frac{1}{d}\sum_{i=1}^{d} x_i\bigg)\bigg)^2\quad \text{(output var.)}\]</span> Unbiased estimator: <span class="math display">\[s^2 = \frac{1}{d-1}\sum_{i=1}^{d} \bigg(x_i - \bigg(\frac{1}{d}\sum_{i=1}^{d} x_i\bigg)\bigg)^2\quad \text{(output var.)}\]</span></p>
</section>
<section id="scaled-normalization" class="level2">
<h2 class="anchored" data-anchor-id="scaled-normalization">Scaled normalization</h2>
<p><span class="math display">\[\text{BatchNorm}(x) = \frac{ x - \mathbb{E}[x]}{\sqrt{\text{Var}[x] + \epsilon}} \gamma + \kappa\]</span> <span class="math display">\[\text{LayerNorm}(\mathbf{x}) = \frac{\mathbf{x} - \bar{x}}{\sqrt{s^2 + \epsilon}} \gamma + \kappa\]</span></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>