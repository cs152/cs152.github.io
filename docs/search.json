[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course Syllabus",
    "section": "",
    "text": "In this course, we will introduce neural networks as a tool for machine learning and function approximation. We will start with the fundamentals of how to build and train a neural network from scratch. We will work our way towards discussing state-of-the-art neural networks including networks that power applications like object recognition, image generation and large language models. Throughout the course we will keep a strong focus on the implications of these models and how to apply them responsibly.\nFinal project description\n\n\n\n\n\n\n\nProf. Gabe Hope (he/him)\nEmail: ghope@g.hmc.edu\nOffice: MacGregor 322\nCourse Slack: https://join.slack.com/t/slack-hox8054/shared_invite/zt-220qv4i92-wFrivrGQBQbtzy0fSElV~w\n\n\n\nYou can call me any combination of Prof./Professor/Dr. and Hope/Gabe. My full name is actually John Gabriel Hope.\nI am originally from New York City\nI have a Bachelor of Science (BS) in computer science and finance from Washington University in St. Louis.\nI have a Master of Science (MS) from Brown University (This was actually the start of my Ph.D.)\nI earned my Ph.D. from the University of California, Irvine advised by Erik Sudderth. I graduated just this June!\nMy research focuses on using neural networks to find interpretable structures in data. I mainly focus on image data, though I have also worked on analyzing motion-capture, audio and climate data among other things!\n\n\n\n\n\n\nMax McKnight (he/they)\nEmail: mmcknigh@pitzer.edu\n\n\n\n\nLinear and logistic regression\nGradient descent and optimization\nFeature transforms and feed-forward networks\nPerformance tuning and analysis for neural networks\nConvolutional neural networks\nRegularization and normalization\nResidual networks and large-scale architectures\nAttention and transformers\nBiases and fairness in machine learning\n\n\n\n\nProbabilistic Machine Learning by Kevin Murphy. Full text for book 1 and book 2 are available for free online.\n\n\n\n\n\nMondays 4-5:30pm MacGregor 322\nThursdays 5:30-6:30pm MacGregor 322\n\n\n\nTuesdays 8-9pm MacGregor lab 203\nOpen door policy: If my door is open, feel free to stop in, say hi and ask questions about the course, research or any other academic issues. If the door is closed, feel free to knock. I often like to close my door to focus, but it does not always mean I am unable to talk. If I don’t answer promptly I am either out of office or in a meeting and am unable to talk. If in doubt, feel free contact me on slack. Note that I generally prefer to keep longer discussion of course materials to designated office hours."
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "Course Syllabus",
    "section": "",
    "text": "Prof. Gabe Hope (he/him)\nEmail: ghope@g.hmc.edu\nOffice: MacGregor 322\nCourse Slack: https://join.slack.com/t/slack-hox8054/shared_invite/zt-220qv4i92-wFrivrGQBQbtzy0fSElV~w\n\n\n\nYou can call me any combination of Prof./Professor/Dr. and Hope/Gabe. My full name is actually John Gabriel Hope.\nI am originally from New York City\nI have a Bachelor of Science (BS) in computer science and finance from Washington University in St. Louis.\nI have a Master of Science (MS) from Brown University (This was actually the start of my Ph.D.)\nI earned my Ph.D. from the University of California, Irvine advised by Erik Sudderth. I graduated just this June!\nMy research focuses on using neural networks to find interpretable structures in data. I mainly focus on image data, though I have also worked on analyzing motion-capture, audio and climate data among other things!"
  },
  {
    "objectID": "index.html#grutor",
    "href": "index.html#grutor",
    "title": "Course Syllabus",
    "section": "",
    "text": "Max McKnight (he/they)\nEmail: mmcknigh@pitzer.edu"
  },
  {
    "objectID": "index.html#topics-covered-tentative",
    "href": "index.html#topics-covered-tentative",
    "title": "Course Syllabus",
    "section": "",
    "text": "Linear and logistic regression\nGradient descent and optimization\nFeature transforms and feed-forward networks\nPerformance tuning and analysis for neural networks\nConvolutional neural networks\nRegularization and normalization\nResidual networks and large-scale architectures\nAttention and transformers\nBiases and fairness in machine learning"
  },
  {
    "objectID": "index.html#textbook-not-required",
    "href": "index.html#textbook-not-required",
    "title": "Course Syllabus",
    "section": "",
    "text": "Probabilistic Machine Learning by Kevin Murphy. Full text for book 1 and book 2 are available for free online."
  },
  {
    "objectID": "index.html#office-hours",
    "href": "index.html#office-hours",
    "title": "Course Syllabus",
    "section": "",
    "text": "Mondays 4-5:30pm MacGregor 322\nThursdays 5:30-6:30pm MacGregor 322\n\n\n\nTuesdays 8-9pm MacGregor lab 203\nOpen door policy: If my door is open, feel free to stop in, say hi and ask questions about the course, research or any other academic issues. If the door is closed, feel free to knock. I often like to close my door to focus, but it does not always mean I am unable to talk. If I don’t answer promptly I am either out of office or in a meeting and am unable to talk. If in doubt, feel free contact me on slack. Note that I generally prefer to keep longer discussion of course materials to designated office hours."
  },
  {
    "objectID": "index.html#vscode-optional",
    "href": "index.html#vscode-optional",
    "title": "Course Syllabus",
    "section": "VSCode (Optional)",
    "text": "VSCode (Optional)\nVisual Studio Code is a free development environment developed by Microsoft. It is available for Mac, Windows and Linux, and provides convenient tools for working with Python, Git and Jupyter. It is what I use to develop the materials for this course, and it is what I would recommend using for homework assignments. This is completely optional however. You are welcome to use whatever environment you feel most comfortable with.\nHere are resources for getting started:\n\nRecommended extensions for data science and working with Jupyter notebooks are listed here.\nInstructions for setting up Python in VSCode are here.\nInstructions for working with Jupyter notebooks in VSCode are here.\nInstructions for setting up Git in VSCode are here."
  },
  {
    "objectID": "index.html#python",
    "href": "index.html#python",
    "title": "Course Syllabus",
    "section": "Python",
    "text": "Python\nAssignments and projects in this course will be based on Python 3. We will be using the following packages throughout this course:\n\nNumpy: The industry-standard Python library for working with vectors, matrices and general numerical computing.\nMatplotlib: The most popular and widely supported library for data visualization in Python.\nSciKit-Learn: A popular library for basic machine learning.\nPyTorch: A deep learning library. Currently the most popular library for neural networks research and competitive with TensorFlow in terms of industry deployment.\n\nYou can find this course’s requirements file here. It will also be included in homework distributions. You can install the full set of packages using the command:\npip install -r requirements.txt"
  },
  {
    "objectID": "index.html#jupyter",
    "href": "index.html#jupyter",
    "title": "Course Syllabus",
    "section": "Jupyter",
    "text": "Jupyter\nMost homework assignments will be distributed and submitted as Jupyter notebooks. Jupyterlab is included in the course requirements.txt file, but instructions for installing it are also available here. Once installed, you can launch a JupyterLab server locally on you computer using the command:\njupyter lab\nThis will open the Jupyter Lab application in a web browser. From there you can navigate to the homework’s main.ipynb file. Resources and documentation for working with Jupyter notebooks are available here."
  },
  {
    "objectID": "index.html#latex-style-equations",
    "href": "index.html#latex-style-equations",
    "title": "Course Syllabus",
    "section": "Latex (style) equations",
    "text": "Latex (style) equations\nHomework assignments will ask you to submit answers requiring mathematical derivations as typeset Latex-style equations. Latex equations are supported directly within Jupyter. To write an equation in a text/markdown cell, simply surround the equation with $ symbols as: $y = x^2 + 1$, which produces the output: \\(y=x^2 +1\\). You can write block equation using double dollar-signs as $$y = x^2 + 1$$, which puts the equation on its own centered line.\nAn extensive reference for Latex equations is available here.\nIn general, only the final answer to such problems will be required to be submitted in this way, intermediate steps in derivations can be submitted separately as handwritten notes. To do this, scan or photograph (clearly!) the handwritten derivations and include them as a derivations.pdf file in the homework repository. You may also include separate files for each question with the format derivations-q1.pdf. PNG and JPG files are also acceptable. You may also omit intermediate steps altogether, but this is not recommended as it may limit your ability to earn partial credit if your answer is incorrect."
  },
  {
    "objectID": "index.html#git-and-github",
    "href": "index.html#git-and-github",
    "title": "Course Syllabus",
    "section": "Git and GitHub",
    "text": "Git and GitHub\nAssignments for this course will be distributed and collected via GitHub. You can sign up for a GitHub account at the link. If you have an existing GitHub account you may use it; you are not required to use an account associated with your Harvey Mudd (or 5Cs) email. As soon as possible please provide me with your Github username using this form.\nAfter the first class you will receive an email inviting you to a GitHub organization for this course. Please accept the invite as soon as possible. Once you have joined the organization, personal repositories for each homework will appear in your GitHub account under the organization (this link should take you to a list of your repositories for each homework, provided you are logged into GitHub).\nThe workflow for each homework should be as follows:\n\nClone the repository: You can find your repository for each homework using the link above. Running git clone REPOSITORY_URL will copy the repository to your computer. In VSCode, you can simply select Clone Git Repository...under the start menu.\nComplete the assignment: Typically the only file you will need to edit will be main.ipynb in each homework’s repository. This is a Jupyter notebook which will contain instructions for where to answer each question. Other supporting files may be distributed in the repository as well.\nCommit your changes: From the command line in the repository directory run the command: git add main.ipynbto stage the changes to made to the assignment with your answers. Then run the command git commit -m \"COMMIT_MESSAGE\" to save a checkpoint of your changes with the provided message. You can make as many commits as you want. In VSCode you can commit by navigating to the “source control” menu in the toolbar (typically on the left side of the window) and clicking the commit or check mark button.\nPush your changes: Before the deadline, submit your assignment by running git push origin main in the assignment directory. You may push updated solutions multiple times, but please do not push after the deadline. Your last push before the deadline will be considered your submission. Submitting after the deadline will cause issues for me and the gruders. In VSCode you can similarly push your assignment from the source control panel and selecting “push”."
  },
  {
    "objectID": "index.html#course-gpu-server",
    "href": "index.html#course-gpu-server",
    "title": "Course Syllabus",
    "section": "Course GPU Server",
    "text": "Course GPU Server\nWe have a GPU server for this course that will be available to you for your final projects. (Thank you to our system administrator Tim Buchheim for setting this up!). The server is located at teapot.cs.hmc.edu (named for the Utah teapot). We will discuss how to allocate resources on this machine at the start of the course project. You will not need GPU access for most homework assignments."
  },
  {
    "objectID": "index.html#homework-assignments-40-of-course-grade",
    "href": "index.html#homework-assignments-40-of-course-grade",
    "title": "Course Syllabus",
    "section": "Homework assignments (40% of course grade)",
    "text": "Homework assignments (40% of course grade)\nFrequency: Homeworks will be assigned on a weekly basis throughout the semester, with the exception of weeks where midterms and final projects are due.\nDue dates and late policy: Homeworks are assigned on Mondays and must be submitted by the end of the following Tuesday (11:59pm Tuesday). Late assignments will not be accepted, but the lowest 2 homework scores will be dropped unconditionally. For cases of illness, injury, family emergency or other extenuating circumstances, please contact me for exceptions to this policy.\nSubmission Homework assignments are submitted by pushing to the corresponding Github repository. For each homework, I will consider the last push before the submission deadline to be your submission."
  },
  {
    "objectID": "index.html#midterm-exam-20-of-course-grade",
    "href": "index.html#midterm-exam-20-of-course-grade",
    "title": "Course Syllabus",
    "section": "Midterm exam (20% of course grade)",
    "text": "Midterm exam (20% of course grade)\nThis course will have a single in-class midterm exam on Wednesday, November 8th. Details will be discussed closer to the exam date."
  },
  {
    "objectID": "index.html#participation-10-of-course-grade",
    "href": "index.html#participation-10-of-course-grade",
    "title": "Course Syllabus",
    "section": "Participation (10% of course grade)",
    "text": "Participation (10% of course grade)\nThis course is not generally a discussion-based class, however there will be certain lectures with open-ended discussions throughout the semester. The participation grade will be based on the following factors:\n\nParticipation in open-ended discussion sessions during class\nContriubting to the learning environment by asking or answering questions during regular lectures\nFollowing the guidelines for respectful discussion (as outlined in course policies)\nClass attendance\nAttending office hours outside class\n\nEarning a perfect participation grade will not require full marks for all of these criteria. A perfect participation grade will be earned by any student who: attends class regularly (&gt; 80% of the time) and participates respectfully in class at least every 1-2 weeks. Attending office hours is not strictly required, but if you are struggling to participate in class, I will assign bonus points to your participation grade for attending. If you have questions about your participation grade at any point, please contact me."
  },
  {
    "objectID": "index.html#final-project-30-of-course-grade",
    "href": "index.html#final-project-30-of-course-grade",
    "title": "Course Syllabus",
    "section": "Final Project (30% of course grade)",
    "text": "Final Project (30% of course grade)\nThe culmination of this course will be a final project completed in teams of 2-4 students. Full project description to follow. Your grade for the final project will depend on:\n\nThe strength of your team’s final presentation and write-up\nYour strength as a team-member (determined by self, peer and instructor evaluations)\nInitial project proposal\nMid-project check-ins\n\nStudents enter this class with highly varying backgrounds and prior experiences with neural networks, so I will help each team determine an appropriate scope for their project. Grades will be evaluated for each team individually based on how the team approached, analyzed and executed on the goals of the project. The relative technical sophistication of other teams projects will not be considered."
  },
  {
    "objectID": "index.html#letter-grade-assignments",
    "href": "index.html#letter-grade-assignments",
    "title": "Course Syllabus",
    "section": "Letter grade assignments",
    "text": "Letter grade assignments\nAs this course is still under active development I cannot yet guarantee exact cutoffs for each grade. Harvey Mudd does not impose expectations for the grade distribution, so every student that meets the requirements for a given grade will earn it. The following is the maximum cutoff of each letter grade, the actual cutoff for each grade may be lower that what is listed below:\n\n&gt;90%: A\n&gt;80%: B\n&gt;70%: C\n&gt;60%: D\n\nAs the semester progresses, I will update this guide to provide a clearer picture of how grades will be assigned."
  },
  {
    "objectID": "index.html#course-feedback",
    "href": "index.html#course-feedback",
    "title": "Course Syllabus",
    "section": "Course feedback",
    "text": "Course feedback\nThis is my first time teaching a college course, so I will need your help! I want to make sure that we go through the material at an appropriate pace and that concepts are presented in a clear and understandable way. To do this, I will be continuously soliciting feedback from you throughout the semester on both the lectures and assignments. I ask that you provide feedback honestly, but kindly. There are three mechanisms I will use for feedback:\nIn-class: In class we will use a thumbs-up, thumbs down system. When I ask if people are following the lecture you can put your thumb up to indicate that you feel you are understanding the material being presented, down to indicate that you are lost or the lecture is confusing, and sideways to indicate that you followed some parts, but not all. You are, of course, also encouraged to give verbal feedback if appropriate.\nWith homework: Each homework will include a link to a survey to give feedback on that week’s assignment and lectures. Submitting this form is a required part of the homework, but your answers will not be tracked or accounted for in grades. This gives you a chance to indicate any issues (or things you like) with the class.\nGeneral anonymous feedback: If you have an issue with the course that you would like me to be aware of, but do not want your name to be associated with, you can use this form to submit an anonymous issue. Please continue to remain constructive and kind when submitting feedback in this way."
  },
  {
    "objectID": "index.html#academic-issues-and-wellness",
    "href": "index.html#academic-issues-and-wellness",
    "title": "Course Syllabus",
    "section": "Academic issues and wellness",
    "text": "Academic issues and wellness\nMy primary goal is for every student to understand the material to the best extent possible and hopefully enjoy learning the material at the same time. If at any point you are concerned about your grade or feel you are struggling for any reason I encourage you to reach out to me either via slack/email or during office hours. I will also try to reach out to you if I notice you are struggling with the material or are not on track to pass the class.\nI also want you to prioritize your mental and physical well-being. The college has several resources that can help you with this. The academic deans (academicdeans@g.hmc.edu) can help you keep on top of your academic progress. The office of health and wellness (https://www.hmc.edu/health-wellness/) can help you with a wide range of physical and metal health issues. I encourage you to be proactive, if you are starting to feel anxious, overwhelmed or depressed seeking help early can be extremely valuable. If you are unsure where to go, I can help guide you to the appropriate resource. The Claremont Care Guide, provides a useful guide if you or someone you know is in urgent distress."
  },
  {
    "objectID": "index.html#accommodations",
    "href": "index.html#accommodations",
    "title": "Course Syllabus",
    "section": "Accommodations",
    "text": "Accommodations\nIf you have a disability (for example, mental health, learning, chronic health, physical, hearing, vision, neurological, etc.) and expect barriers related to this course, it is important to request accommodations and establish a plan. Requests for accommodations must go through the Office of Accessible Education. I am happy to work with them to establish an appropriate plan for this course. I also encourage reaching out to them if you are unsure of your eligibility for accommodations, they can help determine what is appropriate for you.\nRemember that requests for accommodations must be made each semester. If you are not already registered this process can take time and accommodations cannot be applied retroactively, so starting the process early is important."
  },
  {
    "objectID": "index.html#attendence",
    "href": "index.html#attendence",
    "title": "Course Syllabus",
    "section": "Attendence",
    "text": "Attendence\nAttendence is strongly encouraged as it is beneficial for both your own learning and that of your peers who may learn from your knowledge and viewpoints. Not only is attendance reflected in your participation grade, it is also highly correlated with performance on exams and homework. That said, I understand that there are times where student may miss class for a variety of reasons. If you miss a class (or several) please contact me by email or slack and we can work out a plan to catch you up on the material. Up to 1 unexcused absence per month will not affect your participation grade, neither will excused absences due to illness, injury, etc."
  },
  {
    "objectID": "index.html#guidelines-for-respectful-class-discussion",
    "href": "index.html#guidelines-for-respectful-class-discussion",
    "title": "Course Syllabus",
    "section": "Guidelines for respectful class discussion",
    "text": "Guidelines for respectful class discussion\nThe goal of in-class discussions to understand each others perspectives and to contribute to both our own learning and that of our peers. To make sure that in-class discussions are aligned with these goals please be mindful of the following guidelines:\n\nAvoid judgment: Students enter this class with a variety of backgrounds, experience and viewpoints. Be positive and encouraging to your peers even if you feel they are incorrect. Strive to make sure those around you feel comfortable answering questions even if they are not completely sure of their answer and give opinions that they are not sure others will agree with. Remember that giving an answer different from what the instructor was looking for can lead to productive and informative discussions.\nAllow everyone a chance to speak: We want to give every student a chance to participate in the class and in discussions. If you find yourself speaking, answering or asking questions far more than your peers, consider encouraging others to speak instead. Remember that in-class time is not your only opportunity to discuss this material and you are welcome to ask more questions in office hours.\nPractice active listening: When having in-class discussions make sure to acknowledge the answers and opinions of others before offering your own. Avoid interrupting others. Your thoughts deserve to be heard and understood, so it’s important that we work together to make sure everyone’s contributions are considered.\nBe kind: Do not use harsh or disparaging language in class. Avoid blame or speculation about other students. Aim to be charitable in your interpretations of other peoples words. Respect the boundaries set by others.\nBe inclusive: Be respectful of everyone’s background and identity. Avoid making assumptions or generalizations based on someone’s (perceived) social group. Do not ask individuals to speak for their social group."
  },
  {
    "objectID": "index.html#collaboration-policy",
    "href": "index.html#collaboration-policy",
    "title": "Course Syllabus",
    "section": "Collaboration policy",
    "text": "Collaboration policy\nYou are encouraged to discuss homework assignments with other students, but the final work that you submit for each assignment must be still be your own. This means you may:\n\nDiscuss published course materials and topics\nDiscuss approaches to problems with other students, including while working on the assignments\nShare helpful examples and resources with other students\nHelp other students with technical issues such as setting up GitHub and Python environments.\nView another student’s code for the purpose of debugging small technical issues (exceptions, syntax errors etc.)\n\nYou may not:\n\nCopy/paste another student’s answers to any problem or allow another student to copy/paste your answers\nShare answers to completed problems with other students\nDistribute or post online any assignments, problems and/or solutions.\n\nThis collaboration policy is covered by the Harvey Mudd honor code and violations will be referred to the honor code board.\nEach homework will have space for you to indicate who you discussed the assignment with. If you would like help finding other students to study with, please let me know and I can work to set you up with a study group."
  },
  {
    "objectID": "index.html#ai-policy",
    "href": "index.html#ai-policy",
    "title": "Course Syllabus",
    "section": "AI Policy",
    "text": "AI Policy\nIn this course we will be learning the fundamental tools for building large language models and chat AIs, such as ChatGPT. Therefore I encourage you to experiment with ChatGPT and it’s competitors during this course. However, I consider these models to be covered by the above collaboration policy. That means you may interact with them to discuss course materials, but you may not share assignment problems with them, nor may you copy/paste answers from them. If you have any questions about what is appropriate, please reach out to me."
  },
  {
    "objectID": "index.html#covid-safety",
    "href": "index.html#covid-safety",
    "title": "Course Syllabus",
    "section": "COVID Safety",
    "text": "COVID Safety\nCollege policy states that masks are no longer required indoors for the upcoming semester. I will not require masks in class, but students who prefer to continue wearing masks are should do so. If you are feeling sick please stay home and let me know so that I can provide you with the relevant course materials."
  },
  {
    "objectID": "index.html#courses",
    "href": "index.html#courses",
    "title": "Course Syllabus",
    "section": "Courses",
    "text": "Courses\n\nfastai (website)\nDeep Learning Specialization (Coursera MOOC)\nDeep Learning (Stanford CS230 course)\nConvolutional Neural Networks for Visual Recognition (Stanford CS231n course)\nIntroduction to Deep Learning (MIT 6.S191 course)\nMIT Deep Learning and Artificial Intelligence Lectures (MIT course)\nDeep Reinforcement Learning (Berkeley CS 285 course)\nDeep Reinforcement Learning (free online course)\nDeep Learning Systems"
  },
  {
    "objectID": "index.html#books",
    "href": "index.html#books",
    "title": "Course Syllabus",
    "section": "Books",
    "text": "Books\n\nDive into Deep Learning (UC Berkeley book)\nDeep Learning (free book)\nFirst steps towards Deep Learning with PyTorch (free book)\nNeural Networks and Deep Learning (free book)\nDeep Learning With PyTorch (pdf)\nAnnotated Algorithms in Python (free book)\nLearn Python the Right way (free book)\nThe Linux Command Line by William Shotts"
  },
  {
    "objectID": "index.html#math",
    "href": "index.html#math",
    "title": "Course Syllabus",
    "section": "Math",
    "text": "Math\n\nThe Matrix Calculus You Need For Deep Learning (website)\nThe Mechanics of Machine Learning (free book)\nMathematics for Machine Learning (free book)\nSeeing Theory: A Visual Introduction To Probability And Statistics (free book)"
  },
  {
    "objectID": "index.html#extras",
    "href": "index.html#extras",
    "title": "Course Syllabus",
    "section": "Extras",
    "text": "Extras\n\nCheatsheet (website)\nTensorFlow 2.0 Complete Course - Python Neural Networks for Beginners Tutorial (YouTube)\nNeural Networks (3blue1brown YouTube)\nMachine Learning From Scratch (website)\nA visual introduction to machine learning (website)"
  },
  {
    "objectID": "index.html#python-1",
    "href": "index.html#python-1",
    "title": "Course Syllabus",
    "section": "Python",
    "text": "Python\n\nGoogle’s Python Class\nIntroduction to Python | Microsoft Learn\nList of free free Python books\nPython Programming Tutorials\nLearn Python - Full Course for Beginners (YouTube)\nPython In 1 Minute (YouTube)\nAutomate the Boring Stuff With Python (book)\nIntroduction to Python Programming (free course)\nA Whirlwind Tour of Python (Jupyter Notebooks)\nPython for Everybody Specialization (free course)\nIntroduction to Computer Science and Programming Using Python (MIT course)"
  },
  {
    "objectID": "index.html#ethics",
    "href": "index.html#ethics",
    "title": "Course Syllabus",
    "section": "Ethics",
    "text": "Ethics\n\nAwful AI\nLearning from the past to create Responsible AI\nPractical Data Ethics\nFair ML Book\nMachine Ethics Podcast\nACM Code of Ethics and Professional Conduct\nIEEE Code of Ethics\nCode of Conduct for Professional Data Scientists"
  },
  {
    "objectID": "index.html#librariesframeworkstools",
    "href": "index.html#librariesframeworkstools",
    "title": "Course Syllabus",
    "section": "Libraries/Frameworks/Tools",
    "text": "Libraries/Frameworks/Tools\n\nMlxtend (machine learning extensions)\nStreamlit (Turn data scripts into sharable web apps in minutes)\nDeepnote (The notebook you’ll love to use)"
  },
  {
    "objectID": "misc/hw1-hint.html",
    "href": "misc/hw1-hint.html",
    "title": "Hint for homework 1: Q7",
    "section": "",
    "text": "For this question we are interested in simplifying an expression into matrix/vector notation. In order to do this it may be first helpful to think about how we went the other direction: matrix/vector notation to expanded notation.\nRecall that a dot product between two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), can be written explicitly from its definition as:\n\\[\n\\mathbf{x}^T\\mathbf{y} = \\sum_{i=1}^n x_iy_i\n\\]\nTherefore if we see something like the summation on the right in an expression, we can replace it with the more compact dot product notation.\nIf we have the expression \\(\\mathbf{A} \\mathbf{x}\\), where \\(\\mathbf{A}\\) is a matrix and \\(\\mathbf{x}\\) is a vector, we know that the result of this multiplication will be a vector. Let’s call this vector \\(\\mathbf{c}\\), so that \\(\\mathbf{A}\\mathbf{x}=\\mathbf{c}\\).\nWe know from the definition of matrix-multiplication that each element of \\(\\mathbf{c}\\) can be written as the following summation:\n\\[\nc_i = \\sum_{j=1}^n A_{ij}x_j\n\\]\nTherefore, if we saw such a summation in an expression, we could temporarily replace it with \\(c_i\\), knowing that we defined \\(\\mathbf{c}\\) as \\(\\mathbf{c}=\\mathbf{A}\\mathbf{x}\\). Try doing this as the first step in the homework, then try repeating this idea until you have something that you can write compactly in matrix/vector notation.\nWhen writing you final answer make sure to write the final expression in terms of the original variables \\((\\mathbf{A}, \\mathbf{x}, \\mathbf{b})\\). E.g. if you substituted \\(\\mathbf{c}\\) for \\(\\mathbf{A}\\mathbf{x}\\), make sure to substitute it back in the answer."
  },
  {
    "objectID": "calendar/calendar.html",
    "href": "calendar/calendar.html",
    "title": "Course Calendar",
    "section": "",
    "text": "Lecture\nDate\nTopics\nTextbook\nMaterials\nHomework\n\n\n\n\n1\n8/28\nCourse introduction, linear algebra and calculus review\nBook 1: 7.1.1-7.2.4, 7.8.1, 7.8.2\nNotes\nSlides\nHomework 1 assigned (Github) Due: 9/5 11:59pm\nSolutions\n\n\n2\n8/30\nLinear regression\nBook 1: 4.2.1, 4.2.2, 11.1-11.2.4\nNotes\nSlides\n\n\n\n\n9/4\nNo class (Labor day)\n\n\n\n\n\n3\n9/6\nLogistic regression\nBook 1: 10.1, 10.2.1, 10.2.3, 10.3.1-10.3.3\nNotes\nSlides\nHomework 2 assigned (Github) Due: 9/12 11:59pm\nSolutions\n\n\n4\n9/11\nFeature transforms\nBook 1: 10.2.2\nNotes\nSlides\n\n\n\n5\n9/13\nLinear and logistic regression discussion\n\n\n\n\n\n6\n9/18\nLinear and logistic regression discussion\n\nNotebook\n\n\n\n7\n9/20\nNeural networks introduction\nBook 1: 13.1, 13.2\nNotes\nHomework 3 assigned (Github) Due: 9/26 11:59pm\nSolutions\n\n\n8\n9/25\nMultilayer Neural networks\nBook 1: 13.1, 13.2\nNotes\n\n\n\n9\n9/27\nAutomatic Differentiation\nBook 1: 13.3\nNotes\nHomework 4 assigned (Github) Due: 10/10 11:59pm\nSolutions\n\n\n10\n10/2\nAutomatic Differentiation Cont.\nBook 1: 13.3\nNotes\n\n\n\n11\n10/4\nPytorch\n\nNotebook\nColab\n\n\n\n12\n10/9\nEvaluation and Regularization\nBook 1: 13.3\nNotes\n\n\n\n13\n10/11\nRegularization\nBook 1: 13.5.1-13.5.4\nNotes\n\n\n\n\n10/16\nNo class (Fall break)\n\n\n\n\n\n14\n10/18\nRegularization and initialization\nBook 1: 13.4.3-13.4.5\nNotes\nHomework 5 assigned (Github) Due: 10/31 11:59pm\nSolutions\n\n\n15\n10/23\nStochastic Gradient Descent\nBook 1: 8.4\nNotes\n\n\n\n16\n10/25\nMomentum and Adaptive Optimization\nBook 1: 8.4\nNotes\n\n\n\n17\n10/30\nNormalization\nBook 1: 14.2.4\nNotes\n\n\n\n18\n11/1\nResidual networks\nBook 1: 13.4.4\nNotes\nHomework 6 assigned (Github) Due: 11/9 11:59pm\nSolutions\n\n\n19\n11/6\nConvolutional netwoks\nBook 1: 14.1-14.3\n\n\n\n\n\n11/8\nConvolutional netwoks\nBook 1: 14.1-14.3\n\n\n\n\n20\n11/13\nConvolutional netwoks\nBook 1: 14.1-14.3\n\n\n\n\n21\n11/15\nRecurrent networks\nBook 1: 15.2.1-15.2.6\n\n\n\n\n22\n11/20\nRecurrent networks\nBook 1: 15.2.1-15.2.6\n\n\n\n\n\n11/22\nNo class (Thanksgiving break)\n\n\n\n\n\n23\n11/27\nAttention\nBook 1: 15.4.1, 15.4.2-15.4.6\n\n\n\n\n24\n11/29\nAttention\nBook 1: 15.4.1, 15.4.2-15.4.6\n\n\n\n\n25\n12/5\nTransformers\nhttp://nlp.seas.harvard.edu/annotated-transformer/\nNotes\nHomework 7 assigned (Github) Due: 12/15 11:59pm\n\n\n26\n12/6\nTransformers\nhttp://nlp.seas.harvard.edu/annotated-transformer/\n\n\n\n\n\n12/12\n\n\n\nFinal Project Due (11:59pm)\n(Description)"
  },
  {
    "objectID": "plugins/drawer/Readme.html",
    "href": "plugins/drawer/Readme.html",
    "title": "RevealJS drawer plugin (3.2KB gzipped)",
    "section": "",
    "text": "Allows you to draw over your slides. Drawings are saved per slide and kept when slide is changed. Demo. Works with RevealJS Pointer Plugin.\n\nThis plugin only works with RevealJS v4.x or higher.\n\nNo external dependencies, only 7.5KB | &lt;3.2KB gzipped.\n\n\n\nCopy dist/drawer.js into plugins/drawer/drawer.js and import script:\n[...]\n&lt;script src=\"plugin/drawer/drawer.js\"&gt;&lt;/script&gt;\n[...]\nCopy dist/drawer.css into plugins/drawer/drawer.css and import style in &lt;head&gt;&lt;/head&gt;:\n[...]\n&lt;link rel=\"stylesheet\" href=\"plugin/drawer/drawer.css\" /&gt;\n[...]\nAdd RevealDrawer into your plugins initialization:\nplugins: [RevealDrawer];\n\n\n\n\n\nT - toggle drawing board\nD - toggle mode (drawing or not drawing)\nCtrl + Z - remove last line from current slide\n\"1\", \"2\", \"3\", \"4\" - change selected color (base on the order)\n\nIf you’re not changing anything in the Config then you should be able to show drawing board just by hitting T. By default the drawing is enabled.\n\nIf you hit D then drawing mode is toggled and it is going to switch to disabled mode (the pen icon is grayed out).\n\nIn drawing mode you’re not able to interact with other elements (like code) because it would disturb your drawing. That’s why switching between drawing and not drawing mode is important.\nEach time you draw sth, it is saved for this particular slide (slide includes all fragments). You can switch between slides and have a different drawing on each one. Ctrl + Z is available if you make a mistake in your drawing. It also works per slide even if you’re coming back from the different slide.\nYou’re able to change between colors using color icons or numbers on the keyboard. Each color has a number assigned to it and if you have 4 colors then numbers 1,2,3,4 on your keyboard are responsible for switching between them (default option). If you change default colors then numbers are assigned to new ones (base on how many colors you have). E.g. you’ve decided to have simpler colors, so your list looks like ['#FF0000', '#00FF00', '#0000FF'], now only 1,2,3 keys are available.\n\n\n\n\nYou can configure drawer key and tail length in plugin config.\nReveal.initialize({\n  drawer: {\n    toggleDrawKey: \"d\", // (optional) key to enable drawing, default \"d\"\n    toggleBoardKey: \"t\", // (optional) key to show drawing board, default \"t\"\n    colors: [\"#fa1e0e\", \"#8ac926\", \"#1982c4\", \"#ffca3a\"], // (optional) list of colors avaiable (hex color codes)\n    color: \"#FF0000\", // (optional) color of a cursor, first color from `codes` is a default\n    pathSize: 4, // (optional) path size in px, default 4\n  }\n})\nList of available keys:\n\n[“0”, “1”, “2”, “3”, “4”, “5”, “6”, “7”, “8”, “9”, “backspace”, “tab”, “enter”, “shift”, “ctrl”, “alt”, “pausebreak”, “capslock”, “esc”, “space”, “pageup”, “pagedown”, “end”, “home”, “leftarrow”, “uparrow”, “rightarrow”, “downarrow”, “insert”, “delete”, “a”, “b”, “c”, “d”, “e”, “f”, “g”, “h”, “i”, “j”, “k”, “l”, “m”, “n”, “o”, “p”, “q”, “r”, “s”, “t”, “u”, “v”, “w”, “x”, “y”, “z”, “leftwindowkey”, “rightwindowkey”, “selectkey”, “numpad0”, “numpad1”, “numpad2”, “numpad3”, “numpad4”, “numpad5”, “numpad6”, “numpad7”, “numpad8”, “numpad9”, “multiply”, “add”, “subtract”, “decimalpoint”, “divide”, “f1”, “f2”, “f3”, “f4”, “f5”, “f6”, “f7”, “f8”, “f9”, “f10”, “f11”, “f12”, “numlock”, “scrolllock”, “semicolon”, “equalsign”, “comma”, “dash”, “period”, “forwardslash”, “graveaccent”, “openbracket”, “backslash”, “closebracket”, “singlequote”]\n\n\n\n\n\nMake changes in src/plugin.js and run:\nnpm run build\nThis is going to produce dist/drawer.js with bundled iife file."
  },
  {
    "objectID": "plugins/drawer/Readme.html#installation",
    "href": "plugins/drawer/Readme.html#installation",
    "title": "RevealJS drawer plugin (3.2KB gzipped)",
    "section": "",
    "text": "Copy dist/drawer.js into plugins/drawer/drawer.js and import script:\n[...]\n&lt;script src=\"plugin/drawer/drawer.js\"&gt;&lt;/script&gt;\n[...]\nCopy dist/drawer.css into plugins/drawer/drawer.css and import style in &lt;head&gt;&lt;/head&gt;:\n[...]\n&lt;link rel=\"stylesheet\" href=\"plugin/drawer/drawer.css\" /&gt;\n[...]\nAdd RevealDrawer into your plugins initialization:\nplugins: [RevealDrawer];\n\n\n\n\n\nT - toggle drawing board\nD - toggle mode (drawing or not drawing)\nCtrl + Z - remove last line from current slide\n\"1\", \"2\", \"3\", \"4\" - change selected color (base on the order)\n\nIf you’re not changing anything in the Config then you should be able to show drawing board just by hitting T. By default the drawing is enabled.\n\nIf you hit D then drawing mode is toggled and it is going to switch to disabled mode (the pen icon is grayed out).\n\nIn drawing mode you’re not able to interact with other elements (like code) because it would disturb your drawing. That’s why switching between drawing and not drawing mode is important.\nEach time you draw sth, it is saved for this particular slide (slide includes all fragments). You can switch between slides and have a different drawing on each one. Ctrl + Z is available if you make a mistake in your drawing. It also works per slide even if you’re coming back from the different slide.\nYou’re able to change between colors using color icons or numbers on the keyboard. Each color has a number assigned to it and if you have 4 colors then numbers 1,2,3,4 on your keyboard are responsible for switching between them (default option). If you change default colors then numbers are assigned to new ones (base on how many colors you have). E.g. you’ve decided to have simpler colors, so your list looks like ['#FF0000', '#00FF00', '#0000FF'], now only 1,2,3 keys are available.\n\n\n\n\nYou can configure drawer key and tail length in plugin config.\nReveal.initialize({\n  drawer: {\n    toggleDrawKey: \"d\", // (optional) key to enable drawing, default \"d\"\n    toggleBoardKey: \"t\", // (optional) key to show drawing board, default \"t\"\n    colors: [\"#fa1e0e\", \"#8ac926\", \"#1982c4\", \"#ffca3a\"], // (optional) list of colors avaiable (hex color codes)\n    color: \"#FF0000\", // (optional) color of a cursor, first color from `codes` is a default\n    pathSize: 4, // (optional) path size in px, default 4\n  }\n})\nList of available keys:\n\n[“0”, “1”, “2”, “3”, “4”, “5”, “6”, “7”, “8”, “9”, “backspace”, “tab”, “enter”, “shift”, “ctrl”, “alt”, “pausebreak”, “capslock”, “esc”, “space”, “pageup”, “pagedown”, “end”, “home”, “leftarrow”, “uparrow”, “rightarrow”, “downarrow”, “insert”, “delete”, “a”, “b”, “c”, “d”, “e”, “f”, “g”, “h”, “i”, “j”, “k”, “l”, “m”, “n”, “o”, “p”, “q”, “r”, “s”, “t”, “u”, “v”, “w”, “x”, “y”, “z”, “leftwindowkey”, “rightwindowkey”, “selectkey”, “numpad0”, “numpad1”, “numpad2”, “numpad3”, “numpad4”, “numpad5”, “numpad6”, “numpad7”, “numpad8”, “numpad9”, “multiply”, “add”, “subtract”, “decimalpoint”, “divide”, “f1”, “f2”, “f3”, “f4”, “f5”, “f6”, “f7”, “f8”, “f9”, “f10”, “f11”, “f12”, “numlock”, “scrolllock”, “semicolon”, “equalsign”, “comma”, “dash”, “period”, “forwardslash”, “graveaccent”, “openbracket”, “backslash”, “closebracket”, “singlequote”]"
  },
  {
    "objectID": "plugins/drawer/Readme.html#developing",
    "href": "plugins/drawer/Readme.html#developing",
    "title": "RevealJS drawer plugin (3.2KB gzipped)",
    "section": "",
    "text": "Make changes in src/plugin.js and run:\nnpm run build\nThis is going to produce dist/drawer.js with bundled iife file."
  },
  {
    "objectID": "lecture2-linear-regression/notes.html",
    "href": "lecture2-linear-regression/notes.html",
    "title": "Lecture 2: Linear regression",
    "section": "",
    "text": "Manim Community v0.17.3"
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#functions-revisited",
    "href": "lecture2-linear-regression/notes.html#functions-revisited",
    "title": "Lecture 2: Linear regression",
    "section": "Functions revisited",
    "text": "Functions revisited\nIn the previous lecture we reviewed the concept of a function, which is a mapping from a set of possible inputs to a corresponding set of outputs. Here we’ll consider functions with vector inputs and scalar outputs.\n\\[\ny=f(\\mathbf{x}), \\quad \\text{Input: } \\mathbf{x} \\in\\mathbb{R}^n \\longrightarrow \\text{ Output: }y \\in\\mathbb{R}\n\\]\nMathematically, we can easily definite a function using a sequence of basic operations.\n\n\n\n\n\nThis function gives us the relationship between inputs \\(\\mathbf{x}\\) and outputs \\(f(\\mathbf{x})\\). That is, for any given input \\(x\\), we can find the corresponding output \\(y\\) by applying our function \\(f(\\mathbf{x})\\)."
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#linear-functions",
    "href": "lecture2-linear-regression/notes.html#linear-functions",
    "title": "Lecture 2: Linear regression",
    "section": "Linear Functions",
    "text": "Linear Functions\nA linear function is any function \\(f\\) where the following conditions always hold: \\[ f(\\mathbf{x} + \\mathbf{y}) =f(\\mathbf{x}) + f(\\mathbf{y})\\] and \\[ f(a\\mathbf{x}) = a f(\\mathbf{x})\\]For a linear function, the output can be defined as a weighted sum of the inputs. In other words a linear function of a vector can always be written as:\n\\[\nf(\\mathbf{x}) = \\sum_{i=1}^n x_iw_i\n\\]\nWe can add an offset \\(b\\) to create an affine function:\n\\[\nf(\\mathbf{x}) = \\sum_{i=1}^n x_iw_i +b\n\\]\nWe can also write this using a dot-product between our input \\(\\mathbf{x}\\) and parameter vector \\(\\mathbf{w}\\) as:\n\\[\nf(\\mathbf{x}) = \\mathbf{x} \\cdot \\mathbf{w} + b \\quad \\text{or} \\quad f(\\mathbf{x}) = \\mathbf{x}^T  \\mathbf{w} + b\n\\]\nIn one dimension, a linear (or affine) function is always a line, for example:\n\n\n\n\n\nIn higher dimensions, it is a plane or hyperplane:\n\n\n\n\n\nIn numpy we can easily write a function of this form:\n\ndef f(x):\n    w = np.array([-0.6, -0.2])\n    b = -1\n    return np.dot(x, w) + b"
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#handling-bias-compactly",
    "href": "lecture2-linear-regression/notes.html#handling-bias-compactly",
    "title": "Lecture 2: Linear regression",
    "section": "Handling bias compactly",
    "text": "Handling bias compactly\nNotationally, it can be tedious to always write the bias term. A common approach to compactly describing linear or affine functions is to use augmented inputs and weights, such that for \\(\\mathbf{x}\\) and \\(\\mathbf{w}\\in \\mathbb{R}^n\\), we add \\(x_{n+1}=1\\) and \\(w_{n+1}=b\\). So:\n\\[\n\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\longrightarrow \\mathbf{x}_{aug}= \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\\\ 1 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{bmatrix} \\longrightarrow \\mathbf{w}_{aug}=  \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\\\ b \\end{bmatrix}\n\\]\nWe can easily see then that using this notation:\n\\[\nf(\\mathbf{x}) = \\mathbf{x}^T \\mathbf{w} +b = \\mathbf{x}_{aug}^T \\mathbf{w}_{aug}\n\\]\nThis approach is common enough that we typically won’t bother with the \\(aug\\) notation and just assume that any function defined as \\(f(\\mathbf{x})=\\mathbf{x}^T\\mathbf{w}\\) can be defined to include a bias implicitly. Note that in this case the function is a linear function of the augmented input, thus we will still typically refer to functions of this form as linear functions.\nIn numpy this is similarly straightforward:\n\ndef f(x):\n    w = np.array([-0.6, -0.2, -1])\n    x = np.pad(x, ((0,1),), constant_values=1)\n    return np.dot(x, w)"
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#datasets-and-observations",
    "href": "lecture2-linear-regression/notes.html#datasets-and-observations",
    "title": "Lecture 2: Linear regression",
    "section": "Datasets and observations",
    "text": "Datasets and observations\nIn the real-world we often have access to inputs and outputs in the form of data, but not to an actual function that we can evaluate.\nSpecifically we will say that we have access to a dataset \\(\\mathcal{D}\\) made up of \\(N\\) pairs of inputs ( \\(\\mathbf{x}\\) ) and outputs ( \\(y\\) ):\n\\[\n\\mathcal{D} = \\{ (\\mathbf{x}_1, y_1),\\ (\\mathbf{x}_2, y_2),\\ ...\\ (\\mathbf{x}_N, y_N)\\}\n\\]\nWe call each of these pairs an observation. Let’s take a look at a real world example of a dataset.\n\nFuel efficiency\nLet’s imagine we’re designing a car and we would like to know what the fuel efficiency of the car we’re designing will be in miles per gallon (MPG). We know some properties of our current design, such as the weight and horsepower, that we know should affect the efficiency. Ideally we would have access to a function that would give us the MPG rating if we provide these features.\n\\[\n\\text{mpg} = f(\\text{weight},\\ \\text{horsepower}...)\n\\]\nUnfortunately we don’t know the exact relationship between a car’s features and fuel efficiency. However, we can look at other cars on the market and see what the corresponding inputs and outputs would be:\n\\[\n\\text{Honda Accord: } \\begin{bmatrix} \\text{Weight:} & \\text{2500 lbs} \\\\ \\text{Horsepower:} & \\text{ 123 HP} \\\\ \\text{Displacement:} & \\text{ 2.4 L} \\\\ \\text{0-60mph:} & \\text{ 7.8 Sec} \\end{bmatrix} \\longrightarrow \\text{   MPG: 33mpg}\n\\]\n\\[\n\\text{Dodge Aspen: } \\begin{bmatrix} \\text{Weight:} & \\text{3800 lbs} \\\\ \\text{Horsepower:} & \\text{ 155 HP} \\\\ \\text{Displacement:} & \\text{ 3.2 L} \\\\ \\text{0-60mph:} & \\text{ 6.8 Sec} \\end{bmatrix} \\longrightarrow \\text{   MPG: 21mpg}\n\\]\n\\[\n\\vdots \\quad \\vdots\n\\]\nOur dataset will be this collection of data that we have for all other cars. In general, each observation in this dataset will correspond to a car.\n\\[\n\\text{Dataset: } \\mathcal{D}=\\{(\\mathbf{x}_i,\\ y_i) \\text{  for  } i\\in 1...N\\}\n\\]\n\\[\n\\text{Input: } \\mathbf{x}_i= \\begin{bmatrix} \\text{Weight} \\\\ \\text{Horsepower} \\\\ \\text{Displacement} \\\\ \\text{0-60mph}  \\end{bmatrix}, \\quad \\text{Output: } y_i = MPG\n\\]\nJust as with a known function, we can plot the inputs vs the outputs, however in this case, we only know the outputs for the inputs we’ve seen in our dataset. Let’s take a look at a single feature: the weight of a car."
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#prediction-functions",
    "href": "lecture2-linear-regression/notes.html#prediction-functions",
    "title": "Lecture 2: Linear regression",
    "section": "Prediction functions",
    "text": "Prediction functions\nOur dataset gives us a set of known inputs and outputs for our unknown functions. The central question we will address in this course is then:\n\nHow do we predict the output for an input that we haven’t seen before?\nFor example, in our car scenario, we might know that the car that we’re designing will weigh 3100 lbs. In our dataset we’ve haven’t seen a car that weighs exactly 3100 lbs, so we need a way to predict the output of the function at input 3100 lbs.\n\n\n\n\n\nIn general, our approach to this problem will be to model our unknown function with a known function that we can evaluate at any input. We want to chose a function \\(f\\) such that for any observation our dataset, the output of this function approximates the true target output that we observed.\n\\[\nf(\\mathbf{x}_i) \\approx y_i, \\quad \\forall (\\mathbf{x}_i, y_i) \\in \\mathcal{D}\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#linear-interpolation",
    "href": "lecture2-linear-regression/notes.html#linear-interpolation",
    "title": "Lecture 2: Linear regression",
    "section": "Linear interpolation",
    "text": "Linear interpolation\nOne reasonable approach we might consider is linear interpolation. In this approach, we simply connect all the neighboring points with straight lines:\n\n\n\n\n\nIn some cases this can be a reasonable approach! In fact it’s how the plt.plot function works. Real data however tends to be messy. The measurements in our dataset might not be 100% accurate or they might even conflict! What do we do if we have two observations with the same input and different outputs?\n\\[(\\text{Weight: }3100, \\text{MPG: } 34), \\quad (\\text{Weight: }3100, \\text{MPG: } 23) \\longrightarrow f(3100) = ?\\]\nAs the size and number of features in our inputs gets larger, this become even more complex. We can see this if we try to apply interpolation to a much larger MPG dataset:"
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#linear-regression-1",
    "href": "lecture2-linear-regression/notes.html#linear-regression-1",
    "title": "Lecture 2: Linear regression",
    "section": "Linear regression",
    "text": "Linear regression\nLinear regression is the approach of modeling an unknown function with a linear function. From our discussion of linear functions, we know that this means that we will make predictions using a function of the form:\n\\[\nf(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{w} = \\sum_{i=1}^n x_i w_i\n\\]\nMeaning that the output will be a weighted sum of the features of the input. In the case of our car example, we will make predictions as:\n\\[\n\\text{Predicted MPG} = f(\\mathbf{x})=\n\\]\n\\[\n(\\text{weight})w_1 + (\\text{horsepower})w_2 + (\\text{displacement})w_3 + (\\text{0-60mph})w_4 + b\n\\]\nOr in matrix notation:\n\\[\nf(\\mathbf{x})= \\begin{bmatrix} \\text{Weight} \\\\ \\text{Horsepower} \\\\ \\text{Displacement} \\\\ \\text{0-60mph} \\\\ 1 \\end{bmatrix} \\cdot \\begin{bmatrix} w_1 \\\\ w_2\\\\ w_3 \\\\ w_4\\\\ b\\end{bmatrix}\n\\]\nWe see that under this approach each weight \\((w_1, w_2…)\\) tells us how much our prediction changes as we change the corresponding feature. For example, if we were to increase the weight of our car by 1 lb, the predicted MPG would change by \\(w_1\\).\nThe set of weights defines the particular linear regression function. In numpy we can define a generic class for linear regression:\n\nclass Regression:\n    def __init__(self, weights):\n        self.weights = weights\n    \n    def predict(self, x):\n        return np.dot(x, self.weights)\n\nmodel = Regression(np.array([1, 1, 1, 1, 1]))\nmodel.predict(np.array([5, 2, 3, 3, 1]))\n\n14\n\n\nIf we again look at our plot of weight vs. MPG, we see we could chose many different linear functions to make predictions:"
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#residuals-and-error",
    "href": "lecture2-linear-regression/notes.html#residuals-and-error",
    "title": "Lecture 2: Linear regression",
    "section": "Residuals and error",
    "text": "Residuals and error\nThe residual or error of a prediction is the difference between the prediction and the true output:\n\\[\ne_i = y_i - f(\\mathbf{x}_i)\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#mean-squared-error",
    "href": "lecture2-linear-regression/notes.html#mean-squared-error",
    "title": "Lecture 2: Linear regression",
    "section": "Mean squared error",
    "text": "Mean squared error\nIn deciding what linear function to use, we need a measure of error for the entire dataset. A computationally convenient measure is mean squared error (MSE). The mean squared error is the averaged of the error squared for each observation in our dataset:\n\\[\nMSE = \\frac{1}{N}\\sum_{i=1}^N (f(\\mathbf{x}_i) - y_i)^2 = \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2\n\\]It follows that the best choice of linear function \\(f^*\\) is the one that minimizes the mean squared error for our dataset. Since each linear function is defined by a parameter vector \\(\\mathbf{w}\\), this is equivalent to finding \\(\\mathbf{w}^*\\), the parameters vector that minimizes the mean squared error. \\[\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmin}} \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2 \\]"
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#loss-functions",
    "href": "lecture2-linear-regression/notes.html#loss-functions",
    "title": "Lecture 2: Linear regression",
    "section": "Loss functions",
    "text": "Loss functions\nNote that the mean squared error depends on the data inputs \\((\\mathbf{x}_1,…,\\mathbf{x}_N)\\), the data targets \\((y_1,…,y_N)\\) and the parameters \\((\\mathbf{w})\\). So we can express the MSE as a function of all three:\n\\[\nMSE(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2\n\\]\nHere we have used \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) to refer to the entire collection of inputs and outputs from our dataset \\((\\mathcal{D})\\) respectively, so:\n\\[\n\\mathbf{X} = \\begin{bmatrix}\\mathbf{x}_1 \\\\ \\mathbf{x}_2 \\\\ \\vdots \\\\ \\mathbf{x}_N \\end{bmatrix} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1n} \\\\ x_{21} & x_{22} & \\dots & x_{2n}\\\\ \\vdots & \\vdots  & \\ddots & \\vdots \\\\ x_{N1} & x_{N2} & \\dots & x_{Nn} \\end{bmatrix}, \\quad \\mathbf{y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{bmatrix}\n\\]\nThis is an example of loss function, for our given dataset this function tells us how much error (loss) we are incurring for a given choice of \\(\\mathbf{w}\\). If we assume our dataset is fixed we can drop the explicit dependence on \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\), looking at the loss as purely a function of our choice of parameters:\n\\[\n\\textbf{Loss}(\\mathbf{w})= MSE(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2\n\\]\nAgain, if our goal is to minimize error, we want to choose the parameters \\(\\mathbf{w}^*\\) that minimize this loss:\n\\[\n\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmin}}\\ \\textbf{Loss}(\\mathbf{w})= \\underset{\\mathbf{w}}{\\text{argmin}} \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#visualizing-loss",
    "href": "lecture2-linear-regression/notes.html#visualizing-loss",
    "title": "Lecture 2: Linear regression",
    "section": "Visualizing loss",
    "text": "Visualizing loss\nIf we consider the case where our inputs are 1-dimensional, as in the weight example above, then our parameter vector \\(\\mathbf{w}\\) only has 2 entries: \\(w_1\\) and \\(b\\). In this case, we can actually plot our loss function directly!\n\n\n\n\n\n\n\n\n\n\nWe see that point where the loss is lowest, corresponds to the line that best fits our data!"
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#gradient-descent",
    "href": "lecture2-linear-regression/notes.html#gradient-descent",
    "title": "Lecture 2: Linear regression",
    "section": "Gradient descent",
    "text": "Gradient descent\nNow that we have a way to determine the quality of a choice of parameters \\(\\mathbf{w}\\), using our loss function, we need a way to actually find the \\(\\mathbf{w}^*\\) that minimizes our loss. To do this we will turn to an algorithm called gradient descent. In this lecture we will introduce gradient descent, but we will go into much more depth in a future lecture.\nWe’ll introduce gradient descent as a method to find the minimum of a generic function. We have some function \\(f(\\mathbf{\\cdot})\\) and we would like find the input \\(\\mathbf{w}^*\\) that minimizes the output of the function:\n\\[\n\\text{Find: } \\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmin}}\\ f(\\mathbf{w})\n\\]\nWe don’t know how to find \\(\\mathbf{w}^*\\) directly, but if we have an initial guess \\(\\mathbf{w}^{(0)}\\), we can try to update our guess to improve it.\n\\[\n\\mathbf{w}^{(1)} \\leftarrow \\mathbf{w}^{(0)} + \\mathbf{g}\n\\]\n\n\nAutograd ArrayBox with value [[-5.0059363  -4.47709947 -2.89058898 -4.09935888 -3.04168522 -2.36175215\n  -2.52795801 -2.13510779]\n [-4.33221987 -3.80338304 -2.21687255 -3.42564245 -2.36796879 -1.68803572\n  -1.85424158 -1.46139136]\n [-4.04994516 -3.52110833 -1.93459784 -3.14336774 -2.08569408 -1.40576101\n  -1.57196687 -1.17911665]\n [-3.58364606 -3.05480923 -1.46829873 -2.67706863 -1.61939497 -0.9394619\n  -1.10566776 -0.71281755]\n [-3.13138271 -2.60254588 -1.01603539 -2.22480529 -1.16713163 -0.48719856\n  -0.65340442 -0.2605542 ]\n [-2.54188083 -2.013044   -0.42653351 -1.63530341 -0.57762975  0.10230332\n  -0.06390254  0.32894768]\n [-1.87596204 -1.34712521  0.23938528 -0.96938462  0.08828904  0.76822211\n   0.60201625  0.99486647]\n [-0.98703064 -0.45819381  1.12831668 -0.08045321  0.97722045  1.65715351\n   1.49094765  1.88379787]]\n\n\n\n\n\nHere we are changing \\(\\mathbf{w}^{(0)}\\) by moving in the direction of \\(\\mathbf{g}\\). If we recall that the gradient of a function at point \\(\\mathbf{x}\\) corresponds to the slope of \\(f\\) at \\(\\mathbf{w}\\), or equivalently the direction of maximum change. This gives us a natural choice for the update to our guess.\n\\[\n\\mathbf{w}^{(1)} \\leftarrow \\mathbf{w}^{(0)} - \\nabla f(\\mathbf{w}^{(0)})\n\\]\nNote that because the gradient corresponds to the direction that maximally increases \\(f(\\mathbf{w})\\), we actually need to subtract the gradient in order to minimize our function. We can repeat this process many times, continuously updating our estimate.\n\\[\n\\text{For }i \\text{ in 1,...,T}\\text{ :} \\\\\n\\quad \\mathbf{w}^{(i+1)} \\leftarrow \\mathbf{w}^{(i)} - \\nabla f(\\mathbf{w}^{(i)})\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#gradient-descent-convergence",
    "href": "lecture2-linear-regression/notes.html#gradient-descent-convergence",
    "title": "Lecture 2: Linear regression",
    "section": "Gradient descent convergence",
    "text": "Gradient descent convergence\nRecall that it’s minimum value \\(\\mathbf{w}^*\\), a function \\(f\\) must have a gradient of \\(\\mathbf{0}\\).\n\\[\n\\nabla f(\\mathbf{w}^*) = \\mathbf{0}\n\\]\nIt follows that:\n\\[\n\\mathbf{w}^{*} = \\mathbf{w}^{*} - \\nabla f(\\mathbf{w}^{*})\n\\]\nThis means that if our gradient descent reaches the minimum, it will stop updating the guess and we know that we can stop our iteration. So we could write our algorithm to account for this:\n\\[\n\\text{While } \\nabla f(\\mathbf{w}^{(i)}) \\neq \\mathbf{0} \\text{ :} \\\\\n\\quad \\mathbf{w}^{(i+1)} \\leftarrow \\mathbf{w}^{(i)} - \\nabla f(\\mathbf{w}^{(i)})\n\\]\nIn practice though, it could take infinitely many updates to find the exact minimum. A more common approach is to define a convergence criteria that stops the iteration when the gradient magnitude is sufficiently small:\n\\[\n\\text{While } ||\\nabla f(\\mathbf{w}^{(i)})||_2 &gt; \\epsilon \\text{ :} \\\\\n\\quad \\mathbf{w}^{(i+1)} \\leftarrow \\mathbf{w}^{(i)} - \\nabla f(\\mathbf{w}^{(i)})\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#step-sizes",
    "href": "lecture2-linear-regression/notes.html#step-sizes",
    "title": "Lecture 2: Linear regression",
    "section": "Step sizes",
    "text": "Step sizes\nNotice that the gradient descent algorithm we’ve defined so far not only says that we want to update our guess in the direction of the gradient, it also say that we want to move in that direction a distance equal to the magnitude of the gradient. It turns out this is often a very bad idea!\nThis approach says that when the magnitude of the gradient is large, we should take a large step, and vice-versa. This is desirable for many functions as it means when we’re far from the minimum we take large steps, moving toward the minimum more quickly. While when we’re close to the minimum we take small steps to refine our guess more precisely.\n\n\n\n\n\nHowever, if we take too large a step, we can overshoot the minimum entirely! In the worst case, this can lead to divergence, where gradient descent overshoots the minimum more and more at each step.\n\n\n\n\n\nRemember that the gradient is making a linear approximation to the function. A strait line has no minimum, so the gradient has no information about where along the approximation the true minimum will be.\n\n\n\n\n\nAlso remember that the gradient gives us the direction of maximum change of the function, but this is only true in the limit of a very small step.\n\\[\n\\frac{df}{d\\mathbf{w}}= \\underset{\\gamma \\rightarrow 0}{\\lim}\\ \\underset{\\|\\mathbf{\\epsilon}\\|_2 &lt; \\gamma}{\\max} \\frac{f(\\mathbf{w} + \\mathbf{\\epsilon}) - f(\\mathbf{w})}{\\|\\mathbf{\\epsilon}\\|_2}\n\\]\nSo in higher dimensions, the gradient may not point directly to the minimum.\nAll of these issues motivate the need to control the size of our updates. We will typically do this by introducing an additional control to our algorithm: a step size or learning rate. This is a small constant \\(\\alpha\\), that we will multiply the gradient by in each of our updates.\n\\[\n\\mathbf{w}^{(i+1)} \\leftarrow \\mathbf{w}^{(i)} - \\alpha \\nabla f(\\mathbf{w}^{(i)})\n\\]\nUsing a small learning rate \\((\\alpha &lt;&lt; 1)\\) will make gradient descent slower, but much more reliable. Later on in the semester we will explore how to choose \\(\\alpha\\) (and even update it during optimization)."
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#optimizing-linear-regression",
    "href": "lecture2-linear-regression/notes.html#optimizing-linear-regression",
    "title": "Lecture 2: Linear regression",
    "section": "Optimizing linear regression",
    "text": "Optimizing linear regression\nWe can apply gradient descent to linear regression in order to find the parameters that minimize the mean squared error loss. To do this we need to find the gradient of the mean squared error with respect to the parameters:\n\\[\n\\nabla_{\\mathbf{w}} \\textbf{MSE}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) =\n\\frac{d}{d\\mathbf{w}}\\bigg( \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2 \\bigg)\n\\]\n\\[\n= \\frac{2}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)\\mathbf{x}_i\n\\]\nWith this gradient our gradient descent update becomes:\n\\[\n\\mathbf{w}^{(i+1)} \\longleftarrow \\mathbf{w}^{(i)} - \\alpha\\bigg(\\frac{2 }{N}\\bigg)\\sum_{i=1}^N  (\\mathbf{x}_i^T\\mathbf{w}^{(i)} - y_i)\\mathbf{x}_i\n\\]\nWe can see that this update is a sum of all the inputs weighted by their corresponding residual given the current value of the parameters.\nWe can see how the the parameters of our regression model change as we run gradient descent."
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#optimizing-linear-regression-directly",
    "href": "lecture2-linear-regression/notes.html#optimizing-linear-regression-directly",
    "title": "Lecture 2: Linear regression",
    "section": "Optimizing linear regression directly",
    "text": "Optimizing linear regression directly\nGradient descent works well for finding the optimal parameters for a linear regression model, but in fact we can actually find the optimal set of parameters directly, without needing to run an iterative algorithm.\nWe know that at the minimum, the gradient must be \\(\\mathbf{0}\\), so the following condition must hold:\n\\[\n\\mathbf{0} = \\bigg( \\frac{2}{N}\\bigg)\\sum_{i=1}^N  (\\mathbf{x}_i^T\\mathbf{w} - y_i)\\mathbf{x}_i\n\\]\nWe can solve for a \\(\\mathbf{w}\\) that satisfied this condition by first dropping the constant \\(\\frac{2}{N}\\).\n\\[\n\\mathbf{0} = \\sum_{i=1}^N  (\\mathbf{x}_i^T\\mathbf{w} - y_i)\\mathbf{x}_i\n\\]\n\\[\n\\mathbf{0} = \\sum_{i=1}^N \\big( \\mathbf{x}_i\\mathbf{x}_i^T\\mathbf{w} - y_i \\mathbf{x}_i \\big)\n\\]\n\\[\n\\sum_{i=1}^N  y_i \\mathbf{x}_i  =\\bigg(\\sum_{i=1}^N  \\mathbf{x}_i\\mathbf{x}_i^T \\bigg)  \\mathbf{w}\n\\]\nNote that \\(\\mathbf{x}_i \\mathbf{x}_i^T\\) is a vector outer product:\n\\[\n\\mathbf{x}_i \\mathbf{x}_i^T = \\begin{bmatrix} x_{i1} \\\\ x_{i2} \\\\ \\vdots \\\\  x_{in}\\end{bmatrix} \\begin{bmatrix} x_{i1} & x_{i2} & \\dots &  x_{in}\\end{bmatrix} =\n\\begin{bmatrix} x_{i1} x_{i1} & x_{i1} x_{i2} & \\dots & x_{i1} x_{in} \\\\\nx_{i2} x_{i1} & x_{i2} x_{i2} & \\dots & x_{i2} x_{in} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{in} x_{i1} & x_{in} x_{i2} & \\dots & x_{in} x_{in} \\\\\n\\end{bmatrix}\n\\]\nThus \\(\\bigg(\\sum_{i=1}^N \\mathbf{x}_i\\mathbf{x}_i^T \\bigg)\\) is a matrix. Multiplying both sides by the inverse \\(\\bigg(\\sum_{i=1}^N \\mathbf{x}_i\\mathbf{x}_i^T \\bigg)^{-1}\\) we get:\n\\[\n\\bigg(\\sum_{i=1}^N  \\mathbf{x}_i\\mathbf{x}_i^T \\bigg)^{-1} \\bigg(\\sum_{i=1}^N  y_i \\mathbf{x}_i\\bigg)  =  \\mathbf{w}^*\n\\]\nWe can write this more compactly using the stacked input matrix and label vector notation we saw in homework 1.\n\\[\n\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_{1} \\\\ \\mathbf{x}_{2} \\\\ \\vdots \\\\  \\mathbf{x}_{N} \\end{bmatrix},\\quad \\mathbf{y} = \\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\  y_{N} \\end{bmatrix}\n\\]\nIn this case, the expression becomes:\n\\[\\mathbf{w}^* = \\big( \\mathbf{X}^T \\mathbf{X} \\big)^{-1} \\big(\\mathbf{y}\\mathbf{X}\\big)\\]"
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#normal-distributions",
    "href": "lecture2-linear-regression/notes.html#normal-distributions",
    "title": "Lecture 2: Linear regression",
    "section": "Normal distributions",
    "text": "Normal distributions\nThe Normal distribution (also known as the Gaussian distribution) is a continuous probability distribution with the following probability density function:\n\\[\np(y) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\text{exp}\\bigg(-\\frac{1}{2\\sigma^2} (y -\\mu)^2\\bigg)\n\\]\nThe normal distribution shows up almost everywhere in probability and statistics. Most notably, the central limit theorem tells us that the mean of many independent and identically distributed random outcomes tends towards a normal distribution."
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#linear-regression-as-a-probabilistic-model",
    "href": "lecture2-linear-regression/notes.html#linear-regression-as-a-probabilistic-model",
    "title": "Lecture 2: Linear regression",
    "section": "Linear regression as a probabilistic model",
    "text": "Linear regression as a probabilistic model\nOur function approximation view of linear regression says that we can approximate an unknown function with a linear function. An alternate approach is to define a distribution over the output \\(y_i\\) of our unknown function given an input \\(\\mathbf{x}_i\\). In particular, the probabilistic model for linear regression will make the assumption that the output is normally distributed conditioned on the input:\n\\[\ny_i \\sim \\mathcal{N}\\big(\\mathbf{x}_i^T \\mathbf{w},\\ \\sigma^2\\big)\n\\]\nHere we see the assumption we’re making is that the mean of the distribution is a linear function of the input, while the variance is fixed. Under this model, we can write the conditional probability or likelihood of an output as:\n\\[\np(y_i\\mid\\mathbf{x}_i, \\mathbf{w}) =  \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\text{exp}\\bigg(-\\frac{1}{2\\sigma^2} (y_i - \\mathbf{x}_i^T\\mathbf{w})^2\\bigg)\n\\]\nWhy view linear regression as a probabilistic model? Well, generally for real data we can’t know if there actually is a function that perfectly maps inputs to outputs. It could be that there are variables we’re not accounting for, that there errors in our measurements for the data we collected or simply that there is some inherent randomness in the outputs. This view of linear regression makes the uncertainty in our predictions explicit."
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#maximum-likelihood-estimation-1",
    "href": "lecture2-linear-regression/notes.html#maximum-likelihood-estimation-1",
    "title": "Lecture 2: Linear regression",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nWith this view of linear regression in mind, let’s ask again how we find the optimal value for \\(\\mathbf{w}\\). Possibly the most widely used approach to this is to simply choose the \\(\\mathbf{w}\\) that maximizes the likelihood (conditional probability) of all of the outputs in our dataset:\n\\[\n\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmax}} \\ p(\\mathbf{y} \\mid \\mathbf{X}, \\mathbf{w}) =\\underset{\\mathbf{w}}{\\text{argmax}} \\ p(y_1,...,y_N \\mid \\mathbf{x}_1, ...,\\mathbf{x}_N, \\mathbf{w}) \\]\nGenerally our model also assumes conditional independence across observations so:\n\\[\np(y_1,...,y_N \\mid \\mathbf{x}_1, ...,\\mathbf{x}_N, \\mathbf{w}) = \\prod_{i=1}^N p(y_i\\mid \\mathbf{x}_i, \\mathbf{w})\n\\]\nFor convenience, it is typical to frame the optimal value in terms of the negative log-likelihood rather than the likelihood, but the two are equivalent.\n\\[\n\\underset{\\mathbf{w}}{\\text{argmax}} \\prod_{i=1}^N p(y_i\\mid \\mathbf{x}_i, \\mathbf{w}) = \\underset{\\mathbf{w}}{\\text{argmin}} - \\sum_{i=1}^N \\log p(y_i \\mid \\mathbf{x}_i, \\mathbf{w}) = \\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})\n\\]\nWe see that the negative log-likelihood is a natural loss function to optimize to find \\(\\mathbf{w}^*\\).\n\\[\n\\textbf{Loss}(\\mathbf{w}) =\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})=- \\sum_{i=1}^N \\log p(y_i \\mid \\mathbf{x}_i, \\mathbf{w})\n\\]\nWe can write out the negative log-likelihood explicitly using the normal PDF:\n\\[\n\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = -\\sum_{i=1}^N\\log\\bigg[\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\text{exp}\\bigg(-\\frac{1}{2\\sigma^2} (y_i - \\mathbf{x}_i^T\\mathbf{w})^2\\bigg)\\bigg]\n\\]\n\\[\n= \\frac{1}{2\\sigma^2} \\sum_{i=1}^N(y_i - \\mathbf{x}_i^T\\mathbf{w})^2 + N \\log \\sigma \\sqrt{2 \\pi}\n\\]\nWe see that this loss is very similar to the MSE loss. Taking the gradient this becomes even more clear.\n\\[\n\\nabla_{\\mathbf{w}}\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) =\n\\frac{d}{d\\mathbf{w}}\\bigg( \\frac{1}{2\\sigma^2} \\sum_{i=1}^N(y_i - \\mathbf{x}_i^T\\mathbf{w})^2 + N \\log \\sigma \\sqrt{2 \\pi} \\bigg)\n\\]\n\\[\n= \\frac{1}{2\\sigma^2}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)\\mathbf{x}_i\n\\]\nAs we saw in the MSE case, the optimal value \\(\\mathbf{w}^*\\) does not depend on the constant value outside the summation. This means that the optimal value for \\(\\mathbf{w}\\) is the same for both MSE and negative log-likelihood and the optimal value does not depend on \\(\\sigma^2\\)!\n\\[\n\\underset{\\mathbf{w}}{\\text{argmin}}\\  MSE(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = \\underset{\\mathbf{w}}{\\text{argmin}}\\ \\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html",
    "href": "lecture2-linear-regression/slides.html",
    "title": "Lecture 2: Linear regression",
    "section": "",
    "text": "Manim Community v0.17.3\n\n\n\n\n\n\n\nHomework 1 due next Tuesday 9/5 at 11:59pm\nFill out Github username survey!\nOffice hours tomorrow 4-5:30pm in MacGregor 322\nCourse calendar added to website with lecture notes.\nLecture notes are open source!"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#logistics-1",
    "href": "lecture2-linear-regression/slides.html#logistics-1",
    "title": "Lecture 2: Linear regression",
    "section": "Logistics",
    "text": "Logistics\n\nHomework 1 due next Tuesday 9/5 at 11:59pm\nFill out Github username survey!\nOffice hours tomorrow 4-5:30pm in MacGregor 322\nCourse calendar added to website with lecture notes.\nLecture notes are open source!"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#functions-revisited",
    "href": "lecture2-linear-regression/slides.html#functions-revisited",
    "title": "Lecture 2: Linear regression",
    "section": "Functions revisited",
    "text": "Functions revisited\n\\[\ny=f(\\mathbf{x}), \\quad \\text{Input: } \\mathbf{x} \\in\\mathbb{R}^n \\longrightarrow \\text{ Output: }y \\in\\mathbb{R}\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#linear-functions",
    "href": "lecture2-linear-regression/slides.html#linear-functions",
    "title": "Lecture 2: Linear regression",
    "section": "Linear Functions",
    "text": "Linear Functions\nA linear function is any function \\(f\\) where the following conditions always hold: \\[ f(\\mathbf{x} + \\mathbf{y}) =f(\\mathbf{x}) + f(\\mathbf{y})\\] and \\[ f(a\\mathbf{x}) = a f(\\mathbf{x})\\] For a linear function, the output can be defined as a weighted sum of the inputs.\n\\[\nf(\\mathbf{x}) = \\sum_{i=1}^n x_iw_i + b\n\\]\nHere, \\(w_i\\) and \\(b\\) are the parameters of the function."
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#linear-functions-1",
    "href": "lecture2-linear-regression/slides.html#linear-functions-1",
    "title": "Lecture 2: Linear regression",
    "section": "Linear Functions",
    "text": "Linear Functions\nWe can also write a linear function using a dot-product between our input \\(\\mathbf{x}\\) and parameter vector \\(\\mathbf{w}\\) as:\n\\[\nf(\\mathbf{x}) = \\mathbf{x} \\cdot \\mathbf{w} + b \\quad \\text{or} \\quad f(\\mathbf{x}) = \\mathbf{x}^T  \\mathbf{w} + b\n\\]\nWe typically refer to \\(\\mathbf{w}\\) specifically as the weight vector (or weights) and \\(b\\) as the bias.\n\\[\n\\textbf{Linear function:  }f(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{w}+b,\\quad \\textbf{Parameters:}\\quad \\big(\\text{Weights:  } \\mathbf{w},\\ \\text{Bias:  } b \\big)\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#linear-functions-2",
    "href": "lecture2-linear-regression/slides.html#linear-functions-2",
    "title": "Lecture 2: Linear regression",
    "section": "Linear Functions",
    "text": "Linear Functions\nIn one dimension, a linear function is always a line, for example:"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#linear-functions-3",
    "href": "lecture2-linear-regression/slides.html#linear-functions-3",
    "title": "Lecture 2: Linear regression",
    "section": "Linear Functions",
    "text": "Linear Functions\nIn higher dimensions, it is a plane or hyperplane:"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#linear-functions-4",
    "href": "lecture2-linear-regression/slides.html#linear-functions-4",
    "title": "Lecture 2: Linear regression",
    "section": "Linear Functions",
    "text": "Linear Functions\nIn numpy we can easily write a linear function of this form:\n\ndef f(x):\n    w = np.array([-0.6, -0.2])\n    b = -1\n    return np.dot(x, w) + b"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#handling-bias-compactly",
    "href": "lecture2-linear-regression/slides.html#handling-bias-compactly",
    "title": "Lecture 2: Linear regression",
    "section": "Handling bias compactly",
    "text": "Handling bias compactly\n\\[\n\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\longrightarrow \\mathbf{x}_{aug}= \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\\\ 1 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{bmatrix} \\longrightarrow \\mathbf{w}_{aug}=  \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\\\ b \\end{bmatrix}\n\\]\nWe can easily see then that using this notation:\n\\[\nf(\\mathbf{x}) = \\mathbf{x}^T \\mathbf{w} +b = \\mathbf{x}_{aug}^T \\mathbf{w}_{aug}\n\\]\nWe won’t bother with the \\(aug\\) notation and just assume that any linear function defined as \\(f(\\mathbf{x})=\\mathbf{x}^T\\mathbf{w}\\) can be defined to include a bias implicitly.\nIn numpy this is similarly straightforward:\n\ndef f(x):\n    w = np.array([-0.6, -0.2, -1])\n    x = np.pad(x, ((0,1),), constant_values=1)\n    return np.dot(x, w)"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#datasets-and-observations",
    "href": "lecture2-linear-regression/slides.html#datasets-and-observations",
    "title": "Lecture 2: Linear regression",
    "section": "Datasets and observations",
    "text": "Datasets and observations\nDataset \\(\\mathbf{D}\\) made up of \\(N\\) pairs of inputs ( \\(\\mathbf{x}\\) ) and outputs ( \\(y\\) ):\n\\[\n\\mathbf{D} = \\{ (\\mathbf{x}_1, y_1),\\ (\\mathbf{x}_2, y_2),\\ ...\\ (\\mathbf{x}_N, y_N)\\}\n\\]\nWe call each of these pairs an observation."
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#fuel-efficiency",
    "href": "lecture2-linear-regression/slides.html#fuel-efficiency",
    "title": "Lecture 2: Linear regression",
    "section": "Fuel efficiency",
    "text": "Fuel efficiency\nLet’s imagine we’re designing a car and we would like to know what the fuel efficiency of the car we’re designing will be in miles per gallon (MPG). Ideally we would have access to a function that would give us the MPG rating if we provide some features.\n\\[\n\\text{mpg} = f(\\text{weight},\\ \\text{horsepower}...)\n\\]\nWe don’t know the exact relationship between a car’s features and fuel efficiency. However, we can look at other cars on the market and see what the corresponding inputs and outputs would be:\n\\[\n\\text{Honda Accord: } \\begin{bmatrix} \\text{Weight:} & \\text{2500 lbs} \\\\ \\text{Horsepower:} & \\text{ 123 HP} \\\\ \\text{Displacement:} & \\text{ 2.4 L} \\\\ \\text{0-60mph:} & \\text{ 7.8 Sec} \\end{bmatrix} \\longrightarrow \\text{   MPG: 33mpg}\n\\]\n\\[\n\\text{Dodge Aspen: } \\begin{bmatrix} \\text{Weight:} & \\text{3800 lbs} \\\\ \\text{Horsepower:} & \\text{ 155 HP} \\\\ \\text{Displacement:} & \\text{ 3.2 L} \\\\ \\text{0-60mph:} & \\text{ 6.8 Sec} \\end{bmatrix} \\longrightarrow \\text{   MPG: 21mpg}\n\\]\n\\[\n\\vdots \\quad \\vdots\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#fuel-efficiency-1",
    "href": "lecture2-linear-regression/slides.html#fuel-efficiency-1",
    "title": "Lecture 2: Linear regression",
    "section": "Fuel efficiency",
    "text": "Fuel efficiency\nOur dataset will be this collection of data that we have for all other cars. In general, each observation in this dataset will correspond to a car.\n\\[\n\\text{Dataset: } \\mathbf{D}=\\{(\\mathbf{x}_i,\\ y_i) \\text{  for  } i\\in 1...N\\}\n\\]\n\\[\n\\text{Input: } \\mathbf{x}_i= \\begin{bmatrix} \\text{Weight} \\\\ \\text{Horsepower} \\\\ \\text{Displacement} \\\\ \\text{0-60mph}  \\end{bmatrix}, \\quad \\text{Output: } y_i = MPG\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#fuel-efficiency-2",
    "href": "lecture2-linear-regression/slides.html#fuel-efficiency-2",
    "title": "Lecture 2: Linear regression",
    "section": "Fuel efficiency",
    "text": "Fuel efficiency\nLet’s take a look at a single feature: the weight of a car."
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#prediction-functions",
    "href": "lecture2-linear-regression/slides.html#prediction-functions",
    "title": "Lecture 2: Linear regression",
    "section": "Prediction functions",
    "text": "Prediction functions\nHow do we predict the output for an input that we haven’t seen before?"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#prediction-functions-1",
    "href": "lecture2-linear-regression/slides.html#prediction-functions-1",
    "title": "Lecture 2: Linear regression",
    "section": "Prediction functions",
    "text": "Prediction functions\nModel our unknown function with a known function that we can evaluate at any input. Chose a function \\(f\\) such that for any observation our dataset, the output of this function approximates the true target output that we observed.\n\\[\nf(\\mathbf{x}_i) \\approx y_i, \\quad \\forall (\\mathbf{x}_i, y_i) \\in \\mathbf{D}\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#linear-interpolation",
    "href": "lecture2-linear-regression/slides.html#linear-interpolation",
    "title": "Lecture 2: Linear regression",
    "section": "Linear interpolation",
    "text": "Linear interpolation\n\nIn some cases this can be a reasonable approach! In fact it’s how the plt.plot function works."
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#linear-interpolation-1",
    "href": "lecture2-linear-regression/slides.html#linear-interpolation-1",
    "title": "Lecture 2: Linear regression",
    "section": "Linear interpolation",
    "text": "Linear interpolation\nReal data however is messy. Measurements in our dataset might not be 100% accurate or might even conflict!\n\\[(\\text{Weight: }3100, \\text{MPG: } 34), \\quad (\\text{Weight: }3100, \\text{MPG: } 23) \\longrightarrow f(3100) = ?\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#linear-interpolation-2",
    "href": "lecture2-linear-regression/slides.html#linear-interpolation-2",
    "title": "Lecture 2: Linear regression",
    "section": "Linear interpolation",
    "text": "Linear interpolation"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#linear-regression-1",
    "href": "lecture2-linear-regression/slides.html#linear-regression-1",
    "title": "Lecture 2: Linear regression",
    "section": "Linear regression",
    "text": "Linear regression\nLinear regression models an unknown function with a linear function.\n\\[\nf(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{w} = \\sum_{i=1}^n x_i w_i\n\\]\nMeaning that the output will be a weighted sum of the features of the input.\n\\[\n\\text{Predicted MPG} = f(\\mathbf{x})=\n\\]\n\\[\n(\\text{weight})w_1 + (\\text{horsepower})w_2 + (\\text{displacement})w_3 + (\\text{0-60mph})w_4 + b\n\\]\nOr in matrix notation:\n\\[\nf(\\mathbf{x})= \\begin{bmatrix} \\text{Weight} \\\\ \\text{Horsepower} \\\\ \\text{Displacement} \\\\ \\text{0-60mph} \\\\ 1 \\end{bmatrix} \\cdot \\begin{bmatrix} w_1 \\\\ w_2\\\\ w_3 \\\\ w_4\\\\ b\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#linear-regression-2",
    "href": "lecture2-linear-regression/slides.html#linear-regression-2",
    "title": "Lecture 2: Linear regression",
    "section": "Linear regression",
    "text": "Linear regression\n\nclass Regression:\n    def __init__(self, weights):\n        self.weights = weights\n    \n    def predict(self, x):\n        return np.dot(x, self.weights)\n\nmodel = Regression(np.array([1, 1, 1, 1, 1]))\nmodel.predict(np.array([5, 2, 3, 3, 1]))\n\n14"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#linear-regression-3",
    "href": "lecture2-linear-regression/slides.html#linear-regression-3",
    "title": "Lecture 2: Linear regression",
    "section": "Linear regression",
    "text": "Linear regression\nWe could chose many different linear functions to make predictions:"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#residuals-and-error",
    "href": "lecture2-linear-regression/slides.html#residuals-and-error",
    "title": "Lecture 2: Linear regression",
    "section": "Residuals and error",
    "text": "Residuals and error\nThe residual or error of a prediction is the difference between the prediction and the true output:\n\\[\ne_i = y_i - f(\\mathbf{x}_i)\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#mean-squared-error",
    "href": "lecture2-linear-regression/slides.html#mean-squared-error",
    "title": "Lecture 2: Linear regression",
    "section": "Mean squared error",
    "text": "Mean squared error\nWe need a measure of error for the entire dataset. The mean squared error is the average of the residual squared for each observation in our dataset:\n\\[\nMSE = \\frac{1}{N}\\sum_{i=1}^N (f(\\mathbf{x}_i) - y_i)^2 = \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2\n\\]It follows that the best choice of linear function \\(f^*\\) is the one that minimizes the mean squared error for our dataset. \\[\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmin}} \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2 \\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#loss-functions",
    "href": "lecture2-linear-regression/slides.html#loss-functions",
    "title": "Lecture 2: Linear regression",
    "section": "Loss functions",
    "text": "Loss functions\nMean squared error depends on the data inputs \\((\\mathbf{x}_1,…,\\mathbf{x}_N)\\), the data targets \\((y_1,…,y_N)\\) and the parameters \\((\\mathbf{w})\\). So we can express the MSE as a function of all three:\n\\[\nMSE(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2\n\\]\nHere we have used \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) to refer to the entire collection of inputs and outputs from our dataset \\(( \\mathbf{D})\\) respectively, so:\n\\[\n\\mathbf{X} = \\begin{bmatrix}\\mathbf{x}_1 \\\\ \\mathbf{x}_2 \\\\ \\vdots \\\\ \\mathbf{x}_N \\end{bmatrix} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1n} \\\\ x_{21} & x_{22} & \\dots & x_{2n}\\\\ \\vdots & \\vdots  & \\ddots & \\vdots \\\\ x_{N1} & x_{N2} & \\dots & x_{Nn} \\end{bmatrix}, \\quad \\mathbf{y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#loss-functions-1",
    "href": "lecture2-linear-regression/slides.html#loss-functions-1",
    "title": "Lecture 2: Linear regression",
    "section": "Loss functions",
    "text": "Loss functions\nThis is an example of loss function. We can drop the explicit dependence on \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\), looking at the loss as purely a function of our choice of parameters:\n\\[\n\\textbf{Loss}(\\mathbf{w})= MSE(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2\n\\]\nAgain, if our goal is to minimize error, we want to choose the parameters \\(\\mathbf{w}^*\\) that minimize this loss:\n\\[\n\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmin}}\\ \\textbf{Loss}(\\mathbf{w})= \\underset{\\mathbf{w}}{\\text{argmin}} \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#visualizing-loss",
    "href": "lecture2-linear-regression/slides.html#visualizing-loss",
    "title": "Lecture 2: Linear regression",
    "section": "Visualizing loss",
    "text": "Visualizing loss\nIf we consider the case where our inputs are 1-dimensional, as in the weight example above, then our parameter vector \\(\\mathbf{w}\\) only has 2 entries: \\(w_1\\) and \\(b\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe see that point where the loss is lowest, corresponds to the line that best fits our data!"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#gradient-descent",
    "href": "lecture2-linear-regression/slides.html#gradient-descent",
    "title": "Lecture 2: Linear regression",
    "section": "Gradient descent",
    "text": "Gradient descent\nWe have a function \\(f(\\mathbf{\\cdot})\\) and we would like find the input \\(\\mathbf{w}^*\\) that minimizes the output of the function:\n\\[\n\\text{Find: } \\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmin}}\\ f(\\mathbf{w})\n\\]\nWe don’t know how to find \\(\\mathbf{w}^*\\) directly, but if we have an initial guess \\(\\mathbf{w}^{(0)}\\), we can try to update our guess to improve it.\n\\[\n\\mathbf{w}^{(1)} \\leftarrow \\mathbf{w}^{(0)} + \\mathbf{g}\n\\]\n\n\n[ 0.5 -0.6]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#gradient-descent-1",
    "href": "lecture2-linear-regression/slides.html#gradient-descent-1",
    "title": "Lecture 2: Linear regression",
    "section": "Gradient descent",
    "text": "Gradient descent\nThe gradient of a function at point \\(\\mathbf{x}\\) corresponds to the slope of \\(f\\) at \\(\\mathbf{w}\\), or equivalently the direction of maximum change. This gives us a natural choice for the update to our guess.\n\\[\n\\mathbf{w}^{(1)} \\leftarrow \\mathbf{w}^{(0)} - \\nabla f(\\mathbf{w}^{(0)})\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#gradient-descent-2",
    "href": "lecture2-linear-regression/slides.html#gradient-descent-2",
    "title": "Lecture 2: Linear regression",
    "section": "Gradient descent",
    "text": "Gradient descent\nWe can repeat this process many times, continuously updating our estimate.\n\\[\n\\text{For }i \\text{ in 1,...,T}\\text{ :}\n\\quad \\mathbf{w}^{(i+1)} \\leftarrow \\mathbf{w}^{(i)} - \\nabla f(\\mathbf{w}^{(i)})\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#gradient-descent-convergence",
    "href": "lecture2-linear-regression/slides.html#gradient-descent-convergence",
    "title": "Lecture 2: Linear regression",
    "section": "Gradient descent convergence",
    "text": "Gradient descent convergence\nAt it’s minimum value \\(\\mathbf{w}^*\\), a function \\(f\\) must have a gradient of \\(\\mathbf{0}\\).\n\\[\n\\nabla f(\\mathbf{w}^*) = \\mathbf{0}\n\\]\nIt follows that:\n\\[\n\\mathbf{w}^{*} = \\mathbf{w}^{*} - \\nabla f(\\mathbf{w}^{*})\n\\]\nWe could write our algorithm to account for this:\n\\[\n\\text{While } \\nabla f(\\mathbf{w}^{(i)}) \\neq \\mathbf{0} \\text{ :}\n\\quad \\mathbf{w}^{(i+1)} \\leftarrow \\mathbf{w}^{(i)} - \\nabla f(\\mathbf{w}^{(i)})\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#gradient-descent-convergence-1",
    "href": "lecture2-linear-regression/slides.html#gradient-descent-convergence-1",
    "title": "Lecture 2: Linear regression",
    "section": "Gradient descent convergence",
    "text": "Gradient descent convergence\nStops the iteration when the gradient magnitude is sufficiently small:\n\\[\n\\text{While } ||\\nabla f(\\mathbf{w}^{(i)})||_2 &gt; \\epsilon \\text{ :}\n\\quad \\mathbf{w}^{(i+1)} \\leftarrow \\mathbf{w}^{(i)} - \\nabla f(\\mathbf{w}^{(i)})\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#step-sizes",
    "href": "lecture2-linear-regression/slides.html#step-sizes",
    "title": "Lecture 2: Linear regression",
    "section": "Step sizes",
    "text": "Step sizes\nThis approach says that when the magnitude of the gradient is large, we should take a large step, and vice-versa."
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#step-sizes-1",
    "href": "lecture2-linear-regression/slides.html#step-sizes-1",
    "title": "Lecture 2: Linear regression",
    "section": "Step sizes",
    "text": "Step sizes\nHowever, if we take too large a step, we can overshoot the minimum entirely! In the worst case, this can lead to divergence, where gradient descent overshoots the minimum more and more at each step."
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#step-sizes-2",
    "href": "lecture2-linear-regression/slides.html#step-sizes-2",
    "title": "Lecture 2: Linear regression",
    "section": "Step sizes",
    "text": "Step sizes\nThe gradient is making a linear approximation to the function. A strait line has no minimum, so the gradient has no information about where along the approximation the true minimum will be."
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#step-sizes-3",
    "href": "lecture2-linear-regression/slides.html#step-sizes-3",
    "title": "Lecture 2: Linear regression",
    "section": "Step sizes",
    "text": "Step sizes\nThe gradient gives us the direction of maximum change of the function, but this is only true in the limit of a very small step.\n\\[\n\\frac{df}{d\\mathbf{w}}= \\underset{\\gamma \\rightarrow 0}{\\lim}\\ \\underset{\\|\\mathbf{\\epsilon}\\|_2 &lt; \\gamma}{\\max} \\frac{f(\\mathbf{w} + \\mathbf{\\epsilon}) - f(\\mathbf{w})}{\\|\\mathbf{\\epsilon}\\|_2}\n\\]\nIn higher dimensions, the gradient may not point directly to the minimum."
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#step-sizes-4",
    "href": "lecture2-linear-regression/slides.html#step-sizes-4",
    "title": "Lecture 2: Linear regression",
    "section": "Step sizes",
    "text": "Step sizes\nWe can introduce an additional control to our algorithm: a step size or learning rate. This is a small constant \\(\\alpha\\), that we will multiply the gradient by in each of our updates.\n\\[\n\\mathbf{w}^{(i+1)} \\leftarrow \\mathbf{w}^{(i)} - \\alpha \\nabla f(\\mathbf{w}^{(i)})\n\\]\nUsing a small learning rate \\((\\alpha &lt;&lt; 1)\\) will make gradient descent slower, but much more reliable."
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#optimizing-linear-regression",
    "href": "lecture2-linear-regression/slides.html#optimizing-linear-regression",
    "title": "Lecture 2: Linear regression",
    "section": "Optimizing linear regression",
    "text": "Optimizing linear regression\nWe can apply gradient descent to linear regression in order to find the parameters that minimize the mean squared error loss.\n\\[\n\\nabla_{\\mathbf{w}} \\textbf{MSE}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) =\n\\frac{d}{d\\mathbf{w}}\\bigg( \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2 \\bigg)\n\\]\n\\[\n= \\frac{2}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)\\mathbf{x}_i\n\\]\nWith this gradient our gradient descent update becomes:\n\\[\n\\mathbf{w}^{(i+1)} \\longleftarrow \\mathbf{w}^{(i)} - \\alpha\\bigg(\\frac{2 }{N}\\bigg)\\sum_{i=1}^N  (\\mathbf{x}_i^T\\mathbf{w}^{(i)} - y_i)\\mathbf{x}_i\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#optimizing-linear-regression-directly",
    "href": "lecture2-linear-regression/slides.html#optimizing-linear-regression-directly",
    "title": "Lecture 2: Linear regression",
    "section": "Optimizing linear regression directly",
    "text": "Optimizing linear regression directly\nWe know that at the minimum, the gradient must be \\(\\mathbf{0}\\), so the following condition must hold:\n\\[\n\\mathbf{0} = \\bigg( \\frac{2}{N}\\bigg)\\sum_{i=1}^N  (\\mathbf{x}_i^T\\mathbf{w} - y_i)\\mathbf{x}_i\n\\]\nWe can solve for a \\(\\mathbf{w}\\) that satisfied this condition by first dropping the constant \\(\\frac{2}{N}\\).\n\\[\n\\mathbf{0} = \\sum_{i=1}^N  (\\mathbf{x}_i^T\\mathbf{w} - y_i)\\mathbf{x}_i\n\\]\n\\[\n\\mathbf{0} = \\sum_{i=1}^N \\big( \\mathbf{x}_i\\mathbf{x}_i^T\\mathbf{w} - y_i \\mathbf{x}_i \\big)\n\\]\n\\[\n\\sum_{i=1}^N  y_i \\mathbf{x}_i  =\\bigg(\\sum_{i=1}^N  \\mathbf{x}_i\\mathbf{x}_i^T \\bigg)  \\mathbf{w}\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#optimizing-linear-regression-directly-1",
    "href": "lecture2-linear-regression/slides.html#optimizing-linear-regression-directly-1",
    "title": "Lecture 2: Linear regression",
    "section": "Optimizing linear regression directly",
    "text": "Optimizing linear regression directly\nNote that \\(\\mathbf{x}_i \\mathbf{x}_i^T\\) is a vector outer product:\n\\[\n\\mathbf{x}_i \\mathbf{x}_i^T = \\begin{bmatrix} x_{i1} \\\\ x_{i2} \\\\ \\vdots \\\\  x_{in}\\end{bmatrix} \\begin{bmatrix} x_{i1} & x_{i2} & \\dots &  x_{in}\\end{bmatrix} =\n\\begin{bmatrix} x_{i1} x_{i1} & x_{i1} x_{i2} & \\dots & x_{i1} x_{in} \\\\\nx_{i2} x_{i1} & x_{i2} x_{i2} & \\dots & x_{i2} x_{in} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{in} x_{i1} & x_{in} x_{i2} & \\dots & x_{in} x_{in} \\\\\n\\end{bmatrix}\n\\]\nThus \\(\\bigg(\\sum_{i=1}^N \\mathbf{x}_i\\mathbf{x}_i^T \\bigg)\\) is a matrix."
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#optimizing-linear-regression-directly-2",
    "href": "lecture2-linear-regression/slides.html#optimizing-linear-regression-directly-2",
    "title": "Lecture 2: Linear regression",
    "section": "Optimizing linear regression directly",
    "text": "Optimizing linear regression directly\n\\[\n\\sum_{i=1}^N  y_i \\mathbf{x}_i  =\\bigg(\\sum_{i=1}^N  \\mathbf{x}_i\\mathbf{x}_i^T \\bigg)  \\mathbf{w}\n\\]\nMultiplying both sides by the inverse \\(\\bigg(\\sum_{i=1}^N \\mathbf{x}_i\\mathbf{x}_i^T \\bigg)^{-1}\\) we get:\n\\[\n\\bigg(\\sum_{i=1}^N  \\mathbf{x}_i\\mathbf{x}_i^T \\bigg)^{-1} \\bigg(\\sum_{i=1}^N  y_i \\mathbf{x}_i\\bigg)  =  \\mathbf{w}^*\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#optimizing-linear-regression-directly-3",
    "href": "lecture2-linear-regression/slides.html#optimizing-linear-regression-directly-3",
    "title": "Lecture 2: Linear regression",
    "section": "Optimizing linear regression directly",
    "text": "Optimizing linear regression directly\n\\[\n\\bigg(\\sum_{i=1}^N  \\mathbf{x}_i\\mathbf{x}_i^T \\bigg)^{-1} \\bigg(\\sum_{i=1}^N  y_i \\mathbf{x}_i\\bigg)  =  \\mathbf{w}^*\n\\]\nWe can write this more compactly using the stacked input matrix and label vector notation we saw in homework 1.\n\\[\n\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_{1} \\\\ \\mathbf{x}_{2} \\\\ \\vdots \\\\  \\mathbf{x}_{N} \\end{bmatrix},\\quad \\mathbf{y} = \\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\  y_{N} \\end{bmatrix}\n\\]\nIn this case, the expression becomes:\n\\[\\mathbf{w}^* = \\big( \\mathbf{X}^T \\mathbf{X} \\big)^{-1} \\big(\\mathbf{y}\\mathbf{X}\\big)\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#normal-distributions",
    "href": "lecture2-linear-regression/slides.html#normal-distributions",
    "title": "Lecture 2: Linear regression",
    "section": "Normal distributions",
    "text": "Normal distributions\nThe Normal distribution (also known as the Gaussian distribution) is a continuous probability distribution with the following probability density function:\n\\[\np(y) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\mathbf{exp}\\bigg(-\\frac{1}{2\\sigma^2} (y -\\mu)^2\\bigg)\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#linear-regression-as-a-probabilistic-model",
    "href": "lecture2-linear-regression/slides.html#linear-regression-as-a-probabilistic-model",
    "title": "Lecture 2: Linear regression",
    "section": "Linear regression as a probabilistic model",
    "text": "Linear regression as a probabilistic model\nThe probabilistic model for linear regression will make the assumption that the output is normally distributed conditioned on the input:\n\\[\ny_i \\sim N\\big(\\mathbf{x}_i^T \\mathbf{w},\\ \\sigma^2\\big)\n\\]\nWe can write the conditional probability or likelihood of an output as:\n\\[\np(y_i\\mid\\mathbf{x}_i, \\mathbf{w}) =  \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\mathbf{exp}\\bigg(-\\frac{1}{2\\sigma^2} (y_i - \\mathbf{x}_i^T\\mathbf{w})^2\\bigg)\n\\]\n\nImage credit: Lily Chen Towards Data Science"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#maximum-likelihood-estimation-1",
    "href": "lecture2-linear-regression/slides.html#maximum-likelihood-estimation-1",
    "title": "Lecture 2: Linear regression",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nHow do we find the optimal value for \\(\\mathbf{w}\\)? Choose the \\(\\mathbf{w}\\) that maximizes the likelihood (conditional probability) of all of the outputs in our dataset:\n\\[\n\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmax}} \\ p(\\mathbf{y} \\mid \\mathbf{X}, \\mathbf{w}) =\\underset{\\mathbf{w}}{\\text{argmax}} \\ p(y_1,...,y_N \\mid \\mathbf{x}_1, ...,\\mathbf{x}_N, \\mathbf{w}) \\]\nGenerally our model also assumes conditional independence across observations so:\n\\[\np(y_1,...,y_N \\mid \\mathbf{x}_1, ...,\\mathbf{x}_N, \\mathbf{w}) = \\prod_{i=1}^N p(y_i\\mid \\mathbf{x}_i, \\mathbf{w})\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#maximum-likelihood-estimation-2",
    "href": "lecture2-linear-regression/slides.html#maximum-likelihood-estimation-2",
    "title": "Lecture 2: Linear regression",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nEquivalently frame the optimal value in terms of the negative log-likelihood rather than the likelihood.\n\\[\n\\underset{\\mathbf{w}}{\\text{argmax}} \\prod_{i=1}^N p(y_i\\mid \\mathbf{x}_i, \\mathbf{w}) = \\underset{\\mathbf{w}}{\\text{argmin}} - \\sum_{i=1}^N \\log p(y_i \\mid \\mathbf{x}_i, \\mathbf{w}) = \\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})\n\\]\nWe see that the negative log-likelihood is a natural loss function to optimize to find \\(\\mathbf{w}^*\\).\n\\[\n\\textbf{Loss}(\\mathbf{w}) =\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})=- \\sum_{i=1}^N \\log p(y_i \\mid \\mathbf{x}_i, \\mathbf{w})\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#maximum-likelihood-estimation-3",
    "href": "lecture2-linear-regression/slides.html#maximum-likelihood-estimation-3",
    "title": "Lecture 2: Linear regression",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nWe can write out the negative log-likelihood explicitly using the normal PDF:\n\\[\n\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = -\\sum_{i=1}^N\\log\\bigg[\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\text{exp}\\bigg(-\\frac{1}{2\\sigma^2} (y_i - \\mathbf{x}_i^T\\mathbf{w})^2\\bigg)\\bigg]\n\\]\n\\[\n= \\frac{1}{2\\sigma^2} \\sum_{i=1}^N(y_i - \\mathbf{x}_i^T\\mathbf{w})^2 - \\frac{N}{\\sigma \\sqrt{2 \\pi}}\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#maximum-likelihood-estimation-4",
    "href": "lecture2-linear-regression/slides.html#maximum-likelihood-estimation-4",
    "title": "Lecture 2: Linear regression",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\n\\[\n\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^N(y_i - \\mathbf{x}_i^T\\mathbf{w})^2 - \\frac{N}{\\sigma \\sqrt{2 \\pi}}\n\\]\nWe see that this loss is very similar to the MSE loss. Taking the gradient this becomes even more clear.\n\\[\n\\nabla_{\\mathbf{w}}\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) =\n\\frac{d}{d\\mathbf{w}}\\bigg( \\frac{1}{2\\sigma^2} \\sum_{i=1}^N(y_i - \\mathbf{x}_i^T\\mathbf{w})^2 - \\frac{N}{\\sigma \\sqrt{2 \\pi}} \\bigg)\n\\]\n\\[\n= \\frac{1}{2\\sigma^2}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)\\mathbf{x}_i\n\\]\nThe optimal value for \\(\\mathbf{w}\\) is the same for both MSE and negative log-likelihood and the optimal value does not depend on \\(\\sigma^2\\)!\n\\[\n\\underset{\\mathbf{w}}{\\text{argmin}}\\  MSE(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = \\underset{\\mathbf{w}}{\\text{argmin}}\\ \\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})\n\\]"
  },
  {
    "objectID": "lecture1-background/notes.html",
    "href": "lecture1-background/notes.html",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "We will begin with a review of many of the basic concepts from linear algebra that we will need for this course. As we go through these concepts we will see how to implement them in Python.\n\n\nNumpy Is the linear algebra library that we will be using for much of this class (later on we will introduce PyTorch, which is more geared towards neural networks). The standard way to import Numpy is:\n\nimport numpy as np\n\nThe most basic object from Numpy that we will just is the array (np.array), which will represent vectors matrices and even higher-dimensional objects known as tensors.\n\n\n\nScalars are just single numbers. In our math we will typically denote scalars with a lower-case letter. For example, we might call a scalar \\(x\\) and give it a value as \\(x=3.5\\). In code this would be written as:\n\nx = 3.5\n\n# or as a 0-dimensional numpy array\nx = np.array(3.5)\n\nIn general, we will assume that scalars are real numbers meaning decimal numbers in the range \\((-\\infty, \\infty)\\). We can denote this as \\(x \\in \\mathbb{R}\\), meaning “\\(x\\) is in the set of real numbers”.\n\n\n\nVectors are ordered lists of numbers, as shown below. We will typically denote vectors with bold lower case letters, such as \\(\\mathbf{x}\\).\n\\[\n\\mathbf{x} = \\begin{bmatrix} 2\\\\ 5\\\\ 1\\end{bmatrix}\n\\]\nThis bold notation is common, but not universal, so expect that external resources may denote things differently!\nIn Numpy, we can create an array object representing a vector by passing np.array a list of numbers.\n\nx = np.array([2, 5, 1])\n\n\n\n\n\n\n\nOther useful vector creation functions\n\n\n\n\n\nConstants:\n\na = np.zeros(5) # Create a size 5 vector filled with zeros\nprint(a)\na = np.ones(5) # Create a size 5 vector filled with ones\nprint(a)\n\n[0. 0. 0. 0. 0.]\n[1. 1. 1. 1. 1.]\n\n\nInteger ranges:\n\na = np.array([1,2,3,4,5,6,7])\nprint(a)\n# or equivalently, using Python iterables:\na = np.array( range(1,8) )\nprint(a)\n# or the NumPy function np.arange:\na = np.arange(1, 8)\nprint(a)\n\n[1 2 3 4 5 6 7]\n[1 2 3 4 5 6 7]\n[1 2 3 4 5 6 7]\n\n\nDecimal ranges:\n\nb = np.linspace(1.0, 7.0, 4) # length-4 vector interpolating between 1.0 and 7.0\nprint(b)\nc = np.logspace(0.0, 2.0, 5) # length-7 vector interpolating between 10^0 and 10^2 logarithmically\nprint(c)\n\n[1. 3. 5. 7.]\n[  1.           3.16227766  10.          31.6227766  100.        ]\n\n\nCombining vectors\n\na = np.concatenate([b, c]) # length 4 + 7 vector with the elements of both b and c\n\n\n\n\nThe individual numbers in the vector are called the entries and we will refer to them individually as subscripted scalars: \\(x_1, x_2,…,x_n\\), where \\(n\\) is the number of entries or size of the vector.\n\\[\n\\mathbf{x} = \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\end{bmatrix}\n\\]\nWe may say that \\(\\mathbf{x} \\in \\mathbb{R}^n\\) to mean that \\(\\mathbf{x}\\) is in the set of vectors with \\(n\\) real-valued entries. In numpy we can access individual elements of a vector using [] operators (note that numpy is 0-indexed!).\n\nx[0]\n\n2\n\n\nA vector represents either a location or a change in location in \\(n\\) -dimensional space. For instance, the \\(\\mathbf{x}\\) vector we defined could represent the point with coordinates \\((2, 5, 1)\\) in 3-D space or a movement of 2 along the first axis, 5 along the second axis and 1 along the third axis.\n\n\n\n\n\n\n\n\nVectors do not necessarily need to represent geometric points or directions. As they are simply ordered collections of numbers, we can a vector to represent any type of data in a more formal way.\nFor example, a space of vectors could represent students, with each entry corresponding to a student’s grade in a different subject:\n\\[\n\\text{Student } \\mathbf{x} = \\begin{bmatrix} \\text{Math} \\\\ \\text{CS} \\\\ \\text{Literature} \\\\\\text{History} \\end{bmatrix}\n\\]\nAny two students might have different grades across the subjects. We can represent these two students (say \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\)) as two vectors.\n\\[\n\\mathbf{x}_1 = \\begin{bmatrix} 3.0 \\\\ 4.0 \\\\ 3.7 \\\\ 2.3 \\end{bmatrix}, \\quad \\mathbf{x}_2 = \\begin{bmatrix} 3.7 \\\\ 3.3 \\\\ 3.3 \\\\ 4.0 \\end{bmatrix}\n\\]\n\n\n\nWe say that two vectors are equal if and only if all of the corresponding elements are equal, so \\(\\mathbf{x} = \\mathbf{y}\\) implies that \\(x_1=y_1, x_2=y_2…\\). In numpy when we compare two vectors for equality, we get a new vector that indicates which entries are equal.\n\nx = np.array([2, 5, 1])\ny = np.array([3, 5, 2])\nx == y\n\narray([False,  True, False])\n\n\nWe can check if all entries are equal (and therefore the vectors are equal) using the np.all function.\n\nnp.all(x == y), np.all(x == x)\n\n(False, True)\n\n\nOther comparison operators (&gt;,&lt;, &gt;=, &lt;=, !=) also perform element-wise comparison in numpy.\n\n\n\nWhen we add or subtract vectors, we add or subtract the corresponding elements.\n\\[\\mathbf{x} + \\mathbf{y} = \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\end{bmatrix} + \\begin{bmatrix} y_1\\\\ y_2\\\\ y_3\\end{bmatrix} = \\begin{bmatrix} x_1 + y_1\\\\ x_2 + y_2\\\\ x_3 + y_3\\end{bmatrix}\\] This works the same in numpy\n\nx + y\n\narray([ 5, 10,  3])\n\n\nThis corresponds to shifting the point \\(\\mathbf{x}\\) by the vector \\(\\mathbf{y}\\).\n\n\n\n\n\n\n\n\nIn both mathematical notation and numpy, most operations on vectors will be assumed to be taken element-wise. That is,\n\\[\n\\mathbf{x}^2 = \\begin{bmatrix} x_1^2 \\\\ x_2^2 \\\\ x_3^2 \\end{bmatrix} , \\quad \\log(\\mathbf{x}) = \\begin{bmatrix} \\log x_1 \\\\ \\log x_2 \\\\ \\log x_3 \\end{bmatrix},\\ \\text{etc.}\n\\]\nIn numpy:\n\nx ** 2, np.log(x)\n\n(array([ 4, 25,  1]), array([0.69314718, 1.60943791, 0.        ]))\n\n\nNote that in this class (and in numpy!) logarithms are assumed to be base \\(e\\), otherwise known as natural logarithms.\nOperations between scalars and vectors are also assumed to be element-wise as in this scalar-vector multiplication\n\\[\na\\mathbf{x} = \\begin{bmatrix} a x_1 \\\\ a x_2 \\\\ a x_3 \\end{bmatrix}, \\quad a + \\mathbf{x} = \\begin{bmatrix} a + x_1 \\\\ a + x_2 \\\\ a + x_3 \\end{bmatrix}\n\\]\n\n\n\nThe magnitude of a vector its length in \\(\\mathbb{R}^n\\), or equivalently the Euclidean distance from the origin to the point the vector represents. It is denoted as\\(\\|\\mathbf{x}\\|_2\\) and defined as:\n\\[\n\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}\n\\]\nThe subscript \\(2\\) specifies that we are talking about Euclidean distance. Because of this we will also refer to the magnitude as the two-norm.\nIn numpy we can compute this using the np.linalg.norm function.\n\nxnorm = np.linalg.norm(x)\n\nWe can also compute this explicitly with the np.sum function, which computes the sum of the elements of a vector.\n\nxnorm_explicit = np.sqrt(np.sum(x ** 2))\nxnorm, xnorm_explicit\n\n(5.477225575051661, 5.477225575051661)\n\n\n\n\n\n\n\n\nOther useful aggregation functions\n\n\n\n\n\n\nprint(np.mean(x))    # Take the mean of the elements in x\nprint(np.std(x))     # Take the standard deviation of the elements in x\nprint(np.min(x))     # Find the minimum element in x\nprint(np.max(x))     # Find the maximum element in x\n\n2.6666666666666665\n1.699673171197595\n1\n5\n\n\n\n\n\nA unit vector is a vector with length \\(1\\). We can find a unit vector that has the same direction as \\(\\mathbf{x}\\) by dividing \\(\\mathbf{x}\\) by it’s magnitude:\n\\[\n\\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|_2}\n\\]\n\nx / np.linalg.norm(x)\n\narray([0.36514837, 0.91287093, 0.18257419])\n\n\n\n\n\nThe dot-product operation between two vectors is defined as\n\\[\n\\mathbf{x} \\cdot \\mathbf{y} = \\sum_{i=1}^n x_i y_i\n\\]\nMore commonly in this class we will write the dot-product between the vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) as: \\[\\mathbf{x}^T \\mathbf{y} = \\sum_{i=1}^n x_i y_i\\]$\nThe result of a dot product is a scalar whose value is equal to \\(\\|\\mathbf{x}\\|_2\\|\\mathbf{y}\\|_2 \\cos\\theta\\), where \\(\\theta\\) is the angle between them. If\\(\\theta=\\frac{\\pi}{2}\\), the vectors are orthogonal and the dot product with be \\(0\\). If \\(\\theta&gt;\\frac{\\pi}{2}\\) or \\(\\theta &lt; -\\frac{\\pi}{2}\\) , the dot product will be negative.\nIf \\(\\theta=0\\) then \\(\\cos(\\theta)=1\\). In this case we say the vectors are colinear. This formulation implies that given two vectors of fixed length, the dot product is maximized with they are colinear ( \\(\\theta=0\\) ).\nWe can compute dot products in numpy using the np.dot function\n\nnp.dot(x, y)\n\n33\n\n\nGeometrically, \\(\\|\\mathbf{x}\\|_2\\cos\\theta\\) is the length of the projection of \\(\\mathbf{x}\\) onto \\(\\mathbf{y}\\). Thus, we can compute the length of this projection using the dot-product as \\(\\frac{\\mathbf{x}\\cdot\\mathbf{y}}{\\|\\mathbf{y}\\|}\\) .\n\n\n\nA matrix is a 2-dimensional collection of numbers. We will denote matrices using bold capital letters, e.g. \\(\\mathbf{A}\\).\n\\[\n\\mathbf{A} = \\begin{bmatrix} 3 & 5 & 4 \\\\ 1 & 1 & 2  \\end{bmatrix}\n\\]\nIn numpy we can create a matrix by passing np.array as list-of-lists, where each inner list specifies a row of the matrix.\n\nA = np.array([[3, 5, 4], [1, 1, 2]])\nA\n\narray([[3, 5, 4],\n       [1, 1, 2]])\n\n\nAs with vectors we will denote individual elements of a matrix using subscripts. In this case, each element has 2 coordinates. Conventions is to always list the row first.\n\\[\n\\mathbf{A} = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\end{bmatrix}\n\\]\nIn numpy we can access elements in a matrix similarly.\n\nA[1, 2]\n\n2\n\n\nAs with vectors, we say that two matrices are equal if and only if all of the corresponding elements are equal.\n\n\n\n\n\n\nOther matrix creation routines\n\n\n\n\n\nBasic creation functions\n\nA0 = np.zeros((3, 4))          # create a 3x4 matrix of all zeros\nA1 = np.ones((4, 4))           # create a 4x4 matrix of all ones\nAinf = np.full((3, 3), np.inf) # create a 3x3 matrix of all infinities\nI = np.eye(3)                  # create a 3x3 itentity matrix\n\nCreating a matrix by “reshaping” a vector with the same number of elements\n\nV = np.arange(1, 13).reshape((3, 4)) # Create a 3x4 matrix with elements 1-12\n\nCreating matrices by combining matrices\n\nB = np.tile(A0, (3, 2))               # create a matrix by \"tiling\" copies of A0 (3 copies by 2 copies)\nB = np.concatenate([A0, A1], axis=0)  # create a (2n x m) matrix by stacking two matrices vertically\nB = np.concatenate([A0, I], axis=1)  # create a (n x 2m) matrix by stacking two matrices horizontally\n\n\n\n\n\n\n\nOften we will refer to an entire row of a matrix (which is itself a vector) using matrix notation with a single index\n\\[\n\\mathbf{A}_i = \\begin{bmatrix} A_{i1} \\\\ A_{i2} \\\\ \\vdots \\end{bmatrix}\n\\]\nIn numpy we can access a single row similarly.\n\nA[1]\n\narray([1, 1, 2])\n\n\nWe can refer to an entire column by replacing the row index with a \\(*\\), indicating we are referring to all rows in that column.\n\\[\n\\mathbf{A}_{*i} = \\begin{bmatrix} A_{1i} \\\\ A_{2i} \\\\ \\vdots \\end{bmatrix}\n\\]\nIn numpy we can access an entire column (or row) using the slice operator :, which takes all elements along an axis.\n\nA[:, 1] # Take all elements in column 1\n\narray([5, 1])\n\n\nWe can also use the slice operator to take a subset of elements along an axis.\n\nA[:, 1:3] #Take the second and third columns of A\n\narray([[5, 4],\n       [1, 2]])\n\n\n\n\n\n\n\n\nAdvanced slicing in numpy\n\n\n\n\n\n\nA = np.arange(1, 13).reshape((3, 4))\nprint(A)\n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\n\nOpen-ended slices\n\nprint(\"A[:2]=\", A[:2]) # Take rows up to 2 (not inucluding 2)\nprint(\"A[1:]=\", A[1:]) # Take rows starting at (and including) 1\n\nA[:2]= [[1 2 3 4]\n [5 6 7 8]]\nA[1:]= [[ 5  6  7  8]\n [ 9 10 11 12]]\n\n\nTaking rows and colmns\n\nprint(\"A[0, :]=\", A[0, :])   # first row, all columns\nprint(\"A[:, 1]=\", A[:, 1])   # all rows, second column\nprint(\"A[-1, :]\", A[-1, :])  # all columns of the last row\nprint(\"A[:, -2]\", A[:, -2])  # second to last column of every row\n\nA[0, :]= [1 2 3 4]\nA[:, 1]= [ 2  6 10]\nA[-1, :] [ 9 10 11 12]\nA[:, -2] [ 3  7 11]\n\n\nMore general slicing with steps\n\nprint(\"A[1,0:2]=\", A[1,0:2])\nprint(\"A[0,0:4:2]=\", A[0,0:4:2])\nprint(\"A[:,0:4:2]=\\n\", A[:,0:4:2])\n\nA[1,0:2]= [5 6]\nA[0,0:4:2]= [1 3]\nA[:,0:4:2]=\n [[ 1  3]\n [ 5  7]\n [ 9 11]]\n\n\nTaking one row and selected columns\n\nprint(\"A[2, [1,4]]=\",A[2, [0,3]])\n\nA[2, [1,4]]= [ 9 12]\n\n\nTaking all rows and selected columns\n\nprint(\"A[:, [1,4]]=\\n\",A[:, [0,3]])\n\nA[:, [1,4]]=\n [[ 1  4]\n [ 5  8]\n [ 9 12]]\n\n\n\n\n\n\n\n\nThe matrix \\(\\mathbf{A}\\) above has 2 rows and 3 columns, thus we would specify it’s shape as \\(2\\times3\\). A square matrix has the same number of rows and columns.\n\\[\n\\mathbf{A} = \\begin{bmatrix} 3 & 5 & 4 \\\\ 1 & 1 & 2 \\\\ 4 & 1 & 3 \\end{bmatrix}\n\\]\nWe can access the shape of a matrix in numpy using its shape property.\n\nA = np.array([[3, 5, 4], [1, 1, 2], [4, 1, 3]])\nprint(A.shape)\n\n(3, 3)\n\n\n\n\n\nThe transpose of a matrix is an operation that swaps the rows and columns of the matrix. We denote the transpose of a matrix \\(\\mathbf{A}\\) as \\(\\mathbf{A}^T\\).\n\\[\n\\mathbf{A} = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\end{bmatrix}, \\quad \\mathbf{A}^T = \\begin{bmatrix} A_{11} & A_{21} \\\\  A_{12} & A_{22} \\\\  A_{13} & A_{23} \\end{bmatrix}\n\\]\nIn numpy we can transpose a matrix by using the T property.\n\nA = np.array([[1, 1, 1], [2, 2, 2]])\nA.T\n\narray([[1, 2],\n       [1, 2],\n       [1, 2]])\n\n\n\n\n\nAs with vectors, many operations on matrices are performed element-wise:\n\\[\n\\mathbf{A} + \\mathbf{B} = \\begin{bmatrix} A_{11} + B_{11}  & A_{12} + B_{12} & A_{13} + B_{13} \\\\ A_{21} + B_{21} & A_{22} + B_{22} & A_{23} + B_{23} \\end{bmatrix}, \\quad \\log\\mathbf{A} = \\begin{bmatrix} \\log A_{11}  & \\log A_{12} & \\log A_{13} \\\\ \\log A_{21} & \\log A_{22} & \\log A_{23} \\end{bmatrix}\n\\]\nScalar-matrix operation are also element-wise:\n\\[\nc\\mathbf{A} = \\begin{bmatrix} cA_{11}  & cA_{12} & cA_{13} \\\\ cA_{21} & cA_{22} & cA_{23} \\end{bmatrix}\n\\]\nIn numpy:\n\nB = 5 * A \nA + B\n\narray([[ 6,  6,  6],\n       [12, 12, 12]])\n\n\n\n\n\n\n\n\nOther element-wise operations\n\n\n\n\n\nUnary operations\n\nnp.sqrt(A)     # Take the square root of every element\nnp.log(A)      # Take the (natural) log of every element\nnp.exp(A)      # Take e^x for every element x in A\nnp.sin(A)      # Take sin(x) for every element x in A\nnp.cos(A)      # Take cos(x) for every element x in A\n\narray([[ 0.54030231,  0.54030231,  0.54030231],\n       [-0.41614684, -0.41614684, -0.41614684]])\n\n\nScalar operations\n\nA + 2          # Add a scalar to every element of A\nA - 1          # Subtract a scalar from every element of A\nA * 4          # Multiply a scalar with every element of A\nA / 6          # Divide every element by a scalar\nA ** 3         # Take every element to a power\n\narray([[1, 1, 1],\n       [8, 8, 8]])\n\n\n\n\n\n\n\n\nA matrix-vector product is an operation between a matrix and a vector that produces a new vector. Given a matrix \\(\\mathbf{A}\\) and a vector \\(\\mathbf{x}\\), we write the matrix-vector product as:\n\\[\n\\mathbf{A}\\mathbf{x} = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\\\ A_{23} & A_{32} & A_{33} \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\end{bmatrix} = \\begin{bmatrix} \\sum_{i=1}^n x_i A_{1i} \\\\  \\sum_{i=1}^n x_i A_{2i} \\\\  \\sum_{i=1}^n x_i A_{3i} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{x}\\cdot  \\mathbf{A}_{1} \\\\ \\mathbf{x}\\cdot  \\mathbf{A}_{2} \\\\ \\mathbf{x} \\cdot \\mathbf{A}_{2} \\end{bmatrix}\n\\]\nIn other words, each entry of the resulting vector is the dot product between \\(\\mathbf{x}\\) and a row of \\(A\\). In numpy we also use np.dot for matrix-vector products:\n\nA = np.array([[3, 5, 4], [1, 1, 2], [4, 1, 3]])\nx = np.array([2, 5, 1])\n\nnp.dot(A, x)\n\narray([35,  9, 16])\n\n\nWe say that a square matrix \\(A\\) defines a linear mapping from \\(\\mathbb{R}^n\\rightarrow\\mathbb{R}^n\\), which simply means if we multiply any vector \\(\\mathbf{x}\\in\\mathbb{R}^n\\) by \\(A\\) we get a new vector in \\(\\mathbb{R}^n\\) where the elements are a linear combination of the elements in \\(\\mathbf{x}\\).\nGeometrically the matrix \\(A\\) defines a transformation that scales and rotates any vector \\(\\mathbf{x}\\) about the origin.\nThe number of columns of \\(A\\) must match the size of the vector \\(\\mathbf{x}\\), but if \\(A\\) has a different number of rows, the output will simply have a different size.\n\nnp.dot(A[:2], x)\n\narray([35,  9])\n\n\nIn general, an \\(n\\times m\\) matrix defines a linear mapping \\(\\mathbb{R}^n\\rightarrow\\mathbb{R}^m\\), transforming \\(\\mathbf{x} \\in \\mathbb{R}^n\\) into a possibly higher or lower dimensional space \\(\\mathbb{R}^m\\).\n\n\n\nMatrix multiplication is a fundamental operation between two matrices. It is defined as:\n\\[\n\\mathbf{A}\\mathbf{B}  = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\\\ A_{23} & A_{32} & A_{33} \\end{bmatrix} \\begin{bmatrix} B_{11} & B_{12} \\\\ B_{21} & B_{22} \\\\ B_{31} & B_{32} \\end{bmatrix}  =\\begin{bmatrix} \\sum_{i=1}^n A_{1i} B_{i1} & \\sum_{i=1}^n A_{i1}B_{i2} \\\\  \\sum_{i=1}^n A_{2i}B_{i1} & \\sum_{i=1}^n A_{2i}B_{i2}  \\\\  \\sum_{i=1}^n A_{3i}B_{i1} & \\sum_{i=1}^n A_{3i}B_{i2} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*2} \\\\ \\mathbf{A}_{2} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{2} \\cdot \\mathbf{B}_{*2}  \\\\ \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*2}  \\end{bmatrix}\n\\]\nThis means that if we multiply an \\(n\\times m\\) matrix \\(\\mathbf{A}\\) with an \\(m\\times k\\) matrix \\(\\mathbf{B}\\) we get an \\(n\\times k\\) matrix \\(\\mathbf{C}\\), such that \\(\\mathbf{C}_{ij}\\) is the dot product of the \\(i\\)-th row of \\(\\mathbf{A}\\) with the \\(j\\)-th row of \\(\\mathbf{B}\\).\nIn numpy we once again use the np.dot function to perform matrix-multiplications.\n\nB = np.array([[2, -1], [3, 1], [-2, 5]])\nC = np.dot(A, B)\n\nNote that the number of rows of \\(\\mathbf{A}\\) must match the number of columns of \\(\\mathbf{B}\\) for the matrix multiplication \\(\\mathbf{A}\\mathbf{B}\\) to be defined. This implies that matrix multiplication is non-communitive:\n\\[\n\\mathbf{A}\\mathbf{B}\\neq \\mathbf{B}\\mathbf{A}\n\\]\nHowever matrix multiplication is associative and distributive:\n\\[\n\\mathbf{A}(\\mathbf{B}\\mathbf{C})=(\\mathbf{A}\\mathbf{B})\\mathbf{C}, \\quad \\mathbf{A}(\\mathbf{B} +\\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{A}\\mathbf{C}\n\\]\nWe can see matrix multiplication as a composition of linear maps, meaning that if we take the product \\(\\mathbf{A}\\mathbf{B}\\) and apply the resulting matrix to a vector \\(\\mathbf{x}\\), it is equivalent to first transforming \\(\\mathbf{x}\\) with \\(\\mathbf{B}\\) and then with \\(\\mathbf{A}\\). We can state this succinctly as:\n\\[\n(\\mathbf{A}\\mathbf{B})\\mathbf{x} = \\mathbf{A}(\\mathbf{B}\\mathbf{x})\n\\]\nWe can see this in numpy:\n\nx = np.array([1, 3])\nnp.dot(np.dot(A, B), x), np.dot(A, np.dot(B, x))\n\n(array([79, 31, 41]), array([79, 31, 41]))\n\n\n\n\n\nAs we’ve seen a vector is a 1-dimensional set of numbers. For example, we can write the vector \\(\\mathbf{x} \\in \\mathbb{R}^3\\) as:\n\\[\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}\\]\nA vector can also be seen as either an \\(n \\times 1\\) matrix (column vector) or a \\(1 \\times n\\) matrix (row vector).\n\\[\\text{Column vector: } \\mathbf{x} = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3\n\\end{bmatrix}, \\quad \\text{Row vector: } \\mathbf{x} = \\begin{bmatrix}\nx_1 & x_2 & x_3\n\\end{bmatrix}\\]\nNote that we may use the same notation for both as they refer to the same concept (a vector).\nThe difference between row and column vectors becomes relevant when we consider matrix-vector multiplication. We can write matrix-vector multiplication in two ways: \\[\\text{Matrix-vector: }\\mathbf{A}\\mathbf{x} = \\mathbf{b}, \\quad \\text{Vector-matrix: }\\mathbf{x}\\mathbf{A}^T= \\mathbf{b}\\] In matrix-vector multiplication we treat \\(\\textbf{x}\\) as a column vector (\\(n \\times 1\\) matrix), while in vector-matrix multiplication we treat it as a row vector (\\(n \\times 1\\) matrix). Transposing \\(A\\) for left multiplication ensures that the two forms give the same answer.\nIn Numpy the np.dot function works in this way. Given a matrix A and a 1-dimensional vector x, performing both operations will give the same result (another 1-dimensional vector):\n\nA = np.array([[ 1,  2, -1],\n              [ 5, -3,  2],\n              [-2,  1, -4],\n             ])\nx = np.array([1, -2, 1])\n\nAx   = np.dot(A, x)\nxA_T = np.dot(x, A.T)\nprint('Ax   = ', Ax)\nprint('xA^T = ', xA_T)\n\nAx   =  [-4 13 -8]\nxA^T =  [-4 13 -8]\n\n\n\n\n\nIt often is much simpler to explicitly define vectors as being either row or column vectors. The common convention in machine learning is to assume that all vectors are column vectors (\\(n \\times 1\\) matricies) and thus a row vector ( \\(1\\times n\\) matrix) is obtained by explicit transposition:\n\\[\\text{Column vector: } \\mathbf{x} = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3\n\\end{bmatrix}, \\quad \\text{Row vector: } \\mathbf{x}^T = \\begin{bmatrix}\nx_1 & x_2 & x_3\n\\end{bmatrix}\\]\nIn this case, we would rewrite the matrix-vector and vector-matrix products we saw above as:\n\\[\\text{Matrix-vector: }\\mathbf{A}\\mathbf{x} = \\mathbf{b}, \\quad \\text{Vector-matrix: }\\mathbf{x}^T\\mathbf{A}^T= \\mathbf{b}^T\\]\nIn Numpy, we can make a vector into an explicit column or row vector by inserting a new dimension, either with the np.expand_dims function or with the indexing operator:\n\nrow_x = np.expand_dims(x, axis=0) # Add a new leading dimension to x\nrow_x\n\narray([[ 1, -2,  1]])\n\n\n\ncolumn_x = np.expand_dims(x, axis=1) # Add a new second dimension to x\nassert np.all(column_x.T == row_x)\ncolumn_x\n\narray([[ 1],\n       [-2],\n       [ 1]])\n\n\nAlternatively:\n\nrow_x = x[None, :]\ncolumn_x = x[:, None]\n\n\n\n\nIt is important to note that in numpy the * operator does not perform matrix multiplication, instead it performs element-wise multiplication for both matrices and vectors.\n\nA = np.array([[1, 1], [2, 2], [3, 3]])\nB = np.array([[1, 2], [1, 2], [1, 2]])\nA * B\n\narray([[1, 2],\n       [2, 4],\n       [3, 6]])\n\n\nIn mathematical notation we will denote element-wise multiplication as:\n\\[\n\\mathbf{A} \\odot \\mathbf{B} = \\begin{bmatrix} A_{11} B_{11}  & A_{12}  B_{12} & A_{13}  B_{13} \\\\ A_{21}  B_{21} & A_{22}  B_{22} & A_{23}  B_{23} \\end{bmatrix}\n\\]\n\n\n\nAs we saw with vectors, we can take the sum of the elements of a matrix using the np.sum function:\n\nnp.sum(A)\n\n12\n\n\nThe result is a scalar of the form:\n\\[\n\\text{sum}(\\mathbf{A}) = \\sum_{i=1}^n\\sum_{j=1}^m A_{ij}\n\\]\nIn many cases we may wish to take the sum along an axis of a matrix. This operation results in a vector where each entry is the sum of elements from the corresponding row or column of the matrix.\n\\[\n\\text{rowsum}(\\mathbf{A}) = \\begin{bmatrix} \\sum_{i=1}^n A_{i1} \\\\ \\sum_{i=1}^n A_{i2} \\\\ \\sum_{i=1}^n A_{i3} \\\\ \\vdots\\end{bmatrix}, \\quad \\text{colsum}(\\mathbf{A}) = \\begin{bmatrix} \\sum_{j=1}^m A_{1j} \\\\ \\sum_{j=1}^m A_{2j} \\\\ \\sum_{j=1}^m A_{3j} \\\\ \\vdots\\end{bmatrix}\n\\]\nIn numpy we can specify a sum along an axis by providing an axis argument to np.sum. Setting axis=0 specifies a row-sum, while axis=1 specifies a column sum.\n\nA = np.array([[1, 1], [2, 2], [3, 3]])\nprint(A)\nnp.sum(A, axis=0), np.sum(A, axis=1)\n\n[[1 1]\n [2 2]\n [3 3]]\n\n\n(array([6, 6]), array([2, 4, 6]))\n\n\n\n\n\n\n\n\nOther matrix reduction examples\n\n\n\n\n\n\nprint(np.mean(A))            # Take the mean of all elements in x\nprint(np.std(A, axis=0))     # Take the standard deviation of each column of x\nprint(np.min(A, axis=1))     # Find the minimum element in each row of x\nprint(np.max(A))             # Find the maximum element in x\n\n2.0\n[0.81649658 0.81649658]\n[1 2 3]\n3\n\n\n\n\n\n\n\n\nThe identity matrix, denoted at \\(\\mathbf{I}\\) is a special type of square matrix. It is defined as the matrix with \\(1\\) for every diagonal element (\\(\\mathbf{I}_{i=j}=1\\)) and \\(0\\) for every other element (\\(\\mathbf{I}_{i\\neq j}=0\\)). A \\(3\\times 3\\) identity matrix looks like:\n\\[\\mathbf{I} = \\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}\\]\nThe identify matrix has the unique property that any appropriately sized matrix (or vector) multiplied with \\(\\mathbf{I}\\) will equal itself:\n\\[\n\\mathbf{I}\\mathbf{A} = \\mathbf{A}\n\\]\nIf we think of matrices as linear mappings the identity matrix simply maps any vector to itself.\n\\[\n\\mathbf{I} \\mathbf{x} = \\mathbf{x}\n\\]\nIn numpy we can create an identity matrix using the np.eye function:\n\nI = np.eye(3) # Create a 3x3 identity matrix\n\n\n\n\nConsider the matrix-vector product between a matrix \\(\\mathbf{A}\\) and a vector \\(\\mathbf{x}\\), we can denote the result of this multiplication as \\(\\mathbf{b}\\):\n\\[\n\\mathbf{A}\\mathbf{x} =\\mathbf{b}\n\\]\nIn many common cases we will know the matrix \\(\\mathbf{A}\\) and the vector \\(\\mathbf{b}\\), but not the vector \\(\\mathbf{x}\\). In such cases we need to solve this equation for \\(\\mathbf{x}\\):\n\\[\n\\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\\\ A_{23} & A_{32} & A_{33} \\end{bmatrix} \\begin{bmatrix} \\textbf{?}\\\\ \\textbf{?}\\\\ \\textbf{?}\\end{bmatrix} = \\begin{bmatrix} b_1 \\\\  b_2 \\\\  b_3 \\end{bmatrix}\n\\]\nWe call this solving a system of linear equations. A common algorithm used to find \\(\\mathbf{x}\\) is Gaussian elimination, the details of which are outside the scope of this course. Luckily, numpy gives us a convenient function for solving this problem: np.linalg.solve.\n\nA = np.array([[3, 1], [-1, 4]])\nb = np.array([-2, 1])\nx = np.linalg.solve(A, b)\n\nNote that in some cases there may not be any \\(\\mathbf{x}\\) that satisfies the equation (or there may be infinitely many). The conditions for this are beyond the scope of this course.\n\n\n\nThe inverse of a square matrix is denoted by \\(\\mathbf{A}^{-1}\\) and is defined as the matrix such that:\n\\[\n\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n\\]\nThis corresponds to the inverse of the linear map defined by \\(\\mathbf{A}\\). Any vector transformed by \\(\\mathbf{A}\\) can be transformed back by applying the inverse matrix:\n\\[\n\\mathbf{A}^{-1}\\left(\\mathbf{A}\\mathbf{x}\\right) =\\mathbf{x}\n\\]\nWe can also write the solution to a system of linear equations in terms of the inverse, by multiplying both sides by \\(\\mathbf{A}^{-1}\\):\n\\[\n\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\quad \\longrightarrow \\quad \\mathbf{A}^{-1}(\\mathbf{A}\\mathbf{x})=\\mathbf{A}^{-1}\\mathbf{b} \\quad \\longrightarrow \\quad \\mathbf{x}=\\mathbf{A}^{-1}\\mathbf{b}\n\\]\nIn numpy we can find the inverse of a matrix using the np.linalg.inv function:\n\nA_inv = np.linalg.inv(A)\n\nNote that not every matrix has an inverse! Again, we won’t worry about these cases for now."
  },
  {
    "objectID": "lecture1-background/notes.html#numpy",
    "href": "lecture1-background/notes.html#numpy",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "Numpy Is the linear algebra library that we will be using for much of this class (later on we will introduce PyTorch, which is more geared towards neural networks). The standard way to import Numpy is:\n\nimport numpy as np\n\nThe most basic object from Numpy that we will just is the array (np.array), which will represent vectors matrices and even higher-dimensional objects known as tensors."
  },
  {
    "objectID": "lecture1-background/notes.html#scalars",
    "href": "lecture1-background/notes.html#scalars",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "Scalars are just single numbers. In our math we will typically denote scalars with a lower-case letter. For example, we might call a scalar \\(x\\) and give it a value as \\(x=3.5\\). In code this would be written as:\n\nx = 3.5\n\n# or as a 0-dimensional numpy array\nx = np.array(3.5)\n\nIn general, we will assume that scalars are real numbers meaning decimal numbers in the range \\((-\\infty, \\infty)\\). We can denote this as \\(x \\in \\mathbb{R}\\), meaning “\\(x\\) is in the set of real numbers”."
  },
  {
    "objectID": "lecture1-background/notes.html#vectors",
    "href": "lecture1-background/notes.html#vectors",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "Vectors are ordered lists of numbers, as shown below. We will typically denote vectors with bold lower case letters, such as \\(\\mathbf{x}\\).\n\\[\n\\mathbf{x} = \\begin{bmatrix} 2\\\\ 5\\\\ 1\\end{bmatrix}\n\\]\nThis bold notation is common, but not universal, so expect that external resources may denote things differently!\nIn Numpy, we can create an array object representing a vector by passing np.array a list of numbers.\n\nx = np.array([2, 5, 1])\n\n\n\n\n\n\n\nOther useful vector creation functions\n\n\n\n\n\nConstants:\n\na = np.zeros(5) # Create a size 5 vector filled with zeros\nprint(a)\na = np.ones(5) # Create a size 5 vector filled with ones\nprint(a)\n\n[0. 0. 0. 0. 0.]\n[1. 1. 1. 1. 1.]\n\n\nInteger ranges:\n\na = np.array([1,2,3,4,5,6,7])\nprint(a)\n# or equivalently, using Python iterables:\na = np.array( range(1,8) )\nprint(a)\n# or the NumPy function np.arange:\na = np.arange(1, 8)\nprint(a)\n\n[1 2 3 4 5 6 7]\n[1 2 3 4 5 6 7]\n[1 2 3 4 5 6 7]\n\n\nDecimal ranges:\n\nb = np.linspace(1.0, 7.0, 4) # length-4 vector interpolating between 1.0 and 7.0\nprint(b)\nc = np.logspace(0.0, 2.0, 5) # length-7 vector interpolating between 10^0 and 10^2 logarithmically\nprint(c)\n\n[1. 3. 5. 7.]\n[  1.           3.16227766  10.          31.6227766  100.        ]\n\n\nCombining vectors\n\na = np.concatenate([b, c]) # length 4 + 7 vector with the elements of both b and c\n\n\n\n\nThe individual numbers in the vector are called the entries and we will refer to them individually as subscripted scalars: \\(x_1, x_2,…,x_n\\), where \\(n\\) is the number of entries or size of the vector.\n\\[\n\\mathbf{x} = \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\end{bmatrix}\n\\]\nWe may say that \\(\\mathbf{x} \\in \\mathbb{R}^n\\) to mean that \\(\\mathbf{x}\\) is in the set of vectors with \\(n\\) real-valued entries. In numpy we can access individual elements of a vector using [] operators (note that numpy is 0-indexed!).\n\nx[0]\n\n2\n\n\nA vector represents either a location or a change in location in \\(n\\) -dimensional space. For instance, the \\(\\mathbf{x}\\) vector we defined could represent the point with coordinates \\((2, 5, 1)\\) in 3-D space or a movement of 2 along the first axis, 5 along the second axis and 1 along the third axis."
  },
  {
    "objectID": "lecture1-background/notes.html#vectors-as-data",
    "href": "lecture1-background/notes.html#vectors-as-data",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "Vectors do not necessarily need to represent geometric points or directions. As they are simply ordered collections of numbers, we can a vector to represent any type of data in a more formal way.\nFor example, a space of vectors could represent students, with each entry corresponding to a student’s grade in a different subject:\n\\[\n\\text{Student } \\mathbf{x} = \\begin{bmatrix} \\text{Math} \\\\ \\text{CS} \\\\ \\text{Literature} \\\\\\text{History} \\end{bmatrix}\n\\]\nAny two students might have different grades across the subjects. We can represent these two students (say \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\)) as two vectors.\n\\[\n\\mathbf{x}_1 = \\begin{bmatrix} 3.0 \\\\ 4.0 \\\\ 3.7 \\\\ 2.3 \\end{bmatrix}, \\quad \\mathbf{x}_2 = \\begin{bmatrix} 3.7 \\\\ 3.3 \\\\ 3.3 \\\\ 4.0 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lecture1-background/notes.html#vector-equality",
    "href": "lecture1-background/notes.html#vector-equality",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "We say that two vectors are equal if and only if all of the corresponding elements are equal, so \\(\\mathbf{x} = \\mathbf{y}\\) implies that \\(x_1=y_1, x_2=y_2…\\). In numpy when we compare two vectors for equality, we get a new vector that indicates which entries are equal.\n\nx = np.array([2, 5, 1])\ny = np.array([3, 5, 2])\nx == y\n\narray([False,  True, False])\n\n\nWe can check if all entries are equal (and therefore the vectors are equal) using the np.all function.\n\nnp.all(x == y), np.all(x == x)\n\n(False, True)\n\n\nOther comparison operators (&gt;,&lt;, &gt;=, &lt;=, !=) also perform element-wise comparison in numpy."
  },
  {
    "objectID": "lecture1-background/notes.html#vector-addition",
    "href": "lecture1-background/notes.html#vector-addition",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "When we add or subtract vectors, we add or subtract the corresponding elements.\n\\[\\mathbf{x} + \\mathbf{y} = \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\end{bmatrix} + \\begin{bmatrix} y_1\\\\ y_2\\\\ y_3\\end{bmatrix} = \\begin{bmatrix} x_1 + y_1\\\\ x_2 + y_2\\\\ x_3 + y_3\\end{bmatrix}\\] This works the same in numpy\n\nx + y\n\narray([ 5, 10,  3])\n\n\nThis corresponds to shifting the point \\(\\mathbf{x}\\) by the vector \\(\\mathbf{y}\\)."
  },
  {
    "objectID": "lecture1-background/notes.html#element-wise-operations",
    "href": "lecture1-background/notes.html#element-wise-operations",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "In both mathematical notation and numpy, most operations on vectors will be assumed to be taken element-wise. That is,\n\\[\n\\mathbf{x}^2 = \\begin{bmatrix} x_1^2 \\\\ x_2^2 \\\\ x_3^2 \\end{bmatrix} , \\quad \\log(\\mathbf{x}) = \\begin{bmatrix} \\log x_1 \\\\ \\log x_2 \\\\ \\log x_3 \\end{bmatrix},\\ \\text{etc.}\n\\]\nIn numpy:\n\nx ** 2, np.log(x)\n\n(array([ 4, 25,  1]), array([0.69314718, 1.60943791, 0.        ]))\n\n\nNote that in this class (and in numpy!) logarithms are assumed to be base \\(e\\), otherwise known as natural logarithms.\nOperations between scalars and vectors are also assumed to be element-wise as in this scalar-vector multiplication\n\\[\na\\mathbf{x} = \\begin{bmatrix} a x_1 \\\\ a x_2 \\\\ a x_3 \\end{bmatrix}, \\quad a + \\mathbf{x} = \\begin{bmatrix} a + x_1 \\\\ a + x_2 \\\\ a + x_3 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lecture1-background/notes.html#vector-magnitude",
    "href": "lecture1-background/notes.html#vector-magnitude",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "The magnitude of a vector its length in \\(\\mathbb{R}^n\\), or equivalently the Euclidean distance from the origin to the point the vector represents. It is denoted as\\(\\|\\mathbf{x}\\|_2\\) and defined as:\n\\[\n\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}\n\\]\nThe subscript \\(2\\) specifies that we are talking about Euclidean distance. Because of this we will also refer to the magnitude as the two-norm.\nIn numpy we can compute this using the np.linalg.norm function.\n\nxnorm = np.linalg.norm(x)\n\nWe can also compute this explicitly with the np.sum function, which computes the sum of the elements of a vector.\n\nxnorm_explicit = np.sqrt(np.sum(x ** 2))\nxnorm, xnorm_explicit\n\n(5.477225575051661, 5.477225575051661)\n\n\n\n\n\n\n\n\nOther useful aggregation functions\n\n\n\n\n\n\nprint(np.mean(x))    # Take the mean of the elements in x\nprint(np.std(x))     # Take the standard deviation of the elements in x\nprint(np.min(x))     # Find the minimum element in x\nprint(np.max(x))     # Find the maximum element in x\n\n2.6666666666666665\n1.699673171197595\n1\n5\n\n\n\n\n\nA unit vector is a vector with length \\(1\\). We can find a unit vector that has the same direction as \\(\\mathbf{x}\\) by dividing \\(\\mathbf{x}\\) by it’s magnitude:\n\\[\n\\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|_2}\n\\]\n\nx / np.linalg.norm(x)\n\narray([0.36514837, 0.91287093, 0.18257419])"
  },
  {
    "objectID": "lecture1-background/notes.html#dot-products",
    "href": "lecture1-background/notes.html#dot-products",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "The dot-product operation between two vectors is defined as\n\\[\n\\mathbf{x} \\cdot \\mathbf{y} = \\sum_{i=1}^n x_i y_i\n\\]\nMore commonly in this class we will write the dot-product between the vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) as: \\[\\mathbf{x}^T \\mathbf{y} = \\sum_{i=1}^n x_i y_i\\]$\nThe result of a dot product is a scalar whose value is equal to \\(\\|\\mathbf{x}\\|_2\\|\\mathbf{y}\\|_2 \\cos\\theta\\), where \\(\\theta\\) is the angle between them. If\\(\\theta=\\frac{\\pi}{2}\\), the vectors are orthogonal and the dot product with be \\(0\\). If \\(\\theta&gt;\\frac{\\pi}{2}\\) or \\(\\theta &lt; -\\frac{\\pi}{2}\\) , the dot product will be negative.\nIf \\(\\theta=0\\) then \\(\\cos(\\theta)=1\\). In this case we say the vectors are colinear. This formulation implies that given two vectors of fixed length, the dot product is maximized with they are colinear ( \\(\\theta=0\\) ).\nWe can compute dot products in numpy using the np.dot function\n\nnp.dot(x, y)\n\n33\n\n\nGeometrically, \\(\\|\\mathbf{x}\\|_2\\cos\\theta\\) is the length of the projection of \\(\\mathbf{x}\\) onto \\(\\mathbf{y}\\). Thus, we can compute the length of this projection using the dot-product as \\(\\frac{\\mathbf{x}\\cdot\\mathbf{y}}{\\|\\mathbf{y}\\|}\\) ."
  },
  {
    "objectID": "lecture1-background/notes.html#matrices",
    "href": "lecture1-background/notes.html#matrices",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "A matrix is a 2-dimensional collection of numbers. We will denote matrices using bold capital letters, e.g. \\(\\mathbf{A}\\).\n\\[\n\\mathbf{A} = \\begin{bmatrix} 3 & 5 & 4 \\\\ 1 & 1 & 2  \\end{bmatrix}\n\\]\nIn numpy we can create a matrix by passing np.array as list-of-lists, where each inner list specifies a row of the matrix.\n\nA = np.array([[3, 5, 4], [1, 1, 2]])\nA\n\narray([[3, 5, 4],\n       [1, 1, 2]])\n\n\nAs with vectors we will denote individual elements of a matrix using subscripts. In this case, each element has 2 coordinates. Conventions is to always list the row first.\n\\[\n\\mathbf{A} = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\end{bmatrix}\n\\]\nIn numpy we can access elements in a matrix similarly.\n\nA[1, 2]\n\n2\n\n\nAs with vectors, we say that two matrices are equal if and only if all of the corresponding elements are equal.\n\n\n\n\n\n\nOther matrix creation routines\n\n\n\n\n\nBasic creation functions\n\nA0 = np.zeros((3, 4))          # create a 3x4 matrix of all zeros\nA1 = np.ones((4, 4))           # create a 4x4 matrix of all ones\nAinf = np.full((3, 3), np.inf) # create a 3x3 matrix of all infinities\nI = np.eye(3)                  # create a 3x3 itentity matrix\n\nCreating a matrix by “reshaping” a vector with the same number of elements\n\nV = np.arange(1, 13).reshape((3, 4)) # Create a 3x4 matrix with elements 1-12\n\nCreating matrices by combining matrices\n\nB = np.tile(A0, (3, 2))               # create a matrix by \"tiling\" copies of A0 (3 copies by 2 copies)\nB = np.concatenate([A0, A1], axis=0)  # create a (2n x m) matrix by stacking two matrices vertically\nB = np.concatenate([A0, I], axis=1)  # create a (n x 2m) matrix by stacking two matrices horizontally"
  },
  {
    "objectID": "lecture1-background/notes.html#slicing-matrices",
    "href": "lecture1-background/notes.html#slicing-matrices",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "Often we will refer to an entire row of a matrix (which is itself a vector) using matrix notation with a single index\n\\[\n\\mathbf{A}_i = \\begin{bmatrix} A_{i1} \\\\ A_{i2} \\\\ \\vdots \\end{bmatrix}\n\\]\nIn numpy we can access a single row similarly.\n\nA[1]\n\narray([1, 1, 2])\n\n\nWe can refer to an entire column by replacing the row index with a \\(*\\), indicating we are referring to all rows in that column.\n\\[\n\\mathbf{A}_{*i} = \\begin{bmatrix} A_{1i} \\\\ A_{2i} \\\\ \\vdots \\end{bmatrix}\n\\]\nIn numpy we can access an entire column (or row) using the slice operator :, which takes all elements along an axis.\n\nA[:, 1] # Take all elements in column 1\n\narray([5, 1])\n\n\nWe can also use the slice operator to take a subset of elements along an axis.\n\nA[:, 1:3] #Take the second and third columns of A\n\narray([[5, 4],\n       [1, 2]])\n\n\n\n\n\n\n\n\nAdvanced slicing in numpy\n\n\n\n\n\n\nA = np.arange(1, 13).reshape((3, 4))\nprint(A)\n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\n\nOpen-ended slices\n\nprint(\"A[:2]=\", A[:2]) # Take rows up to 2 (not inucluding 2)\nprint(\"A[1:]=\", A[1:]) # Take rows starting at (and including) 1\n\nA[:2]= [[1 2 3 4]\n [5 6 7 8]]\nA[1:]= [[ 5  6  7  8]\n [ 9 10 11 12]]\n\n\nTaking rows and colmns\n\nprint(\"A[0, :]=\", A[0, :])   # first row, all columns\nprint(\"A[:, 1]=\", A[:, 1])   # all rows, second column\nprint(\"A[-1, :]\", A[-1, :])  # all columns of the last row\nprint(\"A[:, -2]\", A[:, -2])  # second to last column of every row\n\nA[0, :]= [1 2 3 4]\nA[:, 1]= [ 2  6 10]\nA[-1, :] [ 9 10 11 12]\nA[:, -2] [ 3  7 11]\n\n\nMore general slicing with steps\n\nprint(\"A[1,0:2]=\", A[1,0:2])\nprint(\"A[0,0:4:2]=\", A[0,0:4:2])\nprint(\"A[:,0:4:2]=\\n\", A[:,0:4:2])\n\nA[1,0:2]= [5 6]\nA[0,0:4:2]= [1 3]\nA[:,0:4:2]=\n [[ 1  3]\n [ 5  7]\n [ 9 11]]\n\n\nTaking one row and selected columns\n\nprint(\"A[2, [1,4]]=\",A[2, [0,3]])\n\nA[2, [1,4]]= [ 9 12]\n\n\nTaking all rows and selected columns\n\nprint(\"A[:, [1,4]]=\\n\",A[:, [0,3]])\n\nA[:, [1,4]]=\n [[ 1  4]\n [ 5  8]\n [ 9 12]]"
  },
  {
    "objectID": "lecture1-background/notes.html#matrix-shapes",
    "href": "lecture1-background/notes.html#matrix-shapes",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "The matrix \\(\\mathbf{A}\\) above has 2 rows and 3 columns, thus we would specify it’s shape as \\(2\\times3\\). A square matrix has the same number of rows and columns.\n\\[\n\\mathbf{A} = \\begin{bmatrix} 3 & 5 & 4 \\\\ 1 & 1 & 2 \\\\ 4 & 1 & 3 \\end{bmatrix}\n\\]\nWe can access the shape of a matrix in numpy using its shape property.\n\nA = np.array([[3, 5, 4], [1, 1, 2], [4, 1, 3]])\nprint(A.shape)\n\n(3, 3)"
  },
  {
    "objectID": "lecture1-background/notes.html#matrix-transpose",
    "href": "lecture1-background/notes.html#matrix-transpose",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "The transpose of a matrix is an operation that swaps the rows and columns of the matrix. We denote the transpose of a matrix \\(\\mathbf{A}\\) as \\(\\mathbf{A}^T\\).\n\\[\n\\mathbf{A} = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\end{bmatrix}, \\quad \\mathbf{A}^T = \\begin{bmatrix} A_{11} & A_{21} \\\\  A_{12} & A_{22} \\\\  A_{13} & A_{23} \\end{bmatrix}\n\\]\nIn numpy we can transpose a matrix by using the T property.\n\nA = np.array([[1, 1, 1], [2, 2, 2]])\nA.T\n\narray([[1, 2],\n       [1, 2],\n       [1, 2]])"
  },
  {
    "objectID": "lecture1-background/notes.html#element-wise-matrix-operations",
    "href": "lecture1-background/notes.html#element-wise-matrix-operations",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "As with vectors, many operations on matrices are performed element-wise:\n\\[\n\\mathbf{A} + \\mathbf{B} = \\begin{bmatrix} A_{11} + B_{11}  & A_{12} + B_{12} & A_{13} + B_{13} \\\\ A_{21} + B_{21} & A_{22} + B_{22} & A_{23} + B_{23} \\end{bmatrix}, \\quad \\log\\mathbf{A} = \\begin{bmatrix} \\log A_{11}  & \\log A_{12} & \\log A_{13} \\\\ \\log A_{21} & \\log A_{22} & \\log A_{23} \\end{bmatrix}\n\\]\nScalar-matrix operation are also element-wise:\n\\[\nc\\mathbf{A} = \\begin{bmatrix} cA_{11}  & cA_{12} & cA_{13} \\\\ cA_{21} & cA_{22} & cA_{23} \\end{bmatrix}\n\\]\nIn numpy:\n\nB = 5 * A \nA + B\n\narray([[ 6,  6,  6],\n       [12, 12, 12]])\n\n\n\n\n\n\n\n\nOther element-wise operations\n\n\n\n\n\nUnary operations\n\nnp.sqrt(A)     # Take the square root of every element\nnp.log(A)      # Take the (natural) log of every element\nnp.exp(A)      # Take e^x for every element x in A\nnp.sin(A)      # Take sin(x) for every element x in A\nnp.cos(A)      # Take cos(x) for every element x in A\n\narray([[ 0.54030231,  0.54030231,  0.54030231],\n       [-0.41614684, -0.41614684, -0.41614684]])\n\n\nScalar operations\n\nA + 2          # Add a scalar to every element of A\nA - 1          # Subtract a scalar from every element of A\nA * 4          # Multiply a scalar with every element of A\nA / 6          # Divide every element by a scalar\nA ** 3         # Take every element to a power\n\narray([[1, 1, 1],\n       [8, 8, 8]])"
  },
  {
    "objectID": "lecture1-background/notes.html#matrix-vector-products",
    "href": "lecture1-background/notes.html#matrix-vector-products",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "A matrix-vector product is an operation between a matrix and a vector that produces a new vector. Given a matrix \\(\\mathbf{A}\\) and a vector \\(\\mathbf{x}\\), we write the matrix-vector product as:\n\\[\n\\mathbf{A}\\mathbf{x} = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\\\ A_{23} & A_{32} & A_{33} \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\end{bmatrix} = \\begin{bmatrix} \\sum_{i=1}^n x_i A_{1i} \\\\  \\sum_{i=1}^n x_i A_{2i} \\\\  \\sum_{i=1}^n x_i A_{3i} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{x}\\cdot  \\mathbf{A}_{1} \\\\ \\mathbf{x}\\cdot  \\mathbf{A}_{2} \\\\ \\mathbf{x} \\cdot \\mathbf{A}_{2} \\end{bmatrix}\n\\]\nIn other words, each entry of the resulting vector is the dot product between \\(\\mathbf{x}\\) and a row of \\(A\\). In numpy we also use np.dot for matrix-vector products:\n\nA = np.array([[3, 5, 4], [1, 1, 2], [4, 1, 3]])\nx = np.array([2, 5, 1])\n\nnp.dot(A, x)\n\narray([35,  9, 16])\n\n\nWe say that a square matrix \\(A\\) defines a linear mapping from \\(\\mathbb{R}^n\\rightarrow\\mathbb{R}^n\\), which simply means if we multiply any vector \\(\\mathbf{x}\\in\\mathbb{R}^n\\) by \\(A\\) we get a new vector in \\(\\mathbb{R}^n\\) where the elements are a linear combination of the elements in \\(\\mathbf{x}\\).\nGeometrically the matrix \\(A\\) defines a transformation that scales and rotates any vector \\(\\mathbf{x}\\) about the origin.\nThe number of columns of \\(A\\) must match the size of the vector \\(\\mathbf{x}\\), but if \\(A\\) has a different number of rows, the output will simply have a different size.\n\nnp.dot(A[:2], x)\n\narray([35,  9])\n\n\nIn general, an \\(n\\times m\\) matrix defines a linear mapping \\(\\mathbb{R}^n\\rightarrow\\mathbb{R}^m\\), transforming \\(\\mathbf{x} \\in \\mathbb{R}^n\\) into a possibly higher or lower dimensional space \\(\\mathbb{R}^m\\)."
  },
  {
    "objectID": "lecture1-background/notes.html#matrix-multiplication",
    "href": "lecture1-background/notes.html#matrix-multiplication",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "Matrix multiplication is a fundamental operation between two matrices. It is defined as:\n\\[\n\\mathbf{A}\\mathbf{B}  = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\\\ A_{23} & A_{32} & A_{33} \\end{bmatrix} \\begin{bmatrix} B_{11} & B_{12} \\\\ B_{21} & B_{22} \\\\ B_{31} & B_{32} \\end{bmatrix}  =\\begin{bmatrix} \\sum_{i=1}^n A_{1i} B_{i1} & \\sum_{i=1}^n A_{i1}B_{i2} \\\\  \\sum_{i=1}^n A_{2i}B_{i1} & \\sum_{i=1}^n A_{2i}B_{i2}  \\\\  \\sum_{i=1}^n A_{3i}B_{i1} & \\sum_{i=1}^n A_{3i}B_{i2} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*2} \\\\ \\mathbf{A}_{2} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{2} \\cdot \\mathbf{B}_{*2}  \\\\ \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*2}  \\end{bmatrix}\n\\]\nThis means that if we multiply an \\(n\\times m\\) matrix \\(\\mathbf{A}\\) with an \\(m\\times k\\) matrix \\(\\mathbf{B}\\) we get an \\(n\\times k\\) matrix \\(\\mathbf{C}\\), such that \\(\\mathbf{C}_{ij}\\) is the dot product of the \\(i\\)-th row of \\(\\mathbf{A}\\) with the \\(j\\)-th row of \\(\\mathbf{B}\\).\nIn numpy we once again use the np.dot function to perform matrix-multiplications.\n\nB = np.array([[2, -1], [3, 1], [-2, 5]])\nC = np.dot(A, B)\n\nNote that the number of rows of \\(\\mathbf{A}\\) must match the number of columns of \\(\\mathbf{B}\\) for the matrix multiplication \\(\\mathbf{A}\\mathbf{B}\\) to be defined. This implies that matrix multiplication is non-communitive:\n\\[\n\\mathbf{A}\\mathbf{B}\\neq \\mathbf{B}\\mathbf{A}\n\\]\nHowever matrix multiplication is associative and distributive:\n\\[\n\\mathbf{A}(\\mathbf{B}\\mathbf{C})=(\\mathbf{A}\\mathbf{B})\\mathbf{C}, \\quad \\mathbf{A}(\\mathbf{B} +\\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{A}\\mathbf{C}\n\\]\nWe can see matrix multiplication as a composition of linear maps, meaning that if we take the product \\(\\mathbf{A}\\mathbf{B}\\) and apply the resulting matrix to a vector \\(\\mathbf{x}\\), it is equivalent to first transforming \\(\\mathbf{x}\\) with \\(\\mathbf{B}\\) and then with \\(\\mathbf{A}\\). We can state this succinctly as:\n\\[\n(\\mathbf{A}\\mathbf{B})\\mathbf{x} = \\mathbf{A}(\\mathbf{B}\\mathbf{x})\n\\]\nWe can see this in numpy:\n\nx = np.array([1, 3])\nnp.dot(np.dot(A, B), x), np.dot(A, np.dot(B, x))\n\n(array([79, 31, 41]), array([79, 31, 41]))"
  },
  {
    "objectID": "lecture1-background/notes.html#element-wise-multiplication",
    "href": "lecture1-background/notes.html#element-wise-multiplication",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "It is important to note that in numpy the * operator does not perform matrix multiplication, instead it performs element-wise multiplication for both matrices and vectors.\n\nA = np.array([[1, 1], [2, 2], [3, 3]])\nB = np.array([[1, 2], [1, 2], [1, 2]])\nA * B\n\narray([[1, 2],\n       [2, 4],\n       [3, 6]])\n\n\nIn mathematical notation we will denote element-wise multiplication as:\n\\[\n\\mathbf{A} \\odot \\mathbf{B} = \\begin{bmatrix} A_{11} B_{11}  & A_{12}  B_{12} & A_{13}  B_{13} \\\\ A_{21}  B_{21} & A_{22}  B_{22} & A_{23}  B_{23} \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lecture1-background/notes.html#matrix-reductions",
    "href": "lecture1-background/notes.html#matrix-reductions",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "As we saw with vectors, we can take the sum of the elements of a matrix using the np.sum function:\n\nnp.sum(A)\n\n12\n\n\nThe result is a scalar of the form:\n\\[\n\\text{sum}(\\mathbf{A}) = \\sum_{i=1}^n\\sum_{j=1}^m A_{ij}\n\\]\nIn many cases we may wish to take the sum along an axis of a matrix. This operation results in a vector where each entry is the sum of elements from the corresponding row or column of the matrix.\n\\[\n\\text{rowsum}(\\mathbf{A}) = \\begin{bmatrix} \\sum_{i=1}^n A_{i1} \\\\ \\sum_{i=1}^n A_{i2} \\\\ \\sum_{i=1}^n A_{i3} \\\\ \\vdots\\end{bmatrix}, \\quad \\text{colsum}(\\mathbf{A}) = \\begin{bmatrix} \\sum_{j=1}^m A_{1j} \\\\ \\sum_{j=1}^m A_{2j} \\\\ \\sum_{j=1}^m A_{3j} \\\\ \\vdots\\end{bmatrix}\n\\]\nIn numpy we can specify a sum along an axis by providing an axis argument to np.sum. Setting axis=0 specifies a row-sum, while axis=1 specifies a column sum.\n\nA = np.array([[1, 1], [2, 2], [3, 3]])\nprint(A)\nnp.sum(A, axis=0), np.sum(A, axis=1)\n\n[[1 1]\n [2 2]\n [3 3]]\n\n\n(array([6, 6]), array([2, 4, 6]))\n\n\n\n\n\n\n\n\nOther matrix reduction examples\n\n\n\n\n\n\nprint(np.mean(A))            # Take the mean of all elements in x\nprint(np.std(A, axis=0))     # Take the standard deviation of each column of x\nprint(np.min(A, axis=1))     # Find the minimum element in each row of x\nprint(np.max(A))             # Find the maximum element in x\n\n2.0\n[0.81649658 0.81649658]\n[1 2 3]\n3"
  },
  {
    "objectID": "lecture1-background/notes.html#identity-matrices",
    "href": "lecture1-background/notes.html#identity-matrices",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "The identity matrix, denoted at \\(\\mathbf{I}\\) is a special type of square matrix. It is defined as the matrix with \\(1\\) for every diagonal element (\\(\\mathbf{I}_{i=j}=1\\)) and \\(0\\) for every other element (\\(\\mathbf{I}_{i\\neq j}=0\\)). A \\(3\\times 3\\) identity matrix looks like:\n\\[\\mathbf{I} = \\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}\\]\nThe identify matrix has the unique property that any appropriately sized matrix (or vector) multiplied with \\(\\mathbf{I}\\) will equal itself:\n\\[\n\\mathbf{I}\\mathbf{A} = \\mathbf{A}\n\\]\nIf we think of matrices as linear mappings the identity matrix simply maps any vector to itself.\n\\[\n\\mathbf{I} \\mathbf{x} = \\mathbf{x}\n\\]\nIn numpy we can create an identity matrix using the np.eye function:\n\nI = np.eye(3) # Create a 3x3 identity matrix"
  },
  {
    "objectID": "lecture1-background/notes.html#solving-systems-of-linear-equations",
    "href": "lecture1-background/notes.html#solving-systems-of-linear-equations",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "Consider the matrix-vector product between a matrix \\(\\mathbf{A}\\) and a vector \\(\\mathbf{x}\\), we can denote the result of this multiplication as \\(\\mathbf{b}\\):\n\\[\n\\mathbf{A}\\mathbf{x} =\\mathbf{b}\n\\]\nIn many common cases we will know the matrix \\(\\mathbf{A}\\) and the vector \\(\\mathbf{b}\\), but not the vector \\(\\mathbf{x}\\). In such cases we need to solve this equation for \\(\\mathbf{x}\\):\n\\[\n\\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\\\ A_{23} & A_{32} & A_{33} \\end{bmatrix} \\begin{bmatrix} \\textbf{?}\\\\ \\textbf{?}\\\\ \\textbf{?}\\end{bmatrix} = \\begin{bmatrix} b_1 \\\\  b_2 \\\\  b_3 \\end{bmatrix}\n\\]\nWe call this solving a system of linear equations. A common algorithm used to find \\(\\mathbf{x}\\) is Gaussian elimination, the details of which are outside the scope of this course. Luckily, numpy gives us a convenient function for solving this problem: np.linalg.solve.\n\nA = np.array([[3, 1], [-1, 4]])\nb = np.array([-2, 1])\nx = np.linalg.solve(A, b)\n\nNote that in some cases there may not be any \\(\\mathbf{x}\\) that satisfies the equation (or there may be infinitely many). The conditions for this are beyond the scope of this course."
  },
  {
    "objectID": "lecture1-background/notes.html#inverse-matrices",
    "href": "lecture1-background/notes.html#inverse-matrices",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "The inverse of a square matrix is denoted by \\(\\mathbf{A}^{-1}\\) and is defined as the matrix such that:\n\\[\n\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n\\]\nThis corresponds to the inverse of the linear map defined by \\(\\mathbf{A}\\). Any vector transformed by \\(\\mathbf{A}\\) can be transformed back by applying the inverse matrix:\n\\[\n\\mathbf{A}^{-1}\\left(\\mathbf{A}\\mathbf{x}\\right) =\\mathbf{x}\n\\]\nWe can also write the solution to a system of linear equations in terms of the inverse, by multiplying both sides by \\(\\mathbf{A}^{-1}\\):\n\\[\n\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\quad \\longrightarrow \\quad \\mathbf{A}^{-1}(\\mathbf{A}\\mathbf{x})=\\mathbf{A}^{-1}\\mathbf{b} \\quad \\longrightarrow \\quad \\mathbf{x}=\\mathbf{A}^{-1}\\mathbf{b}\n\\]\nIn numpy we can find the inverse of a matrix using the np.linalg.inv function:\n\nA_inv = np.linalg.inv(A)\n\nNote that not every matrix has an inverse! Again, we won’t worry about these cases for now."
  },
  {
    "objectID": "lecture1-background/notes.html#functions",
    "href": "lecture1-background/notes.html#functions",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "Functions",
    "text": "Functions\nA function is a general mapping from one set to another.\n\\[\ny=f(x),\\quad f:\\mathbb{R}\\rightarrow\\mathbb{R}\n\\]\nIn this course will focus on functions of real numbers, that is, mappings from real numbers to other real numbers this is denoted using the notation \\(\\mathbb{R}\\rightarrow\\mathbb{R}\\) above. We call the set of possible inputs the domain of the function (in this case real numbers) and the corresponding set of possible outputs the codomain or range of the function. In the notation above \\(x\\) is the real valued input and \\(y\\) is the real-valued output.\nWe can definite functions as compositions of simple operations. For example we could define a polynomial function as:\n\\[\nf(x) = x^2 + 3x + 1\n\\]\nIn code we can implement functions as, well, functions:\n\ndef f(x):\n    return x ** 2 + 3 * x + 1\n\nf(5)\n\n41"
  },
  {
    "objectID": "lecture1-background/notes.html#derivatives",
    "href": "lecture1-background/notes.html#derivatives",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "Derivatives",
    "text": "Derivatives\nThe derivative of a function at input \\(x\\) defines how the function’s output changes as the input changes from \\(x\\). It is equivalent to the slope of the line tangent to the function at the input \\(x\\). We’ll use the notation \\(\\frac{df}{dx}\\) to denote the derivative of the function \\(f\\) at input \\(x\\). Formally:\n\\[\n\\frac{df}{dx} = \\underset{\\epsilon\\rightarrow0}{\\lim} \\frac{f(x+\\epsilon) - f(x)}{\\epsilon}\n\\]\nIntunitively, this means if we change our input \\(x\\) by some small amount \\(\\epsilon\\), the output of our function will change by approximately \\(\\frac{df}{dx}\\epsilon\\)\n\\[\nf(x+\\epsilon) \\approx f(x)+\\frac{df}{dx}\\epsilon\n\\]\nNote that with respect to \\(\\epsilon\\) this approximation is a line with slope \\(\\frac{df}{dx}\\) and intercept \\(f(x)\\), therefore we say that this is a linear approximation to the function \\(f\\) at input \\(x\\).\nWe can also use the notation \\(\\frac{d}{dx}\\) to denote the derivative operator. This means “find the derivative of the following expression with respect to \\(x\\)”.\n\\[\n\\frac{d}{dx}f(x) = \\frac{df}{dx}\n\\]\n\n\n\n\n\n\nCaveat\n\n\n\n\n\nThis definition assumes that the limit exists at the input \\(x\\), which is not always true. We will see practical examples later in the course where this assumption does not hold."
  },
  {
    "objectID": "lecture1-background/notes.html#derivative-functions",
    "href": "lecture1-background/notes.html#derivative-functions",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "Derivative functions",
    "text": "Derivative functions\nWe can also talk about the function that maps any input \\(x\\) to the derivative \\(\\frac{df}{dx}\\) we call this the derivative function and denote it as \\(f'(x)\\). So:\n\\[\n\\frac{df}{dx}=f'(x)\n\\]\nGiven a function defined as a composition of basic operations, we can use a set of standard rules to find the corresponding derivative function. For example using the rules \\(\\frac{d}{dx}x^a=ax\\) , \\(\\frac{d}{dx}ax=a\\) and \\(\\frac{d}{dx}a=0\\), we can derive the derivative function for the polynomial above:\n\\[\nf(x) = x^2 + 3x + 1\n\\]\n\\[\nf'(x) = 2x + 3\n\\]\n\n\n\n\n\n\nBasic derivative rules\n\n\n\n\n\n\n\n\nOperation\nDerivative \\(\\frac{d}{dx}\\)\n\n\n\n\n\\(a\\)\n\\(0\\)\n\n\n\\(ax\\)\n\\(a\\)\n\n\n\\(x^a\\)\n\\(ax\\)\n\n\n\\(\\log(x)\\)\n\\(\\frac{1}{x}\\)\n\n\n\\(e^x\\)\n\\(e^x\\)\n\n\n\\(f(x) + g(x)\\)\n\\(f'(x)+g'(x)\\)\n\n\n\\(f(x)g(x)\\)\n\\(f'(x)g(x) + f(x)g'(x)\\)\n\n\n\\(\\frac{f(x)}{g(x)}\\)\n\\(\\frac{f'(x)g(x)-f(x)g'(x)}{g(x)^2}\\)"
  },
  {
    "objectID": "lecture1-background/notes.html#chain-rule",
    "href": "lecture1-background/notes.html#chain-rule",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "Chain rule",
    "text": "Chain rule\nComposing two functions means to apply one function to the output of another, for example we could apply \\(f\\) to the output of \\(g\\):\n\\[\ny = f\\big(g\\left(x\\right)\\big)\n\\]\nThis is easily replicated in code:\n\ndef f(x):\n    return x ** 2 + 3 * x + 1\n\ndef g(x):\n    return 5 * x - 2\n\nf(g(3))\n\n209\n\n\nThe chain rule tells us how to find the derivative of a composition of functions like this. We can write the rule either in terms of derivatives or derivative functions\n\\[\n\\frac{d}{dx}f\\big(g\\left(x\\right)\\big) = \\frac{df}{dg}\\frac{dg}{dx} \\quad \\text{or} \\quad \\frac{d}{dx}f\\big(g\\left(x\\right)\\big) =  f'\\big(g\\left(x\\right)\\big)g'\\left(x\\right)\n\\]\nNote that in our derivative notation we’re using \\(f\\) and \\(g\\) to denote the outputs of the respective functions.\n\n\n\n\n\n\nDerivatives in code\n\n\n\n\n\nA natural question to ask at this point is: does the derivative operator exist in Python or numpy?. The answer is amazingly: yes! However implementing such an operator is nontrivial. In fact, a significant portion of this course will be devoted to exploring how to implement exactly such an operation for numpy. As a preview, we will end up with the ability to take derivatives as follows:\n\ndef f(x):\n    return x ** 2 + 3 * x + 1\n\nfx = f(5)\ndf_dx = derivative(f)(5)"
  },
  {
    "objectID": "lecture1-background/notes.html#partial-derivatives",
    "href": "lecture1-background/notes.html#partial-derivatives",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "Partial derivatives",
    "text": "Partial derivatives\nA function does not need to be restricted to having a single input. We can specify a function with multiple inputs as follows:\n\\[\nf(x, y, z) = x^2 + 3xy - \\log(z)\n\\]\nIn code this would look like;\n\ndef f(x, y, z):\n    return x ** 2 + 3 * y + np.log(z)\n\nA partial derivative is the derivative of a multiple-input function with respect to a single input, assuming all other inputs are constant. We will explore the implications of that condition later on in this course. For now, we will simply view partial derivatives as a straightforward extension of derivatives, using the modified notation \\(\\frac{\\partial}{\\partial x}\\).\nMore formally, we can define the partial derivative with respect to each input of a function as:\n\\[\n\\frac{\\partial f}{\\partial x} = \\underset{\\epsilon\\rightarrow0}{\\lim} \\frac{f(x+\\epsilon, y, z) - f(x,y,z)}{\\epsilon}, \\quad \\frac{\\partial f}{\\partial y} = \\underset{\\epsilon\\rightarrow0}{\\lim} \\frac{f(x, y+\\epsilon, z) - f(x,y,z)}{\\epsilon}\n\\]\nThese partial derivatives tell us how the output of the function changes as we change each of the inputs individually."
  },
  {
    "objectID": "lecture1-background/notes.html#partial-derivative-functions",
    "href": "lecture1-background/notes.html#partial-derivative-functions",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "Partial derivative functions",
    "text": "Partial derivative functions\nWe can also specify partial derivative functions in the same way as derivative functions. We’ll use subscript notation to specify which input we are differentiating with respect to.\n\\[\n\\frac{\\partial f}{\\partial x} = f_x'(x, y, z)\n\\]\nWe can derive partial derivative functions using the same set of derivative rules:\n\\[\nf(x, y, z) = x^2 + 3xy - \\log(z)\n\\]\n\\[\nf_x'(x, y, z) = 2x + 3y\n\\]\n\\[\nf_y'(x, y, z) = 3x\n\\]\n\\[\nf_z'(x, y, z) = -\\frac{1}{z}\n\\]"
  },
  {
    "objectID": "lecture1-background/notes.html#functions-of-vectors",
    "href": "lecture1-background/notes.html#functions-of-vectors",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "Functions of vectors",
    "text": "Functions of vectors\nWe can also define functions that take vectors (or matrices) as inputs.\n\\[\ny = f(\\mathbf{x}) \\quad f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\n\\]\nHere \\(f\\) is a mapping from length \\(n\\) vectors to real numbers. As a concrete example we could define the function:\n\\[\nf(\\mathbf{x}) = \\sum_{i=1}^n x_i^3 + 1\n\\]\nHere’s the same function in numpy:\n\ndef f(x):\n    return np.sum(x ** 3) + 1\n\nf(np.array([1, 2, 3]))\n\n37\n\n\nNote that functions of vectors are equivalent to multiple-input functions, but with a more compact notation!\n\\[f(\\mathbf{x}) \\equiv f(x_1, x_2, x_3...)\\]"
  },
  {
    "objectID": "lecture1-background/notes.html#gradients",
    "href": "lecture1-background/notes.html#gradients",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "Gradients",
    "text": "Gradients\nThe gradient of a vector-input function is a vector such that each element is the partial derivative of the function with respect to the corresponding element of the input vector. We’ll use the same notation as derivatives for gradients.\n\\[\n\\frac{df}{d\\mathbf{x}} = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\frac{\\partial f}{\\partial x_3} \\\\ \\vdots \\end{bmatrix}\n\\]\nThe gradient is a vector that tangent to the function \\(f\\) at the input \\(\\mathbf{x}\\). Just as with derivatives, this means that the gradient defines a linear approximation to the function at the point \\(\\mathbf{x}\\).\n\\[\nf(\\mathbf{x}+\\mathbf{\\epsilon}) \\approx f(\\mathbf{x}) + \\frac{df}{d\\mathbf{x}} \\cdot \\mathbf{\\epsilon}\n\\]\nWhere \\(\\mathbf{\\epsilon}\\) is now a small vector. Intuitively, this means that if we take a small step in any direction as defined by \\(\\mathbf{\\epsilon}\\), the gradient will approximate the change in the output of the function. Becuase we are now in more than 1 dimension, this approximation defines a plane in \\(\\mathbb{R}^n\\).\nAnother extremely important property of the gradient is that it points in the direction of maximum change in the function. Meaning that if we were to take an infinitesimal step \\(\\mathbf{\\epsilon}\\) from \\(\\mathbf{x}\\) in any direction, stepping in the gradient direction would give use the maximum value of \\(f(\\mathbf{x} +\\mathbf{\\epsilon})\\). We can see this from the approximation above: \\(f(\\mathbf{x} +\\mathbf{\\epsilon})\\) is maximized when \\(\\frac{df}{d\\mathbf{x}}\\) and \\(\\mathbf{\\epsilon}\\) are colinear.\nWe can define the gradient in this sense this more formally as:\n\\[\n\\frac{df}{d\\mathbf{x}}= \\underset{\\gamma \\rightarrow 0}{\\lim}\\ \\underset{\\|\\mathbf{\\epsilon}\\|_2 &lt; \\gamma}{\\max} \\frac{f(\\mathbf{x} + \\mathbf{\\epsilon}) - f(\\mathbf{x})}{\\|\\mathbf{\\epsilon}\\|_2}\n\\]"
  },
  {
    "objectID": "lecture1-background/notes.html#gradient-functions",
    "href": "lecture1-background/notes.html#gradient-functions",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "Gradient functions",
    "text": "Gradient functions\nJust as with derivatives and partial derivatives, we can define a gradient function that maps an input vector \\(\\mathbf{x}\\) to the gradient of the function \\(f\\) at \\(\\mathbf{x}\\) as:\n\\[\n\\frac{df}{d\\mathbf{x}}=\\nabla f(\\mathbf{x})\n\\]\nHere \\(\\nabla f\\) is the gradient function for \\(f\\). If the function takes multiple vectors as input, we can specify the gradient function with respect to a particular input using subscript notation:\n\\[\n\\frac{df}{d\\mathbf{x}}= \\nabla_{\\mathbf{x}} f(\\mathbf{x}, \\mathbf{y}), \\quad \\frac{df}{d\\mathbf{y}}= \\nabla_{\\mathbf{y}} f(\\mathbf{x}, \\mathbf{y})\n\\]\nNote that the gradient function is a mapping from \\(\\mathbb{R}^n\\rightarrow\\mathbb{R}^n\\), meaning that it returns a vector with the same size as the input."
  },
  {
    "objectID": "lecture1-background/notes.html#getting-started",
    "href": "lecture1-background/notes.html#getting-started",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "Getting started",
    "text": "Getting started\nThe standard way to import MatPlotLib is:\n\nimport matplotlib.pyplot as plt\n\nThe standard approach to interacting with MatPlotLib can take some getting used to. In MatPlotLib the current plot that we’re working on is part of the global state, meaning that we don’t create an explicit plot object, we simply call functions of plt to update what the current plot will look like.\nWhen working in Jupyter notebooks, the current plot is displayed at the end of the current cell when it is run."
  },
  {
    "objectID": "lecture1-background/notes.html#scatterplots-and-line-plots",
    "href": "lecture1-background/notes.html#scatterplots-and-line-plots",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "Scatterplots and line plots",
    "text": "Scatterplots and line plots\nThe most typical action is to plot one sequence (x-values) against another (y-values); this can be done using disconnected points (a scatterplot), or by connecting adjacent points in the sequence (in the order they were provided). The latter is usually used to give a nice (piecewise linear) visualization of a continuous curve, by specifying x-values in order, and the y-values given by the function at those x-values.\nPlotting a scatter of data points:\n\nx_values = np.random.rand(1,10)   # unformly in [0,1)\ny_values = np.random.randn(1,10)  # Gaussian distribution\nplt.plot(x_values, y_values, 'ko');\n\n\n\n\nThe string determines the plot appearance -- in this case, black circles. You can use color strings (‘r’, ‘g’, ‘b’, ‘m’, ‘c’, ‘y’, ...) or use the “Color” keyword to specify an RGB color. Marker appearance (‘o’,‘s’,‘v’,‘.’, ...) controls how the points look.\nIf we connect those points using a line appearance specification (‘-’,‘--’,‘:’,...), it will not look very good, because the points are not ordered in any meaningful way. Let’s try a line plot using an ordered sequence of x values:\n\nx_values = np.linspace(0,8,100)\ny_values = np.sin(x_values)\nplt.plot(x_values,y_values,'b');\n\n\n\n\nThis is actually a plot of a large number of points (100), with no marker shape and connected by a solid line.\nFor plotting multiple point sets or curves, you can pass more vectors into the plot function, or call the function multiple times:\n\nx_values = np.linspace(0,8,100)\ny1 = np.sin(x_values)         # sinusoidal function\ny2 = (x_values - 3)**2 / 12   # a simple quadratic curve\ny3 = 0.5*x_values - 1.0       # a simple linear function\n\nplt.plot(x_values, y1, 'b-', x_values, y2, 'g--');  # plot two curves\nplt.plot(x_values, y3, 'r:'); # add a curve to the plot\n\n\n\n\nYou may want to explicitly set the plot ranges -- perhaps the most common pattern is to plot something, get the plot’s ranges, and then restore them later after plotting another function:\n\nx_values = np.linspace(0,8,100)\ny1 = np.sin(x_values)         # sinusoidal function\ny3 = 0.5*x_values - 1.0       # a simple linear function\n\nplt.plot(x_values, y1, 'b-') \nax = plt.axis()               # get the x and y axis ranges\nprint(ax)\n# now plot something else (which will change the axis ranges):\nplt.plot(x_values, y3, 'r:'); # add the linear curve\nplt.axis(ax);                 # restore the original plot's axis ranges\n\n(-0.4, 8.4, -1.099652011574681, 1.0998559934443881)"
  },
  {
    "objectID": "lecture1-background/notes.html#histograms",
    "href": "lecture1-background/notes.html#histograms",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "Histograms",
    "text": "Histograms\nHistograms are also useful visualizations:\n\nplt.hist(y2, bins=20);\n\n\n\n\nThe outputs of hist include the bin locations, the number of data in each bin, and the “handles” to the plot elements to manipulate their appearance, if desired."
  },
  {
    "objectID": "lecture1-background/notes.html#subplots-and-plot-sizes",
    "href": "lecture1-background/notes.html#subplots-and-plot-sizes",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "Subplots and plot sizes",
    "text": "Subplots and plot sizes\nIt is often useful to put more than one plot together in a group; you can do this using the subplot function. There are various options; for example, “sharex” and “sharey” allow multiple plots to share a single axis range (or, you can set it manually, of course).\nI often find it necessary to also change the geometry of the figure for multiple subplots -- although this is more generally useful as well, if you have a plot that looks better wider and shorter, for example.\n\nfig,ax = plt.subplots(1,3, figsize=(8.0, 2.0))      # make a 1 x 3 grid of plots:\nax[0].plot(x_values, y1, 'b-');   # plot y1 in the first subplot\nax[1].plot(x_values, y2, 'g--');  #   y2 in the 2nd\nax[2].plot(x_values, y3, 'r:');   #   and y3 in the last"
  },
  {
    "objectID": "lecture1-background/slides.html",
    "href": "lecture1-background/slides.html",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "We will begin with a review of many of the basic concepts from linear algebra that we will need for this course. As we go through these concepts we will see how to implement them in Python.\n\n\nNumpy Is the linear algebra library that we will be using for much of this class. The standard way to import Numpy is:\n\nimport numpy as np\n\nThe most basic object from Numpy that we will just is the array (np.array), which will represent vectors matrices and even higher-dimensional objects known as tensors.\n\n\n\nScalars are just single numbers. In our math we will typically denote scalars with a lower-case letter. For example, we might call a scalar \\(x\\) and give it a value as \\[x=3.5\\] In code:\n\nx = 3.5\n\n# or as a 0-dimensional numpy array\nx = np.array(3.5)\n\n\n\n\nVectors are ordered lists of numbers, as shown below. We will typically denote vectors with bold lower case letters, such as \\(\\mathbf{x}\\).\n\\[\n\\mathbf{x} = \\begin{bmatrix} 2\\\\ 5\\\\ 1\\end{bmatrix}\n\\]\nIn Numpy, we can create an array object representing a vector by passing np.array a list of numbers.\n\nx = np.array([2, 5, 1])\n\n\n\n\nThe individual numbers in the vector are called the entries and we will refer to them individually as subscripted scalars: \\(x_1, x_2,…,x_n\\).\n\\[\n\\mathbf{x} = \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\end{bmatrix}\n\\]\nIn numpy we can access individual elements of a vector using [] operators (note that numpy is 0-indexed!).\n\nx[0]\n\n2\n\n\n\n\n\nA vector represents either a location or a change in location in \\(n\\) -dimensional space.\n\n\n/Users/gabe/Documents/Courses/CS152-Neural-Networks-Fall-2023.github.io/.venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n  warnings.warn(\n\n\nManim Community v0.17.3\n\n\n\n\n\n\n\n\n\n\n\n\nA space of vectors could represent students, with each entry corresponding to a student’s grade in a different subject:\n\\[\n\\text{Student } \\mathbf{x} = \\begin{bmatrix} \\text{Math} \\\\ \\text{CS} \\\\ \\text{Literature} \\\\\\text{History} \\end{bmatrix}\n\\]\nAny two students might have different grades across the subjects. We can represent these two students (say \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\)) as two vectors.\n\\[\n\\mathbf{x}_1 = \\begin{bmatrix} 3.0 \\\\ 4.0 \\\\ 3.7 \\\\ 2.3 \\end{bmatrix}, \\quad \\mathbf{x}_2 = \\begin{bmatrix} 3.7 \\\\ 3.3 \\\\ 3.3 \\\\ 4.0 \\end{bmatrix}\n\\]\n\n\n\nWe say that two vectors are equal if and only if all of the corresponding elements are equal, so \\[\\mathbf{x} = \\mathbf{y}\\] implies that \\[x_1=y_1, x_2=y_2…\\]\n\n\n\n\nx = np.array([2, 5, 1])\ny = np.array([3, 5, 2])\nx == y\n\narray([False,  True, False])\n\n\nWe can check if all entries are equal (and therefore the vectors are equal) using the np.all function.\n\nnp.all(x == y), np.all(x == x)\n\n(False, True)\n\n\nOther comparison operators (&gt;,&lt;, &gt;=, &lt;=, !=) also perform element-wise comparison in numpy.\n\n\n\nWhen we add or subtract vectors, we add or subtract the corresponding elements.\n\\[\\mathbf{x} + \\mathbf{y} = \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\end{bmatrix} + \\begin{bmatrix} y_1\\\\ y_2\\\\ y_3\\end{bmatrix} = \\begin{bmatrix} x_1 + y_1\\\\ x_2 + y_2\\\\ x_3 + y_3\\end{bmatrix}\\] This works the same in numpy\n\nx + y\n\narray([ 5, 10,  3])\n\n\n\n\n\nAddition corresponds to shifting the point \\(\\mathbf{x}\\) by the vector \\(\\mathbf{y}\\).\n\n\n\n\n\n\n\n\nIn both mathematical notation and numpy, most operations on vectors will be assumed to be taken element-wise. That is,\n\\[\n\\mathbf{x}^2 = \\begin{bmatrix} x_1^2 \\\\ x_2^2 \\\\ x_3^2 \\end{bmatrix} , \\quad \\log(\\mathbf{x}) = \\begin{bmatrix} \\log x_1 \\\\ \\log x_2 \\\\ \\log x_3 \\end{bmatrix},\\ \\text{etc.}\n\\]\nIn numpy:\n\nx ** 2, np.log(x)\n\n(array([ 4, 25,  1]), array([0.69314718, 1.60943791, 0.        ]))\n\n\n\n\n\nNote that in this class (and in numpy!) logarithms are assumed to be base \\(e\\), otherwise known as natural logarithms.\n\n\n\nOperations between scalars and vectors are also assumed to be element-wise as in this scalar-vector multiplication\n\\[\na\\mathbf{x} = \\begin{bmatrix} a x_1 \\\\ a x_2 \\\\ a x_3 \\end{bmatrix}, \\quad a + \\mathbf{x} = \\begin{bmatrix} a + x_1 \\\\ a + x_2 \\\\ a + x_3 \\end{bmatrix}\n\\]\n\n\n\nThe magnitude of a vector its length in \\(\\mathbb{R}^n\\), or equivalently the Euclidean distance from the origin to the point the vector represents. It is denoted as\\(\\|\\mathbf{x}\\|_2\\) and defined as:\n\\[\n\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}\n\\]\nThe subscript \\(2\\) specifies that we are talking about Euclidean distance. Because of this we will also refer to the magnitude as the two-norm.\nIn numpy we can compute this using the np.linalg.norm function.\n\nxnorm = np.linalg.norm(x)\n\n\n\n\nWe can also compute magnitude explicitly with the np.sum function, which computes the sum of the elements of a vector.\n\nxnorm_explicit = np.sqrt(np.sum(x ** 2))\nxnorm, xnorm_explicit\n\n(5.477225575051661, 5.477225575051661)\n\n\n\n\n\n\n\n\nOther useful aggregation functions\n\n\n\n\n\n\nprint(np.mean(x))    # Take the mean of the elements in x\nprint(np.std(x))     # Take the standard deviation of the elements in x\nprint(np.min(x))     # Find the minimum element in x\nprint(np.max(x))     # Find the maximum element in x\n\n2.6666666666666665\n1.699673171197595\n1\n5\n\n\n\n\n\n\n\n\nA unit vector is a vector with length \\(1\\). We can find a unit vector that has the same direction as \\(\\mathbf{x}\\) by dividing \\(\\mathbf{x}\\) by it’s magnitude:\n\\[\n\\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|_2}\n\\]\n\nx / np.linalg.norm(x)\n\narray([0.36514837, 0.91287093, 0.18257419])\n\n\n\n\n\nThe dot-product operation between two vectors is defined as\n\\[\n\\mathbf{x} \\cdot \\mathbf{y} = \\sum_{i=1}^n x_i y_i\n\\]\nThe result is a scalar whose value is equal to \\(\\|\\mathbf{x}\\|_2\\|\\mathbf{y}\\|_2 \\cos\\theta\\), where \\(\\theta\\) is the angle between them.\nWe can compute dot products in numpy using the np.dot function\n\nnp.dot(x, y)\n\n33\n\n\n\n\n\nIf \\(\\theta=\\frac{\\pi}{2}\\), the vectors are orthogonal and the dot product with be \\(0\\). If \\(\\theta&gt;\\frac{\\pi}{2}\\) or \\(\\theta &lt; -\\frac{\\pi}{2}\\) , the dot product will be negative.\nIf \\(\\theta=0\\) then \\(\\cos(\\theta)=1\\). In this case we say the vectors are colinear. This formulation implies that given two vectors of fixed length, the dot product is maximized with they are colinear ( \\(\\theta=0\\) ).\n\n\n\nGeometrically, \\(\\|\\mathbf{x}\\|_2\\cos\\theta\\) is the length of the projection of \\(\\mathbf{x}\\) onto \\(\\mathbf{y}\\). Thus, we can compute the length of this projection using the dot-product as \\(\\frac{\\mathbf{x}\\cdot\\mathbf{y}}{\\|\\mathbf{y}\\|}\\) .\n\n\n\nA matrix is a 2-dimensional collection of numbers. We will denote matrices using bold capital letters, e.g. \\(\\mathbf{A}\\).\n\\[\n\\mathbf{A} = \\begin{bmatrix} 3 & 5 & 4 \\\\ 1 & 1 & 2  \\end{bmatrix}\n\\]\nIn numpy we can create a matrix by passing np.array as list-of-lists, where each inner list specifies a row of the matrix.\n\nA = np.array([[3, 5, 4], [1, 1, 2]])\nA\n\narray([[3, 5, 4],\n       [1, 1, 2]])\n\n\n\n\n\nAs with vectors we will denote individual elements of a matrix using subscripts. In this case, each element has 2 coordinates. Conventions is to always list the row first.\n\\[\n\\mathbf{A} = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\end{bmatrix}\n\\]\nIn numpy we can access elements in a matrix similarly.\n\nA[1, 2]\n\n2\n\n\nAs with vectors, we say that two matrices are equal if and only if all of the corresponding elements are equal.\n\n\n\nBasic creation functions\n\nA0 = np.zeros((3, 4))          # create a 3x4 matrix of all zeros\nA1 = np.ones((4, 4))           # create a 4x4 matrix of all ones\nAinf = np.full((3, 3), np.inf) # create a 3x3 matrix of all infinities\nI = np.eye(3)                  # create a 3x3 itentity matrix\n\nCreating a matrix by “reshaping” a vector with the same number of elements\n\nV = np.arange(1, 13).reshape((3, 4)) # Create a 3x4 matrix with elements 1-12\n\nCreating matrices by combining matrices\n\nB = np.tile(A0, (3, 2))               # create a matrix by \"tiling\" copies of A0 (3 copies by 2 copies)\nB = np.concatenate([A0, A1], axis=0)  # create a (2n x m) matrix by stacking two matrices vertically\nB = np.concatenate([A0, I], axis=1)  # create a (n x 2m) matrix by stacking two matrices horizontally\n\n\n\n\nOften we will refer to an entire row of a matrix (which is itself a vector) using matrix notation with a single index\n\\[\n\\mathbf{A}_i = \\begin{bmatrix} A_{i1} \\\\ A_{i2} \\\\ \\vdots \\end{bmatrix}\n\\]\nIn numpy we can access a single row similarly.\n\nA[1]\n\narray([1, 1, 2])\n\n\n\n\n\nWe can refer to an entire column by replacing the row index with a \\(*\\), indicating we are referring to all rows in that column.\n\\[\n\\mathbf{A}_{*i} = \\begin{bmatrix} A_{1i} \\\\ A_{2i} \\\\ \\vdots \\end{bmatrix}\n\\]\nIn numpy we can access an entire column (or row) using the slice operator :, which takes all elements along an axis.\n\nA[:, 1] # Take all elements in column 1\n\narray([5, 1])\n\n\n\n\n\nWe can also use the slice operator to take a subset of elements along an axis.\n\nA[:, 1:3] #Take the second and third columns of A\n\narray([[5, 4],\n       [1, 2]])\n\n\n\n\n\nOpen-ended slices\n\nprint(\"A[:2]=\", A[:2]) # Take rows up to 2 (not inucluding 2)\nprint(\"A[1:]=\", A[1:]) # Take rows starting at (and including) 1\n\nA[:2]= [[1 2 3 4]\n [5 6 7 8]]\nA[1:]= [[ 5  6  7  8]\n [ 9 10 11 12]]\n\n\nTaking rows and colmns\n\nprint(\"A[0, :]=\", A[0, :])   # first row, all columns\nprint(\"A[:, 1]=\", A[:, 1])   # all rows, second column\nprint(\"A[-1, :]\", A[-1, :])  # all columns of the last row\nprint(\"A[:, -2]\", A[:, -2])  # second to last column of every row\n\nA[0, :]= [1 2 3 4]\nA[:, 1]= [ 2  6 10]\nA[-1, :] [ 9 10 11 12]\nA[:, -2] [ 3  7 11]\n\n\n\n\n\nMore general slicing with steps\n\nprint(\"A[1,0:2]=\", A[1,0:2])\nprint(\"A[0,0:4:2]=\", A[0,0:4:2])\nprint(\"A[:,0:4:2]=\\n\", A[:,0:4:2])\n\nA[1,0:2]= [5 6]\nA[0,0:4:2]= [1 3]\nA[:,0:4:2]=\n [[ 1  3]\n [ 5  7]\n [ 9 11]]\n\n\nTaking one row and selected columns\n\nprint(\"A[2, [1,4]]=\",A[2, [0,3]])\n\nA[2, [1,4]]= [ 9 12]\n\n\nTaking all rows and selected columns\n\nprint(\"A[:, [1,4]]=\\n\",A[:, [0,3]])\n\nA[:, [1,4]]=\n [[ 1  4]\n [ 5  8]\n [ 9 12]]\n\n\n\n\n\nThe matrix \\(\\mathbf{A}\\) above has 2 rows and 3 columns, thus we would specify it’s shape as \\(2\\times3\\). A square matrix has the same number of rows and columns.\n\\[\n\\mathbf{A} = \\begin{bmatrix} 3 & 5 & 4 \\\\ 1 & 1 & 2 \\\\ 4 & 1 & 3 \\end{bmatrix}\n\\]\nWe can access the shape of a matrix in numpy using its shape property.\n\nA = np.array([[3, 5, 4], [1, 1, 2], [4, 1, 3]])\nprint(A.shape)\n\n(3, 3)\n\n\n\n\n\nThe transpose of a matrix is an operation that swaps the rows and columns of the matrix. We denote the transpose of a matrix \\(\\mathbf{A}\\) as \\(\\mathbf{A}^T\\).\n\\[\n\\mathbf{A} = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\end{bmatrix}, \\quad \\mathbf{A}^T = \\begin{bmatrix} A_{11} & A_{21} \\\\  A_{12} & A_{22} \\\\  A_{13} & A_{23} \\end{bmatrix}\n\\]\nIn numpy we can transpose a matrix by using the T property.\n\nA = np.array([[1, 1, 1], [2, 2, 2]])\nA.T\n\narray([[1, 2],\n       [1, 2],\n       [1, 2]])\n\n\n\n\n\nAs with vectors, many operations on matrices are performed element-wise:\n\\[\n\\mathbf{A} + \\mathbf{B} = \\begin{bmatrix} A_{11} + B_{11}  & A_{12} + B_{12} & A_{13} + B_{13} \\\\ A_{21} + B_{21} & A_{22} + B_{22} & A_{23} + B_{23} \\end{bmatrix}, \\quad \\log\\mathbf{A} = \\begin{bmatrix} \\log A_{11}  & \\log A_{12} & \\log A_{13} \\\\ \\log A_{21} & \\log A_{22} & \\log A_{23} \\end{bmatrix}\n\\]\nScalar-matrix operation are also element-wise:\n\\[\nc\\mathbf{A} = \\begin{bmatrix} cA_{11}  & cA_{12} & cA_{13} \\\\ cA_{21} & cA_{22} & cA_{23} \\end{bmatrix}\n\\]\nIn numpy:\n\nB = 5 * A \nA + B\n\narray([[ 6,  6,  6],\n       [12, 12, 12]])\n\n\n\n\n\nA matrix-vector product is an operation between a matrix and a vector that produces a new vector. Given a matrix \\(\\mathbf{A}\\) and a vector \\(\\mathbf{x}\\), we write the matrix-vector product as:\n\\[\n\\mathbf{A}\\mathbf{x} = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\\\ A_{23} & A_{32} & A_{33} \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\end{bmatrix} = \\begin{bmatrix} \\sum_{i=1}^n x_i A_{1i} \\\\  \\sum_{i=1}^n x_i A_{2i} \\\\  \\sum_{i=1}^n x_i A_{3i} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{x}\\cdot  \\mathbf{A}_{1} \\\\ \\mathbf{x}\\cdot  \\mathbf{A}_{2} \\\\ \\mathbf{x} \\cdot \\mathbf{A}_{2} \\end{bmatrix}\n\\]\nIn other words, each entry of the resulting vector is the dot product between \\(\\mathbf{x}\\) and a row of \\(A\\). In numpy we also use np.dot for matrix-vector products:\n\nA = np.array([[3, 5, 4], [1, 1, 2], [4, 1, 3]])\nx = np.array([2, 5, 1])\n\nnp.dot(A, x)\n\narray([35,  9, 16])\n\n\nGeometrically the matrix \\(A\\) defines a transformation that scales and rotates any vector \\(\\mathbf{x}\\) about the origin.\n\n\n\nThe number of columns of \\(A\\) must match the size of the vector \\(\\mathbf{x}\\), but if \\(A\\) has a different number of rows, the output will simply have a different size.\n\nnp.dot(A[:2], x)\n\narray([35,  9])\n\n\n\n\n\nMatrix multiplication is a fundamental operation between two matrices. It is defined as:\n\\[\n\\mathbf{A}\\mathbf{B}  = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\\\ A_{23} & A_{32} & A_{33} \\end{bmatrix} \\begin{bmatrix} B_{11} & B_{12} \\\\ B_{21} & B_{22} \\\\ B_{31} & B_{32} \\end{bmatrix}  =\\begin{bmatrix} \\sum_{i=1}^n A_{1i} B_{i1} & \\sum_{i=1}^n A_{i1}B_{i2} \\\\  \\sum_{i=1}^n A_{2i}B_{i1} & \\sum_{i=1}^n A_{2i}B_{i2}  \\\\  \\sum_{i=1}^n A_{3i}B_{i1} & \\sum_{i=1}^n A_{3i}B_{i2} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*2} \\\\ \\mathbf{A}_{2} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{2} \\cdot \\mathbf{B}_{*2}  \\\\ \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*2}  \\end{bmatrix}\n\\]\nThis means that if we multiply an \\(n\\times m\\) matrix \\(\\mathbf{A}\\) with an \\(m\\times k\\) matrix \\(\\mathbf{B}\\) we get an \\(n\\times k\\) matrix \\(\\mathbf{C}\\), such that \\(\\mathbf{C}_{ij}\\) is the dot product of the \\(i\\)-th row of \\(\\mathbf{A}\\) with the \\(j\\)-th row of \\(\\mathbf{B}\\).\nIn numpy we once again use the np.dot function to perform matrix-multiplications.\n\nB = np.array([[2, -1], [3, 1], [-2, 5]])\nC = np.dot(A, B)\n\n\n\n\nThe number of rows of \\(\\mathbf{A}\\) must match the number of columns of \\(\\mathbf{B}\\) for the matrix multiplication \\(\\mathbf{A}\\mathbf{B}\\) to be defined. This implies that matrix multiplication is non-communitive:\n\\[\n\\mathbf{A}\\mathbf{B}\\neq \\mathbf{B}\\mathbf{A}\n\\]\nHowever matrix multiplication is associative and distributive:\n\\[\n\\mathbf{A}(\\mathbf{B}\\mathbf{C})=(\\mathbf{A}\\mathbf{B})\\mathbf{C}, \\quad \\mathbf{A}(\\mathbf{B} +\\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{A}\\mathbf{C}\n\\]\n\n\n\nMatrix multiplication is a composition of linear maps, meaning that if we take the product \\(\\mathbf{A}\\mathbf{B}\\) and apply the resulting matrix to a vector \\(\\mathbf{x}\\), it is equivalent to first transforming \\(\\mathbf{x}\\) with \\(\\mathbf{B}\\) and then with \\(\\mathbf{A}\\). We can state this succinctly as:\n\\[\n(\\mathbf{A}\\mathbf{B})\\mathbf{x} = \\mathbf{A}(\\mathbf{B}\\mathbf{x})\n\\]\nWe can see this in numpy:\n\nx = np.array([1, 3])\nnp.dot(np.dot(A, B), x), np.dot(A, np.dot(B, x))\n\n(array([79, 31, 41]), array([79, 31, 41]))\n\n\n\n\n\nIt is important to note that in numpy the * operator does not perform matrix multiplication, instead it performs element-wise multiplication for both matrices and vectors.\n\nA = np.array([[1, 1], [2, 2], [3, 3]])\nB = np.array([[1, 2], [1, 2], [1, 2]])\nA * B\n\narray([[1, 2],\n       [2, 4],\n       [3, 6]])\n\n\nIn mathematical notation we will denote element-wise multiplication as:\n\\[\n\\mathbf{A} \\odot \\mathbf{B} = \\begin{bmatrix} A_{11} B_{11}  & A_{12}  B_{12} & A_{13}  B_{13} \\\\ A_{21}  B_{21} & A_{22}  B_{22} & A_{23}  B_{23} \\end{bmatrix}\n\\]\n\n\n\nAs we saw with vectors, we can take the sum of the elements of a matrix using the np.sum function:\n\nnp.sum(A)\n\n12\n\n\nThe result is a scalar of the form:\n\\[\n\\text{sum}(\\mathbf{A}) = \\sum_{i=1}^n\\sum_{j=1}^m A_{ij}\n\\]\n\n\n\nIn many cases we may wish to take the sum along an axis of a matrix. This operation results in a vector where each entry is the sum of elements from the corresponding row or column of the matrix.\n\\[\n\\text{rowsum}(\\mathbf{A}) = \\begin{bmatrix} \\sum_{i=1}^n A_{i1} \\\\ \\sum_{i=1}^n A_{i2} \\\\ \\sum_{i=1}^n A_{i3} \\\\ \\vdots\\end{bmatrix}, \\quad \\text{colsum}(\\mathbf{A}) = \\begin{bmatrix} \\sum_{j=1}^m A_{1j} \\\\ \\sum_{j=1}^m A_{2j} \\\\ \\sum_{j=1}^m A_{3j} \\\\ \\vdots\\end{bmatrix}\n\\]\nIn numpy we can specify a sum along an axis by providing an axis argument to np.sum. Setting axis=0 specifies a row-sum, while axis=1 specifies a column sum.\n\nA = np.array([[1, 1], [2, 2], [3, 3]])\nprint(A)\nnp.sum(A, axis=0), np.sum(A, axis=1)\n\n[[1 1]\n [2 2]\n [3 3]]\n\n\n(array([6, 6]), array([2, 4, 6]))\n\n\n\n\n\n\nprint(np.mean(A))            # Take the mean of all elements in x\nprint(np.std(A, axis=0))     # Take the standard deviation of each column of x\nprint(np.min(A, axis=1))     # Find the minimum element in each row of x\nprint(np.max(A))             # Find the maximum element in x\n\n2.0\n[0.81649658 0.81649658]\n[1 2 3]\n3\n\n\n\n\n\nThe identity matrix, denoted at \\(\\mathbf{I}\\) is a special type of square matrix. It is defined as the matrix with \\(1\\) for every diagonal element (\\(\\mathbf{I}_{i=j}=1\\)) and \\(0\\) for every other element (\\(\\mathbf{I}_{i\\neq j}=0\\)). A \\(3\\times 3\\) identity matrix looks like:\n\\[\\mathbf{I} = \\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}\\]\nThe identify matrix has the unique property that any appropriately sized matrix (or vector) multiplied with \\(\\mathbf{I}\\) will equal itself:\n\\[\n\\mathbf{I}\\mathbf{A} = \\mathbf{A}\n\\]\nIf we think of matrices as linear mappings the identity matrix simply maps any vector to itself.\n\\[\n\\mathbf{I} \\mathbf{x} = \\mathbf{x}\n\\]\nIn numpy we can create an identity matrix using the np.eye function:\n\nI = np.eye(3) # Create a 3x3 identity matrix\n\n\n\n\nConsider the matrix-vector product between a matrix \\(\\mathbf{A}\\) and a vector \\(\\mathbf{x}\\), we can denote the result of this multiplication as \\(\\mathbf{b}\\):\n\\[\n\\mathbf{A}\\mathbf{x} =\\mathbf{b}\n\\]\nIn many common cases we will know the matrix \\(\\mathbf{A}\\) and the vector \\(\\mathbf{b}\\), but not the vector \\(\\mathbf{x}\\). In such cases we need to solve this equation for \\(\\mathbf{x}\\):\n\\[\n\\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\\\ A_{23} & A_{32} & A_{33} \\end{bmatrix} \\begin{bmatrix} \\textbf{?}\\\\ \\textbf{?}\\\\ \\textbf{?}\\end{bmatrix} = \\begin{bmatrix} b_1 \\\\  b_2 \\\\  b_3 \\end{bmatrix}\n\\]\nIn Numpy:\n\nA = np.array([[3, 1], [-1, 4]])\nb = np.array([-2, 1])\nx = np.linalg.solve(A, b)\n\nNote that in some cases there may not be any \\(\\mathbf{x}\\) that satisfies the equation (or there may be infinitely many). The conditions for this are beyond the scope of this course.\n\n\n\nThe inverse of a square matrix is denoted by \\(\\mathbf{A}^{-1}\\) and is defined as the matrix such that:\n\\[\n\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n\\]\nThis corresponds to the inverse of the linear map defined by \\(\\mathbf{A}\\). Any vector transformed by \\(\\mathbf{A}\\) can be transformed back by applying the inverse matrix:\n\\[\n\\mathbf{A}^{-1}\\left(\\mathbf{A}\\mathbf{x}\\right) =\\mathbf{x}\n\\]\nWe can also write the solution to a system of linear equations in terms of the inverse, by multiplying both sides by \\(\\mathbf{A}^{-1}\\):\n\\[\n\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\quad \\longrightarrow \\quad \\mathbf{A}^{-1}(\\mathbf{A}\\mathbf{x})=\\mathbf{A}^{-1}\\mathbf{b} \\quad \\longrightarrow \\quad \\mathbf{x}=\\mathbf{A}^{-1}\\mathbf{b}\n\\]\nIn numpy we can find the inverse of a matrix using the np.linalg.inv function:\n\nA_inv = np.linalg.inv(A)\n\nNote that not every matrix has an inverse! Again, we won’t worry about these cases for now."
  },
  {
    "objectID": "lecture1-background/slides.html#numpy",
    "href": "lecture1-background/slides.html#numpy",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Numpy",
    "text": "Numpy\nNumpy Is the linear algebra library that we will be using for much of this class. The standard way to import Numpy is:\n\nimport numpy as np\n\nThe most basic object from Numpy that we will just is the array (np.array), which will represent vectors matrices and even higher-dimensional objects known as tensors."
  },
  {
    "objectID": "lecture1-background/slides.html#scalars",
    "href": "lecture1-background/slides.html#scalars",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Scalars",
    "text": "Scalars\nScalars are just single numbers. In our math we will typically denote scalars with a lower-case letter. For example, we might call a scalar \\(x\\) and give it a value as \\[x=3.5\\] In code:\n\nx = 3.5\n\n# or as a 0-dimensional numpy array\nx = np.array(3.5)"
  },
  {
    "objectID": "lecture1-background/slides.html#vectors",
    "href": "lecture1-background/slides.html#vectors",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Vectors",
    "text": "Vectors\nVectors are ordered lists of numbers, as shown below. We will typically denote vectors with bold lower case letters, such as \\(\\mathbf{x}\\).\n\\[\n\\mathbf{x} = \\begin{bmatrix} 2\\\\ 5\\\\ 1\\end{bmatrix}\n\\]\nIn Numpy, we can create an array object representing a vector by passing np.array a list of numbers.\n\nx = np.array([2, 5, 1])"
  },
  {
    "objectID": "lecture1-background/slides.html#entries",
    "href": "lecture1-background/slides.html#entries",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Entries",
    "text": "Entries\nThe individual numbers in the vector are called the entries and we will refer to them individually as subscripted scalars: \\(x_1, x_2,…,x_n\\).\n\\[\n\\mathbf{x} = \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\end{bmatrix}\n\\]\nIn numpy we can access individual elements of a vector using [] operators (note that numpy is 0-indexed!).\n\nx[0]\n\n2"
  },
  {
    "objectID": "lecture1-background/slides.html#vector-interpretation",
    "href": "lecture1-background/slides.html#vector-interpretation",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Vector interpretation",
    "text": "Vector interpretation\nA vector represents either a location or a change in location in \\(n\\) -dimensional space.\n\n\nManim Community v0.17.3"
  },
  {
    "objectID": "lecture1-background/slides.html#vectors-as-data",
    "href": "lecture1-background/slides.html#vectors-as-data",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Vectors as data",
    "text": "Vectors as data\nA space of vectors could represent students, with each entry corresponding to a student’s grade in a different subject:\n\\[\n\\text{Student } \\mathbf{x} = \\begin{bmatrix} \\text{Math} \\\\ \\text{CS} \\\\ \\text{Literature} \\\\\\text{History} \\end{bmatrix}\n\\]\nAny two students might have different grades across the subjects. We can represent these two students (say \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\)) as two vectors.\n\\[\n\\mathbf{x}_1 = \\begin{bmatrix} 3.0 \\\\ 4.0 \\\\ 3.7 \\\\ 2.3 \\end{bmatrix}, \\quad \\mathbf{x}_2 = \\begin{bmatrix} 3.7 \\\\ 3.3 \\\\ 3.3 \\\\ 4.0 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lecture1-background/slides.html#vector-equality",
    "href": "lecture1-background/slides.html#vector-equality",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Vector equality",
    "text": "Vector equality\nWe say that two vectors are equal if and only if all of the corresponding elements are equal, so \\[\\mathbf{x} = \\mathbf{y}\\] implies that \\[x_1=y_1, x_2=y_2…\\]"
  },
  {
    "objectID": "lecture1-background/slides.html#vector-equality-in-numpy",
    "href": "lecture1-background/slides.html#vector-equality-in-numpy",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Vector equality in Numpy",
    "text": "Vector equality in Numpy\n\nx = np.array([2, 5, 1])\ny = np.array([3, 5, 2])\nx == y\n\narray([False,  True, False])\n\n\nWe can check if all entries are equal (and therefore the vectors are equal) using the np.all function.\n\nnp.all(x == y), np.all(x == x)\n\n(False, True)\n\n\nOther comparison operators (&gt;,&lt;, &gt;=, &lt;=, !=) also perform element-wise comparison in numpy."
  },
  {
    "objectID": "lecture1-background/slides.html#vector-addition",
    "href": "lecture1-background/slides.html#vector-addition",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Vector addition",
    "text": "Vector addition\nWhen we add or subtract vectors, we add or subtract the corresponding elements.\n\\[\\mathbf{x} + \\mathbf{y} = \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\end{bmatrix} + \\begin{bmatrix} y_1\\\\ y_2\\\\ y_3\\end{bmatrix} = \\begin{bmatrix} x_1 + y_1\\\\ x_2 + y_2\\\\ x_3 + y_3\\end{bmatrix}\\] This works the same in numpy\n\nx + y\n\narray([ 5, 10,  3])"
  },
  {
    "objectID": "lecture1-background/slides.html#vector-addition-1",
    "href": "lecture1-background/slides.html#vector-addition-1",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Vector addition",
    "text": "Vector addition\nAddition corresponds to shifting the point \\(\\mathbf{x}\\) by the vector \\(\\mathbf{y}\\)."
  },
  {
    "objectID": "lecture1-background/slides.html#element-wise-operations",
    "href": "lecture1-background/slides.html#element-wise-operations",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Element-wise operations",
    "text": "Element-wise operations\nIn both mathematical notation and numpy, most operations on vectors will be assumed to be taken element-wise. That is,\n\\[\n\\mathbf{x}^2 = \\begin{bmatrix} x_1^2 \\\\ x_2^2 \\\\ x_3^2 \\end{bmatrix} , \\quad \\log(\\mathbf{x}) = \\begin{bmatrix} \\log x_1 \\\\ \\log x_2 \\\\ \\log x_3 \\end{bmatrix},\\ \\text{etc.}\n\\]\nIn numpy:\n\nx ** 2, np.log(x)\n\n(array([ 4, 25,  1]), array([0.69314718, 1.60943791, 0.        ]))"
  },
  {
    "objectID": "lecture1-background/slides.html#note-on-logarithms",
    "href": "lecture1-background/slides.html#note-on-logarithms",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Note on logarithms",
    "text": "Note on logarithms\nNote that in this class (and in numpy!) logarithms are assumed to be base \\(e\\), otherwise known as natural logarithms."
  },
  {
    "objectID": "lecture1-background/slides.html#scalar-vector-operations",
    "href": "lecture1-background/slides.html#scalar-vector-operations",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Scalar-vector operations",
    "text": "Scalar-vector operations\nOperations between scalars and vectors are also assumed to be element-wise as in this scalar-vector multiplication\n\\[\na\\mathbf{x} = \\begin{bmatrix} a x_1 \\\\ a x_2 \\\\ a x_3 \\end{bmatrix}, \\quad a + \\mathbf{x} = \\begin{bmatrix} a + x_1 \\\\ a + x_2 \\\\ a + x_3 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lecture1-background/slides.html#vector-magnitude",
    "href": "lecture1-background/slides.html#vector-magnitude",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Vector magnitude",
    "text": "Vector magnitude\nThe magnitude of a vector its length in \\(\\mathbb{R}^n\\), or equivalently the Euclidean distance from the origin to the point the vector represents. It is denoted as\\(\\|\\mathbf{x}\\|_2\\) and defined as:\n\\[\n\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}\n\\]\nThe subscript \\(2\\) specifies that we are talking about Euclidean distance. Because of this we will also refer to the magnitude as the two-norm.\nIn numpy we can compute this using the np.linalg.norm function.\n\nxnorm = np.linalg.norm(x)"
  },
  {
    "objectID": "lecture1-background/slides.html#vector-magnitude-1",
    "href": "lecture1-background/slides.html#vector-magnitude-1",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Vector magnitude",
    "text": "Vector magnitude\nWe can also compute magnitude explicitly with the np.sum function, which computes the sum of the elements of a vector.\n\nxnorm_explicit = np.sqrt(np.sum(x ** 2))\nxnorm, xnorm_explicit\n\n(5.477225575051661, 5.477225575051661)\n\n\n\n\n\n\n\n\nOther useful aggregation functions\n\n\n\nprint(np.mean(x))    # Take the mean of the elements in x\nprint(np.std(x))     # Take the standard deviation of the elements in x\nprint(np.min(x))     # Find the minimum element in x\nprint(np.max(x))     # Find the maximum element in x\n\n2.6666666666666665\n1.699673171197595\n1\n5"
  },
  {
    "objectID": "lecture1-background/slides.html#unit-vectors",
    "href": "lecture1-background/slides.html#unit-vectors",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Unit vectors",
    "text": "Unit vectors\nA unit vector is a vector with length \\(1\\). We can find a unit vector that has the same direction as \\(\\mathbf{x}\\) by dividing \\(\\mathbf{x}\\) by it’s magnitude:\n\\[\n\\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|_2}\n\\]\n\nx / np.linalg.norm(x)\n\narray([0.36514837, 0.91287093, 0.18257419])"
  },
  {
    "objectID": "lecture1-background/slides.html#dot-products",
    "href": "lecture1-background/slides.html#dot-products",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Dot products",
    "text": "Dot products\nThe dot-product operation between two vectors is defined as\n\\[\n\\mathbf{x} \\cdot \\mathbf{y} = \\sum_{i=1}^n x_i y_i\n\\]\nThe result is a scalar whose value is equal to \\(\\|\\mathbf{x}\\|_2\\|\\mathbf{y}\\|_2 \\cos\\theta\\), where \\(\\theta\\) is the angle between them.\nWe can compute dot products in numpy using the np.dot function\n\nnp.dot(x, y)\n\n33"
  },
  {
    "objectID": "lecture1-background/slides.html#dot-products-1",
    "href": "lecture1-background/slides.html#dot-products-1",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Dot products",
    "text": "Dot products\nIf \\(\\theta=\\frac{\\pi}{2}\\), the vectors are orthogonal and the dot product with be \\(0\\). If \\(\\theta&gt;\\frac{\\pi}{2}\\) or \\(\\theta &lt; -\\frac{\\pi}{2}\\) , the dot product will be negative.\nIf \\(\\theta=0\\) then \\(\\cos(\\theta)=1\\). In this case we say the vectors are colinear. This formulation implies that given two vectors of fixed length, the dot product is maximized with they are colinear ( \\(\\theta=0\\) )."
  },
  {
    "objectID": "lecture1-background/slides.html#dot-products-2",
    "href": "lecture1-background/slides.html#dot-products-2",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Dot products",
    "text": "Dot products\nGeometrically, \\(\\|\\mathbf{x}\\|_2\\cos\\theta\\) is the length of the projection of \\(\\mathbf{x}\\) onto \\(\\mathbf{y}\\). Thus, we can compute the length of this projection using the dot-product as \\(\\frac{\\mathbf{x}\\cdot\\mathbf{y}}{\\|\\mathbf{y}\\|}\\) ."
  },
  {
    "objectID": "lecture1-background/slides.html#matrices",
    "href": "lecture1-background/slides.html#matrices",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Matrices",
    "text": "Matrices\nA matrix is a 2-dimensional collection of numbers. We will denote matrices using bold capital letters, e.g. \\(\\mathbf{A}\\).\n\\[\n\\mathbf{A} = \\begin{bmatrix} 3 & 5 & 4 \\\\ 1 & 1 & 2  \\end{bmatrix}\n\\]\nIn numpy we can create a matrix by passing np.array as list-of-lists, where each inner list specifies a row of the matrix.\n\nA = np.array([[3, 5, 4], [1, 1, 2]])\nA\n\narray([[3, 5, 4],\n       [1, 1, 2]])"
  },
  {
    "objectID": "lecture1-background/slides.html#matrix-elements",
    "href": "lecture1-background/slides.html#matrix-elements",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Matrix elements",
    "text": "Matrix elements\nAs with vectors we will denote individual elements of a matrix using subscripts. In this case, each element has 2 coordinates. Conventions is to always list the row first.\n\\[\n\\mathbf{A} = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\end{bmatrix}\n\\]\nIn numpy we can access elements in a matrix similarly.\n\nA[1, 2]\n\n2\n\n\nAs with vectors, we say that two matrices are equal if and only if all of the corresponding elements are equal."
  },
  {
    "objectID": "lecture1-background/slides.html#matrix-creation-in-numpy",
    "href": "lecture1-background/slides.html#matrix-creation-in-numpy",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Matrix creation in Numpy",
    "text": "Matrix creation in Numpy\nBasic creation functions\n\nA0 = np.zeros((3, 4))          # create a 3x4 matrix of all zeros\nA1 = np.ones((4, 4))           # create a 4x4 matrix of all ones\nAinf = np.full((3, 3), np.inf) # create a 3x3 matrix of all infinities\nI = np.eye(3)                  # create a 3x3 itentity matrix\n\nCreating a matrix by “reshaping” a vector with the same number of elements\n\nV = np.arange(1, 13).reshape((3, 4)) # Create a 3x4 matrix with elements 1-12\n\nCreating matrices by combining matrices\n\nB = np.tile(A0, (3, 2))               # create a matrix by \"tiling\" copies of A0 (3 copies by 2 copies)\nB = np.concatenate([A0, A1], axis=0)  # create a (2n x m) matrix by stacking two matrices vertically\nB = np.concatenate([A0, I], axis=1)  # create a (n x 2m) matrix by stacking two matrices horizontally"
  },
  {
    "objectID": "lecture1-background/slides.html#slicing-matrices",
    "href": "lecture1-background/slides.html#slicing-matrices",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Slicing matrices",
    "text": "Slicing matrices\nOften we will refer to an entire row of a matrix (which is itself a vector) using matrix notation with a single index\n\\[\n\\mathbf{A}_i = \\begin{bmatrix} A_{i1} \\\\ A_{i2} \\\\ \\vdots \\end{bmatrix}\n\\]\nIn numpy we can access a single row similarly.\n\nA[1]\n\narray([1, 1, 2])"
  },
  {
    "objectID": "lecture1-background/slides.html#slicing-matrices-1",
    "href": "lecture1-background/slides.html#slicing-matrices-1",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Slicing matrices",
    "text": "Slicing matrices\nWe can refer to an entire column by replacing the row index with a \\(*\\), indicating we are referring to all rows in that column.\n\\[\n\\mathbf{A}_{*i} = \\begin{bmatrix} A_{1i} \\\\ A_{2i} \\\\ \\vdots \\end{bmatrix}\n\\]\nIn numpy we can access an entire column (or row) using the slice operator :, which takes all elements along an axis.\n\nA[:, 1] # Take all elements in column 1\n\narray([5, 1])"
  },
  {
    "objectID": "lecture1-background/slides.html#slicing-matrices-2",
    "href": "lecture1-background/slides.html#slicing-matrices-2",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Slicing matrices",
    "text": "Slicing matrices\nWe can also use the slice operator to take a subset of elements along an axis.\n\nA[:, 1:3] #Take the second and third columns of A\n\narray([[5, 4],\n       [1, 2]])"
  },
  {
    "objectID": "lecture1-background/slides.html#advanced-slicing-in-numpy",
    "href": "lecture1-background/slides.html#advanced-slicing-in-numpy",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Advanced slicing in numpy",
    "text": "Advanced slicing in numpy\nOpen-ended slices\n\nprint(\"A[:2]=\", A[:2]) # Take rows up to 2 (not inucluding 2)\nprint(\"A[1:]=\", A[1:]) # Take rows starting at (and including) 1\n\nA[:2]= [[1 2 3 4]\n [5 6 7 8]]\nA[1:]= [[ 5  6  7  8]\n [ 9 10 11 12]]\n\n\nTaking rows and colmns\n\nprint(\"A[0, :]=\", A[0, :])   # first row, all columns\nprint(\"A[:, 1]=\", A[:, 1])   # all rows, second column\nprint(\"A[-1, :]\", A[-1, :])  # all columns of the last row\nprint(\"A[:, -2]\", A[:, -2])  # second to last column of every row\n\nA[0, :]= [1 2 3 4]\nA[:, 1]= [ 2  6 10]\nA[-1, :] [ 9 10 11 12]\nA[:, -2] [ 3  7 11]"
  },
  {
    "objectID": "lecture1-background/slides.html#advanced-slicing-in-numpy-1",
    "href": "lecture1-background/slides.html#advanced-slicing-in-numpy-1",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Advanced slicing in numpy",
    "text": "Advanced slicing in numpy\nMore general slicing with steps\n\nprint(\"A[1,0:2]=\", A[1,0:2])\nprint(\"A[0,0:4:2]=\", A[0,0:4:2])\nprint(\"A[:,0:4:2]=\\n\", A[:,0:4:2])\n\nA[1,0:2]= [5 6]\nA[0,0:4:2]= [1 3]\nA[:,0:4:2]=\n [[ 1  3]\n [ 5  7]\n [ 9 11]]\n\n\nTaking one row and selected columns\n\nprint(\"A[2, [1,4]]=\",A[2, [0,3]])\n\nA[2, [1,4]]= [ 9 12]\n\n\nTaking all rows and selected columns\n\nprint(\"A[:, [1,4]]=\\n\",A[:, [0,3]])\n\nA[:, [1,4]]=\n [[ 1  4]\n [ 5  8]\n [ 9 12]]"
  },
  {
    "objectID": "lecture1-background/slides.html#matrix-shapes",
    "href": "lecture1-background/slides.html#matrix-shapes",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Matrix shapes",
    "text": "Matrix shapes\nThe matrix \\(\\mathbf{A}\\) above has 2 rows and 3 columns, thus we would specify it’s shape as \\(2\\times3\\). A square matrix has the same number of rows and columns.\n\\[\n\\mathbf{A} = \\begin{bmatrix} 3 & 5 & 4 \\\\ 1 & 1 & 2 \\\\ 4 & 1 & 3 \\end{bmatrix}\n\\]\nWe can access the shape of a matrix in numpy using its shape property.\n\nA = np.array([[3, 5, 4], [1, 1, 2], [4, 1, 3]])\nprint(A.shape)\n\n(3, 3)"
  },
  {
    "objectID": "lecture1-background/slides.html#matrix-transpose",
    "href": "lecture1-background/slides.html#matrix-transpose",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Matrix transpose",
    "text": "Matrix transpose\nThe transpose of a matrix is an operation that swaps the rows and columns of the matrix. We denote the transpose of a matrix \\(\\mathbf{A}\\) as \\(\\mathbf{A}^T\\).\n\\[\n\\mathbf{A} = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\end{bmatrix}, \\quad \\mathbf{A}^T = \\begin{bmatrix} A_{11} & A_{21} \\\\  A_{12} & A_{22} \\\\  A_{13} & A_{23} \\end{bmatrix}\n\\]\nIn numpy we can transpose a matrix by using the T property.\n\nA = np.array([[1, 1, 1], [2, 2, 2]])\nA.T\n\narray([[1, 2],\n       [1, 2],\n       [1, 2]])"
  },
  {
    "objectID": "lecture1-background/slides.html#element-wise-matrix-operations",
    "href": "lecture1-background/slides.html#element-wise-matrix-operations",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Element-wise matrix operations",
    "text": "Element-wise matrix operations\nAs with vectors, many operations on matrices are performed element-wise:\n\\[\n\\mathbf{A} + \\mathbf{B} = \\begin{bmatrix} A_{11} + B_{11}  & A_{12} + B_{12} & A_{13} + B_{13} \\\\ A_{21} + B_{21} & A_{22} + B_{22} & A_{23} + B_{23} \\end{bmatrix}, \\quad \\log\\mathbf{A} = \\begin{bmatrix} \\log A_{11}  & \\log A_{12} & \\log A_{13} \\\\ \\log A_{21} & \\log A_{22} & \\log A_{23} \\end{bmatrix}\n\\]\nScalar-matrix operation are also element-wise:\n\\[\nc\\mathbf{A} = \\begin{bmatrix} cA_{11}  & cA_{12} & cA_{13} \\\\ cA_{21} & cA_{22} & cA_{23} \\end{bmatrix}\n\\]\nIn numpy:\n\nB = 5 * A \nA + B\n\narray([[ 6,  6,  6],\n       [12, 12, 12]])"
  },
  {
    "objectID": "lecture1-background/slides.html#matrix-vector-products",
    "href": "lecture1-background/slides.html#matrix-vector-products",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Matrix-vector products",
    "text": "Matrix-vector products\nA matrix-vector product is an operation between a matrix and a vector that produces a new vector. Given a matrix \\(\\mathbf{A}\\) and a vector \\(\\mathbf{x}\\), we write the matrix-vector product as:\n\\[\n\\mathbf{A}\\mathbf{x} = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\\\ A_{23} & A_{32} & A_{33} \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\end{bmatrix} = \\begin{bmatrix} \\sum_{i=1}^n x_i A_{1i} \\\\  \\sum_{i=1}^n x_i A_{2i} \\\\  \\sum_{i=1}^n x_i A_{3i} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{x}\\cdot  \\mathbf{A}_{1} \\\\ \\mathbf{x}\\cdot  \\mathbf{A}_{2} \\\\ \\mathbf{x} \\cdot \\mathbf{A}_{2} \\end{bmatrix}\n\\]\nIn other words, each entry of the resulting vector is the dot product between \\(\\mathbf{x}\\) and a row of \\(A\\). In numpy we also use np.dot for matrix-vector products:\n\nA = np.array([[3, 5, 4], [1, 1, 2], [4, 1, 3]])\nx = np.array([2, 5, 1])\n\nnp.dot(A, x)\n\narray([35,  9, 16])\n\n\nGeometrically the matrix \\(A\\) defines a transformation that scales and rotates any vector \\(\\mathbf{x}\\) about the origin."
  },
  {
    "objectID": "lecture1-background/slides.html#matrix-vector-products-1",
    "href": "lecture1-background/slides.html#matrix-vector-products-1",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Matrix-vector products",
    "text": "Matrix-vector products\nThe number of columns of \\(A\\) must match the size of the vector \\(\\mathbf{x}\\), but if \\(A\\) has a different number of rows, the output will simply have a different size.\n\nnp.dot(A[:2], x)\n\narray([35,  9])"
  },
  {
    "objectID": "lecture1-background/slides.html#matrix-multiplication",
    "href": "lecture1-background/slides.html#matrix-multiplication",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\nMatrix multiplication is a fundamental operation between two matrices. It is defined as:\n\\[\n\\mathbf{A}\\mathbf{B}  = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\\\ A_{23} & A_{32} & A_{33} \\end{bmatrix} \\begin{bmatrix} B_{11} & B_{12} \\\\ B_{21} & B_{22} \\\\ B_{31} & B_{32} \\end{bmatrix}  =\\begin{bmatrix} \\sum_{i=1}^n A_{1i} B_{i1} & \\sum_{i=1}^n A_{i1}B_{i2} \\\\  \\sum_{i=1}^n A_{2i}B_{i1} & \\sum_{i=1}^n A_{2i}B_{i2}  \\\\  \\sum_{i=1}^n A_{3i}B_{i1} & \\sum_{i=1}^n A_{3i}B_{i2} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*2} \\\\ \\mathbf{A}_{2} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{2} \\cdot \\mathbf{B}_{*2}  \\\\ \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*2}  \\end{bmatrix}\n\\]\nThis means that if we multiply an \\(n\\times m\\) matrix \\(\\mathbf{A}\\) with an \\(m\\times k\\) matrix \\(\\mathbf{B}\\) we get an \\(n\\times k\\) matrix \\(\\mathbf{C}\\), such that \\(\\mathbf{C}_{ij}\\) is the dot product of the \\(i\\)-th row of \\(\\mathbf{A}\\) with the \\(j\\)-th row of \\(\\mathbf{B}\\).\nIn numpy we once again use the np.dot function to perform matrix-multiplications.\n\nB = np.array([[2, -1], [3, 1], [-2, 5]])\nC = np.dot(A, B)"
  },
  {
    "objectID": "lecture1-background/slides.html#matrix-multiplication-1",
    "href": "lecture1-background/slides.html#matrix-multiplication-1",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\nThe number of rows of \\(\\mathbf{A}\\) must match the number of columns of \\(\\mathbf{B}\\) for the matrix multiplication \\(\\mathbf{A}\\mathbf{B}\\) to be defined. This implies that matrix multiplication is non-communitive:\n\\[\n\\mathbf{A}\\mathbf{B}\\neq \\mathbf{B}\\mathbf{A}\n\\]\nHowever matrix multiplication is associative and distributive:\n\\[\n\\mathbf{A}(\\mathbf{B}\\mathbf{C})=(\\mathbf{A}\\mathbf{B})\\mathbf{C}, \\quad \\mathbf{A}(\\mathbf{B} +\\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{A}\\mathbf{C}\n\\]"
  },
  {
    "objectID": "lecture1-background/slides.html#matrix-multiplication-2",
    "href": "lecture1-background/slides.html#matrix-multiplication-2",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\nMatrix multiplication is a composition of linear maps, meaning that if we take the product \\(\\mathbf{A}\\mathbf{B}\\) and apply the resulting matrix to a vector \\(\\mathbf{x}\\), it is equivalent to first transforming \\(\\mathbf{x}\\) with \\(\\mathbf{B}\\) and then with \\(\\mathbf{A}\\). We can state this succinctly as:\n\\[\n(\\mathbf{A}\\mathbf{B})\\mathbf{x} = \\mathbf{A}(\\mathbf{B}\\mathbf{x})\n\\]\nWe can see this in numpy:\n\nx = np.array([1, 3])\nnp.dot(np.dot(A, B), x), np.dot(A, np.dot(B, x))\n\n(array([79, 31, 41]), array([79, 31, 41]))"
  },
  {
    "objectID": "lecture1-background/slides.html#element-wise-multiplication",
    "href": "lecture1-background/slides.html#element-wise-multiplication",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Element-wise multiplication",
    "text": "Element-wise multiplication\nIt is important to note that in numpy the * operator does not perform matrix multiplication, instead it performs element-wise multiplication for both matrices and vectors.\n\nA = np.array([[1, 1], [2, 2], [3, 3]])\nB = np.array([[1, 2], [1, 2], [1, 2]])\nA * B\n\narray([[1, 2],\n       [2, 4],\n       [3, 6]])\n\n\nIn mathematical notation we will denote element-wise multiplication as:\n\\[\n\\mathbf{A} \\odot \\mathbf{B} = \\begin{bmatrix} A_{11} B_{11}  & A_{12}  B_{12} & A_{13}  B_{13} \\\\ A_{21}  B_{21} & A_{22}  B_{22} & A_{23}  B_{23} \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lecture1-background/slides.html#matrix-reductions",
    "href": "lecture1-background/slides.html#matrix-reductions",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Matrix reductions",
    "text": "Matrix reductions\nAs we saw with vectors, we can take the sum of the elements of a matrix using the np.sum function:\n\nnp.sum(A)\n\n12\n\n\nThe result is a scalar of the form:\n\\[\n\\text{sum}(\\mathbf{A}) = \\sum_{i=1}^n\\sum_{j=1}^m A_{ij}\n\\]"
  },
  {
    "objectID": "lecture1-background/slides.html#matrix-reductions-1",
    "href": "lecture1-background/slides.html#matrix-reductions-1",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Matrix reductions",
    "text": "Matrix reductions\nIn many cases we may wish to take the sum along an axis of a matrix. This operation results in a vector where each entry is the sum of elements from the corresponding row or column of the matrix.\n\\[\n\\text{rowsum}(\\mathbf{A}) = \\begin{bmatrix} \\sum_{i=1}^n A_{i1} \\\\ \\sum_{i=1}^n A_{i2} \\\\ \\sum_{i=1}^n A_{i3} \\\\ \\vdots\\end{bmatrix}, \\quad \\text{colsum}(\\mathbf{A}) = \\begin{bmatrix} \\sum_{j=1}^m A_{1j} \\\\ \\sum_{j=1}^m A_{2j} \\\\ \\sum_{j=1}^m A_{3j} \\\\ \\vdots\\end{bmatrix}\n\\]\nIn numpy we can specify a sum along an axis by providing an axis argument to np.sum. Setting axis=0 specifies a row-sum, while axis=1 specifies a column sum.\n\nA = np.array([[1, 1], [2, 2], [3, 3]])\nprint(A)\nnp.sum(A, axis=0), np.sum(A, axis=1)\n\n[[1 1]\n [2 2]\n [3 3]]\n\n\n(array([6, 6]), array([2, 4, 6]))"
  },
  {
    "objectID": "lecture1-background/slides.html#other-matrix-reduction-examples",
    "href": "lecture1-background/slides.html#other-matrix-reduction-examples",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Other matrix reduction examples",
    "text": "Other matrix reduction examples\n\nprint(np.mean(A))            # Take the mean of all elements in x\nprint(np.std(A, axis=0))     # Take the standard deviation of each column of x\nprint(np.min(A, axis=1))     # Find the minimum element in each row of x\nprint(np.max(A))             # Find the maximum element in x\n\n2.0\n[0.81649658 0.81649658]\n[1 2 3]\n3"
  },
  {
    "objectID": "lecture1-background/slides.html#identity-matrices",
    "href": "lecture1-background/slides.html#identity-matrices",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Identity Matrices",
    "text": "Identity Matrices\nThe identity matrix, denoted at \\(\\mathbf{I}\\) is a special type of square matrix. It is defined as the matrix with \\(1\\) for every diagonal element (\\(\\mathbf{I}_{i=j}=1\\)) and \\(0\\) for every other element (\\(\\mathbf{I}_{i\\neq j}=0\\)). A \\(3\\times 3\\) identity matrix looks like:\n\\[\\mathbf{I} = \\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}\\]\nThe identify matrix has the unique property that any appropriately sized matrix (or vector) multiplied with \\(\\mathbf{I}\\) will equal itself:\n\\[\n\\mathbf{I}\\mathbf{A} = \\mathbf{A}\n\\]\nIf we think of matrices as linear mappings the identity matrix simply maps any vector to itself.\n\\[\n\\mathbf{I} \\mathbf{x} = \\mathbf{x}\n\\]\nIn numpy we can create an identity matrix using the np.eye function:\n\nI = np.eye(3) # Create a 3x3 identity matrix"
  },
  {
    "objectID": "lecture1-background/slides.html#solving-systems-of-linear-equations",
    "href": "lecture1-background/slides.html#solving-systems-of-linear-equations",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Solving systems of linear equations",
    "text": "Solving systems of linear equations\nConsider the matrix-vector product between a matrix \\(\\mathbf{A}\\) and a vector \\(\\mathbf{x}\\), we can denote the result of this multiplication as \\(\\mathbf{b}\\):\n\\[\n\\mathbf{A}\\mathbf{x} =\\mathbf{b}\n\\]\nIn many common cases we will know the matrix \\(\\mathbf{A}\\) and the vector \\(\\mathbf{b}\\), but not the vector \\(\\mathbf{x}\\). In such cases we need to solve this equation for \\(\\mathbf{x}\\):\n\\[\n\\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\\\ A_{23} & A_{32} & A_{33} \\end{bmatrix} \\begin{bmatrix} \\textbf{?}\\\\ \\textbf{?}\\\\ \\textbf{?}\\end{bmatrix} = \\begin{bmatrix} b_1 \\\\  b_2 \\\\  b_3 \\end{bmatrix}\n\\]\nIn Numpy:\n\nA = np.array([[3, 1], [-1, 4]])\nb = np.array([-2, 1])\nx = np.linalg.solve(A, b)\n\nNote that in some cases there may not be any \\(\\mathbf{x}\\) that satisfies the equation (or there may be infinitely many). The conditions for this are beyond the scope of this course."
  },
  {
    "objectID": "lecture1-background/slides.html#inverse-matrices",
    "href": "lecture1-background/slides.html#inverse-matrices",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Inverse matrices",
    "text": "Inverse matrices\nThe inverse of a square matrix is denoted by \\(\\mathbf{A}^{-1}\\) and is defined as the matrix such that:\n\\[\n\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n\\]\nThis corresponds to the inverse of the linear map defined by \\(\\mathbf{A}\\). Any vector transformed by \\(\\mathbf{A}\\) can be transformed back by applying the inverse matrix:\n\\[\n\\mathbf{A}^{-1}\\left(\\mathbf{A}\\mathbf{x}\\right) =\\mathbf{x}\n\\]\nWe can also write the solution to a system of linear equations in terms of the inverse, by multiplying both sides by \\(\\mathbf{A}^{-1}\\):\n\\[\n\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\quad \\longrightarrow \\quad \\mathbf{A}^{-1}(\\mathbf{A}\\mathbf{x})=\\mathbf{A}^{-1}\\mathbf{b} \\quad \\longrightarrow \\quad \\mathbf{x}=\\mathbf{A}^{-1}\\mathbf{b}\n\\]\nIn numpy we can find the inverse of a matrix using the np.linalg.inv function:\n\nA_inv = np.linalg.inv(A)\n\nNote that not every matrix has an inverse! Again, we won’t worry about these cases for now."
  },
  {
    "objectID": "lecture1-background/slides.html#functions",
    "href": "lecture1-background/slides.html#functions",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Functions",
    "text": "Functions\nA function is a general mapping from one set to another.\n\\[\ny=f(x),\\quad f:\\mathbb{R}\\rightarrow\\mathbb{R}\n\\]\nWe can definite functions as compositions of simple operations. For example we could define a polynomial function as:\n\\[\nf(x) = x^2 + 3x + 1\n\\]\nIn code we can implement functions as, well, functions:\n\ndef f(x):\n    return x ** 2 + 3 * x + 1\n\nf(5)\n\n41"
  },
  {
    "objectID": "lecture1-background/slides.html#derivatives",
    "href": "lecture1-background/slides.html#derivatives",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Derivatives",
    "text": "Derivatives\nThe derivative of a function at input \\(x\\) defines how the function’s output changes as the input changes from \\(x\\). It is equivalent to the slope of the line tangent to the function at the input \\(x\\). We’ll use the notation \\(\\frac{df}{dx}\\) to denote the derivative of the function \\(f\\) at input \\(x\\). Formally:\n\\[\n\\frac{df}{dx} = \\underset{\\epsilon\\rightarrow0}{\\lim} \\frac{f(x+\\epsilon) - f(x)}{\\epsilon}\n\\]\nIntunitively, this means if we change our input \\(x\\) by some small amount \\(\\epsilon\\), the output of our function will change by approximately \\(\\frac{df}{dx}\\epsilon\\)\n\\[\nf(x+\\epsilon) \\approx f(x)+\\frac{df}{dx}\\epsilon\n\\]"
  },
  {
    "objectID": "lecture1-background/slides.html#derivative-operator",
    "href": "lecture1-background/slides.html#derivative-operator",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Derivative operator",
    "text": "Derivative operator\nWe can also use the notation \\(\\frac{d}{dx}\\) to denote the derivative operator. This means “find the derivative of the following expression with respect to \\(x\\)”.\n\\[\n\\frac{d}{dx}f(x) = \\frac{df}{dx}\n\\]"
  },
  {
    "objectID": "lecture1-background/slides.html#derivative-functions",
    "href": "lecture1-background/slides.html#derivative-functions",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Derivative functions",
    "text": "Derivative functions\nWe can also talk about the function that maps any input \\(x\\) to the derivative \\(\\frac{df}{dx}\\) we call this the derivative function and denote it as \\(f'(x)\\). So:\n\\[\n\\frac{df}{dx}=f'(x)\n\\]\nGiven a function defined as a composition of basic operations, we can use a set of standard rules to find the corresponding derivative function. For example using the rules \\(\\frac{d}{dx}x^a=ax\\) , \\(\\frac{d}{dx}ax=a\\) and \\(\\frac{d}{dx}a=0\\), we can derive the derivative function for the polynomial above:\n\\[\nf(x) = x^2 + 3x + 1\n\\]\n\\[\nf'(x) = 2x + 3\n\\]"
  },
  {
    "objectID": "lecture1-background/slides.html#basic-derivative-rules",
    "href": "lecture1-background/slides.html#basic-derivative-rules",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Basic derivative rules",
    "text": "Basic derivative rules\n\n\n\nOperation\nDerivative \\(\\frac{d}{dx}\\)\n\n\n\n\n\\(a\\)\n\\(0\\)\n\n\n\\(ax\\)\n\\(a\\)\n\n\n\\(x^a\\)\n\\(ax\\)\n\n\n\\(\\log(x)\\)\n\\(\\frac{1}{x}\\)\n\n\n\\(e^x\\)\n\\(e^x\\)\n\n\n\\(f(x) + g(x)\\)\n\\(f'(x)+g'(x)\\)\n\n\n\\(f(x)g(x)\\)\n\\(f'(x)g(x) + f(x)g'(x)\\)\n\n\n\\(\\frac{f(x)}{g(x)}\\)\n\\(\\frac{f'(x)g(x)-f(x)g'(x)}{g(x)^2}\\)"
  },
  {
    "objectID": "lecture1-background/slides.html#compositions-of-functions",
    "href": "lecture1-background/slides.html#compositions-of-functions",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Compositions of functions",
    "text": "Compositions of functions\nComposing two functions means to apply one function to the output of another, for example we could apply \\(f\\) to the output of \\(g\\):\n\\[\ny = f\\big(g\\left(x\\right)\\big)\n\\]\nThis is easily replicated in code:\n\ndef f(x):\n    return x ** 2 + 3 * x + 1\n\ndef g(x):\n    return 5 * x - 2\n\nf(g(3))\n\n209"
  },
  {
    "objectID": "lecture1-background/slides.html#chain-rule",
    "href": "lecture1-background/slides.html#chain-rule",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Chain rule",
    "text": "Chain rule\nThe chain rule tells us how to find the derivative of a composition of functions like this. We can write the rule either in terms of derivatives or derivative functions\n\\[\n\\frac{d}{dx}f\\big(g\\left(x\\right)\\big) = \\frac{df}{dg}\\frac{dg}{dx} \\quad \\text{or} \\quad \\frac{d}{dx}f\\big(g\\left(x\\right)\\big) =  f'\\big(g\\left(x\\right)\\big)g'\\left(x\\right)\n\\]\nNote that in our derivative notation we’re using \\(f\\) and \\(g\\) to denote the outputs of the respective functions."
  },
  {
    "objectID": "lecture1-background/slides.html#multivariate-functions",
    "href": "lecture1-background/slides.html#multivariate-functions",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Multivariate functions",
    "text": "Multivariate functions\nA function does not need to be restricted to having a single input. We can specify a function with multiple inputs as follows:\n\\[\nf(x, y, z) = x^2 + 3xy - \\log(z)\n\\]\nIn code this would look like;\n\ndef f(x, y, z):\n    return x ** 2 + 3 * y + np.log(z)"
  },
  {
    "objectID": "lecture1-background/slides.html#partial-derivatives",
    "href": "lecture1-background/slides.html#partial-derivatives",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Partial derivatives",
    "text": "Partial derivatives\nA partial derivative is the derivative of a multiple-input function with respect to a single input, assuming all other inputs are constant.\n\\[\n\\frac{\\partial f}{\\partial x} = \\underset{\\epsilon\\rightarrow0}{\\lim} \\frac{f(x+\\epsilon, y, z) - f(x,y,z)}{\\epsilon}, \\quad \\frac{\\partial f}{\\partial y} = \\underset{\\epsilon\\rightarrow0}{\\lim} \\frac{f(x, y+\\epsilon, z) - f(x,y,z)}{\\epsilon}\n\\]\nThese partial derivatives tell us how the output of the function changes as we change each of the inputs individually."
  },
  {
    "objectID": "lecture1-background/slides.html#partial-derivative-functions",
    "href": "lecture1-background/slides.html#partial-derivative-functions",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Partial derivative functions",
    "text": "Partial derivative functions\nWe can also specify partial derivative functions in the same way as derivative functions. We’ll use subscript notation to specify which input we are differentiating with respect to.\n\\[\n\\frac{\\partial f}{\\partial x} = f_x'(x, y, z)\n\\]\nWe can derive partial derivative functions using the same set of derivative rules:\n\\[\nf(x, y, z) = x^2 + 3xy - \\log(z)\n\\]\n\\[\nf_x'(x, y, z) = 2x + 3y\n\\]\n\\[\nf_y'(x, y, z) = 3x\n\\]\n\\[\nf_z'(x, y, z) = -\\frac{1}{z}\n\\]"
  },
  {
    "objectID": "lecture1-background/slides.html#functions-of-vectors",
    "href": "lecture1-background/slides.html#functions-of-vectors",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Functions of vectors",
    "text": "Functions of vectors\nWe can also define functions that take vectors (or matrices) as inputs.\n\\[\ny = f(\\mathbf{x}) \\quad f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\n\\]\nHere \\(f\\) is a mapping from length \\(n\\) vectors to real numbers. As a concrete example we could define the function:\n\\[\nf(\\mathbf{x}) = \\sum_{i=1}^n x_i^3 + 1\n\\]\nHere’s the same function in numpy:\n\ndef f(x):\n    return np.sum(x ** 3) + 1\n\nf(np.array([1, 2, 3]))\n\n37\n\n\nNote that functions of vectors are equivalent to multiple-input functions, but with a more compact notation!"
  },
  {
    "objectID": "lecture1-background/slides.html#gradients",
    "href": "lecture1-background/slides.html#gradients",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Gradients",
    "text": "Gradients\nThe gradient of a vector-input function is a vector such that each element is the partial derivative of the function with respect to the corresponding element of the input vector. We’ll use the same notation as derivatives for gradients.\n\\[\n\\frac{df}{d\\mathbf{x}} = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\frac{\\partial f}{\\partial x_3} \\\\ \\vdots \\end{bmatrix}\n\\]\nThe gradient is a vector that tangent to the function \\(f\\) at the input \\(\\mathbf{x}\\). Just as with derivatives, this means that the gradient defines a linear approximation to the function at the point \\(\\mathbf{x}\\).\n\\[\nf(\\mathbf{x}+\\mathbf{\\epsilon}) \\approx f(\\mathbf{x}) + \\frac{df}{d\\mathbf{x}} \\cdot \\mathbf{\\epsilon}\n\\]"
  },
  {
    "objectID": "lecture1-background/slides.html#gradient-functions",
    "href": "lecture1-background/slides.html#gradient-functions",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Gradient functions",
    "text": "Gradient functions\nJust as with derivatives and partial derivatives, we can define a gradient function that maps an input vector \\(\\mathbf{x}\\) to the gradient of the function \\(f\\) at \\(\\mathbf{x}\\) as:\n\\[\n\\frac{df}{d\\mathbf{x}}=\\nabla f(\\mathbf{x})\n\\]\nHere \\(\\nabla f\\) is the gradient function for \\(f\\). If the function takes multiple vectors as input, we can specify the gradient function with respect to a particular input using subscript notation:\n\\[\n\\frac{df}{d\\mathbf{x}}= \\nabla_{\\mathbf{x}} f(\\mathbf{x}, \\mathbf{y}), \\quad \\frac{df}{d\\mathbf{y}}= \\nabla_{\\mathbf{y}} f(\\mathbf{x}, \\mathbf{y})\n\\]"
  },
  {
    "objectID": "lecture1-background/slides.html#getting-started",
    "href": "lecture1-background/slides.html#getting-started",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Getting started",
    "text": "Getting started\nThe standard way to import MatPlotLib is:\n\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "lecture1-background/slides.html#scatterplots",
    "href": "lecture1-background/slides.html#scatterplots",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Scatterplots",
    "text": "Scatterplots\nPlotting a scatter of data points:\n\nx_values = np.random.rand(1,10)   # unformly in [0,1)\ny_values = np.random.randn(1,10)  # Gaussian distribution\nplt.plot(x_values, y_values, 'ko');\n\n\n\n\nThe string determines the plot appearance -- in this case, black circles. You can use color strings (‘r’, ‘g’, ‘b’, ‘m’, ‘c’, ‘y’, ...) or use the “Color” keyword to specify an RGB color. Marker appearance (‘o’,‘s’,‘v’,‘.’, ...) controls how the points look."
  },
  {
    "objectID": "lecture1-background/slides.html#line-plots",
    "href": "lecture1-background/slides.html#line-plots",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Line plots",
    "text": "Line plots\nIf we connect points using a line appearance specification (‘-’,‘--’,‘:’,...), it will not look very good, because the points are not ordered in any meaningful way. Let’s try a line plot using an ordered sequence of x values:\n\nx_values = np.linspace(0,8,100)\ny_values = np.sin(x_values)\nplt.plot(x_values,y_values,'b');\n\n\n\n\nThis is actually a plot of a large number of points (100), with no marker shape and connected by a solid line."
  },
  {
    "objectID": "lecture1-background/slides.html#multiple-plots",
    "href": "lecture1-background/slides.html#multiple-plots",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Multiple plots",
    "text": "Multiple plots\nFor plotting multiple point sets or curves, you can pass more vectors into the plot function, or call the function multiple times:\n\nx_values = np.linspace(0,8,100)\ny1 = np.sin(x_values)         # sinusoidal function\ny2 = (x_values - 3)**2 / 12   # a simple quadratic curve\ny3 = 0.5*x_values - 1.0       # a simple linear function\n\nplt.plot(x_values, y1, 'b-', x_values, y2, 'g--');  # plot two curves\nplt.plot(x_values, y3, 'r:'); # add a curve to the plot"
  },
  {
    "objectID": "lecture1-background/slides.html#plot-ranges",
    "href": "lecture1-background/slides.html#plot-ranges",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Plot ranges",
    "text": "Plot ranges\nYou may want to explicitly set the plot ranges -- perhaps the most common pattern is to plot something, get the plot’s ranges, and then restore them later after plotting another function:\n\nx_values = np.linspace(0,8,100)\ny1 = np.sin(x_values)         # sinusoidal function\ny3 = 0.5*x_values - 1.0       # a simple linear function\n\nplt.plot(x_values, y1, 'b-') \nax = plt.axis()               # get the x and y axis ranges\nprint(ax)\n# now plot something else (which will change the axis ranges):\nplt.plot(x_values, y3, 'r:'); # add the linear curve\nplt.axis(ax);                 # restore the original plot's axis ranges\n\n(-0.4, 8.4, -1.099652011574681, 1.0998559934443881)"
  },
  {
    "objectID": "lecture1-background/slides.html#histograms",
    "href": "lecture1-background/slides.html#histograms",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Histograms",
    "text": "Histograms\nHistograms are also useful visualizations:\n\nplt.hist(y2, bins=20);\n\n\n\n\nThe outputs of hist include the bin locations, the number of data in each bin, and the “handles” to the plot elements to manipulate their appearance, if desired."
  },
  {
    "objectID": "lecture1-background/slides.html#subplots-and-plot-sizes",
    "href": "lecture1-background/slides.html#subplots-and-plot-sizes",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Subplots and plot sizes",
    "text": "Subplots and plot sizes\nIt is often useful to put more than one plot together in a group; you can do this using the subplot function. There are various options; for example, “sharex” and “sharey” allow multiple plots to share a single axis range (or, you can set it manually, of course).\n\nfig,ax = plt.subplots(1,3, figsize=(8.0, 2.0))      # make a 1 x 3 grid of plots:\nax[0].plot(x_values, y1, 'b-');   # plot y1 in the first subplot\nax[1].plot(x_values, y2, 'g--');  #   y2 in the 2nd\nax[2].plot(x_values, y3, 'r:');   #   and y3 in the last"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html",
    "href": "lecture3-logistic-regression/notes.html",
    "title": "Lecture 3: Logistic regression",
    "section": "",
    "text": "Manim Community v0.17.3"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#partial-derivatives",
    "href": "lecture3-logistic-regression/notes.html#partial-derivatives",
    "title": "Lecture 3: Logistic regression",
    "section": "Partial derivatives",
    "text": "Partial derivatives\nA function does not need to be restricted to having a single input. We can specify a function with multiple inputs as follows:\n\\[\nf(x, y, z) = x^2 + 3xy - \\log(z)\n\\]\nIn code this would look like;\n\ndef f(x, y, z):\n    return x ** 2 + 3 * y + np.log(z)\n\nA partial derivative is the derivative of a multiple-input function with respect to a single input, assuming all other inputs are constant. We will explore the implications of that condition later on in this course. For now, we will simply view partial derivatives as a straightforward extension of derivatives, using the modified notation \\(\\frac{\\partial}{\\partial x}\\).\nMore formally, we can define the partial derivative with respect to each input of a function as:\n\\[\n\\frac{\\partial f}{\\partial x} = \\underset{\\epsilon\\rightarrow0}{\\lim} \\frac{f(x+\\epsilon, y, z) - f(x,y,z)}{\\epsilon}, \\quad \\frac{\\partial f}{\\partial y} = \\underset{\\epsilon\\rightarrow0}{\\lim} \\frac{f(x, y+\\epsilon, z) - f(x,y,z)}{\\epsilon}\n\\]\nThese partial derivatives tell us how the output of the function changes as we change each of the inputs individually."
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#partial-derivative-functions",
    "href": "lecture3-logistic-regression/notes.html#partial-derivative-functions",
    "title": "Lecture 3: Logistic regression",
    "section": "Partial derivative functions",
    "text": "Partial derivative functions\nWe can also specify partial derivative functions in the same way as derivative functions. We’ll use subscript notation to specify which input we are differentiating with respect to.\n\\[\n\\frac{\\partial f}{\\partial x} = f_x'(x, y, z)\n\\]\nWe can derive partial derivative functions using the same set of derivative rules:\n\\[\nf(x, y, z) = x^2 + 3xy - \\log(z)\n\\]\n\\[\nf_x'(x, y, z) = 2x + 3y\n\\]\n\\[\nf_y'(x, y, z) = 3x\n\\]\n\\[\nf_z'(x, y, z) = -\\frac{1}{z}\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#functions-of-vectors",
    "href": "lecture3-logistic-regression/notes.html#functions-of-vectors",
    "title": "Lecture 3: Logistic regression",
    "section": "Functions of vectors",
    "text": "Functions of vectors\nWe can also define functions that take vectors (or matrices) as inputs.\n\\[\ny = f(\\mathbf{x}) \\quad f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\n\\]\nHere \\(f\\) is a mapping from length \\(n\\) vectors to real numbers. As a concrete example we could define the function:\n\\[\nf(\\mathbf{x}) = \\sum_{i=1}^n x_i^3 + 1\n\\]\nHere’s the same function in numpy:\n\ndef f(x):\n    return np.sum(x ** 3) + 1\n\nf(np.array([1, 2, 3]))\n\n37\n\n\nNote that functions of vectors are equivalent to multiple-input functions, but with a more compact notation!"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#gradients",
    "href": "lecture3-logistic-regression/notes.html#gradients",
    "title": "Lecture 3: Logistic regression",
    "section": "Gradients",
    "text": "Gradients\nThe gradient of a vector-input function is a vector such that each element is the partial derivative of the function with respect to the corresponding element of the input vector. We’ll use the same notation as derivatives for gradients.\n\\[\n\\frac{df}{d\\mathbf{x}} = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\frac{\\partial f}{\\partial x_3} \\\\ \\vdots \\end{bmatrix}\n\\]\nThe gradient is a vector that tangent to the function \\(f\\) at the input \\(\\mathbf{x}\\). Just as with derivatives, this means that the gradient defines a linear approximation to the function at the point \\(\\mathbf{x}\\).\n\\[\nf(\\mathbf{x}+\\mathbf{\\epsilon}) \\approx f(\\mathbf{x}) + \\frac{df}{d\\mathbf{x}} \\cdot \\mathbf{\\epsilon}\n\\]\nWhere \\(\\mathbf{\\epsilon}\\) is now a small vector. Intuitively, this means that if we take a small step in any direction as defined by \\(\\mathbf{\\epsilon}\\), the gradient will approximate the change in the output of the function. Becuase we are now in more than 1 dimension, this approximation defines a plane in \\(\\mathbb{R}^n\\).\nAnother extremely important property of the gradient is that it points in the direction of maximum change in the function. Meaning that if we were to take an infinitesimal step \\(\\mathbf{\\epsilon}\\) from \\(\\mathbf{x}\\) in any direction, stepping in the gradient direction would give use the maximum value of \\(f(\\mathbf{x} +\\mathbf{\\epsilon})\\). We can see this from the approximation above: \\(f(\\mathbf{x} +\\mathbf{\\epsilon})\\) is maximized when \\(\\frac{df}{d\\mathbf{x}}\\) and \\(\\mathbf{\\epsilon}\\) are colinear.\nWe can define the gradient in this sense this more formally as:\n\\[\n\\frac{df}{d\\mathbf{x}}= \\underset{\\gamma \\rightarrow 0}{\\lim}\\ \\underset{\\|\\mathbf{\\epsilon}\\|_2 &lt; \\gamma}{\\max} \\frac{f(\\mathbf{x} + \\mathbf{\\epsilon}) - f(\\mathbf{x})}{\\|\\mathbf{\\epsilon}\\|_2}\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#gradient-functions",
    "href": "lecture3-logistic-regression/notes.html#gradient-functions",
    "title": "Lecture 3: Logistic regression",
    "section": "Gradient functions",
    "text": "Gradient functions\nJust as with derivatives and partial derivatives, we can define a gradient function that maps an input vector \\(\\mathbf{x}\\) to the gradient of the function \\(f\\) at \\(\\mathbf{x}\\) as:\n\\[\n\\frac{df}{d\\mathbf{x}}=\\nabla f(\\mathbf{x})\n\\]\nHere \\(\\nabla f\\) is the gradient function for \\(f\\). If the function takes multiple vectors as input, we can specify the gradient function with respect to a particular input using subscript notation:\n\\[\n\\frac{df}{d\\mathbf{x}}= \\nabla_{\\mathbf{x}} f(\\mathbf{x}, \\mathbf{y}), \\quad \\frac{df}{d\\mathbf{y}}= \\nabla_{\\mathbf{y}} f(\\mathbf{x}, \\mathbf{y})\n\\]\nNote that the gradient function is a mapping from \\(\\mathbb{R}^n\\rightarrow\\mathbb{R}^n\\), meaning that it returns a vector with the same size as the input."
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#notation-revisited",
    "href": "lecture3-logistic-regression/notes.html#notation-revisited",
    "title": "Lecture 3: Logistic regression",
    "section": "Notation revisited",
    "text": "Notation revisited"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#partial-derivatives-revisited",
    "href": "lecture3-logistic-regression/notes.html#partial-derivatives-revisited",
    "title": "Lecture 3: Logistic regression",
    "section": "Partial derivatives revisited",
    "text": "Partial derivatives revisited"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#gradients-revisited",
    "href": "lecture3-logistic-regression/notes.html#gradients-revisited",
    "title": "Lecture 3: Logistic regression",
    "section": "Gradients revisited",
    "text": "Gradients revisited"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#dot-products-as-summations",
    "href": "lecture3-logistic-regression/notes.html#dot-products-as-summations",
    "title": "Lecture 3: Logistic regression",
    "section": "Dot products as summations",
    "text": "Dot products as summations"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#derivatives-of-summations",
    "href": "lecture3-logistic-regression/notes.html#derivatives-of-summations",
    "title": "Lecture 3: Logistic regression",
    "section": "Derivatives of summations",
    "text": "Derivatives of summations"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#nested-summations",
    "href": "lecture3-logistic-regression/notes.html#nested-summations",
    "title": "Lecture 3: Logistic regression",
    "section": "Nested summations",
    "text": "Nested summations"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#derivatives-with-matrix-vector-products",
    "href": "lecture3-logistic-regression/notes.html#derivatives-with-matrix-vector-products",
    "title": "Lecture 3: Logistic regression",
    "section": "Derivatives with matrix-vector products",
    "text": "Derivatives with matrix-vector products"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#vector-notation-revisited",
    "href": "lecture3-logistic-regression/notes.html#vector-notation-revisited",
    "title": "Lecture 3: Logistic regression",
    "section": "Vector notation revisited",
    "text": "Vector notation revisited\nIt often is much simpler to explicitly define vectors as being either row or column vectors. The common convention in machine learning is to assume that all vectors are column vectors (\\(n \\times 1\\) matricies) and thus a row vector ( \\(1\\times n\\) matrix) is obtained by explicit transposition:\n\\[\\text{Column vector: } \\mathbf{x} = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3\n\\end{bmatrix}, \\quad \\text{Row vector: } \\mathbf{x}^T = \\begin{bmatrix}\nx_1 & x_2 & x_3\n\\end{bmatrix}\\]\nIn this case, we would rewrite the matrix-vector and vector-matrix products we saw above as:\n\\[\\text{Matrix-vector: }\\mathbf{A}\\mathbf{x} = \\mathbf{b}, \\quad \\text{Vector-matrix: }\\mathbf{x}^T\\mathbf{A}^T= \\mathbf{b}^T\\]\nIn Numpy, we can make a vector into an explicit column or row vector by inserting a new dimension, either with the np.expand_dims function or with the indexing operator:\n\nrow_x = np.expand_dims(x, axis=0) # Add a new leading dimension to x\nrow_x\n\narray([[ 1, -2,  1]])\n\n\n\ncolumn_x = np.expand_dims(x, axis=1) # Add a new second dimension to x\nassert np.all(column_x.T == row_x)\ncolumn_x\n\narray([[ 1],\n       [-2],\n       [ 1]])\n\n\nAlternatively:\n\nrow_x = x[None, :]\ncolumn_x = x[:, None]"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#vector-notation",
    "href": "lecture3-logistic-regression/notes.html#vector-notation",
    "title": "Lecture 3: Logistic regression",
    "section": "Vector notation",
    "text": "Vector notation\nAs we’ve seen a vector is a 1-dimensional set of numbers. For example, we can write the vector \\(\\mathbf{x} \\in \\mathbb{R}^3\\) as:\n\\[\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}\\]\nRecall that a vector can also be seen as either an \\(n \\times 1\\) matrix (column vector) or a \\(1 \\times n\\) matrix (row vector).\n\\[\\text{Column vector: } \\mathbf{x} = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3\n\\end{bmatrix}, \\quad \\text{Row vector: } \\mathbf{x} = \\begin{bmatrix}\nx_1 & x_2 & x_3\n\\end{bmatrix}\\]\nNote that we may use the same notation for both as they refer to the same concept (a vector)."
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#vector-notation-1",
    "href": "lecture3-logistic-regression/notes.html#vector-notation-1",
    "title": "Lecture 3: Logistic regression",
    "section": "Vector notation",
    "text": "Vector notation\nThe difference between row and column vectors becomes relevant when we consider matrix-vector multiplication. We can write matrix-vector multiplication in two ways: \\[\\text{Matrix-vector: }\\mathbf{A}\\mathbf{x} = \\mathbf{b}, \\quad \\text{Vector-matrix: }\\mathbf{x}\\mathbf{A}^T= \\mathbf{b}\\] In matrix-vector multiplication we treat \\(\\textbf{x}\\) as a column vector (\\(n \\times 1\\) matrix), while in vector-matrix multiplication we treat it as a row vector (\\(n \\times 1\\) matrix). Transposing \\(A\\) for left multiplication ensures that the two forms give the same answer.\nIn Numpy the np.dot function works in this way. Given a matrix A and a 1-dimensional vector x, performing both operations will give the same result (another 1-dimensional vector):\n\nA = np.array([[ 1,  2, -1],\n              [ 5, -3,  2],\n              [-2,  1, -4],\n             ])\nx = np.array([1, -2, 1])\n\nAx   = np.dot(A, x)\nxA_T = np.dot(x, A.T)\nprint('Ax   = ', Ax)\nprint('xA^T = ', xA_T)\n\nAx   =  [-4 13 -8]\nxA^T =  [-4 13 -8]"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#binary-outputs",
    "href": "lecture3-logistic-regression/notes.html#binary-outputs",
    "title": "Lecture 3: Logistic regression",
    "section": "Binary outputs",
    "text": "Binary outputs\nIn the simplest binary case our function produces one of two possible outputs.\nFor example: consider the problem of labeling images as containing either cats or dogs. Conceptually we would like a function that maps images to either a cat label or a dog label:\n\n\n\n\n\nFor convenience and generality, we will typically use the set \\(\\{0, 1\\}\\) to denote the possible outputs for a binary classification function. Therefore in general we are considering functions of the form:\n\\[\ny=f(\\mathbf{x}), \\quad \\text{Input: } \\mathbf{x} \\in\\mathbb{R}^n \\longrightarrow \\text{ Output: }y \\in \\{0, 1\\}\n\\]\nWe can assign these outputs to correspond to our actual target labels. For instance we might say that \\(0 = \\textbf{\"cat\"}\\) and \\(1=\\textbf{\"dog\"}\\)."
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#visualizing-categorical-functions",
    "href": "lecture3-logistic-regression/notes.html#visualizing-categorical-functions",
    "title": "Lecture 3: Logistic regression",
    "section": "Visualizing categorical functions",
    "text": "Visualizing categorical functions\nAs a simpler example, let’s again consider the fuel efficiency example from the previous lecture. Perhaps our company has set a target fuel efficiency of 30 miles per gallon for our new model and we want to predict whether our design will meet that target. In this case our inputs will be the same as before, but our output will become a binary label:\n\\[\n\\text{Input: } \\mathbf{x}_i= \\begin{bmatrix} \\text{Weight} \\\\ \\text{Horsepower} \\\\ \\text{Displacement} \\\\ \\text{0-60mph}  \\end{bmatrix}, \\quad \\text{Output: } y_i = \\begin{cases} 1: \\text{Meets target } (MPG \\geq 30) \\\\ 0:\n\\text{Fails to meet target } (MPG &lt; 30) \\\\  \\end{cases}\n\\]\nWe can visualize which observations meet our target efficiency by again plotting weight against MPG and using colors to distinguish observations would have label \\(1\\) vs. label \\(0\\).\n\n\n\n\n\nWith this new output definition our dataset will look like:\n\\[\n\\text{Honda Accord: } \\begin{bmatrix} \\text{Weight:} & \\text{2500 lbs} \\\\ \\text{Horsepower:} & \\text{ 123 HP} \\\\ \\text{Displacement:} & \\text{ 2.4 L} \\\\ \\text{0-60mph:} & \\text{ 7.8 Sec} \\end{bmatrix} \\longrightarrow \\text{1   (Meets target)}\n\\]\n\\[\n\\text{Dodge Aspen: } \\begin{bmatrix} \\text{Weight:} & \\text{3800 lbs} \\\\ \\text{Horsepower:} & \\text{ 155 HP} \\\\ \\text{Displacement:} & \\text{ 3.2 L} \\\\ \\text{0-60mph:} & \\text{ 6.8 Sec} \\end{bmatrix} \\longrightarrow  \\text{0   (Does not meet target)}\n\\]\n\\[\n\\vdots \\quad \\vdots\n\\]\nIn this case, we’ve gotten rid of the \\(MPG\\) output variable and replaced it with a binary output \\(y_i \\in \\{0, 1\\}\\). If we plot this version of the data, we can see more directly how this classification task differs from the regression task we saw in the last lecture."
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#making-binary-predictions",
    "href": "lecture3-logistic-regression/notes.html#making-binary-predictions",
    "title": "Lecture 3: Logistic regression",
    "section": "Making binary predictions",
    "text": "Making binary predictions\nWe could fit a linear regression model to our binary data, by simply treating the labels \\(0\\) and \\(1\\) as real-valued outputs. For our fuel economy example, such a model would look like this:\n\n\n\n\n\nHowever, this doesn’t really address our problem. How do we interpret a prediction of \\(-1\\) or \\(10\\) or \\(0.5\\)?\nA more suitable prediction function would only output one of our two possible labels \\(\\{0, 1\\}\\). Fortunately, we can adapt our linear regression function in this way by defining a cutoff (typically 0), as follows:\n\\[\nf(\\mathbf{x})=\\mathbf{x}^T\\mathbf{w} \\quad \\longrightarrow \\quad f(\\mathbf{x})=\\begin{cases} 1\\ \\text{   if   }\\ \\mathbf{x}^T\\mathbf{w} \\geq 0 \\\\\n0\\ \\text{   if   }\\ \\mathbf{x}^T\\mathbf{w} &lt; 0\\end{cases}\n\\]\nWe might also write this as:\n\\[\nf(\\mathbf{x}) = \\mathbb{I}(\\mathbf{x}^T\\mathbf{w} \\geq 0)\n\\]\nWhere \\(\\mathbb{I}\\) is an indicator function that is \\(1\\) if the boolean expression is true and \\(0\\) otherwise.\nThis gives us a prediction function that looks like step function in 1 dimension:"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#descision-boundaries",
    "href": "lecture3-logistic-regression/notes.html#descision-boundaries",
    "title": "Lecture 3: Logistic regression",
    "section": "Descision boundaries",
    "text": "Descision boundaries\nWe can visualize a classification dataset as a function of two variables using color to distinguish between observations with each label. In this example we’ll look at weight and engine displacement.\n\n\n\n\n\nFor a binary classification model the decision boundary is the border between regions of the input space corresponding to each prediction. For a linear classification model the decision boundary is line or plane:\n\\[\\mathbf{x}^T\\mathbf{w}=0\\]\nWe can similarly plot the predictions made by a linear model using colors.\n\n\nModel accuracy: 0.8291"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#measuring-error",
    "href": "lecture3-logistic-regression/notes.html#measuring-error",
    "title": "Lecture 3: Logistic regression",
    "section": "Measuring error",
    "text": "Measuring error\nA natural measure for error for binary classifiers is accuracy. The accuracy of a prediction function is the fraction of observations where the prediction matches the true output:\n\\[\n\\textbf{Accuracy: }\\quad \\frac{\\text{\\# of correct predictions}}{\\text{Total predictions}}\n\\]\nWe can write this in terms of our prediction function as:\n\\[\n\\textbf{Accuracy} = \\frac{1}{N} \\sum_{i=1}^N \\mathbb{I}\\big(f(\\mathbf{x}_i) = y_i\\big)\n\\]\nBelow we can plot the decision boundary compared to the true outputs and calculate the accuracy of our predictions.\n\n\nAccuracy: 0.8291"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#defining-a-loss-function",
    "href": "lecture3-logistic-regression/notes.html#defining-a-loss-function",
    "title": "Lecture 3: Logistic regression",
    "section": "Defining a loss function",
    "text": "Defining a loss function\nIn the last lecture we saw that we can find an optimal choice of parameters \\(\\mathbf{w}\\) for a linear regression model by defining a measure of error or loss for our approximation on our dataset and minimizing that error as a function of \\(\\mathbf{w}\\), either directly or with gradient descent.\n\\[\n\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmin}} \\ \\mathbf{Loss}(\\mathbf{w})\n\\]\nGradient descent update:\n\\[\n\\mathbf{w}^{(k+1)} \\quad \\longleftarrow \\quad \\mathbf{w}^{(k)} - \\alpha \\nabla_{\\mathbf{w}} \\mathbf{Loss}(\\mathbf{w})\n\\]\nWe might consider using (negative) accuracy as a loss function or the same mean squared error that we used for linear regression. However, if we tried to minimize one of these losses with gradient descent, we would run into a fundamental problem: the derivative of the indicator function is always \\(0\\), meaning gradient descent will never update our model.\nTo get around this problem, we need to turn back to our maximum likelihood estimation approach."
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#a-pro",
    "href": "lecture3-logistic-regression/notes.html#a-pro",
    "title": "Lecture 3: Logistic regression",
    "section": "A pro",
    "text": "A pro"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#the-bernoulli-distribution",
    "href": "lecture3-logistic-regression/notes.html#the-bernoulli-distribution",
    "title": "Lecture 3: Logistic regression",
    "section": "The Bernoulli distribution",
    "text": "The Bernoulli distribution\nThe Bernoulli distribution is a probability distribution over two possible outcomes. It is often thought of as the distribution of a coin flip, where the probability of heads is defined by a parameter \\(q\\) in the range \\([0,1]\\).\n\\[\n\\text{Probability of }\\textbf{heads: } \\ \\ q, \\quad \\text{Probability of }\\textbf{tails: } 1-q\n\\]\nAgain we typically use \\(0\\) and \\(1\\) to denote the two possible outcomes, so we can write the probability mass function (or likelihood) of the Bernoulli distribution as:\n\\[\np(y)=\\begin{cases} q\\quad\\ \\ \\ \\ \\ \\ \\  \\text{if }\\ y=1\\\\\n1-q\\quad \\text{if }\\ y=0\\\\\n\\end{cases}\\quad q\\in[0,1],\\ y\\in\\{0, 1\\}\n\\]\nUsing the fact that \\(y\\) can only be \\(0\\) or \\(1\\), we can write this more compactly as:\n\\[\np(y) = q^y(1-q)^{1-y}\n\\]\nRecall that the probability mass function tells us the probability of any outcome under our distribution. We can write the log probability mass function as:\n\\[\n\\log p(y) = y\\log q + (1-y)\\log(1-q)\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#a-probabilistic-model-for-binary-classification",
    "href": "lecture3-logistic-regression/notes.html#a-probabilistic-model-for-binary-classification",
    "title": "Lecture 3: Logistic regression",
    "section": "A probabilistic model for binary classification",
    "text": "A probabilistic model for binary classification\nIn the previous lecture we saw that we could define a probabilistic model for outcomes given inputs by making an strong assumption about how the observed outputs were generated. In particular, we assumed that each \\(y_i\\) was sampled from a Normal distribution where the mean was a linear function of the input \\(\\mathbf{x}_i\\).\n\\[\ny_i \\sim \\mathcal{N}(\\mathbf{x}_i^T\\mathbf{w},\\ \\sigma^2)\n\\]\nGiven everything we’ve seen, we might want to do the same for binary outputs by defining a probabilistic model where each binary label $y$_i$ is drawn from a Bernoulli where \\(q\\) is a linear function of \\(\\mathbf{x}_i\\). Unfortunately \\(q\\) needs to be restricted to the interval \\([0,1]\\) and a linear function can make no such guarantee about its output.\n\\[\n\\mathbf{x}^T\\mathbf{w}\\notin [0, 1] \\quad \\longrightarrow \\quad y_i \\sim \\mathbf{Bernoulli}(\\mathbf{ q=? })\\quad\n\\]\nHowever, if we had a way to map the outputs of our linear function into the range \\([0,1]\\), we could define such a model. This means we need a function of the form:\n\\[\n\\textbf{Need }\\ g(x):\\ \\mathbb{R} \\longrightarrow [0,1]\n\\]\n\\[\n\\textbf{Input: } x \\in \\mathbb{R} \\longrightarrow \\textbf{Output: } y \\in [0,1]\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#sigmoid-function",
    "href": "lecture3-logistic-regression/notes.html#sigmoid-function",
    "title": "Lecture 3: Logistic regression",
    "section": "Sigmoid function",
    "text": "Sigmoid function\nThe sigmoid (or logistic) function is exactly such a function.\n\\[\n\\sigma(x) = \\frac{1}{1+e^{-x}}\n\\]\n\n\n\n\n\nThis “S”-shaped function squashes any real number into the range \\([0,1]\\). The sigmoid function has a number of other nice properties. It is smooth, monotonic and differentiable. It’s derivative has a convenient form that can be written in terms of the sigmoid function itself.\n\\[\n\\frac{d}{dx}\\sigma(x) = \\sigma(x)\\big(1-\\sigma(x)\\big)\n\\]\n\n\n\n\n\nIt’s particularly useful for modeling probabilities because:\n\\[\n\\sigma(0) = 0.5\n\\]\nand\n\\[\n1-\\sigma(x) = \\sigma(-x)\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#a-probabilistic-model-for-binary-classification-1",
    "href": "lecture3-logistic-regression/notes.html#a-probabilistic-model-for-binary-classification-1",
    "title": "Lecture 3: Logistic regression",
    "section": "A probabilistic model for binary classification",
    "text": "A probabilistic model for binary classification\nWith the sigmoid as our mapping function, we can now define our linear probabilistic model for binary classification as:\n\\[\ny_i \\sim \\mathbf{Bernoulli}\\big(\\mathbf{ \\sigma(\\mathbf{x}_i^T\\mathbf{w} })\\big)\n\\]\nUsing this definition, we can easily write out the probability of each output given the input \\((\\mathbf{x}_i)\\) and model parameters \\((\\mathbf{w})\\).\n\\[\np(y_i = 1\\mid \\mathbf{x}_i, \\mathbf{w}) = \\sigma(\\mathbf{x}_i^T\\mathbf{w}), \\quad p(y_i=0\\mid \\mathbf{x}_i, \\mathbf{w})=1-\\sigma(\\mathbf{x}_i^T\\mathbf{w})=\\sigma(-\\mathbf{x}_i^T\\mathbf{w})\n\\]\nFor our fuel efficiency example, we can plot the predicted probability that our target is met, \\(p(y=1\\mid \\mathbf{x}, \\mathbf{w})\\) under our model as a function of the input (in this case weight). We see that the result is again an s-curve.\n\n\n\n\n\nWe call this probabilistic model for binary outputs: logistic regression."
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#maximum-likelihood-estimation",
    "href": "lecture3-logistic-regression/notes.html#maximum-likelihood-estimation",
    "title": "Lecture 3: Logistic regression",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nIn this case our parameters are a matrix\n\\[\n\\nabla_{\\mathbf{W}} \\mathbf{NLL}(\\mathbf{W}, \\mathbf{X}, \\mathbf{y})= \\begin{bmatrix} \\frac{\\partial \\mathbf{NLL}}{\\partial W_{11}} & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{12}} & \\dots & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{1C}} \\\\\n\\frac{\\partial \\mathbf{NLL}}{\\partial W_{21}} & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{22}} & \\dots & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{2C}} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\frac{\\partial \\mathbf{NLL}}{\\partial W_{d1}} & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{d2}} & \\dots & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{dC}}\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{W}^{(i+1)} \\leftarrow \\mathbf{W}^{(i)} - \\alpha \\nabla_{\\mathbf{W}} \\mathbf{NLL}(\\mathbf{W}^{(i)}, \\mathbf{X}, \\mathbf{y})\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#comparing-loss-functions",
    "href": "lecture3-logistic-regression/notes.html#comparing-loss-functions",
    "title": "Lecture 3: Logistic regression",
    "section": "Comparing loss functions",
    "text": "Comparing loss functions\nLet’s look at how this loss function compares to the mean squared error loss we derived for logistic regression. One way to do this is to visualize the loss for a single observation as a function of the output of \\(\\mathbf{x}^T\\mathbf{w}\\). Here we’ll look at the loss for different models trying to predict an output of \\(y=0\\):\n\\[\n\\textbf{Let: }\\ y=0, \\quad z=\\mathbf{x}^T\\mathbf{w}\n\\]\n\n\n\n\n\nWe see that the squared error loss is best when the output is exactly 0, while the logistic regression NLL wants the output of \\(\\mathbf{x}^T\\mathbf{w}\\) to be a negative as possible so that \\(p(y=0\\mid \\mathbf{x}, \\mathbf{w}) \\longrightarrow 1\\). Meanwhile the “accuracy” loss has no slope, making it impossible to optimize with gradient descent."
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#multi-class-prediction-functions",
    "href": "lecture3-logistic-regression/notes.html#multi-class-prediction-functions",
    "title": "Lecture 3: Logistic regression",
    "section": "Multi-class prediction functions",
    "text": "Multi-class prediction functions\nA symmetric approach to defining a prediction function for multi-class classification is to define a separate linear function for each class and choose the class whose function gives the largest output.\nIf \\(C\\) is the number of possible classes, we will therefore have \\(C\\) different parameter vectors \\(\\mathbf{w}_1,…,\\mathbf{w}_C\\) and our prediction function will be defined as:\n\\[\nf(\\mathbf{x}) = \\underset{c\\in\\{1...C\\}}{\\text{argmax}}\\ \\mathbf{x}^T\\mathbf{w}_c\n\\]\nFor convenience, we can also define a matrix that contains all \\(C\\) parameter vectors:\n\\[\n\\mathbf{W} = \\begin{bmatrix} \\mathbf{w}_1^T \\\\ \\mathbf{w}_2^T \\\\ \\vdots \\\\ \\mathbf{w}_C^T\\end{bmatrix} = \\begin{bmatrix} W_{11} & W_{12} & \\dots & W_{1d} \\\\\nW_{21} & W_{22} & \\dots & W_{2d} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nW_{C1} & W_{C2} & \\dots & W_{Cd}\n\\end{bmatrix}\n\\]\nWith this notation, our prediction function becomes:\n\\[\nf(\\mathbf{x}) = \\underset{c\\in\\{1...C\\}}{\\text{argmax}}\\ (\\mathbf{x}^T\\mathbf{W}^T)_c, \\quad \\mathbf{W} \\in \\mathbb{R}^{C\\times d}\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#categorical-distribution",
    "href": "lecture3-logistic-regression/notes.html#categorical-distribution",
    "title": "Lecture 3: Logistic regression",
    "section": "Categorical distribution",
    "text": "Categorical distribution\nAs a first step towards finding the optimal \\(\\mathbf{W}\\) for a multi-class model, let’s look at a distribution over multiple discrete outcomes: the Categorical distribution.\nA categorical distribution needs to define a probability for each possible output. We’ll use \\(q_c\\) to denote the probability of output \\(c\\).\n\\[\np(y=c) = q_c, \\quad y\\in \\{1...C\\}\n\\]\nWe can then denote the vector of all \\(C\\) probabilities as \\(\\mathbf{q}\\). Note that in order for this to be valid, every probability needs to be in the range \\([0,1]\\) and the total probability of all outcomes needs to be \\(1\\), so:\n\\[\n\\mathbf{q} \\in \\mathbb{R}^C\\quad q_c \\geq 0\\ \\forall c\\in \\{1...C\\}\\quad \\sum_{c=1}^C q_c=1\n\\]\nAs with the Bernoulli distribution, we can write this in a more compact form. Here we see that the probability of a given outcome is simply the corresponding entry in \\(\\mathbf{q}\\)\n\\[\np(y)=\\prod q_c^{\\mathbb{I}(y=c)} = q_y\n\\]\nThus the log-probability is simply:\n\\[\n\\log p(y) = \\sum_{c=1}^C \\mathbb{I}(y=c)\\log q_c = \\log q_y\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#a-probabilistic-model-for-multi-class-classification",
    "href": "lecture3-logistic-regression/notes.html#a-probabilistic-model-for-multi-class-classification",
    "title": "Lecture 3: Logistic regression",
    "section": "A probabilistic model for multi-class classification",
    "text": "A probabilistic model for multi-class classification\nWith the Categorical distribution defined, we can now ask if we can use it to define a linear probabilistic model for multi-class categorical outputs. As with our other models we’ll consider making the distribution parameter a linear function of our input.\n\\[\ny_i\\sim \\mathbf{Categorical}(\\mathbf{q}=?), \\quad \\mathbf{q}=\\mathbf{x}_i^T\\mathbf{W}^T?\n\\]\nHowever, we once again run into the issue that the output of our linear function likely won’t satisfy the conditions we need for the parameter of a categorical distribution. In particular, the output is not guaranteed to be positive or to sum to \\(1\\).\n\\[\n\\mathbf{x}^T\\mathbf{W}^T\\in \\mathbb{R}^C,\\quad  q_c \\ngeq 0\\ \\forall c\\in \\{1...C\\}, \\quad \\sum_{c=1}^C q_c\\neq1\n\\]\nIn this case we need a way to map arbitrary vectors to vectors that satisfy these conditions:\n\\[\n\\textbf{Need }\\ f(\\mathbf{x}):\\ \\mathbb{R}^C \\longrightarrow [0,\\infty)^C,\\ \\sum_{i=1}^Cf(\\mathbf{x})_c = 1\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#softmax-function",
    "href": "lecture3-logistic-regression/notes.html#softmax-function",
    "title": "Lecture 3: Logistic regression",
    "section": "Softmax function",
    "text": "Softmax function\nSuch a mapping exists in the softmax function. This function maps vectors to positive vectors such that the entries sum to \\(1\\). Entry \\(c\\) of \\(\\text{softmax}(\\mathbf{x})\\) can be written as:\n\\[\n\\text{softmax}(\\mathbf{x})_c = \\frac{e^{x_c}}{\\sum_{j=1}^Ce^{x_j}}\n\\]\nWe can also define the softmax function using vector notation as:\n\\[\n\\text{softmax}(\\mathbf{x}) = \\begin{bmatrix}\\frac{e^{x_1}}{\\sum_{j=1}^Ce^{x_j}} \\\\ \\frac{e^{x_2}}{\\sum_{j=1}^Ce^{x_j}} \\\\ \\vdots \\\\ \\frac{e^{x_C}}{\\sum_{j=1}^Ce^{x_j}} \\end{bmatrix}\n\\]\nIntuitively, \\(e^x\\) is positive for any \\(x\\), while dividing by the sum ensure the entries sum to 1 as:\n\\[\n\\sum_{i=1}^C \\frac{e^{x_i}}{\\sum_{j=1}^Ce^{x_j}} = \\frac{\\sum_{i=1}^C e^{x_i}}{\\sum_{j=1}^Ce^{x_j}} = 1\n\\]\nThe softmax function also has the nice property that\n\\[\n\\underset{c\\in\\{1,...,C\\}}{\\text{argmax}}\\ \\mathbf{x}_c = \\underset{c\\in\\{1,...,C\\}}{\\text{argmax}}\\ \\text{softmax}(\\mathbf{x})_c\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#a-probabilistic-model-for-multi-class-classification-1",
    "href": "lecture3-logistic-regression/notes.html#a-probabilistic-model-for-multi-class-classification-1",
    "title": "Lecture 3: Logistic regression",
    "section": "A probabilistic model for multi-class classification",
    "text": "A probabilistic model for multi-class classification\n\\[\ny_i\\sim \\mathbf{Categorical}\\big(\\text{softmax}(\\mathbf{x}^T\\mathbf{W})\\big)\n\\]\n\\[\np(y_i=c) = \\text{softmax}(\\mathbf{x}^T\\mathbf{W})_c=\\frac{e^{\\mathbf{x}^T\\mathbf{w}_c}}{\\sum_{j=1}^Ce^{\\mathbf{x}^T\\mathbf{w}_j}}\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#maximum-likelihood-estimation-1",
    "href": "lecture3-logistic-regression/notes.html#maximum-likelihood-estimation-1",
    "title": "Lecture 3: Logistic regression",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nIn this case our parameters are a matrix\n\\[\n\\nabla_{\\mathbf{W}} \\mathbf{NLL}(\\mathbf{W}, \\mathbf{X}, \\mathbf{y})= \\begin{bmatrix} \\frac{\\partial \\mathbf{NLL}}{\\partial W_{11}} & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{12}} & \\dots & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{1C}} \\\\\n\\frac{\\partial \\mathbf{NLL}}{\\partial W_{21}} & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{22}} & \\dots & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{2C}} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\frac{\\partial \\mathbf{NLL}}{\\partial W_{d1}} & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{d2}} & \\dots & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{dC}}\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{W}^{(i+1)} \\leftarrow \\mathbf{W}^{(i)} - \\alpha \\nabla_{\\mathbf{W}} \\mathbf{NLL}(\\mathbf{W}^{(i)}, \\mathbf{X}, \\mathbf{y})\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#training-and-test-datasets",
    "href": "lecture3-logistic-regression/notes.html#training-and-test-datasets",
    "title": "Lecture 3: Logistic regression",
    "section": "Training and test datasets",
    "text": "Training and test datasets\nIn machine learning we are typically less interested in how our model predicts the data we’ve already seen than we are in how well it makes predictions for new data. One way to estimate how well our model our model will generalize to new data is to hold out data while fitting our model. To do this we will split our dataset into two smaller datasets: a training dataset that we will use to fit our model, and a test or held-out dataset that we will only use to evaluate our model. By computing the loss on this test dataset, we can get a sense of how well our model will make prediction for new data.\n\\[\\mathcal{D} = \\{ (\\mathbf{x}_1, y_1),\\ (\\mathbf{x}_2, y_2),\\ ... \\,(\\mathbf{x}_N, y_N) \\}\\quad \\longrightarrow \\quad\n\\]\n\\[\n\\mathcal{D}_{train} = \\{ (\\mathbf{x}_1, y_1),\\ (\\mathbf{x}_2, y_2),\\ ... \\,(\\mathbf{x}_{Ntrain}, y_{Ntrain}) \\},\\  \n\\mathcal{D}_{test} = \\{ (\\mathbf{x}_1, y_1),\\ (\\mathbf{x}_2, y_2),\\ ... \\,(\\mathbf{x}_{Ntest}, y_{Ntest}) \\}\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#partial-derivatives",
    "href": "lecture3-logistic-regression/slides.html#partial-derivatives",
    "title": "Lecture 3: Logistic regression",
    "section": "Partial derivatives",
    "text": "Partial derivatives\nA function does not need to be restricted to having a single input. We can specify a function with multiple inputs as follows:\n\\[\nf(x, y, z) = x^2 + 3xy - \\log(z)\n\\]\nIn code this would look like;\n\ndef f(x, y, z):\n    return x ** 2 + 3 * y + np.log(z)\n\nA partial derivative is the derivative of a multiple-input function with respect to a single input, assuming all other inputs are constant. We will explore the implications of that condition later on in this course. For now, we will simply view partial derivatives as a straightforward extension of derivatives, using the modified notation \\(\\frac{\\partial}{\\partial x}\\).\nMore formally, we can define the partial derivative with respect to each input of a function as:\n\\[\n\\frac{\\partial f}{\\partial x} = \\underset{\\epsilon\\rightarrow0}{\\lim} \\frac{f(x+\\epsilon, y, z) - f(x,y,z)}{\\epsilon}, \\quad \\frac{\\partial f}{\\partial y} = \\underset{\\epsilon\\rightarrow0}{\\lim} \\frac{f(x, y+\\epsilon, z) - f(x,y,z)}{\\epsilon}\n\\]\nThese partial derivatives tell us how the output of the function changes as we change each of the inputs individually."
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#partial-derivative-functions",
    "href": "lecture3-logistic-regression/slides.html#partial-derivative-functions",
    "title": "Lecture 3: Logistic regression",
    "section": "Partial derivative functions",
    "text": "Partial derivative functions\nWe can also specify partial derivative functions in the same way as derivative functions. We’ll use subscript notation to specify which input we are differentiating with respect to.\n\\[\n\\frac{\\partial f}{\\partial x} = f_x'(x, y, z)\n\\]\nWe can derive partial derivative functions using the same set of derivative rules:\n\\[\nf(x, y, z) = x^2 + 3xy - \\log(z)\n\\]\n\\[\nf_x'(x, y, z) = 2x + 3y\n\\]\n\\[\nf_y'(x, y, z) = 3x\n\\]\n\\[\nf_z'(x, y, z) = -\\frac{1}{z}\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#functions-of-vectors",
    "href": "lecture3-logistic-regression/slides.html#functions-of-vectors",
    "title": "Lecture 3: Logistic regression",
    "section": "Functions of vectors",
    "text": "Functions of vectors\nWe can also define functions that take vectors (or matrices) as inputs.\n\\[\ny = f(\\mathbf{x}) \\quad f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\n\\]\nHere \\(f\\) is a mapping from length \\(n\\) vectors to real numbers. As a concrete example we could define the function:\n\\[\nf(\\mathbf{x}) = \\sum_{i=1}^n x_i^3 + 1\n\\]\nHere’s the same function in numpy:\n\ndef f(x):\n    return np.sum(x ** 3) + 1\n\nf(np.array([1, 2, 3]))\n\n37\n\n\nNote that functions of vectors are equivalent to multiple-input functions, but with a more compact notation!"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#gradients",
    "href": "lecture3-logistic-regression/slides.html#gradients",
    "title": "Lecture 3: Logistic regression",
    "section": "Gradients",
    "text": "Gradients\nThe gradient of a vector-input function is a vector such that each element is the partial derivative of the function with respect to the corresponding element of the input vector. We’ll use the same notation as derivatives for gradients.\n\\[\n\\frac{df}{d\\mathbf{x}} = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\frac{\\partial f}{\\partial x_3} \\\\ \\vdots \\end{bmatrix}\n\\]\nThe gradient is a vector that tangent to the function \\(f\\) at the input \\(\\mathbf{x}\\). Just as with derivatives, this means that the gradient defines a linear approximation to the function at the point \\(\\mathbf{x}\\).\n\\[\nf(\\mathbf{x}+\\mathbf{\\epsilon}) \\approx f(\\mathbf{x}) + \\frac{df}{d\\mathbf{x}} \\cdot \\mathbf{\\epsilon}\n\\]\nWhere \\(\\mathbf{\\epsilon}\\) is now a small vector. Intuitively, this means that if we take a small step in any direction as defined by \\(\\mathbf{\\epsilon}\\), the gradient will approximate the change in the output of the function. Becuase we are now in more than 1 dimension, this approximation defines a plane in \\(\\mathbb{R}^n\\).\nAnother extremely important property of the gradient is that it points in the direction of maximum change in the function. Meaning that if we were to take an infinitesimal step \\(\\mathbf{\\epsilon}\\) from \\(\\mathbf{x}\\) in any direction, stepping in the gradient direction would give use the maximum value of \\(f(\\mathbf{x} +\\mathbf{\\epsilon})\\). We can see this from the approximation above: \\(f(\\mathbf{x} +\\mathbf{\\epsilon})\\) is maximized when \\(\\frac{df}{d\\mathbf{x}}\\) and \\(\\mathbf{\\epsilon}\\) are colinear.\nWe can define the gradient in this sense this more formally as:\n\\[\n\\frac{df}{d\\mathbf{x}}= \\underset{\\gamma \\rightarrow 0}{\\lim}\\ \\underset{\\|\\mathbf{\\epsilon}\\|_2 &lt; \\gamma}{\\max} \\frac{f(\\mathbf{x} + \\mathbf{\\epsilon}) - f(\\mathbf{x})}{\\|\\mathbf{\\epsilon}\\|_2}\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#gradient-functions",
    "href": "lecture3-logistic-regression/slides.html#gradient-functions",
    "title": "Lecture 3: Logistic regression",
    "section": "Gradient functions",
    "text": "Gradient functions\nJust as with derivatives and partial derivatives, we can define a gradient function that maps an input vector \\(\\mathbf{x}\\) to the gradient of the function \\(f\\) at \\(\\mathbf{x}\\) as:\n\\[\n\\frac{df}{d\\mathbf{x}}=\\nabla f(\\mathbf{x})\n\\]\nHere \\(\\nabla f\\) is the gradient function for \\(f\\). If the function takes multiple vectors as input, we can specify the gradient function with respect to a particular input using subscript notation:\n\\[\n\\frac{df}{d\\mathbf{x}}= \\nabla_{\\mathbf{x}} f(\\mathbf{x}, \\mathbf{y}), \\quad \\frac{df}{d\\mathbf{y}}= \\nabla_{\\mathbf{y}} f(\\mathbf{x}, \\mathbf{y})\n\\]\nNote that the gradient function is a mapping from \\(\\mathbb{R}^n\\rightarrow\\mathbb{R}^n\\), meaning that it returns a vector with the same size as the input."
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#vector-notation",
    "href": "lecture3-logistic-regression/slides.html#vector-notation",
    "title": "Lecture 3: Logistic regression",
    "section": "Vector notation",
    "text": "Vector notation\nAs we’ve seen a vector is a 1-dimensional set of numbers. For example, we can write the vector \\(\\mathbf{x} \\in \\mathbb{R}^3\\) as:\n\\[\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}\\]\nRecall that a vector can also be seen as either an \\(n \\times 1\\) matrix (column vector) or a \\(1 \\times n\\) matrix (row vector).\n\\[\\text{Column vector: } \\mathbf{x} = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3\n\\end{bmatrix}, \\quad \\text{Row vector: } \\mathbf{x} = \\begin{bmatrix}\nx_1 & x_2 & x_3\n\\end{bmatrix}\\]\nNote that we may use the same notation for both as they refer to the same concept (a vector)."
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#vector-notation-1",
    "href": "lecture3-logistic-regression/slides.html#vector-notation-1",
    "title": "Lecture 3: Logistic regression",
    "section": "Vector notation",
    "text": "Vector notation\nThe difference between row and column vectors becomes relevant when we consider matrix-vector multiplication. We can write matrix-vector multiplication in two ways: \\[\\text{Matrix-vector: }\\mathbf{A}\\mathbf{x} = \\mathbf{b}, \\quad \\text{Vector-matrix: }\\mathbf{x}\\mathbf{A}^T= \\mathbf{b}\\] In matrix-vector multiplication we treat \\(\\textbf{x}\\) as a column vector (\\(n \\times 1\\) matrix), while in vector-matrix multiplication we treat it as a row vector (\\(n \\times 1\\) matrix). Transposing \\(A\\) for left multiplication ensures that the two forms give the same answer.\nIn Numpy the np.dot function works in this way. Given a matrix A and a 1-dimensional vector x, performing both operations will give the same result (another 1-dimensional vector):\n\nA = np.array([[ 1,  2, -1],\n              [ 5, -3,  2],\n              [-2,  1, -4],\n             ])\nx = np.array([1, -2, 1])\n\nAx   = np.dot(A, x)\nxA_T = np.dot(x, A.T)\nprint('Ax   = ', Ax)\nprint('xA^T = ', xA_T)\n\nAx   =  [-4 13 -8]\nxA^T =  [-4 13 -8]"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#vector-notation-revisited",
    "href": "lecture3-logistic-regression/slides.html#vector-notation-revisited",
    "title": "Lecture 3: Logistic regression",
    "section": "Vector notation revisited",
    "text": "Vector notation revisited\nIt often is much simpler to explicitly define vectors as being either row or column vectors. The common convention in machine learning is to assume that all vectors are column vectors (\\(n \\times 1\\) matricies) and thus a row vector ( \\(1\\times n\\) matrix) is obtained by explicit transposition:\n\\[\\text{Column vector: } \\mathbf{x} = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3\n\\end{bmatrix}, \\quad \\text{Row vector: } \\mathbf{x}^T = \\begin{bmatrix}\nx_1 & x_2 & x_3\n\\end{bmatrix}\\]\nIn this case, we would rewrite the matrix-vector and vector-matrix products we saw above as:\n\\[\\text{Matrix-vector: }\\mathbf{A}\\mathbf{x} = \\mathbf{b}, \\quad \\text{Vector-matrix: }\\mathbf{x}^T\\mathbf{A}^T= \\mathbf{b}^T\\]\nIn Numpy, we can make a vector into an explicit column or row vector by inserting a new dimension, either with the np.expand_dims function or with the indexing operator:\n\nrow_x = np.expand_dims(x, axis=0) # Add a new leading dimension to x\nrow_x\n\narray([[ 1, -2,  1]])\n\n\n\ncolumn_x = np.expand_dims(x, axis=1) # Add a new second dimension to x\nassert np.all(column_x.T == row_x)\ncolumn_x\n\narray([[ 1],\n       [-2],\n       [ 1]])\n\n\nAlternatively:\n\nrow_x = x[None, :]\ncolumn_x = x[:, None]"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#binary-outputs",
    "href": "lecture3-logistic-regression/slides.html#binary-outputs",
    "title": "Lecture 3: Logistic regression",
    "section": "Binary outputs",
    "text": "Binary outputs\nIn the simplest case there are two possible outputs.\n\n\n\n\n\nWe use the set \\(\\{0, 1\\}\\) to denote the possible outputs for a binary categorical function.\n\\[\ny=f(\\mathbf{x}), \\quad \\text{Input: } \\mathbf{x} \\in\\mathbb{R}^n \\longrightarrow \\text{ Output: }y \\in \\{0, 1\\}\n\\]\nWe might say that \\(0 = \\textbf{\"cat\"}\\) and \\(1=\\textbf{\"dog\"}\\).\nWe call prediction of a categorical output classification."
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#visualizing-categorical-functions",
    "href": "lecture3-logistic-regression/slides.html#visualizing-categorical-functions",
    "title": "Lecture 3: Logistic regression",
    "section": "Visualizing categorical functions",
    "text": "Visualizing categorical functions\nConsider the fuel efficiency example from the previous lecture.\n\\[\n\\text{Input: } \\mathbf{x}_i= \\begin{bmatrix} \\text{Weight} \\\\ \\text{Horsepower} \\\\ \\text{Displacement} \\\\ \\text{0-60mph}  \\end{bmatrix}, \\quad \\text{Output: } y_i = \\begin{cases} 1: \\text{Meets target } (MPG \\geq 30) \\\\ 0:\n\\text{Fails to meet target } (MPG &lt; 30) \\\\  \\end{cases}\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#making-binary-predictions",
    "href": "lecture3-logistic-regression/slides.html#making-binary-predictions",
    "title": "Lecture 3: Logistic regression",
    "section": "Making binary predictions",
    "text": "Making binary predictions\nWe could fit a linear regression model to our binary data, by simply treating the labels \\(0\\) and \\(1\\) as real-valued outputs."
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#descision-boundaries",
    "href": "lecture3-logistic-regression/slides.html#descision-boundaries",
    "title": "Lecture 3: Logistic regression",
    "section": "Descision boundaries",
    "text": "Descision boundaries\nWe can visualize a classification dataset as a function of two variables using color to distinguish between observations with each label. In this example we’ll look at weight and engine displacement."
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#measuring-error",
    "href": "lecture3-logistic-regression/slides.html#measuring-error",
    "title": "Lecture 3: Logistic regression",
    "section": "Measuring error",
    "text": "Measuring error\nA natural measure for error for binary classifiers is accuracy. The accuracy of a prediction function is the fraction of observations where the prediction matches the true output:\n\\[\n\\textbf{Accuracy} = \\frac{1}{N} \\sum_{i=1}^N \\mathbb{I}\\big(f(\\mathbf{x}_i) = y_i\\big)\n\\]\n\n\nModel accuracy: 0.8291"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#defining-a-loss-function",
    "href": "lecture3-logistic-regression/slides.html#defining-a-loss-function",
    "title": "Lecture 3: Logistic regression",
    "section": "Defining a loss function",
    "text": "Defining a loss function\nIn the last lecture we saw that we can find an optimal choice of parameters \\(\\mathbf{w}\\) for a linear regression model by defining a measure of error or loss for our approximation on our dataset and minimizing that error as a function of \\(\\mathbf{w}\\), either directly or with gradient descent.\nWe might consider using (negative) accuracy as a loss function or the same mean squared error that we used for linear regression. However, if we tried to minimize one of these losses with gradient descent, we would run into a fundamental problem: the derivative of the prediction function is always \\(0\\). We can see this if we"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#the-bernoulli-distribution",
    "href": "lecture3-logistic-regression/slides.html#the-bernoulli-distribution",
    "title": "Lecture 3: Logistic regression",
    "section": "The Bernoulli distribution",
    "text": "The Bernoulli distribution\nThe Beroulli distribution is a distribution over binary outcomes (0 or 1). It is parameterized simply by \\(q=p(y=1)\\)\n\\[\np(y)=\\begin{cases} q\\quad\\ \\ \\ \\ \\ \\ \\  \\text{if }\\ y=1\\\\\n1-q\\quad \\text{if }\\ y=0\\\\\n\\end{cases}\\quad q\\in[0,1],\\ y\\in\\{0, 1\\}\n\\]\nWe can also write this as:\n\\[\np(y) = q^y(1-q)^{1-y}\n\\]\nThe log-probability or log-likelihood is then:\n\\[\n\\log p(y) = y\\log q + (1-y)\\log(1-q)\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#a-probabilistic-model-for-binary-classification",
    "href": "lecture3-logistic-regression/slides.html#a-probabilistic-model-for-binary-classification",
    "title": "Lecture 3: Logistic regression",
    "section": "A probabilistic model for binary classification",
    "text": "A probabilistic model for binary classification\nLast lecture saw a probabilistic model for linear regression could be defined as:\n\\[\ny_i \\sim \\mathcal{N}(\\mathbf{x}^T\\mathbf{w},\\ \\sigma^2)\n\\]\nWe’d ideally like to define a similar model for the case of binary outputs using the Bernoulli distribution. However we need to enforce that the Bernoulli parameter is in \\([0,1]\\)\n\\[\n\\mathbf{x}^T\\mathbf{w}\\notin [0, 1] \\quad \\longrightarrow \\quad y_i \\sim \\mathbf{Bernoulli}(\\mathbf{ q=? })\\quad\n\\]\nSo we need a function that can map \\(\\mathbf{x}^T\\mathbf{w}\\) to \\([0,1]\\)\n\\[\n\\textbf{Need }\\ f(x):\\ \\mathbb{R} \\longrightarrow [0,1]\n\\]\n\\[\n\\textbf{Input: } x \\in \\mathbb{R} \\longrightarrow \\textbf{Output: } y \\in [0,1]\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#sigmoid-function",
    "href": "lecture3-logistic-regression/slides.html#sigmoid-function",
    "title": "Lecture 3: Logistic regression",
    "section": "Sigmoid function",
    "text": "Sigmoid function\nThe sigmoid (or logistic) function is a convenient choice\n\\[\n\\sigma(x) = \\frac{1}{1+e^{-x}}\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#a-probabilistic-model-for-binary-classification-1",
    "href": "lecture3-logistic-regression/slides.html#a-probabilistic-model-for-binary-classification-1",
    "title": "Lecture 3: Logistic regression",
    "section": "A probabilistic model for binary classification",
    "text": "A probabilistic model for binary classification\nWith the sigmoid function we can define our probabilistic model\n\\[\ny_i \\sim \\mathbf{Bernoulli}\\big(\\mathbf{ \\sigma(\\mathbf{x}_i^T\\mathbf{w} })\\big)\n\\]\n\\[\np(y_i = 1) = \\sigma(\\mathbf{x}_i^T\\mathbf{w}), \\quad p(y_i=0)=1-\\sigma(\\mathbf{x}_i^T\\mathbf{w})=\\sigma(-\\mathbf{x}_i^T\\mathbf{w})\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#maximum-likelihood-estimation",
    "href": "lecture3-logistic-regression/slides.html#maximum-likelihood-estimation",
    "title": "Lecture 3: Logistic regression",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nLet’s review how to find the parameters of a model using maximum likelihood estimation\n\\[\n\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmax}} \\ p(\\mathbf{y} \\mid \\mathbf{X}, \\mathbf{w}) =\\underset{\\mathbf{w}}{\\text{argmax}} \\ p(y_1,...,y_N \\mid \\mathbf{x}_1, ...,\\mathbf{x}_N, \\mathbf{w}) \\]\nGenerally our model also assumes conditional independence across observations so:\n\\[\np(y_1,...,y_N \\mid \\mathbf{x}_1, ...,\\mathbf{x}_N, \\mathbf{w}) = \\prod_{i=1}^N p(y_i\\mid \\mathbf{x}_i, \\mathbf{w})\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#comparing-loss-functions",
    "href": "lecture3-logistic-regression/slides.html#comparing-loss-functions",
    "title": "Lecture 3: Logistic regression",
    "section": "Comparing loss functions",
    "text": "Comparing loss functions\nLoss for \\(y=0\\) as a function of \\(z=\\mathbf{x}^T\\mathbf{w}\\)\n\\[\n\\textbf{Let: }\\ z=\\mathbf{x}^T\\mathbf{w}\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#multi-class-prediction-functions",
    "href": "lecture3-logistic-regression/slides.html#multi-class-prediction-functions",
    "title": "Lecture 3: Logistic regression",
    "section": "Multi-class prediction functions",
    "text": "Multi-class prediction functions\nWe will typically use a set of integers \\(\\{0,1, 2,...,C\\}\\) to denote the possible outputs for a general categorical function.\n\n\n\n\n\nTherefore in general we are considering functions of the form:\n\\[\ny=f(\\mathbf{x}), \\quad \\text{Input: } \\mathbf{x} \\in\\mathbb{R}^n \\longrightarrow \\text{ Output: }y \\in \\{0, 1, 2, ...,C\\}\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#categorical-distribution",
    "href": "lecture3-logistic-regression/slides.html#categorical-distribution",
    "title": "Lecture 3: Logistic regression",
    "section": "Categorical distribution",
    "text": "Categorical distribution\nThe Categorical distribution is a distribution over several distinct (discrete) outcomes. It’s parameterized by a vector of probabilities for each outcome:\n\\[\np(y=c) = q_c, \\quad y\\in \\{1...C\\}\n\\]\n\\[\n\\mathbf{q} \\in \\mathbb{R}^C\\quad q_c \\geq 0\\ \\forall c\\in \\{1...C\\}\\quad \\sum_{c=1}^C q_c=1\n\\]\nIt can also be written as:\n\\[\np(y)=\\prod q_c^{\\mathbb{I}(y=c)}\n\\]\nThe log-likelihood is then:\n\\[\n\\log p(y) = \\sum_{c=1}^C \\mathbb{I}(y=c)\\log q_c = \\log q_y\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#a-probabilistic-model-for-multi-class-classification",
    "href": "lecture3-logistic-regression/slides.html#a-probabilistic-model-for-multi-class-classification",
    "title": "Lecture 3: Logistic regression",
    "section": "A probabilistic model for multi-class classification",
    "text": "A probabilistic model for multi-class classification\nOnce again we need to translate our linear function output into a valid parameter for this distribution:\n\\[\ny_i\\sim \\mathbf{Categorical}(\\mathbf{q}=?)\n\\]\n\\[\n\\mathbf{q}=\\mathbf{x}^T\\mathbf{W}?\n\\]\n\\[\n\\mathbf{x}^T\\mathbf{W}\\in \\mathbb{R}^C,\\quad  q_c \\ngeq 0\\ \\forall c\\in \\{1...C\\}, \\quad \\sum_{c=1}^C q_c\\neq1\n\\]\n\\[\n\\textbf{Need }\\ f(\\mathbf{x}):\\ \\mathbb{R}^C \\longrightarrow [0,\\infty)^C,\\ \\sum_{i=1}^Cf(\\mathbf{x})_c = 1\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#softmax-function",
    "href": "lecture3-logistic-regression/slides.html#softmax-function",
    "title": "Lecture 3: Logistic regression",
    "section": "Softmax function",
    "text": "Softmax function\nHere we can use the softmax function!\n\\[\n\\text{softmax}(\\mathbf{x})_c = \\frac{e^{x_c}}{\\sum_{j=1}^Ce^{x_j}}\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#a-probabilistic-model-for-multi-class-classification-1",
    "href": "lecture3-logistic-regression/slides.html#a-probabilistic-model-for-multi-class-classification-1",
    "title": "Lecture 3: Logistic regression",
    "section": "A probabilistic model for multi-class classification",
    "text": "A probabilistic model for multi-class classification\nNow we can define our probabilistic model as:\n\\[\ny_i\\sim \\mathbf{Categorical}\\big(\\text{softmax}(\\mathbf{x}^T\\mathbf{W})\\big)\n\\]\n\\[\np(y_i=c) = \\text{softmax}(\\mathbf{x}^T\\mathbf{W})_c=\\frac{e^{\\mathbf{x}^T\\mathbf{w}_c}}{\\sum_{j=1}^Ce^{\\mathbf{x}^T\\mathbf{w}_j}}\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#maximum-likelihood-estimation-1",
    "href": "lecture3-logistic-regression/slides.html#maximum-likelihood-estimation-1",
    "title": "Lecture 3: Logistic regression",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nFor convenience, it is typical to frame the optimal value in terms of the negative log-likelihood rather than the likelihood, but the two are equivalent.\n\\[\n\\underset{\\mathbf{w}}{\\text{argmax}} \\prod_{i=1}^N p(y_i\\mid \\mathbf{x}_i, \\mathbf{w}) = \\underset{\\mathbf{w}}{\\text{argmin}} - \\sum_{i=1}^N \\log p(y_i \\mid \\mathbf{x}_i, \\mathbf{w}) = \\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})\n\\]\nWe see that the negative log-likelihood is a natural loss function to optimize to find \\(\\mathbf{w}^*\\).\n\\[\n\\textbf{Loss}(\\mathbf{w}) =\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})=- \\sum_{i=1}^N \\log p(y_i \\mid \\mathbf{x}_i, \\mathbf{w})\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#maximum-likelihood-estimation-2",
    "href": "lecture3-logistic-regression/slides.html#maximum-likelihood-estimation-2",
    "title": "Lecture 3: Logistic regression",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nWe can now write the maximum likelihood loss for logistic regression.\n\\[\n\\mathbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = -\\sum_{i=1}^N y_i\\log \\sigma(\\mathbf{x}^T\\mathbf{w}) + (1-y_i)\\log(1-\\sigma(\\mathbf{x}^T\\mathbf{w}))\n\\]\n\\[\n=-\\sum_{i=1}^N y_i\\log \\sigma(\\mathbf{x}^T\\mathbf{w}) + (1-y_i)\\log \\sigma(-\\mathbf{x}^T\\mathbf{w})\n\\]\n\\[\n\\textbf{Ideally: }\\ p(y_i\\mid \\mathbf{x}_i, \\mathbf{w})=1,\\ \\forall (\\mathbf{x}_i, y_i)\\in \\mathcal{D}\n\\]\n\\[\n\\mathbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = -\\mathbf{y}^T\\log \\sigma(\\mathbf{X}\\mathbf{w})\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#training-and-test-datasets",
    "href": "lecture3-logistic-regression/slides.html#training-and-test-datasets",
    "title": "Lecture 3: Logistic regression",
    "section": "Training and test datasets",
    "text": "Training and test datasets\nIn machine learning we are typically less interested in how our model predicts the data we’ve already seen than we are in how well it makes predictions for new data. One way to estimate how well our model our model will generalize to new data is to hold out data while fitting our model. To do this we will split our dataset into two smaller datasets: a training dataset that we will use to fit our model, and a test or held-out dataset that we will only use to evaluate our model. By computing the loss on this test dataset, we can get a sense of how well our model will make prediction for new data.\n\\[\\mathcal{D} = \\{ (\\mathbf{x}_1, y_1),\\ (\\mathbf{x}_2, y_2),\\ ... \\,(\\mathbf{x}_N, y_N) \\}\\quad \\longrightarrow \\quad\n\\]\n\\[\n\\mathcal{D}_{train} = \\{ (\\mathbf{x}_1, y_1),\\ (\\mathbf{x}_2, y_2),\\ ... \\,(\\mathbf{x}_{Ntrain}, y_{Ntrain}) \\},\\  \n\\mathcal{D}_{test} = \\{ (\\mathbf{x}_1, y_1),\\ (\\mathbf{x}_2, y_2),\\ ... \\,(\\mathbf{x}_{Ntest}, y_{Ntest}) \\}\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#categorical-outputs",
    "href": "lecture3-logistic-regression/slides.html#categorical-outputs",
    "title": "Lecture 3: Logistic regression",
    "section": "Categorical outputs",
    "text": "Categorical outputs\nIn the last lecture we considered approximating functions of the form:\n\\[\ny=f(\\mathbf{x}), \\quad \\text{Input: } \\mathbf{x} \\in\\mathbb{R}^n \\longrightarrow \\text{ Output: }y \\in\\mathbb{R}\n\\]\nIn many real-world problems, the output we want to model is not a continuous value, but a categorical value."
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#visualizing-categorical-functions-1",
    "href": "lecture3-logistic-regression/slides.html#visualizing-categorical-functions-1",
    "title": "Lecture 3: Logistic regression",
    "section": "Visualizing categorical functions",
    "text": "Visualizing categorical functions\nWith this new output definition our dataset will look like:\n\\[\n\\text{Honda Accord: } \\begin{bmatrix} \\text{Weight:} & \\text{2500 lbs} \\\\ \\text{Horsepower:} & \\text{ 123 HP} \\\\ \\text{Displacement:} & \\text{ 2.4 L} \\\\ \\text{0-60mph:} & \\text{ 7.8 Sec} \\end{bmatrix} \\longrightarrow \\text{1   (Meets target)}\n\\]\n\\[\n\\text{Dodge Aspen: } \\begin{bmatrix} \\text{Weight:} & \\text{3800 lbs} \\\\ \\text{Horsepower:} & \\text{ 155 HP} \\\\ \\text{Displacement:} & \\text{ 3.2 L} \\\\ \\text{0-60mph:} & \\text{ 6.8 Sec} \\end{bmatrix} \\longrightarrow  \\text{0   (Does not meet target)}\n\\]\n\\[\n\\vdots \\quad \\vdots\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#visualizing-categorical-functions-2",
    "href": "lecture3-logistic-regression/slides.html#visualizing-categorical-functions-2",
    "title": "Lecture 3: Logistic regression",
    "section": "Visualizing categorical functions",
    "text": "Visualizing categorical functions"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#making-binary-predictions-1",
    "href": "lecture3-logistic-regression/slides.html#making-binary-predictions-1",
    "title": "Lecture 3: Logistic regression",
    "section": "Making binary predictions",
    "text": "Making binary predictions\nA more suitable prediction function would only output one of our two possible labels \\(\\{0, 1\\}\\). Using a cutoff (typically 0), as follows:\n\\[\nf(\\mathbf{x})=\\mathbf{x}^T\\mathbf{w} \\quad \\longrightarrow \\quad f(\\mathbf{x})=\\begin{cases} 1\\ \\text{   if   }\\ \\mathbf{x}^T\\mathbf{w} \\geq 0 \\\\\n0\\ \\text{   if   }\\ \\mathbf{x}^T\\mathbf{w} &lt; 0\\end{cases}\n\\]\nWe might also write this as:\n\\[\nf(\\mathbf{x}) = \\mathbb{I}(\\mathbf{x}^T\\mathbf{w} \\geq 0)\n\\]\nWhere \\(\\mathbb{I}\\) is an indicator function that is simply \\(1\\) if the boolean expression is true and \\(0\\) otherwise."
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#making-binary-predictions-2",
    "href": "lecture3-logistic-regression/slides.html#making-binary-predictions-2",
    "title": "Lecture 3: Logistic regression",
    "section": "Making binary predictions",
    "text": "Making binary predictions\nThis gives us a prediction function that looks like step function in 1 dimension:"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#making-binary-predictions-3",
    "href": "lecture3-logistic-regression/slides.html#making-binary-predictions-3",
    "title": "Lecture 3: Logistic regression",
    "section": "Making binary predictions",
    "text": "Making binary predictions\nFor our efficiency example, the binary prediction function can be written as:\n\\[\n\\text{Meets target} = f(\\mathbf{x})=\n\\]\n\\[\n\\big((\\text{weight})w_1 + (\\text{horsepower})w_2 + (\\text{displacement})w_3 + (\\text{0-60mph})w_4 + b\\big) \\geq 0\n\\]\nOr in matrix notation:\n\\[\nf(\\mathbf{x})= \\left( \\begin{bmatrix} \\text{Weight} \\\\ \\text{Horsepower} \\\\ \\text{Displacement} \\\\ \\text{0-60mph} \\\\ 1 \\end{bmatrix} \\cdot \\begin{bmatrix} w_1 \\\\ w_2\\\\ w_3 \\\\ w_4\\\\ b\\end{bmatrix} \\geq  0\\right)\n\\]\nIn this form we can see that the sign of each weight parameter determines whether the corresponding feature is more predictive of label \\(1\\) or \\(0\\) and to what extent."
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#making-binary-predictions-4",
    "href": "lecture3-logistic-regression/slides.html#making-binary-predictions-4",
    "title": "Lecture 3: Logistic regression",
    "section": "Making binary predictions",
    "text": "Making binary predictions\nThis has a geometric interpretation if we think of \\(\\mathbf{w}\\) and \\(\\mathbf{x}\\) as vectors. If the angle between \\(\\mathbf{w}\\) and \\(\\mathbf{x}\\) is in the range \\([-\\frac{\\pi}{2}, \\frac{\\pi}{2}]\\) (or \\([-90^o, 90^o]\\) in degrees), then the prediction will be \\(1\\)."
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#descision-boundaries-1",
    "href": "lecture3-logistic-regression/slides.html#descision-boundaries-1",
    "title": "Lecture 3: Logistic regression",
    "section": "Descision boundaries",
    "text": "Descision boundaries\nThe decision boundary is the border between regions of the input space corresponding to each prediction. For a linear classification model the decision boundary is line or plane:\n\\[\\mathbf{x}^T\\mathbf{w}=0\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#sigmoid-function-1",
    "href": "lecture3-logistic-regression/slides.html#sigmoid-function-1",
    "title": "Lecture 3: Logistic regression",
    "section": "Sigmoid function",
    "text": "Sigmoid function\nThe sigmoid function has some nice properties\n\\[\n\\sigma(0) = 0.5\n\\]\n\\[\n1-\\sigma(x) = \\sigma(-x)\n\\]\n\\[\n\\frac{d}{dx}\\sigma(x) = \\sigma(x)\\big(1-\\sigma(x)\\big)\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#a-probabilistic-model-for-binary-classification-2",
    "href": "lecture3-logistic-regression/slides.html#a-probabilistic-model-for-binary-classification-2",
    "title": "Lecture 3: Logistic regression",
    "section": "A probabilistic model for binary classification",
    "text": "A probabilistic model for binary classification\nWe see that if we choose a probability cutoff of \\(0.5\\), our decision boundary doesn’t change!\n\\[\np(y_i=1)\\geq 0.5 \\quad \\longrightarrow \\quad \\mathbf{x}^T\\mathbf{w}\\geq 0\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#multi-class-prediction-functions-1",
    "href": "lecture3-logistic-regression/slides.html#multi-class-prediction-functions-1",
    "title": "Lecture 3: Logistic regression",
    "section": "Multi-class prediction functions",
    "text": "Multi-class prediction functions\nIt’s important to note that we are not assuming that the ordering of labels is meaningful For instance if we’re classifying images of animals we might set the labels such that:\n\\[\n\\textbf{0:  Cat},\\quad\n\\textbf{1:  Dog},\\quad\n\\textbf{2:  Mouse}\n\\]\nBut this is equally valid:\n\\[\n\\textbf{0:  Dog},\\quad\n\\textbf{1:  Mouse},\\quad\n\\textbf{2:  Cat}\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#multi-class-prediction-functions-2",
    "href": "lecture3-logistic-regression/slides.html#multi-class-prediction-functions-2",
    "title": "Lecture 3: Logistic regression",
    "section": "Multi-class prediction functions",
    "text": "Multi-class prediction functions\nA simple prediction function for multiclass classification is:\n\\[\nf(\\mathbf{x}) = \\underset{c\\in\\{0...C\\}}{\\text{argmax}}\\ \\mathbf{x}^T\\mathbf{w}_c\n\\]\nAlternatively:\n\\[\nf(\\mathbf{x}) = \\underset{c\\in\\{0...C\\}}{\\text{argmax}}\\ (\\mathbf{x}^T\\mathbf{W})_c, \\quad \\mathbf{W} \\in \\mathbb{R}^{d\\times C}\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#multi-class-prediction-functions-3",
    "href": "lecture3-logistic-regression/slides.html#multi-class-prediction-functions-3",
    "title": "Lecture 3: Logistic regression",
    "section": "Multi-class prediction functions",
    "text": "Multi-class prediction functions\n\\[\nf(\\mathbf{x}) = \\underset{c\\in\\{0...C\\}}{\\text{argmax}}\\ (\\mathbf{x}^T\\mathbf{W})_c, \\quad \\mathbf{W} \\in \\mathbb{R}^{d\\times C}\n\\]\nThis function reduces to the same one we saw before for the case of \\(C=2\\):\n\\[\nf(\\mathbf{x}) = \\underset{c\\in\\{0,1\\}}{\\text{argmax}}\\ (\\mathbf{x}^T\\mathbf{W})_c = \\mathbb{I}(\\mathbf{x}^T\\mathbf{w}_1 - \\mathbf{x}^T\\mathbf{w}_0 \\geq 0)\n\\]\n\\[\n=\\mathbb{I}(\\mathbf{x}^T(\\mathbf{w}_1 - \\mathbf{w}_0) \\geq 0) \\quad \\longrightarrow \\quad \\mathbb{I}(\\mathbf{x}^T\\mathbf{w} \\geq 0), \\quad \\mathbf{w}=\\mathbf{w}_1-\\mathbf{w}_0\n\\]\nThis means the boundary between any two predictions is linear."
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#maximum-likelihood-estimation-3",
    "href": "lecture3-logistic-regression/slides.html#maximum-likelihood-estimation-3",
    "title": "Lecture 3: Logistic regression",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\n\\[\n\\textbf{Loss}(\\mathbf{W}) =\\textbf{NLL}(\\mathbf{W}, \\mathbf{X}, \\mathbf{y})=- \\sum_{i=1}^N \\log p(y_i \\mid \\mathbf{x}_i, \\mathbf{W})\n\\]\n\\[\n= \\sum_{i=1}^N \\log\\ \\text{softmax}(\\mathbf{x}_i^T\\mathbf{W})_{y_i} = \\sum_{i=1}^N  \\log \\frac{e^{\\mathbf{x}_i^T\\mathbf{w}_{y_i}}}{\\sum_{j=1}^Ce^{\\mathbf{x}_i^T\\mathbf{w}_{j}}}\n\\]\n\\[\n=\\sum_{i=1}^N \\bigg(\\mathbf{x}_i^T\\mathbf{w}_{y_i}- \\log\\sum_{j=1}^Ce^{\\mathbf{x}_i^T\\mathbf{w}_{j}}\\bigg)\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html#maximum-likelihood-estimation-4",
    "href": "lecture3-logistic-regression/slides.html#maximum-likelihood-estimation-4",
    "title": "Lecture 3: Logistic regression",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nIn this case our parameters are a matrix\n\\[\n\\nabla_{\\mathbf{W}} \\mathbf{NLL}(\\mathbf{W}, \\mathbf{X}, \\mathbf{y})= \\begin{bmatrix} \\frac{\\partial \\mathbf{NLL}}{\\partial W_{11}} & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{12}} & \\dots & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{1C}} \\\\\n\\frac{\\partial \\mathbf{NLL}}{\\partial W_{21}} & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{22}} & \\dots & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{2C}} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\frac{\\partial \\mathbf{NLL}}{\\partial W_{d1}} & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{d2}} & \\dots & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{dC}}\n\\end{bmatrix}\n\\]\nWe can still perform gradient descent as before.\n\\[\n\\mathbf{W}^{(i+1)} \\leftarrow \\mathbf{W}^{(i)} - \\alpha \\nabla_{\\mathbf{W}} \\mathbf{NLL}(\\mathbf{W}^{(i)}, \\mathbf{X}, \\mathbf{y})\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#maximum-likelihood-estimation-2",
    "href": "lecture3-logistic-regression/notes.html#maximum-likelihood-estimation-2",
    "title": "Lecture 3: Logistic regression",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nIn this case our parameters are a matrix\n\\[\n\\nabla_{\\mathbf{W}} \\mathbf{NLL}(\\mathbf{W}, \\mathbf{X}, \\mathbf{y})= \\begin{bmatrix} \\frac{\\partial \\mathbf{NLL}}{\\partial W_{11}} & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{12}} & \\dots & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{1C}} \\\\\n\\frac{\\partial \\mathbf{NLL}}{\\partial W_{21}} & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{22}} & \\dots & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{2C}} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\frac{\\partial \\mathbf{NLL}}{\\partial W_{d1}} & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{d2}} & \\dots & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{dC}}\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{W}^{(i+1)} \\leftarrow \\mathbf{W}^{(i)} - \\alpha \\nabla_{\\mathbf{W}} \\mathbf{NLL}(\\mathbf{W}^{(i)}, \\mathbf{X}, \\mathbf{y})\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/slides.html",
    "href": "lecture3-logistic-regression/slides.html",
    "title": "Lecture 3: Logistic regression",
    "section": "",
    "text": "Manim Community v0.17.3"
  },
  {
    "objectID": "lecture1-background/notes.html#vector-notation",
    "href": "lecture1-background/notes.html#vector-notation",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "As we’ve seen a vector is a 1-dimensional set of numbers. For example, we can write the vector \\(\\mathbf{x} \\in \\mathbb{R}^3\\) as:\n\\[\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}\\]\nA vector can also be seen as either an \\(n \\times 1\\) matrix (column vector) or a \\(1 \\times n\\) matrix (row vector).\n\\[\\text{Column vector: } \\mathbf{x} = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3\n\\end{bmatrix}, \\quad \\text{Row vector: } \\mathbf{x} = \\begin{bmatrix}\nx_1 & x_2 & x_3\n\\end{bmatrix}\\]\nNote that we may use the same notation for both as they refer to the same concept (a vector).\nThe difference between row and column vectors becomes relevant when we consider matrix-vector multiplication. We can write matrix-vector multiplication in two ways: \\[\\text{Matrix-vector: }\\mathbf{A}\\mathbf{x} = \\mathbf{b}, \\quad \\text{Vector-matrix: }\\mathbf{x}\\mathbf{A}^T= \\mathbf{b}\\] In matrix-vector multiplication we treat \\(\\textbf{x}\\) as a column vector (\\(n \\times 1\\) matrix), while in vector-matrix multiplication we treat it as a row vector (\\(n \\times 1\\) matrix). Transposing \\(A\\) for left multiplication ensures that the two forms give the same answer.\nIn Numpy the np.dot function works in this way. Given a matrix A and a 1-dimensional vector x, performing both operations will give the same result (another 1-dimensional vector):\n\nA = np.array([[ 1,  2, -1],\n              [ 5, -3,  2],\n              [-2,  1, -4],\n             ])\nx = np.array([1, -2, 1])\n\nAx   = np.dot(A, x)\nxA_T = np.dot(x, A.T)\nprint('Ax   = ', Ax)\nprint('xA^T = ', xA_T)\n\nAx   =  [-4 13 -8]\nxA^T =  [-4 13 -8]"
  },
  {
    "objectID": "lecture1-background/notes.html#vector-notation-1",
    "href": "lecture1-background/notes.html#vector-notation-1",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "The difference between row and column vectors becomes relevant when we consider matrix-vector multiplication. We can write matrix-vector multiplication in two ways: \\[\\text{Matrix-vector: }\\mathbf{A}\\mathbf{x} = \\mathbf{b}, \\quad \\text{Vector-matrix: }\\mathbf{x}\\mathbf{A}^T= \\mathbf{b}\\] In matrix-vector multiplication we treat \\(\\textbf{x}\\) as a column vector (\\(n \\times 1\\) matrix), while in vector-matrix multiplication we treat it as a row vector (\\(n \\times 1\\) matrix). Transposing \\(A\\) for left multiplication ensures that the two forms give the same answer.\nIn Numpy the np.dot function works in this way. Given a matrix A and a 1-dimensional vector x, performing both operations will give the same result (another 1-dimensional vector):\n\nA = np.array([[ 1,  2, -1],\n              [ 5, -3,  2],\n              [-2,  1, -4],\n             ])\nx = np.array([1, -2, 1])\n\nAx   = np.dot(A, x)\nxA_T = np.dot(x, A.T)\nprint('Ax   = ', Ax)\nprint('xA^T = ', xA_T)\n\nAx   =  [-4 13 -8]\nxA^T =  [-4 13 -8]"
  },
  {
    "objectID": "lecture1-background/notes.html#vector-notation-revisited",
    "href": "lecture1-background/notes.html#vector-notation-revisited",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "It often is much simpler to explicitly define vectors as being either row or column vectors. The common convention in machine learning is to assume that all vectors are column vectors (\\(n \\times 1\\) matricies) and thus a row vector ( \\(1\\times n\\) matrix) is obtained by explicit transposition:\n\\[\\text{Column vector: } \\mathbf{x} = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3\n\\end{bmatrix}, \\quad \\text{Row vector: } \\mathbf{x}^T = \\begin{bmatrix}\nx_1 & x_2 & x_3\n\\end{bmatrix}\\]\nIn this case, we would rewrite the matrix-vector and vector-matrix products we saw above as:\n\\[\\text{Matrix-vector: }\\mathbf{A}\\mathbf{x} = \\mathbf{b}, \\quad \\text{Vector-matrix: }\\mathbf{x}^T\\mathbf{A}^T= \\mathbf{b}^T\\]\nIn Numpy, we can make a vector into an explicit column or row vector by inserting a new dimension, either with the np.expand_dims function or with the indexing operator:\n\nrow_x = np.expand_dims(x, axis=0) # Add a new leading dimension to x\nrow_x\n\narray([[ 1, -2,  1]])\n\n\n\ncolumn_x = np.expand_dims(x, axis=1) # Add a new second dimension to x\nassert np.all(column_x.T == row_x)\ncolumn_x\n\narray([[ 1],\n       [-2],\n       [ 1]])\n\n\nAlternatively:\n\nrow_x = x[None, :]\ncolumn_x = x[:, None]"
  },
  {
    "objectID": "lecture1-background/notes.html#partial-derivatives-of-vectors",
    "href": "lecture1-background/notes.html#partial-derivatives-of-vectors",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "Partial derivatives of vectors",
    "text": "Partial derivatives of vectors\nWe can take partial derivates of vector functions simply by recognizing that the input vector is a set of variables and applying the standard set of derivative rules.\nFor example, given a vector \\(\\mathbf{x}\\in \\mathbb{R}^3\\): \\[\\mathbf{x} = \\begin{bmatrix}x_1 \\\\ x_2\\\\ x_3 \\end{bmatrix}\\] We can find the partial derivative of the dot-product of \\(\\mathbf{x}\\) with itself with respect to a single entry: \\(x_1\\): \\[\\frac{\\partial }{\\partial x_1} \\mathbf{x}^T\\mathbf{x}=\\] \\[\\frac{\\partial }{\\partial x_1} \\sum_{i=1}^n x_i x_i = \\frac{\\partial }{\\partial x_1}(x_1^2 + x_2^2+x_3^2)\\] We see that the terms \\(x_2^2\\) an \\(x_3^2\\) do not depend on \\(x_1\\), therefore the derivative is \\(0\\) and the final result is: \\[=\\frac{\\partial }{\\partial x_1} x_1^2 = 2 x_1\\]\nIn general, the addition rule applies to summations, so: \\[\\frac{\\partial }{\\partial x}\\sum f(x) = \\sum \\frac{\\partial }{\\partial x} f(x)\\]\nOther derivative rules also apply. For example, the chain rule works as normal: \\[\\frac{\\partial }{\\partial x_1} f(\\mathbf{x}^T\\mathbf{x})=\\] \\[f'(\\mathbf{x}^T\\mathbf{x}) \\frac{\\partial }{\\partial x_1} \\mathbf{x}^T\\mathbf{x} = f'(\\mathbf{x}^T\\mathbf{x}) (2x_1)\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#binary-outputs-1",
    "href": "lecture3-logistic-regression/notes.html#binary-outputs-1",
    "title": "Lecture 3: Logistic regression",
    "section": "Binary outputs",
    "text": "Binary outputs\nIn the simplest binary case our function produces one of two possible outputs.\nFor example: consider the problem of labeling images as containing either cats or dogs. Conceptually we would like a function that maps images to either a cat label or a dog label:\n\n\n\n\n\nFor convenience and generality, we will typically use the set \\(\\{0, 1\\}\\) to denote the possible outputs for a binary classification function. Therefore in general we are considering functions of the form:\n\\[\ny=f(\\mathbf{x}), \\quad \\text{Input: } \\mathbf{x} \\in\\mathbb{R}^n \\longrightarrow \\text{ Output: }y \\in \\{0, 1\\}\n\\]\nWe can assign these outputs to correspond to our actual target labels. For instance we might say that \\(0 = \\textbf{\"cat\"}\\) and \\(1=\\textbf{\"dog\"}\\)."
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#functions-with-categorical-outputs",
    "href": "lecture3-logistic-regression/notes.html#functions-with-categorical-outputs",
    "title": "Lecture 3: Logistic regression",
    "section": "Functions with categorical outputs",
    "text": "Functions with categorical outputs\nIn the last lecture we considered approximating functions of the form:\n\\[\ny=f(\\mathbf{x}), \\quad \\text{Input: } \\mathbf{x} \\in\\mathbb{R}^n \\longrightarrow \\text{ Output: }y \\in\\mathbb{R}\n\\]\nIn that setup our function takes in a vector and produces a real number as an output (for example a miles per gallon rating).\nIn many real-world problems, the output we want to model is not a continuous value, but a categorical value, meaning the function produces one choice from an unordered of possible outputs. A well-studied example of this kind of prediction is labeling; we might want to assign a label to an image based on the image’s content.\n\nWe call the prediction of categorical outputs classification. The output is often also called the class of the obserc"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#visualizing-categorical-outputs",
    "href": "lecture3-logistic-regression/notes.html#visualizing-categorical-outputs",
    "title": "Lecture 3: Logistic regression",
    "section": "Visualizing categorical outputs",
    "text": "Visualizing categorical outputs\nAs a simpler example, let’s again consider the fuel efficiency example from the previous lecture. Perhaps our company has set a target fuel efficiency of 30 miles per gallon for our new model and we want to predict whether our design will meet that target. In this case our inputs will be the same as before, but our output will become a binary label:\n\\[\n\\text{Input: } \\mathbf{x}_i= \\begin{bmatrix} \\text{Weight} \\\\ \\text{Horsepower} \\\\ \\text{Displacement} \\\\ \\text{0-60mph}  \\end{bmatrix}, \\quad \\text{Output: } y_i = \\begin{cases} 1: \\text{Meets target } (MPG \\geq 30) \\\\ 0:\n\\text{Fails to meet target } (MPG &lt; 30) \\\\  \\end{cases}\n\\]\nWe can visualize which observations meet our target efficiency by again plotting weight against MPG and using colors to distinguish observations would have label \\(1\\) vs. label \\(0\\).\n\n\n\n\n\nWith this new output definition our dataset will look like:\n\\[\n\\text{Honda Accord: } \\begin{bmatrix} \\text{Weight:} & \\text{2500 lbs} \\\\ \\text{Horsepower:} & \\text{ 123 HP} \\\\ \\text{Displacement:} & \\text{ 2.4 L} \\\\ \\text{0-60mph:} & \\text{ 7.8 Sec} \\end{bmatrix} \\longrightarrow \\text{1   (Meets target)}\n\\]\n\\[\n\\text{Dodge Aspen: } \\begin{bmatrix} \\text{Weight:} & \\text{3800 lbs} \\\\ \\text{Horsepower:} & \\text{ 155 HP} \\\\ \\text{Displacement:} & \\text{ 3.2 L} \\\\ \\text{0-60mph:} & \\text{ 6.8 Sec} \\end{bmatrix} \\longrightarrow  \\text{0   (Does not meet target)}\n\\]\n\\[\n\\vdots \\quad \\vdots\n\\]\nIn this case, we’ve gotten rid of the \\(MPG\\) output variable and replaced it with a binary output \\(y_i \\in \\{0, 1\\}\\). If we plot this version of the data, we can see more directly how this classification task differs from the regression task we saw in the last lecture."
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#interpreting-parameters",
    "href": "lecture3-logistic-regression/notes.html#interpreting-parameters",
    "title": "Lecture 3: Logistic regression",
    "section": "Interpreting parameters",
    "text": "Interpreting parameters\nFor our efficiency example, the binary prediction function can be written as:\n\\[\n\\text{Meets target} = f(\\mathbf{x})=\n\\]\n\\[\n\\big((\\text{weight})w_1 + (\\text{horsepower})w_2 + (\\text{displacement})w_3 + (\\text{0-60mph})w_4 + b\\big) \\geq 0\n\\]\nOr in matrix notation:\n\\[\nf(\\mathbf{x})= \\left( \\begin{bmatrix} \\text{Weight} \\\\ \\text{Horsepower} \\\\ \\text{Displacement} \\\\ \\text{0-60mph} \\\\ 1 \\end{bmatrix} \\cdot \\begin{bmatrix} w_1 \\\\ w_2\\\\ w_3 \\\\ w_4\\\\ b\\end{bmatrix} \\geq  0\\right)\n\\]\nIn this form we can see that the sign of each weight parameter determines whether the corresponding feature is more predictive of label \\(1\\) or \\(0\\) and to what extent. For instance, large positive weights indicate features that are very predictive of \\(1\\)."
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#geometric-interpretation-of-predictions",
    "href": "lecture3-logistic-regression/notes.html#geometric-interpretation-of-predictions",
    "title": "Lecture 3: Logistic regression",
    "section": "Geometric interpretation of predictions",
    "text": "Geometric interpretation of predictions\nOur binary prediction function also has a geometric interpretation if we think of \\(\\mathbf{w}\\) and \\(\\mathbf{x}\\) as vectors. Reall that the dot product between the vectors \\(\\mathbf{x}\\) and \\(\\mathbf{w}\\) can be written as:\n\\[\n\\mathbf{x}^T\\mathbf{w} = ||\\mathbf{x}||_2 ||\\mathbf{w}||_2 \\cos \\theta\n\\]\nWhere \\(\\theta\\) is the angle between the two vectors. If the angle between \\(\\mathbf{w}\\) and \\(\\mathbf{x}\\) is in the range \\([-\\frac{\\pi}{2}, \\frac{\\pi}{2}]\\) (or \\([-90^o, 90^o]\\) in degrees), then the prediction will be \\(1\\), otherwise it will be 0.\n\n\n\n\n\nThe blue line in the figure above is the set of points such that:\n\\[\n\\mathbf{x}^T \\mathbf{w} = 0\n\\]\nthus it represents the boundary between the regions where \\(1\\) and \\(0\\) predictions are made. By definition, it is perpendicular to the direction of \\(\\mathbf{w}\\)."
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#decision-boundaries",
    "href": "lecture3-logistic-regression/notes.html#decision-boundaries",
    "title": "Lecture 3: Logistic regression",
    "section": "Decision boundaries",
    "text": "Decision boundaries\nWe can visualize a classification dataset as a function of two variables using color to distinguish between observations with each label. In this example we’ll look at weight and engine displacement.\n\n\n\n\n\nFor a binary classification model the decision boundary is the border between regions of the input space corresponding to each prediction that we saw in the previous section. For a linear classification model the decision boundary is line or plane:\n\\[\\mathbf{x}^T\\mathbf{w}=0\\]\nHere we’ll plot the decision boundary in the input space and color code observations by the predicted label."
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#logistic-regression-decision-boundary",
    "href": "lecture3-logistic-regression/notes.html#logistic-regression-decision-boundary",
    "title": "Lecture 3: Logistic regression",
    "section": "Logistic regression decision boundary",
    "text": "Logistic regression decision boundary\nWhen we’re making predictions we typically don’t want to sample an output, we want to make a definite prediction. In this case either \\(0\\) or \\(1\\). A reasonable way to do this is to simply predict the output that is most likely under our model:\n\\[\n\\textbf{Prediction function: } f(\\mathbf{x}) = \\begin{cases}1 \\ \\text{if } p(y=1\\mid\\mathbf{x}, \\mathbf{w}) \\geq p(y=0\\mid\\mathbf{x}, \\mathbf{w}) \\\\\n0 \\text{ otherwise} \\end{cases}\n\\]\nSince there’s only two possible outcomes, this is equivalent to checking if the probability of class \\(1\\) is greater than 50%. \\[p(y=1\\mid \\mathbf{x}, \\mathbf{w}) =\\sigma(\\mathbf{x}^T\\mathbf{w})\\geq 0.5\\]\nSince \\(\\sigma(0) =0.5\\), we see that this is equivalent to the decision rule for classification we defined earlier!\n\\[\np(y_i=1)\\geq 0.5 \\quad \\longrightarrow \\quad \\mathbf{x}^T\\mathbf{w}\\geq 0\n\\]"
  },
  {
    "objectID": "lecture4-feature-transforms/notes.html",
    "href": "lecture4-feature-transforms/notes.html",
    "title": "Lecture 4: Feature transforms",
    "section": "",
    "text": "Plot = import(\"https://esm.sh/@observablehq/plot\") \nd3 = require(\"d3@7\")\ntopojson = require(\"topojson\")\nMathJax = require(\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-svg.min.js\").catch(() =&gt; window.MathJax)\ntf = require(\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js\").catch(() =&gt; window.tf)\n\nTHREE = {\n  const THREE = window.THREE = await require(\"three@0.130.0/build/three.min.js\");\n  await require(\"three@0.130.0/examples/js/controls/OrbitControls.js\").catch(() =&gt; {});\n  await require(\"three@0.130.0/examples/js/loaders/SVGLoader.js\").catch(() =&gt; {});\n  return THREE;\n}\n\nfunction sample(f, start, end, nsamples=100) {\n  let arr = [...Array(nsamples).keys()]\n  let dist = end - start\n  function arrmap(ind) {\n    const x = (ind * dist) / nsamples + start;\n    return [x, f(x)];\n  }\n  return arr.map(arrmap)\n}\n\nfunction sigmoid(x){\n  return 1 / (1 + Math.exp(-x));\n}\n\nfunction sum(x) {\n  let s = 0;\n  for (let i = 0; i &lt; x.length; i++ ) {\n    s += x[i];\n  }\n  return s;\n}\n\nfunction mean(x) {\n  let s = 0;\n  for (let i = 0; i &lt; x.length; i++ ) {\n    s += x[i];\n  }\n  return s / x.length;\n}\n\nfunction cross_ent(x, y) {\n  return y ? -Math.log(sigmoid(x)) : -Math.log(sigmoid(-x));\n}\n\nfunction se(x, y) {\n  return (x - y) * (x - y);\n}\n\nfunction shuffle(array) {\n  let currentIndex = array.length,  randomIndex;\n\n  // While there remain elements to shuffle.\n  while (currentIndex &gt; 0) {\n\n    // Pick a remaining element.\n    randomIndex = Math.floor(Math.random() * currentIndex);\n    currentIndex--;\n\n    // And swap it with the current element.\n    [array[currentIndex], array[randomIndex]] = [\n      array[randomIndex], array[currentIndex]];\n  }\n\n  return array;\n}\n\nfunction acc(x, y) {\n  return Number(y == (x  &gt; 0));\n}\n\nfunction grid_func(f, width, height, x1, y1, x2, y2) {\n  let values = new Array(width * height);\n  const xstride = (x2 - x1) / width;\n  const ystride = (y2 - y1) / height;\n\n  \n  let y = 0;\n  let x = 0;\n  let ind = 0;\n  for (let i = 0; i &lt; height; i++ ) {\n    for (let j = 0; j &lt; width; j++, ind++) {\n      x = x1 + j * xstride;\n      y = y1 + i * ystride;\n      values[ind] = f(x, y);\n    }\n  }\n  return {width: width, height: height, x1: x1, y1: y1, x2: x2, y2: y2, values: values};\n}\n\nfunction get_accessors(keys, byindex=false) {\n  let isString = value =&gt; typeof value === 'string';\n  \n  let index = 0;\n  let indexmap = {};\n  let accessors = [];\n  for (let i = 0; i &lt; keys.length; i++){\n    let k = keys[i];\n    if (Array.isArray(k)) {\n      let access = isString(k[0]) ? (x =&gt; x[k[0]]) : k[0];\n      \n      if (byindex) {\n        if (isString(k[0]) && !(k[0] in indexmap)) {\n          indexmap[k[0]] = index;\n          index++;\n        }\n        let accessindex = indexmap[k[0]];\n        access = x =&gt; x[accessindex];\n        let process = k[1];\n        let final_access = x =&gt; process(access(x));\n        accessors.push(final_access);\n      }\n      else {\n        let process = k[1];\n        let final_access = x =&gt; process(access(x));\n        accessors.push(final_access);\n      }\n      \n    }\n    else {\n      let access = isString(k) ? (x =&gt; x[k]) : k;\n      if (byindex) { \n        if (isString(k) && !(k in indexmap)) {\n          indexmap[k] = index;\n          index++;\n        }\n        let accessindex = indexmap[k];\n        access = x =&gt; x[accessindex];\n      }\n      accessors.push(access); \n    }\n  }\n  return accessors;\n}\n\nfunction predict(obs, weights, keys=[\"0\", \"1\", \"2\", \"3\"], byindex=false) {\n  let isString = value =&gt; typeof value === 'string';\n  let accessors = get_accessors(keys, byindex);\n  \n  let output = weights[0];\n  let wi = 1;\n  for (let i = 0; (i &lt; keys.length) && (wi &lt; weights.length); i++, wi++){\n    output += weights[wi] * accessors[i](obs);\n  }\n  return output;\n}\n\nfunction mean_loss(f, data, weights, keys, label, l2=0) {\n  let reg = 0;\n  if (l2 &gt; 0){\n    for (let i = 1; i &lt; weights.length; i++) {\n      reg += weights[i] * weights[i];\n    }\n  }\n  \n  const isString = value =&gt; typeof value === 'string';\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n  return mean(data.map(x =&gt; f(predict(x, weights, keys), get_label(x)))) + l2 * reg;\n}\n\nfunction get_domains(data, accessors, margin=0.1) {\n  let domains = [];\n  for (let i = 0; i &lt; accessors.length; i++){\n    let xdomain = d3.extent(data, accessors[i]);\n    let xdsize = (xdomain[1] - xdomain[0]);\n    let xmin = xdomain[0] - xdsize * margin;\n    let xmax = xdomain[1] + xdsize * margin;\n    domains.push([xmin, xmax]);\n  }\n  return domains;\n}\n\nfunction logisticPlot2d(data, weights, keys, label, interval=0.05) {\n  const accuracy = mean_loss(acc, data, weights, keys, label);\n  \n  let isString = value =&gt; typeof value === 'string';\n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, accessors);\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n  \n  return Plot.plot({\n    x: {tickSpacing: 80, label: \"x\"},\n    y: {tickSpacing: 80, label: \"y\"},\n    title: \"Accuracy: \" + accuracy.toFixed(3),\n    color: {type: \"linear\", legend: true, scheme: \"BuRd\", domain: [-0.5, 1.5]},\n    marks: [\n      Plot.contour({\n        fill: (x, y) =&gt; sigmoid(predict([x, y], weights, index_accessors)),\n        x1: domains[0][0], y1: domains[1][0], x2: domains[0][1], y2: domains[1][1], interval: interval,\n      }),\n      Plot.dot(data, {x: accessors[0], y: accessors[1], stroke: x=&gt; (get_label(x) ? 1.35 : -0.35)})\n    ]\n  });\n}\n\nfunction logisticLossPlot2d(data, weights, keys, label) {\n  const loss = mean_loss(cross_ent, data, weights, keys, label);\n  \n  let isString = value =&gt; typeof value === 'string';\n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, accessors);\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n  \n  return Plot.plot({\n    x: {tickSpacing: 80, label: \"x\"},\n    y: {tickSpacing: 80, label: \"y\"},\n    title: \"Loss: \" + loss.toFixed(3),\n    color: {type: \"linear\", legend: true, scheme: \"BuRd\", domain: [0, 5]},\n    marks: [\n      Plot.contour({\n        value: (x, y) =&gt; predict([x, y], weights, index_accessors),\n        fillOpacity: 0.2,\n        stroke: \"black\", x1: domains[0][0], y1: domains[1][0], x2: domains[0][1], y2: domains[1][1],\n        thresholds: [-1e6,  0, 0.00001]\n      }),\n      Plot.dot(data, {x: accessors[0], y: accessors[1], stroke: x=&gt; cross_ent(predict(x, weights, keys), get_label(x)), \n                      strokeOpacity: 0.5 })\n    ]\n  });\n}\n\nfunction lossPlot2d(f, data, keys, label, l2=0, res=100, x1=-40, y1=-0.015, x2=40,  y2=0.015, vmax=50, nlines=25, ctype=\"sqrt\", scale=(x =&gt; x)) {\n  let grid = 0;\n  function lossFunc(w, b) {\n    return scale(mean_loss(f, data, [w, b], keys, label, l2));\n  }\n\n  grid = grid_func(lossFunc,\n                 res, res, x1, y1, x2, y2\n                );\n\n  function plot2d(weights) {\n    let w = weights;\n    if (!(Array.isArray(w[0]))){\n      w = [w];\n    }\n\n    var arrows = w.slice(0, w.length - 1).map(function(e, i) {\n      return e.concat(w[i+1]);\n    });\n\n    let interval= vmax / nlines; \n    let thresholds = [];\n    for (let i = 0; i &lt; nlines; i++) {\n      thresholds.push(i * interval);\n    }\n    let loss = mean_loss(f, data, w[w.length - 1], keys, label, l2)\n    return Plot.plot({\n      title: \"Loss: \" + loss.toFixed(3),\n      color: {type: \"linear\", legend: true, label: \"Loss\", scheme: \"BuRd\", domain: [0, vmax], type: ctype},\n      marks: [\n        Plot.contour(grid.values, {width: grid.width, height: grid.height, x1: grid.x1, x2:grid.x2, y1: grid.y1, y2: grid.y2,\n          stroke: Plot.identity, thresholds: thresholds}),\n        Plot.dot(w),\n        Plot.arrow(arrows, {x1: \"0\", y1: \"1\", x2: \"2\", y2: \"3\", stroke: \"black\"})\n      ]\n    })\n  }\n  return plot2d;\n}\n\n\n\nfunction regressionPlot(data, weights, keys, label, l2, f=se, stroke=\"\") {\n  let loss = mean_loss(f, data, weights, keys, label, l2);\n  let isString = value =&gt; typeof value === 'string';\n  \n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, get_accessors([label].concat(keys)));\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n\n  let stroke_shade = stroke;\n  if (stroke == \"\") {\n    stroke_shade = (x =&gt; f(predict(x, weights, keys), get_label(x)))\n  }\n  \n  return Plot.plot({\n    y: {domain: domains[0]},\n    title: \"Loss: \" + loss.toFixed(3),\n    color: {type: \"linear\", legend: true, label: \"Loss\", scheme: \"BuRd\", domain: [0, 100]},\n    marks: [\n      Plot.line(sample((x) =&gt; predict([x], weights, index_accessors), domains[1][0], domains[1][1]), {stroke: 'black'}),\n      Plot.dot(data, {x: accessors[0], y: get_label, stroke: stroke_shade })\n    ]\n  })\n}\n\nfunction errorPlot(data, weights, keys, label, f, options={}) {\n  const isString = value =&gt; typeof value === 'string';\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n  let errors = data.map(x =&gt; [predict(x, weights, keys) - get_label(x), f(predict(x, weights, keys), get_label(x))]);\n\n  \n  let sigma = (options['sigma'] || 1);\n  let plots = [];\n  const xdomain = (options['xdomain'] || [-30, 30]);\n  const ydomain = (options['ydomain'] || [0, 0.1]);\n  \n\n  if (options['plotnormal']){\n    let pdf = x =&gt; Math.exp(-0.5 * x * x / sigma) * ydomain[1];\n    let normal = Plot.line(sample(pdf, xdomain[0], xdomain[1]), {stroke: 'crimson'});    \n    plots.push(normal);\n  }\n  if (options['plotlaplace']){\n    let pdf = x =&gt; Math.exp(-0.5 * Math.abs(x) / sigma) * ydomain[1];\n    let normal = Plot.line(sample(pdf, xdomain[0], xdomain[1]), {stroke: 'green'});    \n    plots.push(normal);\n  }\n  \n  return Plot.plot({\n    y: {grid: true, domain: ydomain},\n    x: {domain: xdomain},\n    color: {type: \"linear\", legend: true, label: \"Loss\", scheme: \"BuRd\", domain: [0, 100]},\n    marks: [\n      //Plot.rectY(errors, Plot.binX({y: \"count\", fill: x =&gt; mean(x.map(v =&gt; v[1]))}, {x: \"0\"})),\n      \n      Plot.rectY(errors, Plot.binX({y: \"proportion\"}, {x: \"0\", fill: 'steelblue', interval: 1})),\n      Plot.ruleY([0])\n    ].concat(plots)\n  })\n}\n\nfunction nnPlot(data, weights, keys, label, l2, f=se, stroke=\"\", options=[]) {\n  let loss = mean_loss(f, data, weights, keys, label, l2);\n  let isString = value =&gt; typeof value === 'string';\n  \n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, get_accessors([label].concat(keys)));\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n\n  let stroke_shade = stroke;\n  if (stroke == \"\") {\n    stroke_shade = (x =&gt; f(predict(x, weights, keys), get_label(x)))\n  }\n\n  let a = []\n  if (options.indexOf(\"Show feature transforms\") &gt;= 0){\n    a = [Plot.line(sample((x) =&gt;  keys[1][1](x), domains[1][0], domains[1][1]), {stroke: 'red'}),\n      Plot.line(sample((x) =&gt; keys[2][1](x), domains[1][0], domains[1][1]), {stroke: 'blue'})]\n  }\n  \n  return Plot.plot({\n    y: {domain: domains[0]},\n    title: \"Loss: \" + loss.toFixed(3),\n    color: {type: \"linear\", legend: true, label: \"Loss\", scheme: \"BuRd\", domain: [0, 100]},\n    marks: [\n      Plot.line(sample((x) =&gt; predict([x], weights, index_accessors), domains[1][0], domains[1][1]), {stroke: 'black'}),\n      Plot.dot(data, {x: accessors[0], y: get_label, stroke: stroke_shade })\n    ].concat(a)\n  })\n}\ndata = FileAttachment(\"data/auto-mpg.csv\").csv({typed: true})\nManim Community v0.17.3"
  },
  {
    "objectID": "lecture4-feature-transforms/notes.html#training-and-test-datasets",
    "href": "lecture4-feature-transforms/notes.html#training-and-test-datasets",
    "title": "Lecture 4: Feature transforms",
    "section": "Training and test datasets",
    "text": "Training and test datasets\nIn machine learning we are typically less interested in how our model predicts the data we’ve already seen than we are in how well it makes predictions for new data. One way to estimate how well our model our model will generalize to new data is to hold out data while fitting our model. To do this we will split our dataset into two smaller datasets: a training dataset that we will use to fit our model, and a test or held-out dataset that we will only use to evaluate our model. By computing the loss on this test dataset, we can get a sense of how well our model will make prediction for new data.\n\\[\\mathcal{D} = \\{ (\\mathbf{x}_1, y_1),\\ (\\mathbf{x}_2, y_2),\\ ... \\,(\\mathbf{x}_N, y_N) \\}\\quad \\longrightarrow \\quad\n\\]\n\\[\n\\mathcal{D}_{train} = \\{ (\\mathbf{x}_1, y_1),\\ (\\mathbf{x}_2, y_2),\\ ... \\,(\\mathbf{x}_{Ntrain}, y_{Ntrain}) \\},\\  \n\\mathcal{D}_{test} = \\{ (\\mathbf{x}_1, y_1),\\ (\\mathbf{x}_2, y_2),\\ ... \\,(\\mathbf{x}_{Ntest}, y_{Ntest}) \\}\n\\]\n\n\n\nTraining data\n\n\n\n\n\n\n\n\n\ncar name\nweight\ndisplacement\nhorsepower\nacceleration\n\n\n\n\n0\nchevrolet chevelle malibu\n3504\n307.0\n130\n12.0\n\n\n1\nbuick skylark 320\n3693\n350.0\n165\n11.5\n\n\n2\nplymouth satellite\n3436\n318.0\n150\n11.0\n\n\n3\namc rebel sst\n3433\n304.0\n150\n12.0\n\n\n4\nford torino\n3449\n302.0\n140\n10.5\n\n\n...\n...\n...\n...\n...\n...\n\n\n295\ndodge colt hatchback custom\n1915\n98.0\n80\n14.4\n\n\n296\namc spirit dl\n2670\n121.0\n80\n15.0\n\n\n297\nmercedes benz 300d\n3530\n183.0\n77\n20.1\n\n\n298\ncadillac eldorado\n3900\n350.0\n125\n17.4\n\n\n299\npeugeot 504\n3190\n141.0\n71\n24.8\n\n\n\n\n300 rows × 5 columns\n\n\n\n\n\n\n\n\nTest data\n\n\n\n\n\n\n\n\n\ncar name\nweight\ndisplacement\nhorsepower\nacceleration\n\n\n\n\n300\noldsmobile cutlass salon brougham\n3420\n260.0\n90\n22.2\n\n\n301\nplymouth horizon\n2200\n105.0\n70\n13.2\n\n\n302\nplymouth horizon tc3\n2150\n105.0\n70\n14.9\n\n\n303\ndatsun 210\n2020\n85.0\n65\n19.2\n\n\n304\nfiat strada custom\n2130\n91.0\n69\n14.7\n\n\n...\n...\n...\n...\n...\n...\n\n\n393\nford mustang gl\n2790\n140.0\n86\n15.6\n\n\n394\nvw pickup\n2130\n97.0\n52\n24.6\n\n\n395\ndodge rampage\n2295\n135.0\n84\n11.6\n\n\n396\nford ranger\n2625\n120.0\n79\n18.6\n\n\n397\nchevy s-10\n2720\n119.0\n82\n19.4\n\n\n\n\n98 rows × 5 columns\n\n\n\n\n\n\nFor example, we might see that our model does well on the data it was fit on and poorly on new data.\n\n\n\nTraining data\n\nregressionPlot(data.slice(0,300), [45.32, -0.0077], [\"weight\"], x =&gt; x.mpg, 0, se, \"crimson\")\n\n\n\n\n\n\n\n\n\nTest data\n\nregressionPlot(data.slice(300), [45.32, -0.0077], [\"weight\"], x =&gt; x.mpg, 0, se, \"crimson\")"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#maximum-likelihood-estimation-review",
    "href": "lecture3-logistic-regression/notes.html#maximum-likelihood-estimation-review",
    "title": "Lecture 3: Logistic regression",
    "section": "Maximum likelihood estimation review",
    "text": "Maximum likelihood estimation review\nNow that we’ve setup our model, we can look at how to find the optimal \\(\\mathbf{w}\\) using the principle of maximum likelihood estimation."
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#section",
    "href": "lecture3-logistic-regression/notes.html#section",
    "title": "Lecture 3: Logistic regression",
    "section": "",
    "text": "Recall that the maximum likelihood estimate of our parameter \\(\\mathbf{w}\\) is the choice of \\(\\mathbf{w}\\) that maximizes the (conditional) probability of the data we observed under our model\n\\[\n\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmax}} \\ p(\\mathbf{y} \\mid \\mathbf{X}, \\mathbf{w}) =\\underset{\\mathbf{w}}{\\text{argmax}} \\ p(y_1,...,y_N \\mid \\mathbf{x}_1, ...,\\mathbf{x}_N, \\mathbf{w}) \\]\nAgain, our model also assumes conditional independence across observations so:\n\\[\np(y_1,...,y_N \\mid \\mathbf{x}_1, ...,\\mathbf{x}_N, \\mathbf{w}) = \\prod_{i=1}^N p(y_i\\mid \\mathbf{x}_i, \\mathbf{w})\n\\]\nFor convenience, it is typical to frame the optimal value in terms of the negative log-likelihood rather than the likelihood, but the two are equivalent.\n\\[\n\\underset{\\mathbf{w}}{\\text{argmax}} \\prod_{i=1}^N p(y_i\\mid \\mathbf{x}_i, \\mathbf{w}) = \\underset{\\mathbf{w}}{\\text{argmin}} - \\sum_{i=1}^N \\log p(y_i \\mid \\mathbf{x}_i, \\mathbf{w}) = \\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})\n\\]\nThus, the negative log-likelihood is a natural loss function to optimize to find \\(\\mathbf{w}^*\\).\n\\[\n\\textbf{Loss}(\\mathbf{w}) =\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})=- \\sum_{i=1}^N \\log p(y_i \\mid \\mathbf{x}_i, \\mathbf{w})\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#maximum-likelihood-for-logistic-regression",
    "href": "lecture3-logistic-regression/notes.html#maximum-likelihood-for-logistic-regression",
    "title": "Lecture 3: Logistic regression",
    "section": "Maximum likelihood for logistic regression",
    "text": "Maximum likelihood for logistic regression\nWe can now write out the negative log-likelihood for our logistic regression model using the Bernoulli PMF we defined above\n\\[\n\\mathbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = -\\sum_{i=1}^N \\bigg[ y_i\\log \\sigma(\\mathbf{x}_i^T\\mathbf{w}) + (1-y_i)\\log(1-\\sigma(\\mathbf{x}_i^T\\mathbf{w})) \\bigg]\n\\]\nUsing our knowledge of the sigmoid function, we can write this even more compactly:\n\\[\n\\mathbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) =-\\sum_{i=1}^N \\bigg[ y_i\\log \\sigma(\\mathbf{x}_i^T\\mathbf{w}) + (1-y_i)\\log \\sigma(-\\mathbf{x}_i^T\\mathbf{w}) \\bigg]\n\\]\n\\[\n= -\\sum_{i=1}^N \\log\\sigma\\big((2y_i-1)\\mathbf{x}_i^T\\mathbf{w}\\big)\n\\]\nNote that \\(2y_i-1\\) is \\(1\\) if \\(y_i=1\\) and is \\(-1\\) if \\(y_i=0\\).\nFor our logistic regression model, maximum likelihood is intuitive. In the ideal case our model would always predict the correct class with probability 1.\n\\[\n\\textbf{Best case scenerio: } p(y_i\\mid \\mathbf{x}_i, \\mathbf{w})=1, \\quad \\forall i \\in \\{1,...,N\\}\n\\]\nThis is generally not possible though due to the constraints of our linear function.\nWe can also write the negative log-likelihood compactly using matrix-vector notation.\n\\[\n\\mathbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = -\\mathbf{y}^T\\log \\sigma(\\mathbf{X}\\mathbf{w}) - (1-\\mathbf{y})^T\\log \\sigma(-\\mathbf{X}\\mathbf{w})\n\\]\nIt’s worth noting that in neural network literature, this loss is often called the binary cross-entropy loss."
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#multi-class-classification",
    "href": "lecture3-logistic-regression/notes.html#multi-class-classification",
    "title": "Lecture 3: Logistic regression",
    "section": "Multi-class classification",
    "text": "Multi-class classification\nWe’ve now seen a useful model for binary classification, but in many cases we want to predict between many different classes.\n\n\n\n\n\nWe will typically use a set of integers \\(\\{1, 2,...,C\\}\\) to denote the possible outputs for a general categorical function. Therefore we are considering functions of the form:\n\\[\ny=f(\\mathbf{x}), \\quad \\text{Input: } \\mathbf{x} \\in\\mathbb{R}^n \\longrightarrow \\text{ Output: }y \\in \\{1, 2, ...,C\\}\n\\]\nIt’s important to note that we do not want to assume that the ordering of labels is meaningful. For instance if we’re classifying images of animals we might set the labels such that:\n\\[\n\\textbf{1:  Cat},\\quad\n\\textbf{2:  Dog},\\quad\n\\textbf{3:  Mouse}\n\\]\nBut this shouldn’t lead to different results to the case where we assign the labels as:\n\\[\n\\textbf{1:  Dog},\\quad\n\\textbf{2:  Mouse},\\quad\n\\textbf{3:  Cat}\n\\]\nWe call prediction of a categorical output with more than two possibilities multi-class classification."
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#multi-class-decision-boundaries",
    "href": "lecture3-logistic-regression/notes.html#multi-class-decision-boundaries",
    "title": "Lecture 3: Logistic regression",
    "section": "Multi-class decision boundaries",
    "text": "Multi-class decision boundaries\nIf we only have two classes \\(0\\) and \\(1\\), so \\(C=2\\), then this multi-class prediction function reduces to the same as our binary prediction function. We can see this by noting that \\(x &gt; y \\equiv x-y&gt;0\\):\n\\[\nf(\\mathbf{x}) = \\underset{c\\in\\{0,1\\}}{\\text{argmax}}\\ (\\mathbf{x}^T\\mathbf{W}^T)_c = \\mathbb{I}(\\mathbf{x}^T\\mathbf{w}_1 - \\mathbf{x}^T\\mathbf{w}_0 \\geq 0)\n\\]\nIf we factor out \\(\\mathbf{x}\\) we see that we can simply define a new parameter vector in order to get the same decision rule.\n\\[\n=\\mathbb{I}(\\mathbf{x}^T(\\mathbf{w}_1 - \\mathbf{w}_0) \\geq 0) \\quad \\longrightarrow \\quad \\mathbb{I}(\\mathbf{x}^T\\mathbf{w} \\geq 0), \\quad \\mathbf{w}=\\mathbf{w}_1-\\mathbf{w}_0\n\\]\nIt follows that the decision boundary between any two classes is also linear! We can see this by plotting a prediction function. In this case for the Iris dataset we saw in the homework."
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#maximum-likelihood-estimation-for-multinomial-logistic-regression",
    "href": "lecture3-logistic-regression/notes.html#maximum-likelihood-estimation-for-multinomial-logistic-regression",
    "title": "Lecture 3: Logistic regression",
    "section": "Maximum likelihood estimation for multinomial logistic regression",
    "text": "Maximum likelihood estimation for multinomial logistic regression\nWe now have everything we need to define our negative log-likelihood loss for the multi-class classification model. Once again our loss is the negative sum of the log-probability of each observed output:\n\\[\n\\textbf{Loss}(\\mathbf{W}) =\\textbf{NLL}(\\mathbf{W}, \\mathbf{X}, \\mathbf{y})=- \\sum_{i=1}^N \\log p(y_i \\mid \\mathbf{x}_i, \\mathbf{W})\n\\]\nUsing the log-probability of the multinomial logistic regression model we get:\n\\[\n\\textbf{NLL}(\\mathbf{W}, \\mathbf{X}, \\mathbf{y})= -\\sum_{i=1}^N \\log\\ \\text{softmax}(\\mathbf{x}_i^T\\mathbf{W}^T)_{y_i} = -\\sum_{i=1}^N  \\log \\frac{e^{\\mathbf{x}_i^T\\mathbf{w}_{y_i}}}{\\sum_{j=1}^Ce^{\\mathbf{x}_i^T\\mathbf{w}_{j}}}\n\\]\nWe can simplify this further to:\n\\[\n\\textbf{NLL}(\\mathbf{W}, \\mathbf{X}, \\mathbf{y})=-\\sum_{i=1}^N \\bigg(\\mathbf{x}_i^T\\mathbf{w}_{y_i}- \\log\\sum_{j=1}^Ce^{\\mathbf{x}_i^T\\mathbf{w}_{j}}\\bigg)\n\\]"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#multonomial-logistic-regression",
    "href": "lecture3-logistic-regression/notes.html#multonomial-logistic-regression",
    "title": "Lecture 3: Logistic regression",
    "section": "Multonomial logistic regression",
    "text": "Multonomial logistic regression\nWith the softmax function we can now define our probabilistic model for categorical labels as:\n\\[\ny_i\\sim \\mathbf{Categorical}\\big(\\text{softmax}(\\mathbf{x}^T\\mathbf{W})\\big)\n\\]\nWe see that under this assumption, the probability of a particular output \\((c)\\) is:\n\\[\np(y_i=c \\mid \\mathbf{x}, \\mathbf{W}) = \\text{softmax}(\\mathbf{x}^T\\mathbf{W})_c=\\frac{e^{\\mathbf{x}^T\\mathbf{w}_c}}{\\sum_{j=1}^Ce^{\\mathbf{x}^T\\mathbf{w}_j}}\n\\]\nWe call this particular probabilistic model: multinomial logistic regression"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#gradient-descent",
    "href": "lecture3-logistic-regression/notes.html#gradient-descent",
    "title": "Lecture 3: Logistic regression",
    "section": "Gradient descent",
    "text": "Gradient descent\nIn this case our parameters are a matrix \\(\\mathbf{W}\\). The concept of a gradient, extends naturally to a matrix; we simply define the gradient matrix such that each element is the partial derivative with respect to the corresponding element of the input. For the multinomial logistic regression loss, the gradient this looks like:\n\\[\n\\nabla_{\\mathbf{W}} \\mathbf{NLL}(\\mathbf{W}, \\mathbf{X}, \\mathbf{y})= \\begin{bmatrix} \\frac{\\partial \\mathbf{NLL}}{\\partial W_{11}} & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{12}} & \\dots & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{1C}} \\\\\n\\frac{\\partial \\mathbf{NLL}}{\\partial W_{21}} & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{22}} & \\dots & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{2C}} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\frac{\\partial \\mathbf{NLL}}{\\partial W_{d1}} & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{d2}} & \\dots & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{dC}}\n\\end{bmatrix}\n\\]\nWe can still apply the same gradient descent updates in this case!\n\\[\n\\mathbf{W}^{(i+1)} \\leftarrow \\mathbf{W}^{(i)} - \\alpha \\nabla_{\\mathbf{W}} \\mathbf{NLL}(\\mathbf{W}^{(i)}, \\mathbf{X}, \\mathbf{y})\n\\]"
  },
  {
    "objectID": "misc/hw2-hint.html",
    "href": "misc/hw2-hint.html",
    "title": "Hints for homework 2",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "misc/hw2-hint.html#question-2",
    "href": "misc/hw2-hint.html#question-2",
    "title": "Hints for homework 2",
    "section": "Question 2",
    "text": "Question 2\nIn order to plot a function with MatPlotLib, we need to provide the np.plot function with a set of inputs and outputs. For example, let’s say we want to plot the function \\[f(x) = \\sin(x)\\] for inputs between \\(0\\) and \\(10\\) (we say we want to plot \\(\\sin(x)\\) on the range \\([0,10]\\)). We first need a set of inputs between \\(0\\) and \\(10\\). The np.linspace(a, b, n) function will give us n equally-spaced values between a and b. We can use this to define the inputs to our function.\n\nx = np.linspace(0, 10, 100)\n\nThen, we can compute the corresponding set of outputs:\n\ny = np.sin(x)\n\nFinally we can plot these values to np.plot, which will “connect-the-dots” to make a smooth plot.\n\nplt.plot(x, y)\n\n\n\n\nWe can see more clearly what np.plot is doing if we lower the number of inputs that we use to plot our function and add markers at each point:\n\nx = np.linspace(0, 10, 10)\ny = np.sin(x)\nplt.plot(x, y, marker='o')\n\n\n\n\nThe prediction function for linear regression takes in vectors and outputs scalars: \\[f(\\mathbf{x})=\\mathbf{x}^T\\mathbf{w}\\] We saw that we can rewrite the prediction function to make predictions for an entire set of inputs: \\[f(\\mathbf{X})=\\mathbf{X}\\mathbf{w}\\] Where \\[\n\\mathbf{X} =\n\\begin{bmatrix} X_{11} & X_{12} & \\dots  & X_{1d} & 1 \\\\\n                X_{21} & X_{22} & \\dots  & X_{2d} & 1 \\\\\n                \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n                X_{N1} & X_{N2} & \\dots  & X_{Nd} & 1 \\\\  \n                \\end{bmatrix} = \\begin{bmatrix} \\mathbf{x}_1^T \\\\ \\mathbf{x}_2^T \\\\ \\vdots \\\\ \\mathbf{x}_N^T \\end{bmatrix}\\] If our data only has a single feature (as in the Q2 ), then this becomes: \\[\nf(\\mathbf{X}) =\\mathbf{X}\\mathbf{w}=\n\\begin{bmatrix} X_{11} &  1 \\\\\n                X_{21} &  1 \\\\\n                \\vdots & \\vdots  \\\\\n                X_{N1} &  1 \\\\  \n                \\end{bmatrix} \\cdot \\begin{bmatrix} w_{1} \\\\\n                b\n                \\end{bmatrix} \\]\nIn numpy, consider converting a range of inputs into a data matrix like the one above:\n\nx = np.linspace(0, 5, 6) # Get range of inputs\nX = x[:, None]             # Convert vector into an Nx1 matrix\n# Add a column of 1s\nXaug = np.pad(X, [(0,0), (0,1)], constant_values=1.)\nprint(Xaug)\n\n[[0. 1.]\n [1. 1.]\n [2. 1.]\n [3. 1.]\n [4. 1.]\n [5. 1.]]\n\n\nThis gives us something we can pass into our prediction function."
  },
  {
    "objectID": "misc/hw2-hint.html#question-1-2",
    "href": "misc/hw2-hint.html#question-1-2",
    "title": "Hints for homework 2",
    "section": "Question 1 & 2",
    "text": "Question 1 & 2\nThere are multiple ways to add a column of ones to a matrix in numpy. \\[\\mathbf{X} =\n\\begin{bmatrix} X_{11} & X_{12} & \\dots  & X_{1d} \\\\\n                X_{21} & X_{22} & \\dots  & X_{2d}  \\\\\n                \\vdots & \\vdots & \\ddots  & \\vdots \\\\\n                X_{N1} & X_{N2} & \\dots  & X_{Nd} \\\\  \n                \\end{bmatrix} \\longrightarrow \\begin{bmatrix} X_{11} & X_{12} & \\dots  & X_{1d} & 1 \\\\\n                X_{21} & X_{22} & \\dots  & X_{2d} & 1 \\\\\n                \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n                X_{N1} & X_{N2} & \\dots  & X_{Nd} & 1 \\\\  \n                \\end{bmatrix}\\]\n\n## Example 4x4 matrix\nX = np.zeros((4,4))\nX\n\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])\n\n\n\n## Using np.concatenate\nones = np.ones((X.shape[0], 1))\nXaug = np.concatenate([X, ones], axis=1)\nXaug\n\narray([[0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 1.]])\n\n\n\n## Using np.pad\nXaug = np.pad(X, [(0,0), (0,1)], constant_values=1.)\nXaug\n\narray([[0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 1.]])\n\n\nWe could also use pad to add a 1 to a one-dimensional vector:\n\n## Example vector\nx = np.zeros((4,))\nxaug = np.pad(x, [(0,1)], constant_values=1.)\nprint(x)\nprint(xaug)\n\n[0. 0. 0. 0.]\n[0. 0. 0. 0. 1.]"
  },
  {
    "objectID": "misc/hw2-hint.html#using-np.concatenate",
    "href": "misc/hw2-hint.html#using-np.concatenate",
    "title": "Hints for homework 2",
    "section": "Using np.concatenate",
    "text": "Using np.concatenate\nones = np.ones((X.shape[0], 1)) Xaug = np.concatenate([X, ones], axis=1) Xaug"
  },
  {
    "objectID": "misc/hw2-hint.html#question-8",
    "href": "misc/hw2-hint.html#question-8",
    "title": "Hints for homework 2",
    "section": "Question 8",
    "text": "Question 8\nIf we want to split a matrix by its rows in numpy as follow:\n\\[\n\\mathbf{X} =\n\\begin{bmatrix} X_{11} & X_{12} \\\\\nX_{21} & X_{22} \\\\\nX_{31} & X_{32} \\\\\nX_{41} & X_{42} \\\\\nX_{51} & X_{52} \\\\\n                \\end{bmatrix}\\longrightarrow \\begin{bmatrix} X_{11} & X_{12} \\\\\nX_{21} & X_{22} \\\\\nX_{31} & X_{32} \\\\\n\\end{bmatrix}, \\quad\n\\begin{bmatrix}\nX_{41} & X_{42} \\\\\nX_{51} & X_{52} \\\\\n                \\end{bmatrix}\\]\nWe can use the index operator [] as follows:\n\nX = np.random.randint(10, size=(5,2))\nprint(X)\n\n[[7 1]\n [6 4]\n [4 1]\n [5 8]\n [2 0]]\n\n\n\nprint(X[:3]) # Get first 3 rows\n\n[[7 1]\n [6 4]\n [4 1]]\n\n\n\nprint(X[3:]) # Get the rest of the rows\n\n[[5 8]\n [2 0]]"
  },
  {
    "objectID": "lecture4-feature-transforms/figures.html",
    "href": "lecture4-feature-transforms/figures.html",
    "title": "CS 152: Neural Networks",
    "section": "",
    "text": "import warnings\nwarnings.filterwarnings(\"ignore\")\nfrom manim import *\nimport autograd.numpy as np\n\n\nclass LectureScene(Scene):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.camera.background_color = \"#ffffff\"\n        self.template = TexTemplate()\n        self.template.add_to_preamble(r\"\\usepackage{amsmath}\")\n\nclass ThreeDLectureScene(ThreeDScene):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.camera.background_color = \"#ffffff\"\n        self.template = TexTemplate()\n        self.template.add_to_preamble(r\"\\usepackage{amsmath}\")\n    \n\nclass VectorScene(LectureScene):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.ax = Axes(\n            x_range=[-7.5, 7.5, 1],\n            y_range=[-5, 5, 1],\n            x_length=12,\n            y_length=8,\n            axis_config={\"color\": GREY},\n        )\n        \n        #axes_labels.set_color(GREY)\n        self.add(self.ax)\n\nclass PositiveVectorScene(LectureScene):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.ax = Axes(\n            x_range=[-2.5, 12.5, 1],\n            y_range=[-1, 9, 1],\n            x_length=12,\n            y_length=8,\n            axis_config={\"color\": GREY},\n        )\n                #axes_labels.set_color(GREY)\n        self.add(self.ax)\n\nclass ComparisonVectorScene(LectureScene):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.ax1 = Axes(\n            x_range=[-5, 5, 1],\n            y_range=[-5, 5, 1],\n            x_length=6,\n            y_length=6,\n            axis_config={\"color\": GREY},\n        )\n        self.ax2 = Axes(\n            x_range=[-5, 5, 1],\n            y_range=[-5, 5, 1],\n            x_length=6,\n            y_length=6,\n            axis_config={\"color\": GREY},\n        )\n        axgroup = Group(self.ax1, self.ax2)\n        axgroup.arrange_in_grid(buf=2)\n        \n\n        #axes_labels.set_color(GREY)\n        self.add(axgroup)\n\nManim Community v0.17.3"
  },
  {
    "objectID": "lecture4-feature-transforms/notes.html#linear-predictions",
    "href": "lecture4-feature-transforms/notes.html#linear-predictions",
    "title": "Lecture 4: Feature transforms",
    "section": "Linear predictions",
    "text": "Linear predictions\nIn the previous two lectures, we looked at examples of linear models. For example, we saw that the linear regression model makes predictions of the form:\n\\[\nf(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{w} = \\sum_{i=1}^n x_i w_i\n\\]\nMeaning that the output will be a weighted sum of the features of the input. In the case of our car example, we will made predictions as:\n\\[\n\\text{Predicted MPG} = f(\\mathbf{x})=\n\\]\n\\[\n(\\text{weight})w_1 + (\\text{horsepower})w_2 + (\\text{displacement})w_3 + (\\text{0-60mph})w_4 + b\n\\]\nGraphically we see this corresponds to a prediction function that is a line or a plane.\n\nregressionPlot(data, [45.32, -0.0077], [\"weight\"], x =&gt; x.mpg, 0, se, \"crimson\")"
  },
  {
    "objectID": "lecture4-feature-transforms/notes.html#non-linear-data",
    "href": "lecture4-feature-transforms/notes.html#non-linear-data",
    "title": "Lecture 4: Feature transforms",
    "section": "Non-linear data",
    "text": "Non-linear data\nUnfortunately, in the real world the relationship between inputs and outputs is not always linear. For example, what if we tried to fit a linear model to the following dataset.\n\nviewof form_quadratic = Inputs.form(\n  [\n    Inputs.range([-10, 10], {step: 0.01, label: \"b\", value: 2.89}),\n    Inputs.range([-10, 10], {step: 0.0001, label: \"w\", value: 2.0165}),\n  ]\n)\n\n\n\n\n\n\n\n\n\n\n\n\nregressionPlot(quadratic_data, form_quadratic, [\"0\"], \"1\", 0, se)\n\n\n\n\n\n\nWe see that there is no straight line that is a good fit to our data. We see this with our real-world fuel efficiency dataset as well: we can find a line that reasonably approximates the relationship between weight and efficiency, but a curve would fit the data better.\n\nviewof form_mpg_linear = Inputs.form(\n  [\n    Inputs.range([-10, 100], {step: 0.01, label: \"b\", value: 45.32}),\n    Inputs.range([-0.03, 0.03], {step: 0.0001, label: \"w\", value: -0.0077}),\n  ]\n)\n\n\n\n\n\n\n\nregressionPlot(data, form_mpg_linear, [\"weight\"], \"mpg\", 0, se)"
  },
  {
    "objectID": "lecture4-feature-transforms/notes.html#polynomial-functions",
    "href": "lecture4-feature-transforms/notes.html#polynomial-functions",
    "title": "Lecture 4: Feature transforms",
    "section": "Polynomial functions",
    "text": "Polynomial functions\nIf we’re trying to approximate a non-linear relationship between inputs and outputs, it follows that we may want to fit a non-linear approximation.\nOne of the simplest types of non-linear functions we could use are polynomial functions. A polynomial function is simply a function that can be expressed as a polynomial, meaning that it allows for (integer) powers of the input.\nThe simplest type of non-linear polynomial is a quadratic function, which involves powers of up to \\(2\\). A quadratic function of a single variable can be written as:\n\\[\nf(x) = w_2 x^2 + w_1x +b\n\\]\n\n\n\n\n\nA quadratic function of \\(2\\) variables can be written as:\n\\[\nf(x, y) = w_5 x^2 + w_4y^2 + w_3 xy + w_2x + w_1y +b\n\\]\nSimilarly a cubic function involves powers up to 3:\n\\[\nf(x) = w_3 x^3 + w_2 x^2 + w_1x +b\n\\]\n\n\n\n\n\nIn general the degree of a polynomial is the largest exponent in any term of the polynomial (or sum of exponents for terms involving more than 1 input). For example we can look at 2 different degree 4 polynomial functions:\n\\[\nf(x, y) = 3 x^4 + 2 xy + y - 2\n\\]\n\\[\nf(x, y) = -2 x^2y^2 + 2 x^3 + y^2 - 5\n\\]"
  },
  {
    "objectID": "lecture4-feature-transforms/notes.html#polynomial-functions-as-vector-functions",
    "href": "lecture4-feature-transforms/notes.html#polynomial-functions-as-vector-functions",
    "title": "Lecture 4: Feature transforms",
    "section": "Polynomial functions as vector functions",
    "text": "Polynomial functions as vector functions\nWe can also write polynomial functions as vector-input functions. For example a quadratic function of two variables could be written as:\n\\[\nf(\\mathbf{x}) = w_5 x_2^2 + w_4x_1^2 + w_3 x_1 x_2 + w_2x_2 + w_1x_1 +b\n\\]\nFrom this form we see that a polynomial is a weighted sum of powers of \\(\\mathbf{x}\\)! This means we could write a vector polynomial as a dot product between a weight vector and a vector containing all the powers of \\(\\mathbf{x}\\):\n\\[\nw_5 x_2^2 + w_4x_1^2 + w_3 x_1 x_2 + w_2x + w_1y +b = \\begin{bmatrix}  x_1 \\\\ x_2 \\\\ x_1 x_2 \\\\ x_1^2 \\\\ x_2^2 \\\\  1 \\end{bmatrix} \\cdot \\begin{bmatrix}  w_1 \\\\ w_2 \\\\ w_3 \\\\ w_4 \\\\ w_5 \\\\  b \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lecture4-feature-transforms/notes.html#quadratic-feature-transforms",
    "href": "lecture4-feature-transforms/notes.html#quadratic-feature-transforms",
    "title": "Lecture 4: Feature transforms",
    "section": "Quadratic feature transforms",
    "text": "Quadratic feature transforms\nLet’s consider the mapping from \\(\\mathbf{x}\\) to powers of the elements of \\(\\mathbf{x}\\). We’ll call this mapping \\(\\phi\\):\n\\[\n\\begin{bmatrix}  x_1 \\\\ x_2 \\end{bmatrix}\\underset{\\phi}{\\longrightarrow}\\begin{bmatrix}  x_1 \\\\ x_2 \\\\ x_1 x_2 \\\\ x_1^2 \\\\ x_2^2 \\\\  1 \\end{bmatrix}\n\\]\nIn this quadratic example \\(\\phi\\) is a non-linear function that maps vectors to vectors \\((\\mathbb{R}^2 \\rightarrow \\mathbb{R}^6)\\). We call this a quadratic feature transform\n\\[\n\\phi(\\mathbf{x}) = \\begin{bmatrix}  x_1 \\\\ x_2 \\\\ x_1 x_2 \\\\ x_1^2 \\\\ x_2^2 \\\\  1 \\end{bmatrix}\n\\]\nWith this mapping we can our quadratic prediction function simply as:\n\\[\nf(\\mathbf{x})=\\phi(\\mathbf{x})^T \\mathbf{w}\n\\]\nThis is a linear function of \\(\\phi(\\mathbf{x})\\) and \\(\\mathbf{w}\\)!\nAs a simpler example, let’s look at the case where our input has only a single element \\((x_1)\\).\n\\[\nf(\\mathbf{x})=\\phi(\\mathbf{x})^T \\mathbf{w} = w_2 x_1^2 + w_1x_1 +b, \\quad\n\\phi(\\mathbf{x}) = \\begin{bmatrix}  x_1 \\\\ x_1^2 \\\\  1 \\end{bmatrix}\n\\]\n\nviewof form_quadratic_2 = Inputs.form(\n  [\n    Inputs.range([-10, 10], {step: 0.01, label: \"b\", value: 2.89}),\n    Inputs.range([-10, 10], {step: 0.0001, label: \"w_1\", value: 2.0165}),\n    Inputs.range([-10, 10], {step: 0.0001, label: \"w_2\", value: 0}),\n  ]\n)\n\n\n\n\n\n\n\nregressionPlot(quadratic_data, form_quadratic_2, [[\"0\", x =&gt; x], [\"0\", x =&gt; x * x]], \"1\", 0, se)"
  },
  {
    "objectID": "lecture4-feature-transforms/notes.html#fitting-quadratic-regression",
    "href": "lecture4-feature-transforms/notes.html#fitting-quadratic-regression",
    "title": "Lecture 4: Feature transforms",
    "section": "Fitting quadratic regression",
    "text": "Fitting quadratic regression\nIf we treat \\(\\phi(\\mathbf{x})\\) as our new set of inputs, we see that we can apply all the same tools of linear regression that we learned before. Again our new prediction function is:\n\\[\nf(\\mathbf{x})=\\phi(\\mathbf{x})^T \\mathbf{w}, \\quad\n\\phi(\\mathbf{x}) = \\begin{bmatrix}  x_1 \\\\ x_2 \\\\ x_1 x_2 \\\\ x_1^2 \\\\ x_2^2 \\\\  1 \\end{bmatrix}\n\\]\nWe can then define a quadratic probabilistic model as:\n\\[\ny_i \\sim \\mathcal{N}\\big(\\phi(\\mathbf{x}_i)^T\\mathbf{w}, \\sigma^2\\big)\n\\]\nThe corresponding negative log-likelihood loss becomes\n\\[\n\\textbf{Loss}(\\mathbf{w})=\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})=- \\sum_{i=1}^N \\log p(y_i \\mid \\mathbf{x}_i, \\mathbf{w})\n\\]\n\\[\n= \\frac{1}{2\\sigma^2} \\sum_{i=1}^N\\big(y_i - \\phi(\\mathbf{x}_i)^T\\mathbf{w}\\big)^2 + N \\log \\sigma \\sqrt{2 \\pi}\n\\]\nWe can now find the optimal \\(\\mathbf{w}\\) by once again minimizing this loss!\n\\[\n\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmin}} \\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})\n\\]\nWe see that the gradient doesn’t change, it simply involves \\(\\phi(\\mathbf{x}_i)\\) instead of \\(\\mathbf{x}_i\\).\n\\[\n\\nabla_{\\mathbf{w}}\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})\n= \\frac{1}{2\\sigma^2}\\sum_{i=1}^N \\big(\\phi(\\mathbf{x}_i)^T\\mathbf{w} - y_i\\big)\\phi(\\mathbf{x}_i)\n\\] This is because we are only taking the gradient with respect to \\(\\mathbf{w}\\). From the perspective of \\(\\mathbf{w}\\), the prediction funciton is still linear."
  },
  {
    "objectID": "lecture4-feature-transforms/notes.html#quadratic-regression-on-real-data",
    "href": "lecture4-feature-transforms/notes.html#quadratic-regression-on-real-data",
    "title": "Lecture 4: Feature transforms",
    "section": "Quadratic regression on real data",
    "text": "Quadratic regression on real data\nLet’s look at our new quadratic regression model on the problem of predicting fuel efficiency from a car’s weight. In this case because our input has only \\(1\\) entry our quadratic feature transform will be simpler:\n\\[\n\\begin{bmatrix}  x_1  \\end{bmatrix}\\underset{\\phi}{\\longrightarrow}\\begin{bmatrix}  x_1 \\\\ x_1^2 \\\\  1 \\end{bmatrix}\n\\]\nOur prediction function will be:\n\\[\nf(\\mathbf{x})=\\phi(\\mathbf{x})^T \\mathbf{w} = w_2 x_1^2 + w_1x_1 +b, \\quad\n\\phi(\\mathbf{x}) = \\begin{bmatrix}  x_1 \\\\ x_1^2 \\\\  1 \\end{bmatrix}\n\\]\nWe see that by varying \\(w_2\\), we can now fit a curve to our data and get a better overall loss!\n\nviewof form_mpg_2 = Inputs.form(\n  [\n    Inputs.range([-10, 100], {step: 0.01, label: \"b\", value: 45.32}),\n    Inputs.range([-0.03, 0.03], {step: 0.0001, label: \"w_1\", value: -0.0077}),\n    Inputs.range([-0.03, 0.03], {step: 0.0001, label: \"w_2\", value: 0}),\n  ]\n)\n\n\n\n\n\n\n\nregressionPlot(data, form_mpg_2, [\"weight\", [\"weight\", x =&gt; (x / 100) * (x / 100)]], \"mpg\", 0, se)"
  },
  {
    "objectID": "lecture4-feature-transforms/notes.html#quadratic-logistic-regression",
    "href": "lecture4-feature-transforms/notes.html#quadratic-logistic-regression",
    "title": "Lecture 4: Feature transforms",
    "section": "Quadratic logistic regression",
    "text": "Quadratic logistic regression\nJust like with our regression example, we can apply our quadratic feature transform to the logistic regression model as well! In this case our prediction function becomes:\n\\[\nf(\\mathbf{x}) = \\mathbb{I}(\\phi(\\mathbf{x})^T\\mathbf{w} \\geq 0), \\quad \\phi(\\mathbf{x}) = \\begin{bmatrix}  x_1 \\\\ x_2 \\\\ x_1 x_2 \\\\ x_1^2 \\\\ x_2^2 \\\\  1 \\end{bmatrix}\n\\]\nOur Bernoulli probabilistic model becomes:\n\\[\ny_i \\sim \\mathbf{Bernoulli}\\big(\\mathbf{ \\sigma(\\phi(\\mathbf{x}_i)^T\\mathbf{w} })\\big), \\quad p(y_i = 1\\mid \\mathbf{x}_i, \\mathbf{w}) = \\sigma\\big(\\phi(\\mathbf{x}_i)^T\\mathbf{w}\\big)\n\\]\nThe corresponding negative log-likelihood is:\n\\[\n\\mathbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = -\\sum_{i=1}^N \\log\\sigma\\big((2y_i-1)\\phi(\\mathbf{x}_i)^T\\mathbf{w}\\big)\n\\]\nWhich we can once again optimize with gradient descent.\nWith this approach our decision boundary is no longer restricted to be a line!\n\n\n\n\n\n\nviewof form_circles = Inputs.form(\n  [\n    Inputs.range([-100, 100], {step: 0.01, label: \"b\", value: 0}),\n    Inputs.range([-100, 100], {step: 0.0001, label: \"w_1\", value: 20}),\n    Inputs.range([-100, 100], {step: 0.0001, label: \"w_2\", value: 20}),\n      Inputs.range([-100, 100], {step: 0.0001, label: \"w_1\", value: 0}),\n      Inputs.range([-100, 100], {step: 0.0001, label: \"w_1\", value: 0}),\n  ]\n)\n\n\n\n\n\n\n\n\n\nlogisticPlot2d(circles, form_circles, [\"0\", \"1\", [\"0\", x=&gt;x*x], [\"1\", x=&gt;x*x]], \"2\")\n\n\n\n\n\n\n\n\nlogisticLossPlot2d(circles, form_circles, [\"0\", \"1\", [\"0\", x=&gt;x*x], [\"1\", x=&gt;x*x]], \"2\")\n\n\n\n\n\n\n\n\nWe can see where this circular decision boundary comes from if we think about the problem in 3-dimensions.\nRecall that our linear classifier made predictions by thresholding a linear function. Our quadratic classifer thresholds a quadratic function of 1 or more variables, producing the curve that we see above.\n\n\n\nLinear decision boundary\n\n\n\n\n\n\n\n\nQuadratic decision boundary"
  },
  {
    "objectID": "lecture4-feature-transforms/notes.html#general-polynomial-transforms",
    "href": "lecture4-feature-transforms/notes.html#general-polynomial-transforms",
    "title": "Lecture 4: Feature transforms",
    "section": "General polynomial transforms",
    "text": "General polynomial transforms\nWe’ve now seen how to define quadratic models by defining a function \\(\\phi\\) that maps inputs to new new inputs with quadratic terms. However we’re not restricted to just quadratic transform! For example, for a model with \\(1\\) input, we could definite a cubic feature transform as:\n\\[\n\\begin{bmatrix}  x_1  \\end{bmatrix}\\underset{\\phi}{\\longrightarrow}\\begin{bmatrix}  x_1 \\\\ x_1^2 \\\\ x_1^3\\\\  1 \\end{bmatrix}\n\\]\nOur prediction function will be:\n\\[\nf(\\mathbf{x})=\\phi(\\mathbf{x})^T \\mathbf{w} = w_3 x_1^3 + w_2 x_1^2 + w_1x_1 +b, \\quad\n\\phi(\\mathbf{x}) = \\begin{bmatrix}  x_1 \\\\ x_1^2\\\\ x_1^3 \\\\  1 \\end{bmatrix}\n\\]\nWe can apply this to our regression model for fuel efficiency as before.\n\nviewof form_mpg_3 = Inputs.form(\n  [\n    Inputs.range([-10, 100], {step: 0.01, label: \"b\", value: 45.32}),\n    Inputs.range([-0.03, 0.03], {step: 0.0001, label: \"w_1\", value: -0.0077}),\n    Inputs.range([-0.03, 0.03], {step: 0.0001, label: \"w_2\", value: 0}),\n    Inputs.range([-0.05, 0.05], {step: 0.0001, label: \"w_3\", value: 0}),\n  ]\n)\n\n\n\n\n\n\n\nregressionPlot(data, form_mpg_3, [\"weight\", [\"weight\", x =&gt; (x / 100) * (x / 100)], [\"weight\", x =&gt; (x / 250) * (x / 250) * (x / 250)]], \"mpg\", 0, se)\n\n\n\n\n\n\nWe can also similarly define general polynomial transforms using polynomials of higher degrees. Note that the number of features in the transformed input grows very quickly with the degree of the polynomial and the number of original input features. We’ll often just use a subset of the possible polynomial terms in our transform. For example we might use only powers of individual elements (i.e. \\(x_i^k\\)) with out considering the cross terms (i.e. \\(x_i^kx_j^p\\)).\nFor example we might define the following quadratic transform for 3-feature inputs:\n\\[\n\\phi(\\mathbf{x}) = \\begin{bmatrix}  x_1 \\\\ x_2 \\\\ x_3 \\\\ x_1^2\\\\ x_2^2 \\\\  x_3^2 \\\\ 1 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lecture4-feature-transforms/notes.html#general-feature-transforms",
    "href": "lecture4-feature-transforms/notes.html#general-feature-transforms",
    "title": "Lecture 4: Feature transforms",
    "section": "General feature transforms",
    "text": "General feature transforms\nIt’s also not necessary to restrict ourselves to transforms defined by integer powers of the inputs. We can use any scalar non-linear functions we want. For example we could define a transform using \\(\\sin\\) and \\(\\cos\\):\n\\[\n\\phi(\\mathbf{x}) = \\begin{bmatrix}  x_1 \\\\ x_2 \\\\ \\sin(x_1) \\\\ \\sin(x_2) \\\\ \\cos(x_1) \\\\ \\cos(x_2) \\\\ 1 \\end{bmatrix}\n\\]\nOr using the sigmoid function:\n\\[\n\\phi(\\mathbf{x}) = \\begin{bmatrix}  x_1 \\\\ x_2 \\\\ \\sigma(x_1) \\\\ \\sigma(x_2)  \\\\ 1 \\end{bmatrix}\n\\]\nWe can see how different features allow us to define different nonlinear functions. In the example below we’ll try the prediction function:\n\\[\nf(\\mathbf{x})=\\phi(\\mathbf{x})^T \\mathbf{w} = w_3 e^{x_1} + w_2 \\sin(x_1) + w_1x_1^2 +b ,\\quad \\phi(\\mathbf{x}) = \\begin{bmatrix}  x_1 \\\\ x_1^2 \\\\ \\sin(x_1) \\\\ e^{x_1}  \\\\ 1 \\end{bmatrix}\n\\]\n\nviewof form_mpg_4 = Inputs.form(\n  [\n    Inputs.range([-10, 10], {step: 0.01, label: \"b\", value: 1}),\n    Inputs.range([-10, 10], {step: 0.0001, label: \"w_1\", value: -0.0077}),\n    Inputs.range([-10, 10], {step: 0.0001, label: \"w_2\", value: 0}),\n    Inputs.range([-10, 10], {step: 0.0001, label: \"w_3\", value: 0}),\n    Inputs.range([-0.5, 0.5], {step: 0.0001, label: \"w_4\", value: 0}),\n  ]\n)\n\n\n\n\n\n\n\nregressionPlot(quadratic_data, form_mpg_4, [\"0\", [\"0\", x =&gt; (x) * (x)], [\"0\", x =&gt; Math.sin(x)], [\"0\", x =&gt; Math.exp(x)]], \"1\", 0, se)\n\n\n\n\n\n\n\nsin_X = np.sin(X)               # sin(x)\nsquared_X = X ** 2              # x^2\nexp_X = np.exp(X)               # e^x\nones = np.ones((X.shape[0], 1)) # Column of 1s\n\ntransformedX = np.concatenate([X, squared_X, sin_X, exp_X, ones], axis=1)"
  },
  {
    "objectID": "lecture4-feature-transforms/notes.html#general-feature-transforms-for-classification",
    "href": "lecture4-feature-transforms/notes.html#general-feature-transforms-for-classification",
    "title": "Lecture 4: Feature transforms",
    "section": "General feature transforms for classification",
    "text": "General feature transforms for classification\nWe can also see general feature transforms for classification! Here’s another example"
  },
  {
    "objectID": "lecture4-feature-transforms/notes.html#implicit-quadratic-transforms",
    "href": "lecture4-feature-transforms/notes.html#implicit-quadratic-transforms",
    "title": "Lecture 4: Feature transforms",
    "section": "Implicit quadratic transforms",
    "text": "Implicit quadratic transforms\nLet’s"
  },
  {
    "objectID": "code/ojs.html",
    "href": "code/ojs.html",
    "title": "CS 152: Neural Networks",
    "section": "",
    "text": "Plot = import(\"https://esm.sh/@observablehq/plot\") \nd3 = require(\"d3@7\")\ntopojson = require(\"topojson\")\nMathJax = require(\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-svg.min.js\").catch(() =&gt; window.MathJax)\ntf = require(\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js\").catch(() =&gt; window.tf)\n\nTHREE = {\n  const THREE = window.THREE = await require(\"three@0.130.0/build/three.min.js\");\n  await require(\"three@0.130.0/examples/js/controls/OrbitControls.js\").catch(() =&gt; {});\n  await require(\"three@0.130.0/examples/js/loaders/SVGLoader.js\").catch(() =&gt; {});\n  return THREE;\n}\n\nfunction sample(f, start, end, nsamples=100) {\n  let arr = [...Array(nsamples).keys()]\n  let dist = end - start\n  function arrmap(ind) {\n    const x = (ind * dist) / nsamples + start;\n    return [x, f(x)];\n  }\n  return arr.map(arrmap)\n}\n\nfunction sigmoid(x){\n  return 1 / (1 + Math.exp(-x));\n}\n\nfunction sum(x) {\n  let s = 0;\n  for (let i = 0; i &lt; x.length; i++ ) {\n    s += x[i];\n  }\n  return s;\n}\n\nfunction mean(x) {\n  let s = 0;\n  for (let i = 0; i &lt; x.length; i++ ) {\n    s += x[i];\n  }\n  return s / x.length;\n}\n\nfunction cross_ent(x, y) {\n  return y ? -Math.log(sigmoid(x)) : -Math.log(sigmoid(-x));\n}\n\nfunction se(x, y) {\n  return (x - y) * (x - y);\n}\n\nfunction shuffle(array) {\n  let currentIndex = array.length,  randomIndex;\n\n  // While there remain elements to shuffle.\n  while (currentIndex &gt; 0) {\n\n    // Pick a remaining element.\n    randomIndex = Math.floor(Math.random() * currentIndex);\n    currentIndex--;\n\n    // And swap it with the current element.\n    [array[currentIndex], array[randomIndex]] = [\n      array[randomIndex], array[currentIndex]];\n  }\n\n  return array;\n}\n\nfunction acc(x, y) {\n  return Number(y == (x  &gt; 0));\n}\n\nfunction grid_func(f, width, height, x1, y1, x2, y2) {\n  let values = new Array(width * height);\n  const xstride = (x2 - x1) / width;\n  const ystride = (y2 - y1) / height;\n\n  \n  let y = 0;\n  let x = 0;\n  let ind = 0;\n  for (let i = 0; i &lt; height; i++ ) {\n    for (let j = 0; j &lt; width; j++, ind++) {\n      x = x1 + j * xstride;\n      y = y1 + i * ystride;\n      values[ind] = f(x, y);\n    }\n  }\n  return {width: width, height: height, x1: x1, y1: y1, x2: x2, y2: y2, values: values};\n}\n\nfunction get_accessors(keys, byindex=false) {\n  let isString = value =&gt; typeof value === 'string';\n  \n  let index = 0;\n  let indexmap = {};\n  let accessors = [];\n  for (let i = 0; i &lt; keys.length; i++){\n    let k = keys[i];\n    if (Array.isArray(k)) {\n      let access = isString(k[0]) ? (x =&gt; x[k[0]]) : k[0];\n      \n      if (byindex) {\n        if (isString(k[0]) && !(k[0] in indexmap)) {\n          indexmap[k[0]] = index;\n          index++;\n        }\n        let accessindex = indexmap[k[0]];\n        access = x =&gt; x[accessindex];\n        let process = k[1];\n        let final_access = x =&gt; process(access(x));\n        accessors.push(final_access);\n      }\n      else {\n        let process = k[1];\n        let final_access = x =&gt; process(access(x));\n        accessors.push(final_access);\n      }\n      \n    }\n    else {\n      let access = isString(k) ? (x =&gt; x[k]) : k;\n      if (byindex) { \n        if (isString(k) && !(k in indexmap)) {\n          indexmap[k] = index;\n          index++;\n        }\n        let accessindex = indexmap[k];\n        access = x =&gt; x[accessindex];\n      }\n      accessors.push(access); \n    }\n  }\n  return accessors;\n}\n\nfunction predict(obs, weights, keys=[\"0\", \"1\", \"2\", \"3\"], byindex=false) {\n  let isString = value =&gt; typeof value === 'string';\n  let accessors = get_accessors(keys, byindex);\n  \n  let output = weights[0];\n  let wi = 1;\n  for (let i = 0; (i &lt; keys.length) && (wi &lt; weights.length); i++, wi++){\n    output += weights[wi] * accessors[i](obs);\n  }\n  return output;\n}\n\nfunction mean_loss(f, data, weights, keys, label, l2=0) {\n  let reg = 0;\n  if (l2 &gt; 0){\n    for (let i = 1; i &lt; weights.length; i++) {\n      reg += weights[i] * weights[i];\n    }\n  }\n  \n  const isString = value =&gt; typeof value === 'string';\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n  return mean(data.map(x =&gt; f(predict(x, weights, keys), get_label(x)))) + l2 * reg;\n}\n\nfunction get_domains(data, accessors, margin=0.1) {\n  let domains = [];\n  for (let i = 0; i &lt; accessors.length; i++){\n    let xdomain = d3.extent(data, accessors[i]);\n    let xdsize = (xdomain[1] - xdomain[0]);\n    let xmin = xdomain[0] - xdsize * margin;\n    let xmax = xdomain[1] + xdsize * margin;\n    domains.push([xmin, xmax]);\n  }\n  return domains;\n}\n\nfunction logisticPlot2d(data, weights, keys, label, interval=0.05) {\n  const accuracy = mean_loss(acc, data, weights, keys, label);\n  \n  let isString = value =&gt; typeof value === 'string';\n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, accessors);\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n  \n  return Plot.plot({\n    x: {tickSpacing: 80, label: \"x\"},\n    y: {tickSpacing: 80, label: \"y\"},\n    title: \"Accuracy: \" + accuracy.toFixed(3),\n    color: {type: \"linear\", legend: true, scheme: \"BuRd\", domain: [-0.5, 1.5]},\n    marks: [\n      Plot.contour({\n        fill: (x, y) =&gt; sigmoid(predict([x, y], weights, index_accessors)),\n        x1: domains[0][0], y1: domains[1][0], x2: domains[0][1], y2: domains[1][1], interval: interval,\n      }),\n      Plot.dot(data, {x: accessors[0], y: accessors[1], stroke: x=&gt; (get_label(x) ? 1.35 : -0.35)})\n    ]\n  });\n}\n\nfunction logisticLossPlot2d(data, weights, keys, label) {\n  const loss = mean_loss(cross_ent, data, weights, keys, label);\n  \n  let isString = value =&gt; typeof value === 'string';\n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, accessors);\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n  \n  return Plot.plot({\n    x: {tickSpacing: 80, label: \"x\"},\n    y: {tickSpacing: 80, label: \"y\"},\n    title: \"Loss: \" + loss.toFixed(3),\n    color: {type: \"linear\", legend: true, scheme: \"BuRd\", domain: [0, 5]},\n    marks: [\n      Plot.contour({\n        value: (x, y) =&gt; predict([x, y], weights, index_accessors),\n        fillOpacity: 0.2,\n        stroke: \"black\", x1: domains[0][0], y1: domains[1][0], x2: domains[0][1], y2: domains[1][1],\n        thresholds: [-1e6,  0, 0.00001]\n      }),\n      Plot.dot(data, {x: accessors[0], y: accessors[1], stroke: x=&gt; cross_ent(predict(x, weights, keys), get_label(x)), \n                      strokeOpacity: 0.5 })\n    ]\n  });\n}\n\nfunction lossPlot2d(f, data, keys, label, l2=0, res=100, x1=-40, y1=-0.015, x2=40,  y2=0.015, vmax=50, nlines=25, ctype=\"sqrt\", scale=(x =&gt; x)) {\n  let grid = 0;\n  function lossFunc(w, b) {\n    return scale(mean_loss(f, data, [w, b], keys, label, l2));\n  }\n\n  grid = grid_func(lossFunc,\n                 res, res, x1, y1, x2, y2\n                );\n\n  function plot2d(weights) {\n    let w = weights;\n    if (!(Array.isArray(w[0]))){\n      w = [w];\n    }\n\n    var arrows = w.slice(0, w.length - 1).map(function(e, i) {\n      return e.concat(w[i+1]);\n    });\n\n    let interval= vmax / nlines; \n    let thresholds = [];\n    for (let i = 0; i &lt; nlines; i++) {\n      thresholds.push(i * interval);\n    }\n    let loss = mean_loss(f, data, w[w.length - 1], keys, label, l2)\n    return Plot.plot({\n      title: \"Loss: \" + loss.toFixed(3),\n      color: {type: \"linear\", legend: true, label: \"Loss\", scheme: \"BuRd\", domain: [0, vmax], type: ctype},\n      marks: [\n        Plot.contour(grid.values, {width: grid.width, height: grid.height, x1: grid.x1, x2:grid.x2, y1: grid.y1, y2: grid.y2,\n          stroke: Plot.identity, thresholds: thresholds}),\n        Plot.dot(w),\n        Plot.arrow(arrows, {x1: \"0\", y1: \"1\", x2: \"2\", y2: \"3\", stroke: \"black\"})\n      ]\n    })\n  }\n  return plot2d;\n}\n\n\n\nfunction regressionPlot(data, weights, keys, label, l2, f=se, stroke=\"\") {\n  let loss = mean_loss(f, data, weights, keys, label, l2);\n  let isString = value =&gt; typeof value === 'string';\n  \n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, get_accessors([label].concat(keys)));\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n\n  let stroke_shade = stroke;\n  if (stroke == \"\") {\n    stroke_shade = (x =&gt; f(predict(x, weights, keys), get_label(x)))\n  }\n  \n  return Plot.plot({\n    y: {domain: domains[0]},\n    title: \"Loss: \" + loss.toFixed(3),\n    color: {type: \"linear\", legend: true, label: \"Loss\", scheme: \"BuRd\", domain: [0, 100]},\n    marks: [\n      Plot.line(sample((x) =&gt; predict([x], weights, index_accessors), domains[1][0], domains[1][1]), {stroke: 'black'}),\n      Plot.dot(data, {x: accessors[0], y: get_label, stroke: stroke_shade })\n    ]\n  })\n}\n\nfunction errorPlot(data, weights, keys, label, f, options={}) {\n  const isString = value =&gt; typeof value === 'string';\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n  let errors = data.map(x =&gt; [predict(x, weights, keys) - get_label(x), f(predict(x, weights, keys), get_label(x))]);\n\n  \n  let sigma = (options['sigma'] || 1);\n  let plots = [];\n  const xdomain = (options['xdomain'] || [-30, 30]);\n  const ydomain = (options['ydomain'] || [0, 0.1]);\n  \n\n  if (options['plotnormal']){\n    let pdf = x =&gt; Math.exp(-0.5 * x * x / sigma) * ydomain[1];\n    let normal = Plot.line(sample(pdf, xdomain[0], xdomain[1]), {stroke: 'crimson'});    \n    plots.push(normal);\n  }\n  if (options['plotlaplace']){\n    let pdf = x =&gt; Math.exp(-0.5 * Math.abs(x) / sigma) * ydomain[1];\n    let normal = Plot.line(sample(pdf, xdomain[0], xdomain[1]), {stroke: 'green'});    \n    plots.push(normal);\n  }\n  \n  return Plot.plot({\n    y: {grid: true, domain: ydomain},\n    x: {domain: xdomain},\n    color: {type: \"linear\", legend: true, label: \"Loss\", scheme: \"BuRd\", domain: [0, 100]},\n    marks: [\n      //Plot.rectY(errors, Plot.binX({y: \"count\", fill: x =&gt; mean(x.map(v =&gt; v[1]))}, {x: \"0\"})),\n      \n      Plot.rectY(errors, Plot.binX({y: \"proportion\"}, {x: \"0\", fill: 'steelblue', interval: 1})),\n      Plot.ruleY([0])\n    ].concat(plots)\n  })\n}\n\nfunction nnPlot(data, weights, keys, label, l2, f=se, stroke=\"\", options=[]) {\n  let loss = mean_loss(f, data, weights, keys, label, l2);\n  let isString = value =&gt; typeof value === 'string';\n  \n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, get_accessors([label].concat(keys)));\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n\n  let stroke_shade = stroke;\n  if (stroke == \"\") {\n    stroke_shade = (x =&gt; f(predict(x, weights, keys), get_label(x)))\n  }\n\n  let a = []\n  if (options.indexOf(\"Show feature transforms\") &gt;= 0){\n    a = [Plot.line(sample((x) =&gt;  keys[1][1](x), domains[1][0], domains[1][1]), {stroke: 'red'}),\n      Plot.line(sample((x) =&gt; keys[2][1](x), domains[1][0], domains[1][1]), {stroke: 'blue'})]\n  }\n  \n  return Plot.plot({\n    y: {domain: domains[0]},\n    title: \"Loss: \" + loss.toFixed(3),\n    color: {type: \"linear\", legend: true, label: \"Loss\", scheme: \"BuRd\", domain: [0, 100]},\n    marks: [\n      Plot.line(sample((x) =&gt; predict([x], weights, index_accessors), domains[1][0], domains[1][1]), {stroke: 'black'}),\n      Plot.dot(data, {x: accessors[0], y: get_label, stroke: stroke_shade })\n    ].concat(a)\n  })\n}"
  },
  {
    "objectID": "lecture4-feature-transforms/notes.html#spliting-data-in-practice",
    "href": "lecture4-feature-transforms/notes.html#spliting-data-in-practice",
    "title": "Lecture 4: Feature transforms",
    "section": "Spliting data in practice",
    "text": "Spliting data in practice\nIn general a good rule of thumb is to reserve \\(30\\%\\) of you data for evaluation, but anywhere from \\(10\\%\\) to \\(50\\%\\) is common in practice.\nIt is also very import very important to split data at random. Often real-world data is stored in a meaningul order and we don’t want this order to bias our results. In fact, the previous example was not split randomly. We see that if we do split randomly our evaluation looks much better.\n\n\n\nTraining data\n\nsdata = data.slice()\na = shuffle(sdata)\nregressionPlot(sdata.slice(0,300), [45.32, -0.0077], [\"weight\"], x =&gt; x.mpg, 0, se, \"crimson\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest data\n\nregressionPlot(sdata.slice(300), [45.32, -0.0077], [\"weight\"], x =&gt; x.mpg, 0, se, \"crimson\")\n\n\n\n\n\n\n\n\n\nIn numpy we can accomplish this splitting by creating a random order of observations and applying it to both \\(X\\) and \\(y\\)\n\norder = np.arange(X.shape[0])    # Get an array of indices (1...N)\nnumTrain = int(X.shape[0] * 0.7) # Get the number of training obs. (70%)\ntrainInds = order[:numTrain]     # Get the indices of training obs. (70%)\ntestInds = order[numTrain:]      # Get the indices of test obs. (30%)\n\n# Get the data and labels for each split\ntrainX, trainy = X[trainInds], y[trainInds]\ntestX, testy = X[testInds], y[testInds]"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#training-and-test-datasets",
    "href": "lecture4-feature-transforms/slides.html#training-and-test-datasets",
    "title": "Lecture 4: Feature transforms",
    "section": "Training and test datasets",
    "text": "Training and test datasets\nTo evaluate our model fairly, we will split our dataset into two smaller datasets: a training dataset that we will use to fit our model, and a test or held-out dataset that we will only use to evaluate our model.\n\\[D = \\{ (\\mathbf{x}_1, y_1),\\ (\\mathbf{x}_2, y_2),\\ ... \\,(\\mathbf{x}_N, y_N) \\}\\quad \\longrightarrow \\quad\n\\]\n\\[\nD_{train} = \\{ (\\mathbf{x}_1, y_1),\\ (\\mathbf{x}_2, y_2),\\ ... \\,(\\mathbf{x}_{Ntrain}, y_{Ntrain}) \\},\\  \nD_{test} = \\{ (\\mathbf{x}_1, y_1),\\ (\\mathbf{x}_2, y_2),\\ ... \\,(\\mathbf{x}_{Ntest}, y_{Ntest}) \\}\n\\]\n\n\n\nTraining data\n\n\n\n\n\n\n\n\n\ncar name\nweight\ndisplacement\nhorsepower\nacceleration\n\n\n\n\n0\nchevrolet chevelle malibu\n3504\n307.0\n130\n12.0\n\n\n1\nbuick skylark 320\n3693\n350.0\n165\n11.5\n\n\n2\nplymouth satellite\n3436\n318.0\n150\n11.0\n\n\n3\namc rebel sst\n3433\n304.0\n150\n12.0\n\n\n4\nford torino\n3449\n302.0\n140\n10.5\n\n\n...\n...\n...\n...\n...\n...\n\n\n295\ndodge colt hatchback custom\n1915\n98.0\n80\n14.4\n\n\n296\namc spirit dl\n2670\n121.0\n80\n15.0\n\n\n297\nmercedes benz 300d\n3530\n183.0\n77\n20.1\n\n\n298\ncadillac eldorado\n3900\n350.0\n125\n17.4\n\n\n299\npeugeot 504\n3190\n141.0\n71\n24.8\n\n\n\n\n300 rows × 5 columns\n\n\n\n\n\n\n\n\nTest data\n\n\n\n\n\n\n\n\n\ncar name\nweight\ndisplacement\nhorsepower\nacceleration\n\n\n\n\n300\noldsmobile cutlass salon brougham\n3420\n260.0\n90\n22.2\n\n\n301\nplymouth horizon\n2200\n105.0\n70\n13.2\n\n\n302\nplymouth horizon tc3\n2150\n105.0\n70\n14.9\n\n\n303\ndatsun 210\n2020\n85.0\n65\n19.2\n\n\n304\nfiat strada custom\n2130\n91.0\n69\n14.7\n\n\n...\n...\n...\n...\n...\n...\n\n\n393\nford mustang gl\n2790\n140.0\n86\n15.6\n\n\n394\nvw pickup\n2130\n97.0\n52\n24.6\n\n\n395\ndodge rampage\n2295\n135.0\n84\n11.6\n\n\n396\nford ranger\n2625\n120.0\n79\n18.6\n\n\n397\nchevy s-10\n2720\n119.0\n82\n19.4\n\n\n\n\n98 rows × 5 columns"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#spliting-data-in-practice",
    "href": "lecture4-feature-transforms/slides.html#spliting-data-in-practice",
    "title": "Lecture 4: Feature transforms",
    "section": "Spliting data in practice",
    "text": "Spliting data in practice\nIn general a good rule of thumb is to reserve \\(30\\%\\) of you data for evaluation, but anywhere from \\(10\\%\\) to \\(50\\%\\) is common in practice.\nIt is also very import very important to split data at random.\n\n\n\nTraining data\n\nsdata = data.slice()\na = shuffle(sdata)\nregressionPlot(sdata.slice(0,300), [45.32, -0.0077], [\"weight\"], x =&gt; x.mpg, 0, se, \"crimson\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest data\n\nregressionPlot(sdata.slice(300), [45.32, -0.0077], [\"weight\"], x =&gt; x.mpg, 0, se, \"crimson\")"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#linear-predictions",
    "href": "lecture4-feature-transforms/slides.html#linear-predictions",
    "title": "Lecture 4: Feature transforms",
    "section": "Linear predictions",
    "text": "Linear predictions\nIn the previous two lectures, we saw that the linear regression model makes predictions of the form:\n\\[\nf(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{w} = \\sum_{i=1}^n x_i w_i\n\\]\nMeaning that the output will be a weighted sum of the features of the input.\n\\[\n\\text{Predicted MPG} = f(\\mathbf{x})=\n\\]\n\\[\n(\\text{weight})w_1 + (\\text{horsepower})w_2 + (\\text{displacement})w_3 + (\\text{0-60mph})w_4 + b\n\\]"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#non-linear-data",
    "href": "lecture4-feature-transforms/slides.html#non-linear-data",
    "title": "Lecture 4: Feature transforms",
    "section": "Non-linear data",
    "text": "Non-linear data\nUnfortunately, in the real world the relationship between inputs and outputs is not always linear.\n\nviewof form_quadratic = Inputs.form(\n  [\n    Inputs.range([-10, 10], {step: 0.01, label: \"b\", value: 2.89}),\n    Inputs.range([-10, 10], {step: 0.0001, label: \"w\", value: 2.0165}),\n  ]\n)\n\n\n\n\n\n\n\n\n\n\n\n\nregressionPlot(quadratic_data, form_quadratic, [\"0\"], \"1\", 0, se)\n\n\n\n\n\n\nWe see that there is no straight line that is a good fit to our data."
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#polynomial-functions",
    "href": "lecture4-feature-transforms/slides.html#polynomial-functions",
    "title": "Lecture 4: Feature transforms",
    "section": "Polynomial functions",
    "text": "Polynomial functions\nIf we’re trying to approximate a non-linear relationship between inputs and outputs, it follows that we may want to fit a non-linear approximation.\nOne of the simplest types of non-linear functions we could use are polynomial functions.\nThe simplest type of non-linear polynomial is a quadratic function, which involves powers of up to \\(2\\).\n\\[\nf(x) = w_2 x^2 + w_1x +b\n\\]"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#polynomial-functions-as-vector-functions",
    "href": "lecture4-feature-transforms/slides.html#polynomial-functions-as-vector-functions",
    "title": "Lecture 4: Feature transforms",
    "section": "Polynomial functions as vector functions",
    "text": "Polynomial functions as vector functions\nWe can also write polynomial functions as vector-input functions. For example a quadratic function of two variables could be written as:\n\\[\nf(\\mathbf{x}) = w_5 x_2^2 + w_4x_1^2 + w_3 x_1 x_2 + w_2x_2 + w_1x_1 +b\n\\]\nFrom this form we see that a polynomial is a weighted sum of powers of \\(\\mathbf{x}\\)!\n\\[\nw_5 x_2^2 + w_4x_1^2 + w_3 x_1 x_2 + w_2x + w_1y +b = \\begin{bmatrix}  x_1 \\\\ x_2 \\\\ x_1 x_2 \\\\ x_1^2 \\\\ x_2^2 \\\\  1 \\end{bmatrix} \\cdot \\begin{bmatrix}  w_1 \\\\ w_2 \\\\ w_3 \\\\ w_4 \\\\ w_5 \\\\  b \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#quadratic-feature-transforms",
    "href": "lecture4-feature-transforms/slides.html#quadratic-feature-transforms",
    "title": "Lecture 4: Feature transforms",
    "section": "Quadratic feature transforms",
    "text": "Quadratic feature transforms\nLet’s consider the mapping from \\(\\mathbf{x}\\) to powers of the elements of \\(\\mathbf{x}\\). We’ll call this mapping \\(\\phi\\):\n\\[\n\\begin{bmatrix}  x_1 \\\\ x_2 \\end{bmatrix}\\underset{\\phi}{\\longrightarrow}\\begin{bmatrix}  x_1 \\\\ x_2 \\\\ x_1 x_2 \\\\ x_1^2 \\\\ x_2^2 \\\\  1 \\end{bmatrix}\n\\]\nIn this quadratic example \\(\\phi\\) is a non-linear function that maps vectors to vectors \\((\\mathbb{R}^2 \\rightarrow \\mathbb{R}^6)\\). We call this a quadratic feature transform\n\\[\n\\phi(\\mathbf{x}) = \\begin{bmatrix}  x_1 \\\\ x_2 \\\\ x_1 x_2 \\\\ x_1^2 \\\\ x_2^2 \\\\  1 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#fitting-quadratic-regression",
    "href": "lecture4-feature-transforms/slides.html#fitting-quadratic-regression",
    "title": "Lecture 4: Feature transforms",
    "section": "Fitting quadratic regression",
    "text": "Fitting quadratic regression\nIf we treat \\(\\phi(\\mathbf{x})\\) as our new set of inputs, we see that we can apply all the same tools of linear regression that we learned before.\n\\[\nf(\\mathbf{x})=\\phi(\\mathbf{x})^T \\mathbf{w}, \\quad\n\\phi(\\mathbf{x}) = \\begin{bmatrix}  x_1 \\\\ x_2 \\\\ x_1 x_2 \\\\ x_1^2 \\\\ x_2^2 \\\\  1 \\end{bmatrix}\n\\]\nWe can then define a quadratic probabilistic model as:\n\\[\ny_i \\sim N\\big(\\phi(\\mathbf{x}_i)^T\\mathbf{w}, \\sigma^2\\big)\n\\]"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#quadratic-regression-on-real-data",
    "href": "lecture4-feature-transforms/slides.html#quadratic-regression-on-real-data",
    "title": "Lecture 4: Feature transforms",
    "section": "Quadratic regression on real data",
    "text": "Quadratic regression on real data\nLet’s look at our new quadratic regression model on the problem of predicting fuel efficiency from a car’s weight.\n\\[\nf(\\mathbf{x})=\\phi(\\mathbf{x})^T \\mathbf{w} = w_2 x_1^2 + w_1x_1 +b, \\quad\n\\phi(\\mathbf{x}) = \\begin{bmatrix}  x_1 \\\\ x_1^2 \\\\  1 \\end{bmatrix}\n\\]\nWe see that by varying \\(w_2\\), we can now fit a curve to our data and get a better overall loss!\n\nviewof form_mpg_2 = Inputs.form(\n  [\n    Inputs.range([-10, 100], {step: 0.01, label: \"b\", value: 45.32}),\n    Inputs.range([-0.03, 0.03], {step: 0.0001, label: \"w_1\", value: -0.0077}),\n    Inputs.range([-0.03, 0.03], {step: 0.0001, label: \"w_2\", value: 0}),\n  ]\n)\n\n\n\n\n\n\n\nregressionPlot(data, form_mpg_2, [\"weight\", [\"weight\", x =&gt; (x / 100) * (x / 100)]], \"mpg\", 0, se)"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#quadratic-logistic-regression",
    "href": "lecture4-feature-transforms/slides.html#quadratic-logistic-regression",
    "title": "Lecture 4: Feature transforms",
    "section": "Quadratic logistic regression",
    "text": "Quadratic logistic regression\nJust like with our regression example, we can apply our quadratic feature transform to the logistic regression model as well! In this case our prediction function becomes:\n\\[\nf(\\mathbf{x}) = \\mathbf{I}(\\phi(\\mathbf{x})^T\\mathbf{w} \\geq 0), \\quad \\phi(\\mathbf{x}) = \\begin{bmatrix}  x_1 \\\\ x_2 \\\\ x_1 x_2 \\\\ x_1^2 \\\\ x_2^2 \\\\  1 \\end{bmatrix}\n\\]\nOur Bernoulli probabilistic model becomes:\n\\[\ny_i \\sim \\mathbf{Bernoulli}\\big(\\mathbf{ \\sigma(\\phi(\\mathbf{x}_i)^T\\mathbf{w} })\\big), \\quad p(y_i = 1\\mid \\mathbf{x}_i, \\mathbf{w}) = \\sigma\\big(\\phi(\\mathbf{x}_i)^T\\mathbf{w}\\big)\n\\]"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#general-polynomial-transforms",
    "href": "lecture4-feature-transforms/slides.html#general-polynomial-transforms",
    "title": "Lecture 4: Feature transforms",
    "section": "General polynomial transforms",
    "text": "General polynomial transforms\nWe’re not restricted to just quadratic transform! For example, for a model with \\(1\\) input, we could definite a cubic feature transform as:\n\\[\n\\begin{bmatrix}  x_1  \\end{bmatrix}\\underset{\\phi}{\\longrightarrow}\\begin{bmatrix}  x_1 \\\\ x_1^2 \\\\ x_1^3\\\\  1 \\end{bmatrix}\n\\]\nOur prediction function will be:\n\\[\nf(\\mathbf{x})=\\phi(\\mathbf{x})^T \\mathbf{w} = w_3 x_1^3 + w_2 x_1^2 + w_1x_1 +b, \\quad\n\\phi(\\mathbf{x}) = \\begin{bmatrix}  x_1 \\\\ x_1^2\\\\ x_1^3 \\\\  1 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#general-feature-transforms",
    "href": "lecture4-feature-transforms/slides.html#general-feature-transforms",
    "title": "Lecture 4: Feature transforms",
    "section": "General feature transforms",
    "text": "General feature transforms\nWe can also use any scalar non-linear functions we want. For example we could define a transform using \\(\\sin\\) and \\(\\cos\\):\n\\[\n\\phi(\\mathbf{x}) = \\begin{bmatrix}  x_1 \\\\ x_2 \\\\ \\sin(x_1) \\\\ \\sin(x_2) \\\\ \\cos(x_1) \\\\ \\cos(x_2) \\\\ 1 \\end{bmatrix}\n\\]\nOr using the sigmoid function:\n\\[\n\\phi(\\mathbf{x}) = \\begin{bmatrix}  x_1 \\\\ x_2 \\\\ \\sigma(x_1) \\\\ \\sigma(x_2)  \\\\ 1 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#training-and-test-datasets-1",
    "href": "lecture4-feature-transforms/slides.html#training-and-test-datasets-1",
    "title": "Lecture 4: Feature transforms",
    "section": "Training and test datasets",
    "text": "Training and test datasets\nFor example, we might see that our model does well on the data it was fit on and poorly on new data.\n\n\n\nTraining data\n\nregressionPlot(data.slice(0,300), [45.32, -0.0077], [\"weight\"], x =&gt; x.mpg, 0, se, \"crimson\")\n\n\n\n\n\n\n\n\n\nTest data\n\nregressionPlot(data.slice(300), [45.32, -0.0077], [\"weight\"], x =&gt; x.mpg, 0, se, \"crimson\")"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#spliting-data-in-practice-1",
    "href": "lecture4-feature-transforms/slides.html#spliting-data-in-practice-1",
    "title": "Lecture 4: Feature transforms",
    "section": "Spliting data in practice",
    "text": "Spliting data in practice\nIn numpy we can accomplish this splitting by creating a random order of observations and applying it to both \\(X\\) and \\(y\\)\n\norder = np.arange(X.shape[0])    # Get an array of indices (1...N)\nnp.random.shuffle(order)         # Permute the data\n\nnumTrain = int(X.shape[0] * 0.7) # Get the number of training obs. (70%)\ntrainInds = order[:numTrain]     # Get the indices of training obs. (70%)\ntestInds = order[numTrain:]      # Get the indices of test obs. (30%)\n\n# Get the data and labels for each split\ntrainX, trainy = X[trainInds], y[trainInds]\ntestX, testy = X[testInds], y[testInds]"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#linear-predictions-1",
    "href": "lecture4-feature-transforms/slides.html#linear-predictions-1",
    "title": "Lecture 4: Feature transforms",
    "section": "Linear predictions",
    "text": "Linear predictions\nGraphically we see this corresponds to a prediction function that is a line or a plane.\n\nregressionPlot(data, [45.32, -0.0077], [\"weight\"], x =&gt; x.mpg, 0, se, \"crimson\")"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#quadratic-functions",
    "href": "lecture4-feature-transforms/slides.html#quadratic-functions",
    "title": "Lecture 4: Feature transforms",
    "section": "Quadratic functions",
    "text": "Quadratic functions\nA quadratic function of \\(2\\) variables can be written as:\n\\[\nf(x, y) = w_5 x^2 + w_4y^2 + w_3 xy + w_2x + w_1y +b\n\\]"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#cubic-functions",
    "href": "lecture4-feature-transforms/slides.html#cubic-functions",
    "title": "Lecture 4: Feature transforms",
    "section": "Cubic functions",
    "text": "Cubic functions\nSimilarly a cubic function involves powers up to 3:\n\\[\nf(x) = w_3 x^3 + w_2 x^2 + w_1x +b\n\\]"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#polynomial-functions-1",
    "href": "lecture4-feature-transforms/slides.html#polynomial-functions-1",
    "title": "Lecture 4: Feature transforms",
    "section": "Polynomial functions",
    "text": "Polynomial functions\nIn general the degree of a polynomial is the largest exponent in any term of the polynomial (or sum of exponents for terms involving more than 1 input). For example we can look at 2 different degree 4 polynomial functions:\n\\[\nf(x, y) = 3 x^4 + 2 xy + y - 2\n\\]\n\\[\nf(x, y) = -2 x^2y^2 + 2 x^3 + y^2 - 5\n\\]"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#quadratic-feature-transforms-1",
    "href": "lecture4-feature-transforms/slides.html#quadratic-feature-transforms-1",
    "title": "Lecture 4: Feature transforms",
    "section": "Quadratic feature transforms",
    "text": "Quadratic feature transforms\n\\[\n\\phi(\\mathbf{x}) = \\begin{bmatrix}  x_1 \\\\ x_2 \\\\ x_1 x_2 \\\\ x_1^2 \\\\ x_2^2 \\\\  1 \\end{bmatrix}\n\\] With this mapping we can our quadratic prediction function simply as:\n\\[\nf(\\mathbf{x})=\\phi(\\mathbf{x})^T \\mathbf{w}\n\\]\nThis is a linear function of \\(\\phi(\\mathbf{x})\\) and \\(\\mathbf{w}\\)!"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#quadratic-feature-transforms-2",
    "href": "lecture4-feature-transforms/slides.html#quadratic-feature-transforms-2",
    "title": "Lecture 4: Feature transforms",
    "section": "Quadratic feature transforms",
    "text": "Quadratic feature transforms\nAs a simpler example, let’s look at the case where our input has only a single element \\((x_1)\\).\n\\[\nf(\\mathbf{x})=\\phi(\\mathbf{x})^T \\mathbf{w} = w_2 x_1^2 + w_1x_1 +b, \\quad\n\\phi(\\mathbf{x}) = \\begin{bmatrix}  x_1 \\\\ x_1^2 \\\\  1 \\end{bmatrix}\n\\]\n\nviewof form_quadratic_2 = Inputs.form(\n  [\n    Inputs.range([-10, 10], {step: 0.01, label: \"b\", value: 2.89}),\n    Inputs.range([-10, 10], {step: 0.0001, label: \"w_1\", value: 2.0165}),\n    Inputs.range([-10, 10], {step: 0.0001, label: \"w_2\", value: 0}),\n  ]\n)\n\n\n\n\n\n\n\nregressionPlot(quadratic_data, form_quadratic_2, [[\"0\", x =&gt; x], [\"0\", x =&gt; x * x]], \"1\", 0, se)"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#fitting-quadratic-regression-1",
    "href": "lecture4-feature-transforms/slides.html#fitting-quadratic-regression-1",
    "title": "Lecture 4: Feature transforms",
    "section": "Fitting quadratic regression",
    "text": "Fitting quadratic regression\n\\[\ny_i \\sim N\\big(\\phi(\\mathbf{x}_i)^T\\mathbf{w}, \\sigma^2\\big)\n\\] The corresponding negative log-likelihood loss becomes\n\\[\n\\textbf{Loss}(\\mathbf{w})=\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})=- \\sum_{i=1}^N \\log p(y_i \\mid \\mathbf{x}_i, \\mathbf{w})\n\\]\n\\[\n= \\frac{1}{2\\sigma^2} \\sum_{i=1}^N\\big(y_i - \\phi(\\mathbf{x}_i)^T\\mathbf{w}\\big)^2 + N \\log \\sigma \\sqrt{2 \\pi}\n\\]"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#quadratic-logistic-regression-1",
    "href": "lecture4-feature-transforms/slides.html#quadratic-logistic-regression-1",
    "title": "Lecture 4: Feature transforms",
    "section": "Quadratic logistic regression",
    "text": "Quadratic logistic regression\nThe corresponding negative log-likelihood is:\n\\[\n\\mathbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = -\\sum_{i=1}^N \\log\\sigma\\big((2y_i-1)\\phi(\\mathbf{x}_i)^T\\mathbf{w}\\big)\n\\]\n\\[\ny_i \\sim \\mathbf{Bernoulli}\\big(\\mathbf{ \\sigma(\\phi(\\mathbf{x}_i)^T\\mathbf{w} })\\big), \\quad p(y_i = 1\\mid \\mathbf{x}_i, \\mathbf{w}) = \\sigma\\big(\\phi(\\mathbf{x}_i)^T\\mathbf{w}\\big)\n\\] Which we can once again optimize with gradient descent."
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#quadratic-logistic-regression-2",
    "href": "lecture4-feature-transforms/slides.html#quadratic-logistic-regression-2",
    "title": "Lecture 4: Feature transforms",
    "section": "Quadratic logistic regression",
    "text": "Quadratic logistic regression\nWith this approach our decision boundary is no longer restricted to be a line!\n\n\n\n\n\n\nviewof form_circles = Inputs.form(\n  [\n    Inputs.range([-100, 100], {step: 0.01, label: \"b\", value: 0}),\n    Inputs.range([-100, 100], {step: 0.0001, label: \"w_1\", value: 20}),\n    Inputs.range([-100, 100], {step: 0.0001, label: \"w_2\", value: 20}),\n      Inputs.range([-100, 100], {step: 0.0001, label: \"w_1\", value: 0}),\n      Inputs.range([-100, 100], {step: 0.0001, label: \"w_1\", value: 0}),\n  ]\n)\n\n\n\n\n\n\n\n\n\nlogisticPlot2d(circles, form_circles, [\"0\", \"1\", [\"0\", x=&gt;x*x], [\"1\", x=&gt;x*x]], \"2\")\n\n\n\n\n\n\n\n\nlogisticLossPlot2d(circles, form_circles, [\"0\", \"1\", [\"0\", x=&gt;x*x], [\"1\", x=&gt;x*x]], \"2\")"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#quadratic-logistic-regression-3",
    "href": "lecture4-feature-transforms/slides.html#quadratic-logistic-regression-3",
    "title": "Lecture 4: Feature transforms",
    "section": "Quadratic logistic regression",
    "text": "Quadratic logistic regression\nRecall that our linear classifier made predictions by thresholding a linear function. Our quadratic classifer thresholds a quadratic function of 1 or more variables.\n\n\n\nLinear decision boundary\n\n\n\n\n\n\n\n\nQuadratic decision boundary"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#general-polynomial-transforms-1",
    "href": "lecture4-feature-transforms/slides.html#general-polynomial-transforms-1",
    "title": "Lecture 4: Feature transforms",
    "section": "General polynomial transforms",
    "text": "General polynomial transforms\n\\[\nf(\\mathbf{x})=\\phi(\\mathbf{x})^T \\mathbf{w} = w_3 x_1^3 + w_2 x_1^2 + w_1x_1 +b, \\quad\n\\phi(\\mathbf{x}) = \\begin{bmatrix}  x_1 \\\\ x_1^2\\\\ x_1^3 \\\\  1 \\end{bmatrix}\n\\] We can apply this to our regression model for fuel efficiency as before.\n\nviewof form_mpg_3 = Inputs.form(\n  [\n    Inputs.range([-10, 100], {step: 0.01, label: \"b\", value: 45.32}),\n    Inputs.range([-0.03, 0.03], {step: 0.0001, label: \"w_1\", value: -0.0077}),\n    Inputs.range([-0.03, 0.03], {step: 0.0001, label: \"w_2\", value: 0}),\n    Inputs.range([-0.05, 0.05], {step: 0.0001, label: \"w_3\", value: 0}),\n  ]\n)\n\n\n\n\n\n\n\nregressionPlot(data, form_mpg_3, [\"weight\", [\"weight\", x =&gt; (x / 100) * (x / 100)], [\"weight\", x =&gt; (x / 250) * (x / 250) * (x / 250)]], \"mpg\", 0, se)"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#general-polynomial-transforms-2",
    "href": "lecture4-feature-transforms/slides.html#general-polynomial-transforms-2",
    "title": "Lecture 4: Feature transforms",
    "section": "General polynomial transforms",
    "text": "General polynomial transforms\nWe can also similarly define general polynomial transforms using polynomials of higher degrees or a subset of the features.\nFor example we might define the following quadratic transform for 3-feature inputs:\n\\[\n\\phi(\\mathbf{x}) = \\begin{bmatrix}  x_1 \\\\ x_2 \\\\ x_3 \\\\ x_1^2\\\\ x_2^2 \\\\  x_3^2 \\\\ 1 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#general-feature-transforms-1",
    "href": "lecture4-feature-transforms/slides.html#general-feature-transforms-1",
    "title": "Lecture 4: Feature transforms",
    "section": "General feature transforms",
    "text": "General feature transforms\nWe can see how different features allow us to define different nonlinear functions.\n\\[\nf(\\mathbf{x})=\\phi(\\mathbf{x})^T \\mathbf{w} = w_3 e^{x_1} + w_2 \\sin(x_1) + w_1x_1^2 +b ,\\quad \\phi(\\mathbf{x}) = \\begin{bmatrix}  x_1 \\\\ x_1^2 \\\\ \\sin(x_1) \\\\ e^{x_1}  \\\\ 1 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#general-feature-transforms-2",
    "href": "lecture4-feature-transforms/slides.html#general-feature-transforms-2",
    "title": "Lecture 4: Feature transforms",
    "section": "General feature transforms",
    "text": "General feature transforms\n\nviewof form_mpg_4 = Inputs.form(\n  [\n    Inputs.range([-10, 10], {step: 0.01, label: \"b\", value: 1}),\n    Inputs.range([-10, 10], {step: 0.0001, label: \"w_1 (x)\", value: -0.0077}),\n    Inputs.range([-10, 10], {step: 0.0001, label: \"w_2 (x^2)\", value: 0}),\n    Inputs.range([-10, 10], {step: 0.0001, label: \"w_3 (sin x)\", value: 0}),\n    Inputs.range([-0.5, 0.5], {step: 0.0001, label: \"w_4 (exp(x))\", value: 0}),\n  ]\n)\n\n\n\n\n\n\n\nregressionPlot(quadratic_data, form_mpg_4, [\"0\", [\"0\", x =&gt; (x) * (x)], [\"0\", x =&gt; Math.sin(x)], [\"0\", x =&gt; Math.exp(x)]], \"1\", 0, se)"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#general-feature-transforms-3",
    "href": "lecture4-feature-transforms/slides.html#general-feature-transforms-3",
    "title": "Lecture 4: Feature transforms",
    "section": "General feature transforms",
    "text": "General feature transforms\nIn numpy we can write this as:\n\nsin_X = np.sin(X)               # sin(x)\nsquared_X = X ** 2              # x^2\nexp_X = np.exp(X)               # e^x\nones = np.ones((X.shape[0], 1)) # Column of 1s\n\ntransformedX = np.concatenate([X, squared_X, sin_X, exp_X, ones], axis=1)"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#non-linear-data-1",
    "href": "lecture4-feature-transforms/slides.html#non-linear-data-1",
    "title": "Lecture 4: Feature transforms",
    "section": "Non-linear data",
    "text": "Non-linear data\nWe see this with our real-world fuel efficiency dataset as well.\n\nviewof form_mpg_linear = Inputs.form(\n  [\n    Inputs.range([-10, 100], {step: 0.01, label: \"b\", value: 45.32}),\n    Inputs.range([-0.03, 0.03], {step: 0.0001, label: \"w\", value: -0.0077}),\n  ]\n)\n\n\n\n\n\n\n\nregressionPlot(data, form_mpg_linear, [\"weight\"], \"mpg\", 0, se)"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#fitting-quadratic-regression-2",
    "href": "lecture4-feature-transforms/slides.html#fitting-quadratic-regression-2",
    "title": "Lecture 4: Feature transforms",
    "section": "Fitting quadratic regression",
    "text": "Fitting quadratic regression\n\\[\ny_i \\sim N\\big(\\phi(\\mathbf{x}_i)^T\\mathbf{w}, \\sigma^2\\big)\n\\]\nWe can now find the optimal \\(\\mathbf{w}\\) by once again minimizing this loss!\n\\[\n\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmin}} \\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})\n\\]"
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html#fitting-quadratic-regression-3",
    "href": "lecture4-feature-transforms/slides.html#fitting-quadratic-regression-3",
    "title": "Lecture 4: Feature transforms",
    "section": "Fitting quadratic regression",
    "text": "Fitting quadratic regression\n\\[\n\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})=y_i \\sim N\\big(\\phi(\\mathbf{x}_i)^T\\mathbf{w}, \\sigma^2\\big)\n\\] We see that the gradient doesn’t change, it simply involves \\(\\phi(\\mathbf{x}_i)\\) instead of \\(\\mathbf{x}_i\\).\n\\[\n\\nabla_{\\mathbf{w}}\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})\n= \\frac{1}{2\\sigma^2}\\sum_{i=1}^N \\big(\\phi(\\mathbf{x}_i)^T\\mathbf{w} - y_i\\big)\\phi(\\mathbf{x}_i)\n\\] This is because we are only taking the gradient with respect to \\(\\mathbf{w}\\). From the perspective of \\(\\mathbf{w}\\), the prediction funciton is still linear."
  },
  {
    "objectID": "lecture4-feature-transforms/slides.html",
    "href": "lecture4-feature-transforms/slides.html",
    "title": "Lecture 4: Feature transforms",
    "section": "",
    "text": "Plot = import(\"https://esm.sh/@observablehq/plot\") \nd3 = require(\"d3@7\")\ntopojson = require(\"topojson\")\nMathJax = require(\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-svg.min.js\").catch(() =&gt; window.MathJax)\ntf = require(\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js\").catch(() =&gt; window.tf)\n\nTHREE = {\n  const THREE = window.THREE = await require(\"three@0.130.0/build/three.min.js\");\n  await require(\"three@0.130.0/examples/js/controls/OrbitControls.js\").catch(() =&gt; {});\n  await require(\"three@0.130.0/examples/js/loaders/SVGLoader.js\").catch(() =&gt; {});\n  return THREE;\n}\n\nfunction sample(f, start, end, nsamples=100) {\n  let arr = [...Array(nsamples).keys()]\n  let dist = end - start\n  function arrmap(ind) {\n    const x = (ind * dist) / nsamples + start;\n    return [x, f(x)];\n  }\n  return arr.map(arrmap)\n}\n\nfunction sigmoid(x){\n  return 1 / (1 + Math.exp(-x));\n}\n\nfunction sum(x) {\n  let s = 0;\n  for (let i = 0; i &lt; x.length; i++ ) {\n    s += x[i];\n  }\n  return s;\n}\n\nfunction mean(x) {\n  let s = 0;\n  for (let i = 0; i &lt; x.length; i++ ) {\n    s += x[i];\n  }\n  return s / x.length;\n}\n\nfunction cross_ent(x, y) {\n  return y ? -Math.log(sigmoid(x)) : -Math.log(sigmoid(-x));\n}\n\nfunction se(x, y) {\n  return (x - y) * (x - y);\n}\n\nfunction shuffle(array) {\n  let currentIndex = array.length,  randomIndex;\n\n  // While there remain elements to shuffle.\n  while (currentIndex &gt; 0) {\n\n    // Pick a remaining element.\n    randomIndex = Math.floor(Math.random() * currentIndex);\n    currentIndex--;\n\n    // And swap it with the current element.\n    [array[currentIndex], array[randomIndex]] = [\n      array[randomIndex], array[currentIndex]];\n  }\n\n  return array;\n}\n\nfunction acc(x, y) {\n  return Number(y == (x  &gt; 0));\n}\n\nfunction grid_func(f, width, height, x1, y1, x2, y2) {\n  let values = new Array(width * height);\n  const xstride = (x2 - x1) / width;\n  const ystride = (y2 - y1) / height;\n\n  \n  let y = 0;\n  let x = 0;\n  let ind = 0;\n  for (let i = 0; i &lt; height; i++ ) {\n    for (let j = 0; j &lt; width; j++, ind++) {\n      x = x1 + j * xstride;\n      y = y1 + i * ystride;\n      values[ind] = f(x, y);\n    }\n  }\n  return {width: width, height: height, x1: x1, y1: y1, x2: x2, y2: y2, values: values};\n}\n\nfunction get_accessors(keys, byindex=false) {\n  let isString = value =&gt; typeof value === 'string';\n  \n  let index = 0;\n  let indexmap = {};\n  let accessors = [];\n  for (let i = 0; i &lt; keys.length; i++){\n    let k = keys[i];\n    if (Array.isArray(k)) {\n      let access = isString(k[0]) ? (x =&gt; x[k[0]]) : k[0];\n      \n      if (byindex) {\n        if (isString(k[0]) && !(k[0] in indexmap)) {\n          indexmap[k[0]] = index;\n          index++;\n        }\n        let accessindex = indexmap[k[0]];\n        access = x =&gt; x[accessindex];\n        let process = k[1];\n        let final_access = x =&gt; process(access(x));\n        accessors.push(final_access);\n      }\n      else {\n        let process = k[1];\n        let final_access = x =&gt; process(access(x));\n        accessors.push(final_access);\n      }\n      \n    }\n    else {\n      let access = isString(k) ? (x =&gt; x[k]) : k;\n      if (byindex) { \n        if (isString(k) && !(k in indexmap)) {\n          indexmap[k] = index;\n          index++;\n        }\n        let accessindex = indexmap[k];\n        access = x =&gt; x[accessindex];\n      }\n      accessors.push(access); \n    }\n  }\n  return accessors;\n}\n\nfunction predict(obs, weights, keys=[\"0\", \"1\", \"2\", \"3\"], byindex=false) {\n  let isString = value =&gt; typeof value === 'string';\n  let accessors = get_accessors(keys, byindex);\n  \n  let output = weights[0];\n  let wi = 1;\n  for (let i = 0; (i &lt; keys.length) && (wi &lt; weights.length); i++, wi++){\n    output += weights[wi] * accessors[i](obs);\n  }\n  return output;\n}\n\nfunction mean_loss(f, data, weights, keys, label, l2=0) {\n  let reg = 0;\n  if (l2 &gt; 0){\n    for (let i = 1; i &lt; weights.length; i++) {\n      reg += weights[i] * weights[i];\n    }\n  }\n  \n  const isString = value =&gt; typeof value === 'string';\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n  return mean(data.map(x =&gt; f(predict(x, weights, keys), get_label(x)))) + l2 * reg;\n}\n\nfunction get_domains(data, accessors, margin=0.1) {\n  let domains = [];\n  for (let i = 0; i &lt; accessors.length; i++){\n    let xdomain = d3.extent(data, accessors[i]);\n    let xdsize = (xdomain[1] - xdomain[0]);\n    let xmin = xdomain[0] - xdsize * margin;\n    let xmax = xdomain[1] + xdsize * margin;\n    domains.push([xmin, xmax]);\n  }\n  return domains;\n}\n\nfunction logisticPlot2d(data, weights, keys, label, interval=0.05) {\n  const accuracy = mean_loss(acc, data, weights, keys, label);\n  \n  let isString = value =&gt; typeof value === 'string';\n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, accessors);\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n  \n  return Plot.plot({\n    x: {tickSpacing: 80, label: \"x\"},\n    y: {tickSpacing: 80, label: \"y\"},\n    title: \"Accuracy: \" + accuracy.toFixed(3),\n    color: {type: \"linear\", legend: true, scheme: \"BuRd\", domain: [-0.5, 1.5]},\n    marks: [\n      Plot.contour({\n        fill: (x, y) =&gt; sigmoid(predict([x, y], weights, index_accessors)),\n        x1: domains[0][0], y1: domains[1][0], x2: domains[0][1], y2: domains[1][1], interval: interval,\n      }),\n      Plot.dot(data, {x: accessors[0], y: accessors[1], stroke: x=&gt; (get_label(x) ? 1.35 : -0.35)})\n    ]\n  });\n}\n\nfunction logisticLossPlot2d(data, weights, keys, label) {\n  const loss = mean_loss(cross_ent, data, weights, keys, label);\n  \n  let isString = value =&gt; typeof value === 'string';\n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, accessors);\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n  \n  return Plot.plot({\n    x: {tickSpacing: 80, label: \"x\"},\n    y: {tickSpacing: 80, label: \"y\"},\n    title: \"Loss: \" + loss.toFixed(3),\n    color: {type: \"linear\", legend: true, scheme: \"BuRd\", domain: [0, 5]},\n    marks: [\n      Plot.contour({\n        value: (x, y) =&gt; predict([x, y], weights, index_accessors),\n        fillOpacity: 0.2,\n        stroke: \"black\", x1: domains[0][0], y1: domains[1][0], x2: domains[0][1], y2: domains[1][1],\n        thresholds: [-1e6,  0, 0.00001]\n      }),\n      Plot.dot(data, {x: accessors[0], y: accessors[1], stroke: x=&gt; cross_ent(predict(x, weights, keys), get_label(x)), \n                      strokeOpacity: 0.5 })\n    ]\n  });\n}\n\nfunction lossPlot2d(f, data, keys, label, l2=0, res=100, x1=-40, y1=-0.015, x2=40,  y2=0.015, vmax=50, nlines=25, ctype=\"sqrt\", scale=(x =&gt; x)) {\n  let grid = 0;\n  function lossFunc(w, b) {\n    return scale(mean_loss(f, data, [w, b], keys, label, l2));\n  }\n\n  grid = grid_func(lossFunc,\n                 res, res, x1, y1, x2, y2\n                );\n\n  function plot2d(weights) {\n    let w = weights;\n    if (!(Array.isArray(w[0]))){\n      w = [w];\n    }\n\n    var arrows = w.slice(0, w.length - 1).map(function(e, i) {\n      return e.concat(w[i+1]);\n    });\n\n    let interval= vmax / nlines; \n    let thresholds = [];\n    for (let i = 0; i &lt; nlines; i++) {\n      thresholds.push(i * interval);\n    }\n    let loss = mean_loss(f, data, w[w.length - 1], keys, label, l2)\n    return Plot.plot({\n      title: \"Loss: \" + loss.toFixed(3),\n      color: {type: \"linear\", legend: true, label: \"Loss\", scheme: \"BuRd\", domain: [0, vmax], type: ctype},\n      marks: [\n        Plot.contour(grid.values, {width: grid.width, height: grid.height, x1: grid.x1, x2:grid.x2, y1: grid.y1, y2: grid.y2,\n          stroke: Plot.identity, thresholds: thresholds}),\n        Plot.dot(w),\n        Plot.arrow(arrows, {x1: \"0\", y1: \"1\", x2: \"2\", y2: \"3\", stroke: \"black\"})\n      ]\n    })\n  }\n  return plot2d;\n}\n\n\n\nfunction regressionPlot(data, weights, keys, label, l2, f=se, stroke=\"\") {\n  let loss = mean_loss(f, data, weights, keys, label, l2);\n  let isString = value =&gt; typeof value === 'string';\n  \n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, get_accessors([label].concat(keys)));\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n\n  let stroke_shade = stroke;\n  if (stroke == \"\") {\n    stroke_shade = (x =&gt; f(predict(x, weights, keys), get_label(x)))\n  }\n  \n  return Plot.plot({\n    y: {domain: domains[0]},\n    title: \"Loss: \" + loss.toFixed(3),\n    color: {type: \"linear\", legend: true, label: \"Loss\", scheme: \"BuRd\", domain: [0, 100]},\n    marks: [\n      Plot.line(sample((x) =&gt; predict([x], weights, index_accessors), domains[1][0], domains[1][1]), {stroke: 'black'}),\n      Plot.dot(data, {x: accessors[0], y: get_label, stroke: stroke_shade })\n    ]\n  })\n}\ndata = transpose(data_raw)\nManim Community v0.17.3"
  },
  {
    "objectID": "lecture5-neural-networks-intro/notes.html",
    "href": "lecture5-neural-networks-intro/notes.html",
    "title": "Lecture 5: Neural networks",
    "section": "",
    "text": "Plot = import(\"https://esm.sh/@observablehq/plot\") \nd3 = require(\"d3@7\")\ntopojson = require(\"topojson\")\nMathJax = require(\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-svg.min.js\").catch(() =&gt; window.MathJax)\ntf = require(\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js\").catch(() =&gt; window.tf)\n\nTHREE = {\n  const THREE = window.THREE = await require(\"three@0.130.0/build/three.min.js\");\n  await require(\"three@0.130.0/examples/js/controls/OrbitControls.js\").catch(() =&gt; {});\n  await require(\"three@0.130.0/examples/js/loaders/SVGLoader.js\").catch(() =&gt; {});\n  return THREE;\n}\n\nfunction sample(f, start, end, nsamples=100) {\n  let arr = [...Array(nsamples).keys()]\n  let dist = end - start\n  function arrmap(ind) {\n    const x = (ind * dist) / nsamples + start;\n    return [x, f(x)];\n  }\n  return arr.map(arrmap)\n}\n\nfunction sigmoid(x){\n  return 1 / (1 + Math.exp(-x));\n}\n\nfunction sum(x) {\n  let s = 0;\n  for (let i = 0; i &lt; x.length; i++ ) {\n    s += x[i];\n  }\n  return s;\n}\n\nfunction mean(x) {\n  let s = 0;\n  for (let i = 0; i &lt; x.length; i++ ) {\n    s += x[i];\n  }\n  return s / x.length;\n}\n\nfunction cross_ent(x, y) {\n  return y ? -Math.log(sigmoid(x)) : -Math.log(sigmoid(-x));\n}\n\nfunction se(x, y) {\n  return (x - y) * (x - y);\n}\n\nfunction shuffle(array) {\n  let currentIndex = array.length,  randomIndex;\n\n  // While there remain elements to shuffle.\n  while (currentIndex &gt; 0) {\n\n    // Pick a remaining element.\n    randomIndex = Math.floor(Math.random() * currentIndex);\n    currentIndex--;\n\n    // And swap it with the current element.\n    [array[currentIndex], array[randomIndex]] = [\n      array[randomIndex], array[currentIndex]];\n  }\n\n  return array;\n}\n\nfunction acc(x, y) {\n  return Number(y == (x  &gt; 0));\n}\n\nfunction grid_func(f, width, height, x1, y1, x2, y2) {\n  let values = new Array(width * height);\n  const xstride = (x2 - x1) / width;\n  const ystride = (y2 - y1) / height;\n\n  \n  let y = 0;\n  let x = 0;\n  let ind = 0;\n  for (let i = 0; i &lt; height; i++ ) {\n    for (let j = 0; j &lt; width; j++, ind++) {\n      x = x1 + j * xstride;\n      y = y1 + i * ystride;\n      values[ind] = f(x, y);\n    }\n  }\n  return {width: width, height: height, x1: x1, y1: y1, x2: x2, y2: y2, values: values};\n}\n\nfunction get_accessors(keys, byindex=false) {\n  let isString = value =&gt; typeof value === 'string';\n  \n  let index = 0;\n  let indexmap = {};\n  let accessors = [];\n  for (let i = 0; i &lt; keys.length; i++){\n    let k = keys[i];\n    if (Array.isArray(k)) {\n      let access = isString(k[0]) ? (x =&gt; x[k[0]]) : k[0];\n      \n      if (byindex) {\n        if (isString(k[0]) && !(k[0] in indexmap)) {\n          indexmap[k[0]] = index;\n          index++;\n        }\n        let accessindex = indexmap[k[0]];\n        access = x =&gt; x[accessindex];\n        let process = k[1];\n        let final_access = x =&gt; process(access(x));\n        accessors.push(final_access);\n      }\n      else {\n        let process = k[1];\n        let final_access = x =&gt; process(access(x));\n        accessors.push(final_access);\n      }\n      \n    }\n    else {\n      let access = isString(k) ? (x =&gt; x[k]) : k;\n      if (byindex) { \n        if (isString(k) && !(k in indexmap)) {\n          indexmap[k] = index;\n          index++;\n        }\n        let accessindex = indexmap[k];\n        access = x =&gt; x[accessindex];\n      }\n      accessors.push(access); \n    }\n  }\n  return accessors;\n}\n\nfunction predict(obs, weights, keys=[\"0\", \"1\", \"2\", \"3\"], byindex=false) {\n  let isString = value =&gt; typeof value === 'string';\n  let accessors = get_accessors(keys, byindex);\n  \n  let output = weights[0];\n  let wi = 1;\n  for (let i = 0; (i &lt; keys.length) && (wi &lt; weights.length); i++, wi++){\n    output += weights[wi] * accessors[i](obs);\n  }\n  return output;\n}\n\nfunction mean_loss(f, data, weights, keys, label, l2=0) {\n  let reg = 0;\n  if (l2 &gt; 0){\n    for (let i = 1; i &lt; weights.length; i++) {\n      reg += weights[i] * weights[i];\n    }\n  }\n  \n  const isString = value =&gt; typeof value === 'string';\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n  return mean(data.map(x =&gt; f(predict(x, weights, keys), get_label(x)))) + l2 * reg;\n}\n\nfunction get_domains(data, accessors, margin=0.1) {\n  let domains = [];\n  for (let i = 0; i &lt; accessors.length; i++){\n    let xdomain = d3.extent(data, accessors[i]);\n    let xdsize = (xdomain[1] - xdomain[0]);\n    let xmin = xdomain[0] - xdsize * margin;\n    let xmax = xdomain[1] + xdsize * margin;\n    domains.push([xmin, xmax]);\n  }\n  return domains;\n}\n\nfunction logisticPlot2d(data, weights, keys, label, interval=0.05) {\n  const accuracy = mean_loss(acc, data, weights, keys, label);\n  \n  let isString = value =&gt; typeof value === 'string';\n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, accessors);\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n  \n  return Plot.plot({\n    x: {tickSpacing: 80, label: \"x\"},\n    y: {tickSpacing: 80, label: \"y\"},\n    title: \"Accuracy: \" + accuracy.toFixed(3),\n    color: {type: \"linear\", legend: true, scheme: \"BuRd\", domain: [-0.5, 1.5]},\n    marks: [\n      Plot.contour({\n        fill: (x, y) =&gt; sigmoid(predict([x, y], weights, index_accessors)),\n        x1: domains[0][0], y1: domains[1][0], x2: domains[0][1], y2: domains[1][1], interval: interval,\n      }),\n      Plot.dot(data, {x: accessors[0], y: accessors[1], stroke: x=&gt; (get_label(x) ? 1.35 : -0.35)})\n    ]\n  });\n}\n\nfunction logisticLossPlot2d(data, weights, keys, label) {\n  const loss = mean_loss(cross_ent, data, weights, keys, label);\n  \n  let isString = value =&gt; typeof value === 'string';\n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, accessors);\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n  \n  return Plot.plot({\n    x: {tickSpacing: 80, label: \"x\"},\n    y: {tickSpacing: 80, label: \"y\"},\n    title: \"Loss: \" + loss.toFixed(3),\n    color: {type: \"linear\", legend: true, scheme: \"BuRd\", domain: [0, 5]},\n    marks: [\n      Plot.contour({\n        value: (x, y) =&gt; predict([x, y], weights, index_accessors),\n        fillOpacity: 0.2,\n        stroke: \"black\", x1: domains[0][0], y1: domains[1][0], x2: domains[0][1], y2: domains[1][1],\n        thresholds: [-1e6,  0, 0.00001]\n      }),\n      Plot.dot(data, {x: accessors[0], y: accessors[1], stroke: x=&gt; cross_ent(predict(x, weights, keys), get_label(x)), \n                      strokeOpacity: 0.5 })\n    ]\n  });\n}\n\nfunction lossPlot2d(f, data, keys, label, l2=0, res=100, x1=-40, y1=-0.015, x2=40,  y2=0.015, vmax=50, nlines=25, ctype=\"sqrt\", scale=(x =&gt; x)) {\n  let grid = 0;\n  function lossFunc(w, b) {\n    return scale(mean_loss(f, data, [w, b], keys, label, l2));\n  }\n\n  grid = grid_func(lossFunc,\n                 res, res, x1, y1, x2, y2\n                );\n\n  function plot2d(weights) {\n    let w = weights;\n    if (!(Array.isArray(w[0]))){\n      w = [w];\n    }\n\n    var arrows = w.slice(0, w.length - 1).map(function(e, i) {\n      return e.concat(w[i+1]);\n    });\n\n    let interval= vmax / nlines; \n    let thresholds = [];\n    for (let i = 0; i &lt; nlines; i++) {\n      thresholds.push(i * interval);\n    }\n    let loss = mean_loss(f, data, w[w.length - 1], keys, label, l2)\n    return Plot.plot({\n      title: \"Loss: \" + loss.toFixed(3),\n      color: {type: \"linear\", legend: true, label: \"Loss\", scheme: \"BuRd\", domain: [0, vmax], type: ctype},\n      marks: [\n        Plot.contour(grid.values, {width: grid.width, height: grid.height, x1: grid.x1, x2:grid.x2, y1: grid.y1, y2: grid.y2,\n          stroke: Plot.identity, thresholds: thresholds}),\n        Plot.dot(w),\n        Plot.arrow(arrows, {x1: \"0\", y1: \"1\", x2: \"2\", y2: \"3\", stroke: \"black\"})\n      ]\n    })\n  }\n  return plot2d;\n}\n\n\n\nfunction regressionPlot(data, weights, keys, label, l2, f=se, stroke=\"\") {\n  let loss = mean_loss(f, data, weights, keys, label, l2);\n  let isString = value =&gt; typeof value === 'string';\n  \n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, get_accessors([label].concat(keys)));\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n\n  let stroke_shade = stroke;\n  if (stroke == \"\") {\n    stroke_shade = (x =&gt; f(predict(x, weights, keys), get_label(x)))\n  }\n  \n  return Plot.plot({\n    y: {domain: domains[0]},\n    title: \"Loss: \" + loss.toFixed(3),\n    color: {type: \"linear\", legend: true, label: \"Loss\", scheme: \"BuRd\", domain: [0, 100]},\n    marks: [\n      Plot.line(sample((x) =&gt; predict([x], weights, index_accessors), domains[1][0], domains[1][1]), {stroke: 'black'}),\n      Plot.dot(data, {x: accessors[0], y: get_label, stroke: stroke_shade })\n    ]\n  })\n}\n\nfunction errorPlot(data, weights, keys, label, f, options={}) {\n  const isString = value =&gt; typeof value === 'string';\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n  let errors = data.map(x =&gt; [predict(x, weights, keys) - get_label(x), f(predict(x, weights, keys), get_label(x))]);\n\n  \n  let sigma = (options['sigma'] || 1);\n  let plots = [];\n  const xdomain = (options['xdomain'] || [-30, 30]);\n  const ydomain = (options['ydomain'] || [0, 0.1]);\n  \n\n  if (options['plotnormal']){\n    let pdf = x =&gt; Math.exp(-0.5 * x * x / sigma) * ydomain[1];\n    let normal = Plot.line(sample(pdf, xdomain[0], xdomain[1]), {stroke: 'crimson'});    \n    plots.push(normal);\n  }\n  if (options['plotlaplace']){\n    let pdf = x =&gt; Math.exp(-0.5 * Math.abs(x) / sigma) * ydomain[1];\n    let normal = Plot.line(sample(pdf, xdomain[0], xdomain[1]), {stroke: 'green'});    \n    plots.push(normal);\n  }\n  \n  return Plot.plot({\n    y: {grid: true, domain: ydomain},\n    x: {domain: xdomain},\n    color: {type: \"linear\", legend: true, label: \"Loss\", scheme: \"BuRd\", domain: [0, 100]},\n    marks: [\n      //Plot.rectY(errors, Plot.binX({y: \"count\", fill: x =&gt; mean(x.map(v =&gt; v[1]))}, {x: \"0\"})),\n      \n      Plot.rectY(errors, Plot.binX({y: \"proportion\"}, {x: \"0\", fill: 'steelblue', interval: 1})),\n      Plot.ruleY([0])\n    ].concat(plots)\n  })\n}\n\nfunction nnPlot(data, weights, keys, label, l2, f=se, stroke=\"\", options=[]) {\n  let loss = mean_loss(f, data, weights, keys, label, l2);\n  let isString = value =&gt; typeof value === 'string';\n  \n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, get_accessors([label].concat(keys)));\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n\n  let stroke_shade = stroke;\n  if (stroke == \"\") {\n    stroke_shade = (x =&gt; f(predict(x, weights, keys), get_label(x)))\n  }\n\n  let a = []\n  if (options.indexOf(\"Show feature transforms\") &gt;= 0){\n    a = [Plot.line(sample((x) =&gt;  keys[1][1](x), domains[1][0], domains[1][1]), {stroke: 'red'}),\n      Plot.line(sample((x) =&gt; keys[2][1](x), domains[1][0], domains[1][1]), {stroke: 'blue'})]\n  }\n  \n  return Plot.plot({\n    y: {domain: domains[0]},\n    title: \"Loss: \" + loss.toFixed(3),\n    color: {type: \"linear\", legend: true, label: \"Loss\", scheme: \"BuRd\", domain: [0, 100]},\n    marks: [\n      Plot.line(sample((x) =&gt; predict([x], weights, index_accessors), domains[1][0], domains[1][1]), {stroke: 'black'}),\n      Plot.dot(data, {x: accessors[0], y: get_label, stroke: stroke_shade })\n    ].concat(a)\n  })\n}\ndata = transpose(data_raw)\nManim Community v0.17.3"
  },
  {
    "objectID": "lecture5-neural-networks-intro/notes.html#data",
    "href": "lecture5-neural-networks-intro/notes.html#data",
    "title": "Lecture 5: Introduction to Neural Networks",
    "section": "Data",
    "text": "Data\n\\[\n\\text{Honda Accord: } \\begin{bmatrix} \\text{Weight:} & \\text{2500 lbs} \\\\ \\text{Horsepower:} & \\text{ 123 HP} \\\\ \\text{Displacement:} & \\text{ 2.4 L} \\\\ \\text{0-60mph:} & \\text{ 7.8 Sec} \\end{bmatrix} \\longrightarrow \\text{   MPG: ?}\n\\]\n\\[\n\\text{Dodge Aspen: } \\begin{bmatrix} \\text{Weight:} & \\text{3800 lbs} \\\\ \\text{Horsepower:} & \\text{ 155 HP} \\\\ \\text{Displacement:} & \\text{ 3.2 L} \\\\ \\text{0-60mph:} & \\text{ 6.8 Sec} \\end{bmatrix} \\longrightarrow \\text{   MPG: ?}\n\\]\n\\[\n\\vdots \\quad \\vdots\n\\]\n\\[\n\\mathcal{D}  = \\{ (\\mathbf{x}_1, y_1),\\ (\\mathbf{x}_2, y_2),\\ ...\\ (\\mathbf{x}_N, y_N)\\} = \\big(\\mathbf{X}, \\mathbf{y}\\big)\n\\]\n\n\n\nInputs\n\n\n\n\n\n\n\n\n\n\nweight\nhorsepower\ndisplacement\nacceleration\n\n\ncar name\n\n\n\n\n\n\n\n\nchevrolet chevelle malibu\n3504\n130.0\n307.0\n12.0\n\n\nbuick skylark 320\n3693\n165.0\n350.0\n11.5\n\n\nplymouth satellite\n3436\n150.0\n318.0\n11.0\n\n\namc rebel sst\n3433\n150.0\n304.0\n12.0\n\n\nford torino\n3449\n140.0\n302.0\n10.5\n\n\n...\n...\n...\n...\n...\n\n\nford mustang gl\n2790\n86.0\n140.0\n15.6\n\n\nvw pickup\n2130\n52.0\n97.0\n24.6\n\n\ndodge rampage\n2295\n84.0\n135.0\n11.6\n\n\nford ranger\n2625\n79.0\n120.0\n18.6\n\n\nchevy s-10\n2720\n82.0\n119.0\n19.4\n\n\n\n\n305 rows × 4 columns\n\n\n\n\\[ \\mathbf{X} =\n\\begin{bmatrix}\n                X_{11} & X_{12} & X_{13} & X_{14} \\\\\n                X_{21} & X_{22} & X_{23} & X_{24} \\\\\n                X_{31} & X_{32} & X_{33} & X_{34} \\\\\n                \\vdots & \\vdots & \\vdots & \\vdots \\\\\n                X_{N1} & X_{N2} & X_{N3} & X_{N4} \\\\\n                                 \\end{bmatrix} \\underset{\\text{Car data}}{\\longrightarrow}\n\\begin{bmatrix}\n                3504 & 130 & 307.0 & 12.0 \\\\\n                3693 & 165 & 350.0 & 11.5 \\\\\n                3493 & 150 & 318.0 & 11.0 \\\\\n                \\vdots & \\vdots & \\vdots & \\vdots \\\\\n                2720 & 82 & 119.0 & 19.4 \\\\\n                                 \\end{bmatrix}\\]\n\\[\\mathbf{X}:\\ N \\times d \\ \\text{matrix} , \\quad (\\mathbf{X} \\in \\mathbb{R}^{N\\times d})\\] \\[ N: \\text{number of observations}, \\quad d: \\text{number of features}\\]\n\nprint(X)\n\n[[3504.   130.   307.    12. ]\n [3693.   165.   350.    11.5]\n [3436.   150.   318.    11. ]\n ...\n [2295.    84.   135.    11.6]\n [2625.    79.   120.    18.6]\n [2720.    82.   119.    19.4]]\n\n\n\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\n\n\nmpg\n\n\ncar name\n\n\n\n\n\nchevrolet chevelle malibu\n18.0\n\n\nbuick skylark 320\n15.0\n\n\nplymouth satellite\n18.0\n\n\namc rebel sst\n16.0\n\n\nford torino\n17.0\n\n\n...\n...\n\n\nford mustang gl\n27.0\n\n\nvw pickup\n44.0\n\n\ndodge rampage\n32.0\n\n\nford ranger\n28.0\n\n\nchevy s-10\n31.0\n\n\n\n\n305 rows × 1 columns\n\n\n\n\\[ \\mathbf{y} =\n\\begin{bmatrix}\n                y_{1} \\\\\n                y_{2} \\\\\n                y_{3} \\\\\n                \\vdots \\\\\n                y_{N} \\\\\n                                 \\end{bmatrix} \\underset{\\text{Car data}}{\\longrightarrow}\n\\begin{bmatrix}\n                18.0  \\\\\n                15.0  \\\\\n                16.0  \\\\\n                \\vdots \\\\\n                31.0 \\\\\n                                 \\end{bmatrix}\\]\n\\[\\mathbf{y}:\\ N \\ \\text{vector}, \\quad (\\mathbf{y} \\in \\mathbb{R}^N)\\] \\[ N: \\text{number of observations}\\]\n\nprint(y)\n\n[18. 15. 18. ... 32. 28. 31.]\n\n\n\n\n\n\n\n\n\nInputs\n\n\n\n\n\n\n\n\n \nweight\nhorsepower\ndisplacement\nacceleration\n\n\ncar name\n \n \n \n \n\n\n\n\nchevrolet chevelle malibu\n3504\n130.000000\n307.000000\n12.000000\n\n\nbuick skylark 320\n3693\n165.000000\n350.000000\n11.500000\n\n\nplymouth satellite\n3436\n150.000000\n318.000000\n11.000000\n\n\namc rebel sst\n3433\n150.000000\n304.000000\n12.000000\n\n\nford torino\n3449\n140.000000\n302.000000\n10.500000\n\n\n\n\n\n\\[\\mathbf{x}_3 = \\begin{bmatrix} 3433 \\\\ 150 \\\\ 304 \\\\ 12 \\end{bmatrix}, \\quad (d \\text{ vector})\\]\n\nprint(X[3])\n\n[3433.  150.  304.   12.]\n\n\n\nAs a column vector\n\\[\\mathbf{x}_3 = \\begin{bmatrix} 3433 \\\\ 150 \\\\ 304 \\\\ 12 \\end{bmatrix}, \\quad (d \\text{ vector})\\quad \\underset{\\text{same notation!}}{\\longleftrightarrow} \\quad \\mathbf{x}_3 = \\begin{bmatrix} 3433 \\\\ 150 \\\\ 304 \\\\ 12 \\end{bmatrix}, \\quad (d \\times 1 \\text{ matrix}) \\]\n\nprint(X[3].reshape((-1, 1)))\n\n[[3433.]\n [ 150.]\n [ 304.]\n [  12.]]\n\n\n\n\nAs a row vector\n\\[\\mathbf{x}_3^T = \\begin{bmatrix} 3433 & 150 & 304 & 12 \\end{bmatrix}, \\quad (d \\times 1 \\text{ matrix}) \\]\n\nprint(X[3].reshape((1, -1)))\n\n[[3433.  150.  304.   12.]]\n\n\n\\[\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1^T \\\\ \\mathbf{x}_2^T \\\\ \\vdots \\\\ \\mathbf{x}_N^ T \\end{bmatrix}\\]\n\n\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\n \nmpg\n\n\ncar name\n \n\n\n\n\nchevrolet chevelle malibu\n18.000000\n\n\nbuick skylark 320\n15.000000\n\n\nplymouth satellite\n18.000000\n\n\namc rebel sst\n16.000000\n\n\nford torino\n17.000000\n\n\n\n\n\n\\[y_3 = 16, \\quad (\\text{scalar})\\]\n\nprint(y[3])\n\n16.0\n\n\n\n\n\n\n\n\n\nInputs\n\n\n\n\n\n\n\n\n \nweight\nhorsepower\ndisplacement\nacceleration\n\n\ncar name\n \n \n \n \n\n\n\n\nchevrolet chevelle malibu\n3504\n130.000000\n307.000000\n12.000000\n\n\nbuick skylark 320\n3693\n165.000000\n350.000000\n11.500000\n\n\nplymouth satellite\n3436\n150.000000\n318.000000\n11.000000\n\n\namc rebel sst\n3433\n150.000000\n304.000000\n12.000000\n\n\nford torino\n3449\n140.000000\n302.000000\n10.500000\n\n\n\n\n\n\nweight = X[:, 0]\nprint(weight[:5])\n\n[3504. 3693. 3436. 3433. 3449.]\n\n\n\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\n \nmpg\n\n\ncar name\n \n\n\n\n\nchevrolet chevelle malibu\n18.000000\n\n\nbuick skylark 320\n15.000000\n\n\nplymouth satellite\n18.000000\n\n\namc rebel sst\n16.000000\n\n\nford torino\n17.000000\n\n\n\n\n\n\nprint(y[:5])\n\n[[18.]\n [15.]\n [18.]\n [16.]\n [17.]]\n\n\n\n\n\n\n\nf = plt.scatter(weight, y)\n\n\n\n\n\nLinear predictions\nFind simple function that predicts output \\[f(\\mathbf{X}) = \\mathbf{x}^T \\mathbf{w}\\] \\[\\mathbf{x}: \\text{input} (d \\text{ vector}), \\quad\\mathbf{w}: \\text{weights or parameters} (d \\text{ vector})\\]\n\n\n\n\n\n\n\n \nweight\nhorsepower\ndisplacement\nacceleration\n\n\ncar name\n \n \n \n \n\n\n\n\nchevrolet chevelle malibu\n3504\n130.000000\n307.000000\n12.000000\n\n\nbuick skylark 320\n3693\n165.000000\n350.000000\n11.500000\n\n\nplymouth satellite\n3436\n150.000000\n318.000000\n11.000000\n\n\namc rebel sst\n3433\n150.000000\n304.000000\n12.000000\n\n\nford torino\n3449\n140.000000\n302.000000\n10.500000\n\n\n\n\n\n\\[f(\\mathbf{x}_4) = \\mathbf{x}_4^T \\mathbf{w} =  \\begin{bmatrix} 3433 \\\\ 150 \\\\ 304 \\\\ 12 \\end{bmatrix}^T \\begin{bmatrix} w_1 \\\\ w_2 \\\\ w_3 \\\\ w_4 \\end{bmatrix} = 3433 w_1  +  150 w_2 + 304  w_3 + 12w_4 \\]\n\nw = np.array([0.02, 0.2, -0.1, -1.5])\n\ndef f(x, w):\n    # Transpose not needed because x and w are 1-dimensional vectors\n    # (Not column/row vectors!)\n    return np.dot(x, w)\n\nprint(f(X[3], w))\n\n50.25999999999999\n\n\n\n# Make everything explicit column vectors\nw = w.reshape((-1, 1))\nx3 = X[3].reshape((-1, 1))\n\n# Works!\ndef f(x, w):\n    return np.dot(x.T, w)\n\nf(X[3], w)\n\narray([50.26])\n\n\nFind simple function that predicts output \\[f(\\mathbf{X}) = \\mathbf{X} \\mathbf{w}\\] \\[\\mathbf{x}: \\text{all inputs} (N\\times d \\text{ matrix}), \\quad\\mathbf{w}: \\text{weights or parameters} (d \\text{ vector})\\]\n\n\n\n\n\n\n\n \nweight\nhorsepower\ndisplacement\nacceleration\n\n\ncar name\n \n \n \n \n\n\n\n\nchevrolet chevelle malibu\n3504\n130.000000\n307.000000\n12.000000\n\n\nbuick skylark 320\n3693\n165.000000\n350.000000\n11.500000\n\n\nplymouth satellite\n3436\n150.000000\n318.000000\n11.000000\n\n\namc rebel sst\n3433\n150.000000\n304.000000\n12.000000\n\n\nford torino\n3449\n140.000000\n302.000000\n10.500000\n\n\n\n\n\n\\[f(\\mathbf{X}) = \\mathbf{X} \\mathbf{w} =  \\begin{bmatrix} 3504 & 130 & 307 & 12 \\\\\n3693 & 165 & 350 & 11.5 \\\\\n3436 & 150 & 318 & 11 \\\\\n3433 & 150 & 304 & 12 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n\\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\\\ w_3 \\\\ w_4 \\end{bmatrix}  \\]\n\nw = np.array([0.02, 0.2, -0.1, -1.5])\n\ndef f(x, w):\n    return np.dot(x, w)\n\nprint(f(X, w) [:4])\n\n[47.38 54.61 50.42 50.26]\n\n\nFind x"
  },
  {
    "objectID": "lecture5-neural-networks-intro/notes.html#feature-transforms-revisited",
    "href": "lecture5-neural-networks-intro/notes.html#feature-transforms-revisited",
    "title": "Lecture 5: Neural networks",
    "section": "Feature transforms revisited",
    "text": "Feature transforms revisited\nIn the last lecture we saw that we can define more complex and expressive functions by transforming the inputs in various ways. For example, we can define a function as:\n\\[\nf(\\mathbf{x})=\\phi(\\mathbf{x})^T \\mathbf{w} ,\\quad \\phi(\\mathbf{x})  = \\begin{bmatrix}  x_1 \\\\ x_2 \\\\ x_1^2 \\\\ x_2^2 \\\\ x_1x_2 \\\\ \\sin(x_1) \\\\ \\sin(x_2) \\end{bmatrix}\n\\]\nWriting this out we get:\n\\[\nf(\\mathbf{x}) = w_1 x_1 + w_2 x_2 + w_3 x_1^2 + w_4 x_2^2 + w_5 x_1 x_2 + w_6 \\sin(x_1) + w_7 \\sin(x_2)\n\\]\n\n\n\n\n\nIn code, we could consider transforming an entire dataset as follows:\n\nsquared_X = X ** 2              # x^2\ncross_X = X[:, :1] * X[:, 1:2]  # x_1 * x_2\nsin_X = np.sin(X)               # sin(x)\n\ntransformedX = np.concatenate([X, squared_X, cross_X, sin_X], axis=1)"
  },
  {
    "objectID": "lecture5-neural-networks-intro/notes.html#learned-feature-transforms",
    "href": "lecture5-neural-networks-intro/notes.html#learned-feature-transforms",
    "title": "Lecture 5: Neural networks",
    "section": "Learned feature transforms",
    "text": "Learned feature transforms\nWe’ve already seen that we can learn a function by defining our function in terms of a set of parameters \\(\\mathbf{w}\\): \\[f(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{w}\\] and then minimizing a loss as a function of \\(\\mathbf{w}\\) \\[\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmin}}\\ \\mathbf{Loss}(\\mathbf{w})\\] Which we can do with gradient descent: \\[\\mathbf{w}^{(k+1)} \\longleftarrow \\mathbf{w}^{(k)} - \\alpha \\nabla_{\\mathbf{w}} \\mathbf{Loss}(\\mathbf{w})\\]\nSo we didn’t choose \\(\\mathbf{w}\\) explicitly, we let our algorithm find the optimal values. Ideally, we could do the same thing for our feature transforms: let our algorithm choose the optimal functions to use. This raises the question:\nCan we learn the functions in our feature transform? The answer is yes! To see how, let’s start by writing out what this would look like. We’ll start with the feature transform framework we’ve already introduced, but now let’s replace the individual transforms with functions that we can learn.\n\\[\nf(\\mathbf{x})=\\phi(\\mathbf{x})^T \\mathbf{w} ,\\quad \\phi(\\mathbf{x}) = \\begin{bmatrix}  g_1(x_1) \\\\ g_2(x_1) \\\\ g_3(x_1) \\\\ g_4(x_1) \\end{bmatrix}\n\\]\nThe key insight we’ll use here is that we’ve already seen how to learn functions: this is exactly what our regression models are doing! So if we want to learn a feature transform, we can try using one of these functions that we know how to learn this case: logistic regression. \\[g_i(\\mathbf{x}) = \\sigma(\\mathbf{x}^T \\mathbf{w}_i)\\] With this form, we get a new feature transform: \\[\nf(\\mathbf{x})=\\phi(\\mathbf{x})^T \\mathbf{w}_0,\\quad \\phi(\\mathbf{x}) = \\begin{bmatrix}  \\sigma(\\mathbf{x}^T \\mathbf{w}_1) \\\\ \\sigma(\\mathbf{x}^T \\mathbf{w}_2) \\\\ \\sigma(\\mathbf{x}^T \\mathbf{w}_3) \\\\ \\sigma(\\mathbf{x}^T \\mathbf{w}_4) \\end{bmatrix}\n\\]\nHere we’ll call our original weight vector \\(\\mathbf{w}_0\\) to distinguish it from the others. If we choose different weights for these different transform functions, we can have different feature transforms!\nLet’s look at a very simple example: \\[\\mathbf{x} = \\begin{bmatrix} x_1\\\\ x_2 \\end{bmatrix}, \\quad \\mathbf{w}_0 = \\begin{bmatrix} w_{01} \\\\ w_{02} \\end{bmatrix}\\] \\[\nf(\\mathbf{x})=\\phi(\\mathbf{x})^T \\mathbf{w}_0,\\quad \\phi(\\mathbf{x}) = \\begin{bmatrix}  \\sigma(\\mathbf{x}^T \\mathbf{w}_1) \\\\ \\sigma(\\mathbf{x}^T \\mathbf{w}_2) \\\\ \\sigma(\\mathbf{x}^T \\mathbf{w}_3) \\end{bmatrix} =\n\\begin{bmatrix}  \\sigma(x_1 w_{11} + x_2 w_{12}) \\\\ \\sigma(x_1 w_{21} + x_2 w_{22}) \\\\ \\sigma(x_1 w_{31} + x_2 w_{32}) \\end{bmatrix}\n\\]\nIn this case, we can write out our prediction function explicitly as: \\[f(\\mathbf{x}) = w_{01} \\cdot\\sigma(x_1 w_{11} + x_2 w_{12}) + w_{02} \\cdot\\sigma(x_1 w_{21} + x_2 w_{22})+ w_{03} \\cdot\\sigma(x_1 w_{31} + x_2 w_{32}) \\]\nWe can represent this pictorially again as a node-link diagram:\n\n\n\n\n\nWe often omit the labels for compactness, which make it easy to draw larger models:"
  },
  {
    "objectID": "lecture5-neural-networks-intro/notes.html#linear-transforms",
    "href": "lecture5-neural-networks-intro/notes.html#linear-transforms",
    "title": "Lecture 5: Neural networks",
    "section": "Linear transforms",
    "text": "Linear transforms\nThus far we’ve looked at a logistic regression feature transform as the basis of our neural network. Can we use linear regression as a feature transform?\nLet’s see what happens in our simple example: \\[\\mathbf{x} = \\begin{bmatrix} x_1\\\\ x_2 \\end{bmatrix}, \\quad \\mathbf{w}_0 = \\begin{bmatrix} w_{01} \\\\ w_{02} \\end{bmatrix}\\] \\[\nf(\\mathbf{x})=\\phi(\\mathbf{x})^T \\mathbf{w}_0,\\quad \\phi(\\mathbf{x}) = \\begin{bmatrix}  \\mathbf{x}^T \\mathbf{w}_1 \\\\ \\mathbf{x}^T \\mathbf{w}_2 \\\\ \\mathbf{x}^T \\mathbf{w}_3 \\\\ \\end{bmatrix} =\n\\begin{bmatrix}  x_1 w_{11} + x_2 w_{12} \\\\ x_1 w_{21} + x_2 w_{22} \\\\ x_1 w_{31} + x_2 w_{32} \\\\\\end{bmatrix}\n\\] In this case, we can write out our prediction function explicitly as: \\[f(\\mathbf{x}) = w_{01}\\cdot (x_1 w_{11} + x_2 w_{12}) + w_{02} \\cdot(x_1 w_{21} + x_2 w_{22})+ w_{03} \\cdot(x_1 w_{31} + x_2 w_{32}) \\] \\[= (w_{11}w_{01}) x_1 +(w_{12}w_{01}) x_2 +  (w_{21}w_{02}) x_1 +(w_{22}w_{02}) x_2  +(w_{31}w_{03}) x_1 +(w_{32}w_{03}) x_2 \\]\n\\[\n= (w_{11}w_{01} + w_{21}w_{02} + w_{31}w_{03}) x_1 + (w_{12}w_{01} + w_{22} w_{02} + w_{32} w_{03}) x_2\n\\]\nWe see that we ultimately just end up with another linear function of \\(\\mathbf{x}\\) and we’re no better off than in our orginal case. We can see this in practice here.\nIn general: \\[\nf(\\mathbf{x})=\\phi(\\mathbf{x})^T \\mathbf{w}_0,\\quad \\phi(\\mathbf{x}) = \\begin{bmatrix}  \\mathbf{x}^T \\mathbf{w}_1 \\\\ \\mathbf{x}^T \\mathbf{w}_2\\\\ \\mathbf{x}^T \\mathbf{w}_3 \\\\ \\mathbf{x}^T \\mathbf{w}_4\\end{bmatrix}\n\\]\n\\[\nf(\\mathbf{x})= w_{01} (\\mathbf{x}^T \\mathbf{w}_1) +  w_{02} (\\mathbf{x}^T \\mathbf{w}_2) +...\n\\] \\[= \\mathbf{x}^T ( w_{01}\\mathbf{w}_1) +  \\mathbf{x}^T (w_{02} \\mathbf{w}_2) +...\n\\] Which is again just a linear function. The motivates the need for using a non-linear function like \\(\\sigma(\\cdot)\\) in our neurons."
  },
  {
    "objectID": "lecture5-neural-networks-intro/notes.html#neural-networks-1",
    "href": "lecture5-neural-networks-intro/notes.html#neural-networks-1",
    "title": "Lecture 5: Neural networks",
    "section": "Neural networks",
    "text": "Neural networks\nWhat we’ve just seen is a neural network!\nTerminology-wise we call a single feature transform like \\(\\sigma(x_1 w_{11} + x_2 w_{12})\\) a neuron.\nWe call the whole set of transformed features the hidden layer: \\[\\begin{bmatrix}  \\sigma(\\mathbf{x}^T \\mathbf{w}_1) \\\\ \\sigma(\\mathbf{x}^T \\mathbf{w}_2) \\\\ \\sigma(\\mathbf{x}^T \\mathbf{w}_3) \\\\ \\sigma(\\mathbf{x}^T \\mathbf{w}_4) \\end{bmatrix} \\]\nWe call \\(\\mathbf{x}\\) the input and \\(f(\\mathbf{x})\\) the output."
  },
  {
    "objectID": "lecture5-neural-networks-intro/notebook.html",
    "href": "lecture5-neural-networks-intro/notebook.html",
    "title": "Logistic regression",
    "section": "",
    "text": "from demo import *\n\n\nimages, labels = get_dataset('cats_and_dogs')\n\nCorrupt JPEG data: 99 extraneous bytes before marker 0xd9\nWarning: unknown JFIF revision number 0.00\nCorrupt JPEG data: 396 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 162 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 252 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 65 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 1403 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 214 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 2226 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 128 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 239 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 1153 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 228 extraneous bytes before marker 0xd9\n\n\n\n\n\n\nprint('Shape of images: ', images.shape)\nprint('Shape of labels:', labels.shape)\n\nShape of images:  (1027, 64, 64)\nShape of labels: (1027,)\n\n\n\nX = images.reshape((images.shape[0], -1))\ny = labels\n\nprint('Shape of X:', X.shape)\nprint('Shape of y:', X.shape)\n\nShape of X: (23262, 4096)\nShape of y: (23262, 4096)\n\n\n\nw = np.zeros(X.shape[1] + 1)\n\n\ndef linear_function(X, w):\n    w, b = w[:-1], w[-1]\n    return np.dot(X, w) + b\n\ndef predict(X, w):\n    return (linear_function(X, w) &gt; 0).astype(float)\n\n\nimage = 3\nplt.imshow(images[image])\nprint('True label: ', y[image], ' prediction: ', predict(X[image], w))\n\nTrue label:  0.0  prediction:  0.0\n\n\n\n\n\n\ndef accuracy(X, y, w):\n    return np.mean(predict(X, w) == y)\n\nprint('Classifier accuracy: ', accuracy(X, y, w))\n\nClassifier accuracy:  0.48685491723466406\n\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef predict_probability(X, w):\n    return sigmoid(linear_function(X, w))\n\n\nimage = 4\nprint('True label: ', y[image], ' prediction: ', predict_probability(X[image], w))\n\nTrue label:  0.0  prediction:  0.5\n\n\n\ndef nll(w, X, y):\n    xw = linear_function(X, w)\n    prob_correct = sigmoid((2 * y - 1) * xw)\n    return -np.sum(np.log(prob_correct))\n\n\nnll(w, X, y)\n\n711.8621544350638\n\n\n\ndef dsigmoid(x):\n    return sigmoid(x) * (1 - sigmoid(x))\n\ndef grad(w, X, y):\n    xw = linear_function(X, w)\n    signed_xw = (2 * y - 1) * xw\n    prob_correct = sigmoid(signed_xw)\n    dw = -np.dot((1 / prob_correct) * dsigmoid(signed_xw) * (2 * y - 1.), X)\n    db = -np.sum((1 / prob_correct) * dsigmoid(signed_xw) * (2 * y - 1.))\n    return np.concatenate([dw, np.atleast_1d(db)])\n\nprint(grad( w, X, y) /X.shape[0]) \n\n[-0.01946607 -0.01871726 -0.02047718 ... -0.02952198 -0.02973923\n -0.01314508]\n\n\n\nf, ax = plt.subplots(1, 2)\nax[0].cla(), ax[1].cla()\nax[0].imshow(w[:-1].reshape(images[0].shape))\nax[1].imshow(grad( w, X, y)[:-1].reshape(images[0].shape))\n\n&lt;matplotlib.image.AxesImage at 0x2e61b1310&gt;\n\n\n\n\n\n\ndef gradient_descent(value_and_grad, w0, lr, steps, X, y, images):\n    f, ax = plt.subplots(1, 3, figsize=(12,4))\n\n    losses = []\n    weights = w0\n    for i in range(steps):\n        loss, g = value_and_grad(weights, X, y)\n        weights = weights - lr * g\n        losses.append(loss)\n\n\n        # Plotting code\n        \n        [a.cla() for a in ax]\n        display.clear_output(wait =True)\n        \n        ax[1].axis('off')\n        ax[2].axis('off')\n        ax[0].plot(losses)\n        ax[1].imshow(weights[:-1].reshape(images[0].shape))\n        ax[2].imshow(g[:-1].reshape(images[0].shape))\n        ax[1].set_title('Loss: %.3f, accuracy: %.3f' % (loss, accuracy(X, y, weights)))\n        display.display(f)\n        time.sleep(0.001)\n        \n    return weights, losses\n\n\nimport time\nfrom IPython import display\n\ndef nll_and_grad(w, X, y):\n    return nll(w, X, y), grad(w, X, y)\n\ndef gradient_descent(value_and_grad, w0, lr, steps, X, y):\n    f, ax = plt.subplots(1, 3, figsize=(12,4))\n\n    losses = []\n    weights = w0\n    for i in range(steps):\n        loss, g = value_and_grad(weights, X, y)\n        weights = weights - lr * g\n        losses.append(loss)\n\n\n        # Plotting code\n        \n        [a.cla() for a in ax]\n        display.clear_output(wait =True)\n        \n        ax[1].axis('off')\n        ax[2].axis('off')\n        ax[0].plot(losses)\n        ax[1].imshow(weights[:-1].reshape(images[0].shape))\n        ax[2].imshow(g[:-1].reshape(images[0].shape))\n        ax[1].set_title('Loss: %.3f, accuracy: %.3f' % (loss, accuracy(X, y, weights)))\n        display.display(f)\n        time.sleep(0.001)\n        \n    return weights, losses\n\nw = np.zeros(X.shape[1] + 1)\nweights, losses = gradient_descent(nll_and_grad, np.zeros_like(w), 0.0000003, 500, X, y)\n\n\n\n\n\n\n\n\ndef gradient_descent(value_and_grad, w0, lr, steps, X, y, images):\n    dims = np.array(images[0].shape).prod()\n\n    f, ax = plt.subplots(X.shape[1] // dims, 3, figsize=(15,8))\n\n    losses = []\n    weights = w0\n    for i in range(steps):\n        loss, g = value_and_grad(weights, X, y)\n        weights = weights - lr * g\n        losses.append(loss)\n\n\n        # Plotting code\n        \n        [a.cla() for a in ax.flatten()]\n        [a.axis('off') for a in ax.flatten()[1:]]\n        display.clear_output(wait =True)\n        \n        ax[0, 0].plot(losses)\n        \n\n        ax[0, 1].imshow(weights[:dims].reshape(images[0].shape))\n        ax[0, 2].imshow(g[:dims].reshape(images[0].shape))\n        ax[0, 1].set_title('Loss: %.3f, accuracy: %.3f' % (loss, accuracy(X, y, weights)))\n\n        for j in range(1, ax.shape[0]):\n            ax[j, 1].imshow(weights[(dims * j):(dims * (j+1))].reshape(images[0].shape))\n            ax[j, 2].imshow(g[(dims * j):(dims * (j+1))].reshape(images[0].shape))\n            ax[j, 0].imshow((X[0, (dims * j):(dims * (j+1))].reshape(images[0].shape)) )\n\n        display.display(f)\n        time.sleep(0.001)\n        \n    return weights, losses\n\nphi_X = np.concatenate([X, X ** 2], axis=1)\nw = np.zeros(phi_X.shape[1] + 1)\nweights, losses = gradient_descent(nll_and_grad, np.zeros_like(w), 0.0000003, 500, phi_X, y, images)\nplt.plot(losses)\n\nKeyboardInterrupt: \n\n\n\n\n\n\ndef gradient_descent(value_and_grad, w0, lr, steps, *args):\n    f, ax = plt.subplots(2, 3, figsize=(15,8))\n    X, y = args\n\n    losses = []\n    weights = w0\n    for i in range(steps):\n        loss, g = value_and_grad(weights, *args)\n        weights = weights - lr * g\n        losses.append(loss)\n\n\n        # Plotting code\n        \n        [a.cla() for a in ax.flatten()]\n        [a.axis('off') for a in ax.flatten()[1:]]\n        display.clear_output(wait =True)\n        \n        ax[0, 0].plot(losses)\n        dims = np.array(images[0].shape).prod()\n\n        ax[0, 1].imshow(weights[:dims].reshape(images[0].shape))\n        ax[0, 2].imshow(g[:dims].reshape(images[0].shape))\n        ax[0, 1].set_title('Loss: %.3f, accuracy: %.3f' % (loss, accuracy(X, y, weights)))\n\n        ax[1, 1].imshow(weights[dims:-1].reshape(images[0].shape))\n        ax[1, 2].imshow(g[dims:-1].reshape(images[0].shape))\n        ax[1, 0].imshow((images[0] ** 2) )\n        display.display(f)\n        time.sleep(0.001)\n        \n    return weights, losses\n\nphi_X = np.concatenate([X, X ** 2], axis=1)\nw = np.zeros(phi_X.shape[1] + 1)\nweights, losses = gradient_descent(nll_and_grad, np.zeros_like(w), 0.0000003, 500, phi_X, y)\nplt.plot(losses)\n\n\n\n\nKeyboardInterrupt: \n\n\n\n\n\n\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport matplotlib.pyplot as plt\nimport autograd.numpy as np\ncd = tfds.image_classification.CatsVsDogs()\ncd.download_and_prepare()\n\ndef preprocess(x):\n    x = tf.image.resize(x, (128, 128)).numpy()\n    x = hog(x, orientations=8, pixels_per_cell=(4, 4),\n                    cells_per_block=(1, 1), visualize=False, channel_axis=-1)\n    return tf.tensor(x)\n\ndata = tfds.as_numpy(cd.as_dataset(split='train', as_supervised=True).map(lambda x, y: (tf.image.resize(x, (128, 128)), y)\n).batch(100000))\nimages, labels = next(iter(data))\nimages, labels = images[labels &lt;= 1], labels[labels &lt;= 1].astype(float)\n\nCorrupt JPEG data: 99 extraneous bytes before marker 0xd9\nWarning: unknown JFIF revision number 0.00\nCorrupt JPEG data: 396 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 162 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 252 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 65 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 1403 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 214 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 2226 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 128 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 239 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 1153 extraneous bytes before marker 0xd9\nCorrupt JPEG data: 228 extraneous bytes before marker 0xd9\n\n\n\nimport tqdm.notebook as tqdm\nX = np.stack([hog(xi, orientations=8, pixels_per_cell=(4, 4),\n                    cells_per_block=(1, 1), visualize=False, channel_axis=-1) for xi in tqdm.tqdm(images)])\n\n\n\n\n\nimport time\nfrom IPython import display\n\ndef nll_and_grad(w, X, y):\n    return nll(w, X, y), grad(w, X, y)\n\ndef gradient_descent(value_and_grad, w0, lr, steps, X, y):\n    f, ax = plt.subplots(1, 3, figsize=(12,4))\n\n    losses = []\n    weights = w0\n    for i in range(steps):\n        loss, g = value_and_grad(weights, X, y)\n        weights = weights - lr * g\n        losses.append(loss)\n\n\n        # Plotting code\n        \n        [a.cla() for a in ax]\n        display.clear_output(wait =True)\n        \n        ax[1].axis('off')\n        ax[2].axis('off')\n        ax[0].plot(losses)\n        #ax[1].imshow(weights[:-1].reshape(images[0].shape))\n        #ax[2].imshow(g[:-1].reshape(images[0].shape))\n        ax[1].set_title('Loss: %.3f, accuracy: %.3f' % (loss, accuracy(X, y, weights)))\n        display.display(f)\n        time.sleep(0.001)\n        \n        \n        \n\n    return weights, losses\n\nw = np.zeros(X.shape[1] + 1)\nweights, losses = gradient_descent(nll_and_grad, np.zeros_like(w), 0.0000003, 500, X, y)\n\n\n\n\n\n\n\n\nimport time\nfrom IPython import display\n\ndef nll_and_grad(w, X, y):\n    return nll(w, X, y), grad(w, X, y)\n\ndef gradient_descent(value_and_grad, w0, lr, steps, X, y):\n    f, ax = plt.subplots(1, 3, figsize=(12,4))\n\n    losses = []\n    weights = w0\n    for i in range(steps):\n        loss, g = value_and_grad(weights, X, y)\n        weights = weights - lr * g\n        losses.append(loss)\n\n\n        # Plotting code\n        \n        [a.cla() for a in ax]\n        display.clear_output(wait =True)\n        \n        ax[1].axis('off')\n        ax[2].axis('off')\n        ax[0].plot(losses)\n        ax[1].imshow(weights[:-1].reshape(images[0].shape))\n        ax[2].imshow(g[:-1].reshape(images[0].shape))\n        ax[1].set_title('Loss: %.3f, accuracy: %.3f' % (loss, accuracy(X, y, weights)))\n        display.display(f)\n        time.sleep(0.001)\n        \n        \n        \n\n    return weights, losses\n\nw = np.zeros(X.shape[1] + 1)\nweights, losses = gradient_descent(nll_and_grad, np.zeros_like(w), 0.0000003, 500, X, y)\n\n\n\nfrom skimage.feature import hog\nfrom skimage import data, exposure\n\n\nimage = data.astronaut()\n\nfd, hog_image = hog(images[0][:, :, None], orientations=8, pixels_per_cell=(4, 4),\n                    cells_per_block=(1, 1), visualize=True, channel_axis=-1)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n\nax1.axis('off')\nax1.imshow(images[0], cmap=plt.cm.gray)\nax1.set_title('Input image')\n\n# Rescale histogram for better display\nhog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n\nax2.axis('off')\nax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)\nax2.set_title('Histogram of Oriented Gradients')\nplt.show()\n\n\n\n\n\n64 * 64\n\n4096\n\n\n\nf, ax = plt.subplots(1, 3, figsize=(12, 4))\nvedges = (images[:, :-2] - images[:, 2:])\nhedges = (images[:, :, :-2] - images[:, :, 2:])\n\nax[0].imshow(images[0])\nax[1].imshow(vedges[0])\nax[2].imshow(hedges[0])\n\n&lt;matplotlib.image.AxesImage at 0x2d1447610&gt;\n\n\n\n\n\n\ndef gradient_descent(value_and_grad, w0, lr, steps, *args):\n    f, ax = plt.subplots(3, 3, figsize=(15,12))\n    X, y = args\n\n    losses = []\n    weights = w0\n    for i in range(steps):\n        loss, g = value_and_grad(weights, *args)\n        weights = weights - lr * g\n        losses.append(loss)\n\n\n        # Plotting code\n        \n        if i % 3 == 0:\n            [a.cla() for a in ax.flatten()]\n            [a.axis('off') for a in ax.flatten()[1:]]\n            display.clear_output(wait =True)\n            \n            ax[0, 0].plot(losses)\n            dims = np.array(images[0].shape).prod()\n            vdims = np.array(vedges[0].shape).prod()\n\n            ax[0, 1].imshow(weights[:dims].reshape(images[0].shape))\n            ax[0, 2].imshow(g[:dims].reshape(images[0].shape))\n            ax[0, 1].set_title('Loss: %.3f, accuracy: %.3f' % (loss, accuracy(X, y, weights)))\n\n            ax[1, 1].imshow(weights[dims:(dims + vdims)].reshape(vedges[0].shape))\n            ax[1, 2].imshow(g[dims:(dims + vdims)].reshape(vedges[0].shape))\n            ax[1, 0].imshow((vedges[0]) )\n            display.display(f)\n            time.sleep(0.001)\n        \n    return weights, losses\nphi_X = np.concatenate([X, vedges.reshape((X.shape[0], -1)), hedges.reshape((X.shape[0], -1))], axis=1)\nw = np.zeros(phi_X.shape[1] + 1)\nweights, losses = gradient_descent(nll_and_grad, np.zeros_like(w), 0.0000003, 500, phi_X, y)\nplt.plot(losses)"
  },
  {
    "objectID": "lecture5-neural-networks-intro/slides.html#feature-transforms-revisited",
    "href": "lecture5-neural-networks-intro/slides.html#feature-transforms-revisited",
    "title": "Lecture 4: Feature transforms",
    "section": "Feature transforms revisited",
    "text": "Feature transforms revisited\nIn the last lecture we saw that we can define more complex and expressive functions by transforming the inputs in various ways. For example, we can define a function as:\n\\[\nf(\\mathbf{x})=\\phi(\\mathbf{x})^T \\mathbf{w} = w_4 e^{x_1} + w_3 \\sin(x_1) + w_2x_1^2 + w_1 x_1 + b ,\\quad \\phi(\\mathbf{x}) = \\begin{bmatrix}  x_1 \\\\ x_1^2 \\\\ \\sin(x_1) \\\\ e^{x_1}  \\\\ 1 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lecture5-neural-networks-intro/slides.html#learned-feature-transforms",
    "href": "lecture5-neural-networks-intro/slides.html#learned-feature-transforms",
    "title": "Lecture 4: Feature transforms",
    "section": "Learned feature transforms",
    "text": "Learned feature transforms\nWe’ve already seen that we can learn a function by defining our function in terms of a set of parameters \\(\\mathbf{w}\\): \\[f(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{w}\\] and then minimizing a loss as a function of \\(\\mathbf{w}\\) \\[\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmin}}\\ \\mathbf{Loss}(\\mathbf{w})\\] Which we can do with gradient descent: \\[\\mathbf{w}^{(k+1)} \\longleftarrow \\mathbf{w}^{(k)} - \\alpha \\nabla_{\\mathbf{w}} \\mathbf{Loss}(\\mathbf{w})\\]\nSo we didn’t choose \\(\\mathbf{w}\\) explicitly, we let our algorithm find the optimal values. Ideally, we could do the same thing for our feature transforms: let our algorithm choose the optimal functions to use."
  },
  {
    "objectID": "lecture5-neural-networks-intro/slides.html#neural-networks-1",
    "href": "lecture5-neural-networks-intro/slides.html#neural-networks-1",
    "title": "Lecture 4: Feature transforms",
    "section": "Neural networks",
    "text": "Neural networks\nWhat we’ve just seen is a neural network!\nTerminology-wise we call a single feature transform like \\[\\sigma(x_1 w_{11} + b_1)\\] a neuron.\nWe call the whole set of transformed features the hidden layer: \\[\\begin{bmatrix}  \\sigma(\\mathbf{x}^T \\mathbf{w}_1) \\\\ \\sigma(\\mathbf{x}^T \\mathbf{w}_2) \\\\ \\sigma(\\mathbf{x}^T \\mathbf{w}_3) \\\\ \\sigma(\\mathbf{x}^T \\mathbf{w}_4) \\\\ 1 \\end{bmatrix} \\]\nWe call \\(\\mathbf{X}\\) the input and \\(f(\\mathbf{X})\\) the output.\nWe often describe neural networks using a node-link diagram:"
  },
  {
    "objectID": "lecture5-neural-networks-intro/slides.html#linear-transforms",
    "href": "lecture5-neural-networks-intro/slides.html#linear-transforms",
    "title": "Lecture 4: Feature transforms",
    "section": "Linear transforms",
    "text": "Linear transforms\nCan we use linear regression as a feature transform?\nLet’s see what happens in our simple example: \\[\\mathbf{x} = \\begin{bmatrix} x_1\\\\ 1 \\end{bmatrix}, \\quad \\mathbf{w}_0 = \\begin{bmatrix} w_{01} \\\\ b_0 \\end{bmatrix}\\] \\[\nf(\\mathbf{x})=\\phi(\\mathbf{x})^T \\mathbf{w}_0,\\quad \\phi(\\mathbf{x}) = \\begin{bmatrix}  \\mathbf{x}^T \\mathbf{w}_1 \\\\ \\mathbf{x}^T \\mathbf{w}_2 \\\\ 1 \\end{bmatrix} =\n\\begin{bmatrix}  x_1 w_{11} + b_1 \\\\ x_1 w_{21} + b_2 \\\\ 1 \\end{bmatrix}\n\\] In this case, we can write out our prediction function explicitly as: \\[f(\\mathbf{x}) = w_{01} \\cdot x_1 w_{11} + b_1 + w_{02}\\cdot x_1 w_{21} + b_2 + b_0 \\] \\[= (w_{11}w_{01}) x_1 + (w_{21}w_{02}) x_1 + (b_0 + b_1 + b2)\\]"
  },
  {
    "objectID": "lecture5-neural-networks-intro/slides.html#feature-transforms-revisited-1",
    "href": "lecture5-neural-networks-intro/slides.html#feature-transforms-revisited-1",
    "title": "Lecture 4: Feature transforms",
    "section": "Feature transforms revisited",
    "text": "Feature transforms revisited\n\n\n\n\n\n\nviewof form_mpg_4 = Inputs.form(\n  [\n    Inputs.range([-10, 10], {step: 0.01, label: \"b\", value: 1}),\n    Inputs.range([-10, 10], {step: 0.0001, label: \"w_1\", value: -0.0077}),\n    Inputs.range([-10, 10], {step: 0.0001, label: \"w_2\", value: 0}),\n    Inputs.range([-10, 10], {step: 0.0001, label: \"w_3\", value: 0}),\n    Inputs.range([-0.5, 0.5], {step: 0.0001, label: \"w_4\", value: 0}),\n  ]\n)\n\n\n\n\n\n\n\nregressionPlot(quadratic_data, form_mpg_4, [\"0\", [\"0\", x =&gt; (x) * (x)], [\"0\", x =&gt; Math.sin(x)], [\"0\", x =&gt; Math.exp(x)]], \"1\", 0, se)\n\n\n\n\n\n\nWe see that by varying the weights \\(w_1...w_4\\), we can get a variety of complex, non-linear functions of our input \\(\\mathbf{x}\\)!"
  },
  {
    "objectID": "lecture5-neural-networks-intro/slides.html#feature-transforms-revisited-2",
    "href": "lecture5-neural-networks-intro/slides.html#feature-transforms-revisited-2",
    "title": "Lecture 4: Feature transforms",
    "section": "Feature transforms revisited",
    "text": "Feature transforms revisited\nHow do we actually choose what transforms of our inputs to use?"
  },
  {
    "objectID": "lecture5-neural-networks-intro/slides.html#feature-transforms-revisited-3",
    "href": "lecture5-neural-networks-intro/slides.html#feature-transforms-revisited-3",
    "title": "Lecture 4: Feature transforms",
    "section": "Feature transforms revisited",
    "text": "Feature transforms revisited\nCan we learn the functions in our feature transform?"
  },
  {
    "objectID": "lecture5-neural-networks-intro/slides.html#feature-transforms-revisited-4",
    "href": "lecture5-neural-networks-intro/slides.html#feature-transforms-revisited-4",
    "title": "Lecture 4: Feature transforms",
    "section": "Feature transforms revisited",
    "text": "Feature transforms revisited\n\\[\nf(\\mathbf{x})=\\phi(\\mathbf{x})^T \\mathbf{w} ,\\quad \\phi(\\mathbf{x}) = \\begin{bmatrix}  g_1(x_1) \\\\ g_2(x_1) \\\\ g_3(x_1) \\\\ g_4(x_1) \\\\ 1 \\end{bmatrix}\n\\]\nThe key insight we’ll use here is that we’ve already seen how to learn functions: this is exactly what our regression models are doing! \\[g_i(\\mathbf{x}) = \\sigma(\\mathbf{x}^T \\mathbf{w}_i)\\] With this form we get a new feature transform: \\[\nf(\\mathbf{x})=\\phi(\\mathbf{x})^T \\mathbf{w}_0,\\quad \\phi(\\mathbf{x}) = \\begin{bmatrix}  \\sigma(\\mathbf{x}^T \\mathbf{w}_1) \\\\ \\sigma(\\mathbf{x}^T \\mathbf{w}_2) \\\\ \\sigma(\\mathbf{x}^T \\mathbf{w}_3) \\\\ \\sigma(\\mathbf{x}^T \\mathbf{w}_4) \\\\ 1 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lecture5-neural-networks-intro/slides.html#feature-transforms-revisited-5",
    "href": "lecture5-neural-networks-intro/slides.html#feature-transforms-revisited-5",
    "title": "Lecture 4: Feature transforms",
    "section": "Feature transforms revisited",
    "text": "Feature transforms revisited\nLet’s look at a very simple example: \\[\\mathbf{x} = \\begin{bmatrix} x_1\\\\ 1 \\end{bmatrix}, \\quad \\mathbf{w}_0 = \\begin{bmatrix} w_{01} \\\\ b_0 \\end{bmatrix}\\] \\[\nf(\\mathbf{x})=\\phi(\\mathbf{x})^T \\mathbf{w}_0,\\quad \\phi(\\mathbf{x}) = \\begin{bmatrix}  \\sigma(\\mathbf{x}^T \\mathbf{w}_1) \\\\ \\sigma(\\mathbf{x}^T \\mathbf{w}_2) \\\\ 1 \\end{bmatrix} =\n\\begin{bmatrix}  \\sigma(x_1 w_{11} + b_1) \\\\ \\sigma(x_1 w_{21} + b_2) \\\\ 1 \\end{bmatrix}\n\\] In this case, we can write out our prediction function explicitly as: \\[f(\\mathbf{x}) = w_{01} \\cdot\\sigma(x_1 w_{11} + b_1) + w_{02}\\cdot \\sigma(x_1 w_{21} + b_2) + b_0 \\]"
  },
  {
    "objectID": "lecture5-neural-networks-intro/slides.html#feature-transforms-revisited-6",
    "href": "lecture5-neural-networks-intro/slides.html#feature-transforms-revisited-6",
    "title": "Lecture 4: Feature transforms",
    "section": "Feature transforms revisited",
    "text": "Feature transforms revisited\n\\[f(\\mathbf{x}) = w_{01} \\cdot\\sigma(x_1 w_{11} + b_1) + w_{02}\\cdot \\sigma(x_1 w_{21} + b_2) + b_0 \\]\n\n\n\nviewof form_nn = Inputs.form(\n  [\n    Inputs.range([-10, 10], {step: 0.01, label: \"b_0\", value: 0.33}),\n    Inputs.range([-10, 10], {step: 0.0001, label: \"w_01\", value: 9.2376}),\n    Inputs.range([-10, 10], {step: 0.0001, label: \"w_02\", value: 8.3719}),\n    Inputs.range([-10, 10], {step: 0.0001, label: \"w_11\", value: -2.4219}),\n    Inputs.range([-10, 10], {step: 0.0001, label: \"b_1\", value: -5.457}),\n    Inputs.range([-10, 10], {step: 0.0001, label: \"w_21\", value: 2.6795}),\n    Inputs.range([-10, 10], {step: 0.0001, label: \"b_2\", value: -5.4557}),\n    Inputs.checkbox([\"Show feature transforms\"], {}),\n\n  ]\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunction nnPlot(data, weights, keys, label, l2, f=se, stroke=\"\", options=[]) {\n  let loss = mean_loss(f, data, weights, keys, label, l2);\n  let isString = value =&gt; typeof value === 'string';\n  \n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, get_accessors([label].concat(keys)));\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n\n  let stroke_shade = stroke;\n  if (stroke == \"\") {\n    stroke_shade = (x =&gt; f(predict(x, weights, keys), get_label(x)))\n  }\n\n  let a = []\n  if (options.indexOf(\"Show feature transforms\") &gt;= 0){\n    a = [Plot.line(sample((x) =&gt;  keys[1][1](x), domains[1][0], domains[1][1]), {stroke: 'red'}),\n      Plot.line(sample((x) =&gt; keys[2][1](x), domains[1][0], domains[1][1]), {stroke: 'blue'})]\n  }\n  \n  return Plot.plot({\n    y: {domain: domains[0]},\n    title: \"Loss: \" + loss.toFixed(3),\n    color: {type: \"linear\", legend: true, label: \"Loss\", scheme: \"BuRd\", domain: [0, 100]},\n    marks: [\n      Plot.line(sample((x) =&gt; predict([x], weights, index_accessors), domains[1][0], domains[1][1]), {stroke: 'black'}),\n      Plot.dot(data, {x: accessors[0], y: get_label, stroke: stroke_shade })\n    ].concat(a)\n  })\n}\n\n\n\n\n\n\n\nnnPlot(quadratic_data2, [form_nn[0], 0, form_nn[1], form_nn[2]],\n    \n        [\"0\", [\"0\", x =&gt; sigmoid(form_nn[3] * x + form_nn[4], 0)], \n        [\"0\", x =&gt; sigmoid(form_nn[5] * x + form_nn[6], 0)], \n        ], \"1\", 0, se, \"\", form_nn[7])"
  },
  {
    "objectID": "lecture5-neural-networks-intro/slides.html#feature-transforms-revisited-7",
    "href": "lecture5-neural-networks-intro/slides.html#feature-transforms-revisited-7",
    "title": "Lecture 4: Feature transforms",
    "section": "Feature transforms revisited",
    "text": "Feature transforms revisited\nIf we let \\[\\mathbf{W}_1 = \\begin{bmatrix} \\mathbf{w}_1^T \\\\ \\mathbf{w}_2^T \\mathbf{w}_3^T \\\\ \\vdots \\end{bmatrix}\\] We can write this more compactly as: \\[f(\\mathbf{x})= \\sigma(\\mathbf{x}^T \\mathbf{W}_1^T)^T \\mathbf{w_0}  \\] Or for a whole dataset: \\[\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1^T \\\\ \\mathbf{x}_2^T \\mathbf{x}_3^T \\\\ \\vdots \\end{bmatrix}\\] \\[f(\\mathbf{X})= \\sigma(\\mathbf{X} \\mathbf{W}_1^T)^T \\mathbf{w_0}  \\]"
  },
  {
    "objectID": "lecture5-neural-networks-intro/slides.html#linear-transforms-1",
    "href": "lecture5-neural-networks-intro/slides.html#linear-transforms-1",
    "title": "Lecture 4: Feature transforms",
    "section": "Linear transforms",
    "text": "Linear transforms\n\n\n\nviewof form_lnn = Inputs.form(\n  [\n    Inputs.range([-10, 10], {step: 0.01, label: \"b_0\", value: 0.33}),\n    Inputs.range([-10, 10], {step: 0.0001, label: \"w_01\", value: 9.2376}),\n    Inputs.range([-10, 10], {step: 0.0001, label: \"w_02\", value: 8.3719}),\n    Inputs.range([-10, 10], {step: 0.0001, label: \"w_11\", value: -2.4219}),\n    Inputs.range([-10, 10], {step: 0.0001, label: \"b_1\", value: -5.457}),\n    Inputs.range([-10, 10], {step: 0.0001, label: \"w_21\", value: 2.6795}),\n    Inputs.range([-10, 10], {step: 0.0001, label: \"b_2\", value: -5.4557}),\n    Inputs.checkbox([\"Show feature transforms\"], {}),\n\n  ]\n)\n\n\n\n\n\n\n\n\nnnPlot(quadratic_data2, [form_lnn[0], 0, form_lnn[1], form_lnn[2]],\n    \n        [\"0\", [\"0\", x =&gt; form_lnn[3] * x + form_lnn[4]], \n        [\"0\", x =&gt; form_lnn[5] * x + form_lnn[6]], \n        ], \"1\", 0, se, \"\", form_lnn[7])"
  },
  {
    "objectID": "lecture5-neural-networks-intro/slides.html#linear-transforms-2",
    "href": "lecture5-neural-networks-intro/slides.html#linear-transforms-2",
    "title": "Lecture 4: Feature transforms",
    "section": "Linear transforms",
    "text": "Linear transforms\nIn general: \\[\nf(\\mathbf{x})=\\phi(\\mathbf{x})^T \\mathbf{w}_0,\\quad \\phi(\\mathbf{x}) = \\begin{bmatrix}  \\mathbf{x}^T \\mathbf{w}_1 \\\\ \\mathbf{x}^T \\mathbf{w}_2\\\\ \\mathbf{x}^T \\mathbf{w}_3 \\\\ \\mathbf{x}^T \\mathbf{w}_4\\\\ 1 \\end{bmatrix}\n\\]\n\\[\nf(\\mathbf{x})= w_{01} (\\mathbf{x}^T \\mathbf{w}_1) +  w_{02} (\\mathbf{x}^T \\mathbf{w}_2) +...\n\\] \\[= \\mathbf{x}^T ( w_{01}\\mathbf{w}_1) +  \\mathbf{x}^T (w_{02} \\mathbf{w}_2) +...\n\\] Which is again just a linear function. The motivates the need for using a non-linear function like \\(\\sigma(\\cdot)\\) in our neurons. We’ll see more about this next week!"
  },
  {
    "objectID": "lecture5-neural-networks-intro/other_notes.html",
    "href": "lecture5-neural-networks-intro/other_notes.html",
    "title": "Lecture 5: Introduction to Neural Networks",
    "section": "",
    "text": "Plot = import(\"https://esm.sh/@observablehq/plot\") \nd3 = require(\"d3@7\")\ntopojson = require(\"topojson\")\nMathJax = require(\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-svg.min.js\").catch(() =&gt; window.MathJax)\ntf = require(\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js\").catch(() =&gt; window.tf)\n\nTHREE = {\n  const THREE = window.THREE = await require(\"three@0.130.0/build/three.min.js\");\n  await require(\"three@0.130.0/examples/js/controls/OrbitControls.js\").catch(() =&gt; {});\n  await require(\"three@0.130.0/examples/js/loaders/SVGLoader.js\").catch(() =&gt; {});\n  return THREE;\n}\n\nfunction sample(f, start, end, nsamples=100) {\n  let arr = [...Array(nsamples).keys()]\n  let dist = end - start\n  function arrmap(ind) {\n    const x = (ind * dist) / nsamples + start;\n    return [x, f(x)];\n  }\n  return arr.map(arrmap)\n}\n\nfunction sigmoid(x){\n  return 1 / (1 + Math.exp(-x));\n}\n\nfunction sum(x) {\n  let s = 0;\n  for (let i = 0; i &lt; x.length; i++ ) {\n    s += x[i];\n  }\n  return s;\n}\n\nfunction mean(x) {\n  let s = 0;\n  for (let i = 0; i &lt; x.length; i++ ) {\n    s += x[i];\n  }\n  return s / x.length;\n}\n\nfunction cross_ent(x, y) {\n  return y ? -Math.log(sigmoid(x)) : -Math.log(sigmoid(-x));\n}\n\nfunction se(x, y) {\n  return (x - y) * (x - y);\n}\n\nfunction shuffle(array) {\n  let currentIndex = array.length,  randomIndex;\n\n  // While there remain elements to shuffle.\n  while (currentIndex &gt; 0) {\n\n    // Pick a remaining element.\n    randomIndex = Math.floor(Math.random() * currentIndex);\n    currentIndex--;\n\n    // And swap it with the current element.\n    [array[currentIndex], array[randomIndex]] = [\n      array[randomIndex], array[currentIndex]];\n  }\n\n  return array;\n}\n\nfunction acc(x, y) {\n  return Number(y == (x  &gt; 0));\n}\n\nfunction grid_func(f, width, height, x1, y1, x2, y2) {\n  let values = new Array(width * height);\n  const xstride = (x2 - x1) / width;\n  const ystride = (y2 - y1) / height;\n\n  \n  let y = 0;\n  let x = 0;\n  let ind = 0;\n  for (let i = 0; i &lt; height; i++ ) {\n    for (let j = 0; j &lt; width; j++, ind++) {\n      x = x1 + j * xstride;\n      y = y1 + i * ystride;\n      values[ind] = f(x, y);\n    }\n  }\n  return {width: width, height: height, x1: x1, y1: y1, x2: x2, y2: y2, values: values};\n}\n\nfunction get_accessors(keys, byindex=false) {\n  let isString = value =&gt; typeof value === 'string';\n  \n  let index = 0;\n  let indexmap = {};\n  let accessors = [];\n  for (let i = 0; i &lt; keys.length; i++){\n    let k = keys[i];\n    if (Array.isArray(k)) {\n      let access = isString(k[0]) ? (x =&gt; x[k[0]]) : k[0];\n      \n      if (byindex) {\n        if (isString(k[0]) && !(k[0] in indexmap)) {\n          indexmap[k[0]] = index;\n          index++;\n        }\n        let accessindex = indexmap[k[0]];\n        access = x =&gt; x[accessindex];\n        let process = k[1];\n        let final_access = x =&gt; process(access(x));\n        accessors.push(final_access);\n      }\n      else {\n        let process = k[1];\n        let final_access = x =&gt; process(access(x));\n        accessors.push(final_access);\n      }\n      \n    }\n    else {\n      let access = isString(k) ? (x =&gt; x[k]) : k;\n      if (byindex) { \n        if (isString(k) && !(k in indexmap)) {\n          indexmap[k] = index;\n          index++;\n        }\n        let accessindex = indexmap[k];\n        access = x =&gt; x[accessindex];\n      }\n      accessors.push(access); \n    }\n  }\n  return accessors;\n}\n\nfunction predict(obs, weights, keys=[\"0\", \"1\", \"2\", \"3\"], byindex=false) {\n  let isString = value =&gt; typeof value === 'string';\n  let accessors = get_accessors(keys, byindex);\n  \n  let output = weights[0];\n  let wi = 1;\n  for (let i = 0; (i &lt; keys.length) && (wi &lt; weights.length); i++, wi++){\n    output += weights[wi] * accessors[i](obs);\n  }\n  return output;\n}\n\nfunction mean_loss(f, data, weights, keys, label, l2=0) {\n  let reg = 0;\n  if (l2 &gt; 0){\n    for (let i = 1; i &lt; weights.length; i++) {\n      reg += weights[i] * weights[i];\n    }\n  }\n  \n  const isString = value =&gt; typeof value === 'string';\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n  return mean(data.map(x =&gt; f(predict(x, weights, keys), get_label(x)))) + l2 * reg;\n}\n\nfunction get_domains(data, accessors, margin=0.1) {\n  let domains = [];\n  for (let i = 0; i &lt; accessors.length; i++){\n    let xdomain = d3.extent(data, accessors[i]);\n    let xdsize = (xdomain[1] - xdomain[0]);\n    let xmin = xdomain[0] - xdsize * margin;\n    let xmax = xdomain[1] + xdsize * margin;\n    domains.push([xmin, xmax]);\n  }\n  return domains;\n}\n\nfunction logisticPlot2d(data, weights, keys, label, interval=0.05) {\n  const accuracy = mean_loss(acc, data, weights, keys, label);\n  \n  let isString = value =&gt; typeof value === 'string';\n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, accessors);\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n  \n  return Plot.plot({\n    x: {tickSpacing: 80, label: \"x\"},\n    y: {tickSpacing: 80, label: \"y\"},\n    title: \"Accuracy: \" + accuracy.toFixed(3),\n    color: {type: \"linear\", legend: true, scheme: \"BuRd\", domain: [-0.5, 1.5]},\n    marks: [\n      Plot.contour({\n        fill: (x, y) =&gt; sigmoid(predict([x, y], weights, index_accessors)),\n        x1: domains[0][0], y1: domains[1][0], x2: domains[0][1], y2: domains[1][1], interval: interval,\n      }),\n      Plot.dot(data, {x: accessors[0], y: accessors[1], stroke: x=&gt; (get_label(x) ? 1.35 : -0.35)})\n    ]\n  });\n}\n\nfunction logisticLossPlot2d(data, weights, keys, label) {\n  const loss = mean_loss(cross_ent, data, weights, keys, label);\n  \n  let isString = value =&gt; typeof value === 'string';\n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, accessors);\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n  \n  return Plot.plot({\n    x: {tickSpacing: 80, label: \"x\"},\n    y: {tickSpacing: 80, label: \"y\"},\n    title: \"Loss: \" + loss.toFixed(3),\n    color: {type: \"linear\", legend: true, scheme: \"BuRd\", domain: [0, 5]},\n    marks: [\n      Plot.contour({\n        value: (x, y) =&gt; predict([x, y], weights, index_accessors),\n        fillOpacity: 0.2,\n        stroke: \"black\", x1: domains[0][0], y1: domains[1][0], x2: domains[0][1], y2: domains[1][1],\n        thresholds: [-1e6,  0, 0.00001]\n      }),\n      Plot.dot(data, {x: accessors[0], y: accessors[1], stroke: x=&gt; cross_ent(predict(x, weights, keys), get_label(x)), \n                      strokeOpacity: 0.5 })\n    ]\n  });\n}\n\nfunction lossPlot2d(f, data, keys, label, l2=0, res=100, x1=-40, y1=-0.015, x2=40,  y2=0.015, vmax=50, nlines=25, ctype=\"sqrt\", scale=(x =&gt; x)) {\n  let grid = 0;\n  function lossFunc(w, b) {\n    return scale(mean_loss(f, data, [w, b], keys, label, l2));\n  }\n\n  grid = grid_func(lossFunc,\n                 res, res, x1, y1, x2, y2\n                );\n\n  function plot2d(weights) {\n    let w = weights;\n    if (!(Array.isArray(w[0]))){\n      w = [w];\n    }\n\n    var arrows = w.slice(0, w.length - 1).map(function(e, i) {\n      return e.concat(w[i+1]);\n    });\n\n    let interval= vmax / nlines; \n    let thresholds = [];\n    for (let i = 0; i &lt; nlines; i++) {\n      thresholds.push(i * interval);\n    }\n    let loss = mean_loss(f, data, w[w.length - 1], keys, label, l2)\n    return Plot.plot({\n      title: \"Loss: \" + loss.toFixed(3),\n      color: {type: \"linear\", legend: true, label: \"Loss\", scheme: \"BuRd\", domain: [0, vmax], type: ctype},\n      marks: [\n        Plot.contour(grid.values, {width: grid.width, height: grid.height, x1: grid.x1, x2:grid.x2, y1: grid.y1, y2: grid.y2,\n          stroke: Plot.identity, thresholds: thresholds}),\n        Plot.dot(w),\n        Plot.arrow(arrows, {x1: \"0\", y1: \"1\", x2: \"2\", y2: \"3\", stroke: \"black\"})\n      ]\n    })\n  }\n  return plot2d;\n}\n\n\n\nfunction regressionPlot(data, weights, keys, label, l2, f=se, stroke=\"\") {\n  let loss = mean_loss(f, data, weights, keys, label, l2);\n  let isString = value =&gt; typeof value === 'string';\n  \n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, get_accessors([label].concat(keys)));\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n\n  let stroke_shade = stroke;\n  if (stroke == \"\") {\n    stroke_shade = (x =&gt; f(predict(x, weights, keys), get_label(x)))\n  }\n  \n  return Plot.plot({\n    y: {domain: domains[0]},\n    title: \"Loss: \" + loss.toFixed(3),\n    color: {type: \"linear\", legend: true, label: \"Loss\", scheme: \"BuRd\", domain: [0, 100]},\n    marks: [\n      Plot.line(sample((x) =&gt; predict([x], weights, index_accessors), domains[1][0], domains[1][1]), {stroke: 'black'}),\n      Plot.dot(data, {x: accessors[0], y: get_label, stroke: stroke_shade })\n    ]\n  })\n}\n\nfunction errorPlot(data, weights, keys, label, f, options={}) {\n  const isString = value =&gt; typeof value === 'string';\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n  let errors = data.map(x =&gt; [predict(x, weights, keys) - get_label(x), f(predict(x, weights, keys), get_label(x))]);\n\n  \n  let sigma = (options['sigma'] || 1);\n  let plots = [];\n  const xdomain = (options['xdomain'] || [-30, 30]);\n  const ydomain = (options['ydomain'] || [0, 0.1]);\n  \n\n  if (options['plotnormal']){\n    let pdf = x =&gt; Math.exp(-0.5 * x * x / sigma) * ydomain[1];\n    let normal = Plot.line(sample(pdf, xdomain[0], xdomain[1]), {stroke: 'crimson'});    \n    plots.push(normal);\n  }\n  if (options['plotlaplace']){\n    let pdf = x =&gt; Math.exp(-0.5 * Math.abs(x) / sigma) * ydomain[1];\n    let normal = Plot.line(sample(pdf, xdomain[0], xdomain[1]), {stroke: 'green'});    \n    plots.push(normal);\n  }\n  \n  return Plot.plot({\n    y: {grid: true, domain: ydomain},\n    x: {domain: xdomain},\n    color: {type: \"linear\", legend: true, label: \"Loss\", scheme: \"BuRd\", domain: [0, 100]},\n    marks: [\n      //Plot.rectY(errors, Plot.binX({y: \"count\", fill: x =&gt; mean(x.map(v =&gt; v[1]))}, {x: \"0\"})),\n      \n      Plot.rectY(errors, Plot.binX({y: \"proportion\"}, {x: \"0\", fill: 'steelblue', interval: 1})),\n      Plot.ruleY([0])\n    ].concat(plots)\n  })\n}\nManim Community v0.17.3"
  },
  {
    "objectID": "lecture5-neural-networks-intro/other_notes.html#data",
    "href": "lecture5-neural-networks-intro/other_notes.html#data",
    "title": "Lecture 5: Introduction to Neural Networks",
    "section": "Data",
    "text": "Data\n\\[\n\\text{Honda Accord: } \\begin{bmatrix} \\text{Weight:} & \\text{2500 lbs} \\\\ \\text{Horsepower:} & \\text{ 123 HP} \\\\ \\text{Displacement:} & \\text{ 2.4 L} \\\\ \\text{0-60mph:} & \\text{ 7.8 Sec} \\end{bmatrix} \\longrightarrow \\text{   MPG: ?}\n\\]\n\\[\n\\text{Dodge Aspen: } \\begin{bmatrix} \\text{Weight:} & \\text{3800 lbs} \\\\ \\text{Horsepower:} & \\text{ 155 HP} \\\\ \\text{Displacement:} & \\text{ 3.2 L} \\\\ \\text{0-60mph:} & \\text{ 6.8 Sec} \\end{bmatrix} \\longrightarrow \\text{   MPG: ?}\n\\]\n\\[\n\\vdots \\quad \\vdots\n\\]\n\\[\n\\mathcal{D}  = \\{ (\\mathbf{x}_1, y_1),\\ (\\mathbf{x}_2, y_2),\\ ...\\ (\\mathbf{x}_N, y_N)\\} = \\big(\\mathbf{X}, \\mathbf{y}\\big)\n\\]\n\n\n\nInputs\n\n\n\n\n\n\n\n\n\n\nweight\nhorsepower\ndisplacement\nacceleration\n\n\ncar name\n\n\n\n\n\n\n\n\nchevrolet chevelle malibu\n3504\n130.0\n307.0\n12.0\n\n\nbuick skylark 320\n3693\n165.0\n350.0\n11.5\n\n\nplymouth satellite\n3436\n150.0\n318.0\n11.0\n\n\namc rebel sst\n3433\n150.0\n304.0\n12.0\n\n\nford torino\n3449\n140.0\n302.0\n10.5\n\n\n...\n...\n...\n...\n...\n\n\nford mustang gl\n2790\n86.0\n140.0\n15.6\n\n\nvw pickup\n2130\n52.0\n97.0\n24.6\n\n\ndodge rampage\n2295\n84.0\n135.0\n11.6\n\n\nford ranger\n2625\n79.0\n120.0\n18.6\n\n\nchevy s-10\n2720\n82.0\n119.0\n19.4\n\n\n\n\n305 rows × 4 columns\n\n\n\n\\[ \\mathbf{X} =\n\\begin{bmatrix}\n                X_{11} & X_{12} & X_{13} & X_{14} \\\\\n                X_{21} & X_{22} & X_{23} & X_{24} \\\\\n                X_{31} & X_{32} & X_{33} & X_{34} \\\\\n                \\vdots & \\vdots & \\vdots & \\vdots \\\\\n                X_{N1} & X_{N2} & X_{N3} & X_{N4} \\\\\n                                 \\end{bmatrix} \\underset{\\text{Car data}}{\\longrightarrow}\n\\begin{bmatrix}\n                3504 & 130 & 307.0 & 12.0 \\\\\n                3693 & 165 & 350.0 & 11.5 \\\\\n                3493 & 150 & 318.0 & 11.0 \\\\\n                \\vdots & \\vdots & \\vdots & \\vdots \\\\\n                2720 & 82 & 119.0 & 19.4 \\\\\n                                 \\end{bmatrix}\\]\n\\[\\mathbf{X}:\\ N \\times d \\ \\text{matrix} , \\quad (\\mathbf{X} \\in \\mathbb{R}^{N\\times d})\\] \\[ N: \\text{number of observations}, \\quad d: \\text{number of features}\\]\n\nprint(X)\n\n[[3504.   130.   307.    12. ]\n [3693.   165.   350.    11.5]\n [3436.   150.   318.    11. ]\n ...\n [2295.    84.   135.    11.6]\n [2625.    79.   120.    18.6]\n [2720.    82.   119.    19.4]]\n\n\n\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\n\n\nmpg\n\n\ncar name\n\n\n\n\n\nchevrolet chevelle malibu\n18.0\n\n\nbuick skylark 320\n15.0\n\n\nplymouth satellite\n18.0\n\n\namc rebel sst\n16.0\n\n\nford torino\n17.0\n\n\n...\n...\n\n\nford mustang gl\n27.0\n\n\nvw pickup\n44.0\n\n\ndodge rampage\n32.0\n\n\nford ranger\n28.0\n\n\nchevy s-10\n31.0\n\n\n\n\n305 rows × 1 columns\n\n\n\n\\[ \\mathbf{y} =\n\\begin{bmatrix}\n                y_{1} \\\\\n                y_{2} \\\\\n                y_{3} \\\\\n                \\vdots \\\\\n                y_{N} \\\\\n                                 \\end{bmatrix} \\underset{\\text{Car data}}{\\longrightarrow}\n\\begin{bmatrix}\n                18.0  \\\\\n                15.0  \\\\\n                16.0  \\\\\n                \\vdots \\\\\n                31.0 \\\\\n                                 \\end{bmatrix}\\]\n\\[\\mathbf{y}:\\ N \\ \\text{vector}, \\quad (\\mathbf{y} \\in \\mathbb{R}^N)\\] \\[ N: \\text{number of observations}\\]\n\nprint(y)\n\n[18. 15. 18. ... 32. 28. 31.]\n\n\n\n\n\n\n\n\n\nInputs\n\n\n\n\n\n\n\n\n \nweight\nhorsepower\ndisplacement\nacceleration\n\n\ncar name\n \n \n \n \n\n\n\n\nchevrolet chevelle malibu\n3504\n130.000000\n307.000000\n12.000000\n\n\nbuick skylark 320\n3693\n165.000000\n350.000000\n11.500000\n\n\nplymouth satellite\n3436\n150.000000\n318.000000\n11.000000\n\n\namc rebel sst\n3433\n150.000000\n304.000000\n12.000000\n\n\nford torino\n3449\n140.000000\n302.000000\n10.500000\n\n\n\n\n\n\\[\\mathbf{x}_3 = \\begin{bmatrix} 3433 \\\\ 150 \\\\ 304 \\\\ 12 \\end{bmatrix}, \\quad (d \\text{ vector})\\]\n\nprint(X[3])\n\n[3433.  150.  304.   12.]\n\n\n\nAs a column vector\n\\[\\mathbf{x}_3 = \\begin{bmatrix} 3433 \\\\ 150 \\\\ 304 \\\\ 12 \\end{bmatrix}, \\quad (d \\text{ vector})\\quad \\underset{\\text{same notation!}}{\\longleftrightarrow} \\quad \\mathbf{x}_3 = \\begin{bmatrix} 3433 \\\\ 150 \\\\ 304 \\\\ 12 \\end{bmatrix}, \\quad (d \\times 1 \\text{ matrix}) \\]\n\nprint(X[3].reshape((-1, 1)))\n\n[[3433.]\n [ 150.]\n [ 304.]\n [  12.]]\n\n\n\n\nAs a row vector\n\\[\\mathbf{x}_3^T = \\begin{bmatrix} 3433 & 150 & 304 & 12 \\end{bmatrix}, \\quad (d \\times 1 \\text{ matrix}) \\]\n\nprint(X[3].reshape((1, -1)))\n\n[[3433.  150.  304.   12.]]\n\n\n\\[\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1^T \\\\ \\mathbf{x}_2^T \\\\ \\vdots \\\\ \\mathbf{x}_N^ T \\end{bmatrix}\\]\n\n\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\n \nmpg\n\n\ncar name\n \n\n\n\n\nchevrolet chevelle malibu\n18.000000\n\n\nbuick skylark 320\n15.000000\n\n\nplymouth satellite\n18.000000\n\n\namc rebel sst\n16.000000\n\n\nford torino\n17.000000\n\n\n\n\n\n\\[y_3 = 16, \\quad (\\text{scalar})\\]\n\nprint(y[3])\n\n16.0\n\n\n\n\n\n\n\n\n\nInputs\n\n\n\n\n\n\n\n\n \nweight\nhorsepower\ndisplacement\nacceleration\n\n\ncar name\n \n \n \n \n\n\n\n\nchevrolet chevelle malibu\n3504\n130.000000\n307.000000\n12.000000\n\n\nbuick skylark 320\n3693\n165.000000\n350.000000\n11.500000\n\n\nplymouth satellite\n3436\n150.000000\n318.000000\n11.000000\n\n\namc rebel sst\n3433\n150.000000\n304.000000\n12.000000\n\n\nford torino\n3449\n140.000000\n302.000000\n10.500000\n\n\n\n\n\n\nweight = X[:, 0]\nprint(weight[:5])\n\n[3504. 3693. 3436. 3433. 3449.]\n\n\n\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\n \nmpg\n\n\ncar name\n \n\n\n\n\nchevrolet chevelle malibu\n18.000000\n\n\nbuick skylark 320\n15.000000\n\n\nplymouth satellite\n18.000000\n\n\namc rebel sst\n16.000000\n\n\nford torino\n17.000000\n\n\n\n\n\n\nprint(y[:5])\n\n[[18.]\n [15.]\n [18.]\n [16.]\n [17.]]\n\n\n\n\n\n\n\nf = plt.scatter(weight, y)\n\n\n\n\n\nLinear predictions\nFind simple function that predicts output \\[f(\\mathbf{X}) = \\mathbf{x}^T \\mathbf{w}\\] \\[\\mathbf{x}: \\text{input} (d \\text{ vector}), \\quad\\mathbf{w}: \\text{weights or parameters} (d \\text{ vector})\\]\n\n\n\n\n\n\n\n \nweight\nhorsepower\ndisplacement\nacceleration\n\n\ncar name\n \n \n \n \n\n\n\n\nchevrolet chevelle malibu\n3504\n130.000000\n307.000000\n12.000000\n\n\nbuick skylark 320\n3693\n165.000000\n350.000000\n11.500000\n\n\nplymouth satellite\n3436\n150.000000\n318.000000\n11.000000\n\n\namc rebel sst\n3433\n150.000000\n304.000000\n12.000000\n\n\nford torino\n3449\n140.000000\n302.000000\n10.500000\n\n\n\n\n\n\\[f(\\mathbf{x}_4) = \\mathbf{x}_4^T \\mathbf{w} =  \\begin{bmatrix} 3433 \\\\ 150 \\\\ 304 \\\\ 12 \\end{bmatrix}^T \\begin{bmatrix} w_1 \\\\ w_2 \\\\ w_3 \\\\ w_4 \\end{bmatrix} = 3433 w_1  +  150 w_2 + 304  w_3 + 12w_4 \\]\n\nw = np.array([0.02, 0.2, -0.1, -1.5])\n\ndef f(x, w):\n    # Transpose not needed because x and w are 1-dimensional vectors\n    # (Not column/row vectors!)\n    return np.dot(x, w)\n\nprint(f(X[3], w))\n\n50.25999999999999\n\n\n\n# Make everything explicit column vectors\nw = w.reshape((-1, 1))\nx3 = X[3].reshape((-1, 1))\n\n# Works!\ndef f(x, w):\n    return np.dot(x.T, w)\n\nf(X[3], w)\n\narray([50.26])\n\n\nFind simple function that predicts output \\[f(\\mathbf{X}) = \\mathbf{X} \\mathbf{w}\\] \\[\\mathbf{x}: \\text{all inputs} (N\\times d \\text{ matrix}), \\quad\\mathbf{w}: \\text{weights or parameters} (d \\text{ vector})\\]\n\n\n\n\n\n\n\n \nweight\nhorsepower\ndisplacement\nacceleration\n\n\ncar name\n \n \n \n \n\n\n\n\nchevrolet chevelle malibu\n3504\n130.000000\n307.000000\n12.000000\n\n\nbuick skylark 320\n3693\n165.000000\n350.000000\n11.500000\n\n\nplymouth satellite\n3436\n150.000000\n318.000000\n11.000000\n\n\namc rebel sst\n3433\n150.000000\n304.000000\n12.000000\n\n\nford torino\n3449\n140.000000\n302.000000\n10.500000\n\n\n\n\n\n\\[f(\\mathbf{X}) = \\mathbf{X} \\mathbf{w} =  \\begin{bmatrix} 3504 & 130 & 307 & 12 \\\\\n3693 & 165 & 350 & 11.5 \\\\\n3436 & 150 & 318 & 11 \\\\\n3433 & 150 & 304 & 12 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n\\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\\\ w_3 \\\\ w_4 \\end{bmatrix}  \\]\n\nw = np.array([0.02, 0.2, -0.1, -1.5])\n\ndef f(x, w):\n    return np.dot(x, w)\n\nprint(f(X, w) [:4])\n\n[47.38 54.61 50.42 50.26]\n\n\nFind x"
  },
  {
    "objectID": "misc/grading.html",
    "href": "misc/grading.html",
    "title": "Hint for homework 1: Q7",
    "section": "",
    "text": "Plot = import(\"https://esm.sh/@observablehq/plot\") \nd3 = require(\"d3@7\")\ntopojson = require(\"topojson\")\nMathJax = require(\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-svg.min.js\").catch(() =&gt; window.MathJax)\ntf = require(\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js\").catch(() =&gt; window.tf)\n\nTHREE = {\n  const THREE = window.THREE = await require(\"three@0.130.0/build/three.min.js\");\n  await require(\"three@0.130.0/examples/js/controls/OrbitControls.js\").catch(() =&gt; {});\n  await require(\"three@0.130.0/examples/js/loaders/SVGLoader.js\").catch(() =&gt; {});\n  return THREE;\n}\n\nfunction sample(f, start, end, nsamples=100) {\n  let arr = [...Array(nsamples).keys()]\n  let dist = end - start\n  function arrmap(ind) {\n    const x = (ind * dist) / nsamples + start;\n    return [x, f(x)];\n  }\n  return arr.map(arrmap)\n}\n\nfunction sigmoid(x){\n  return 1 / (1 + Math.exp(-x));\n}\n\nfunction sum(x) {\n  let s = 0;\n  for (let i = 0; i &lt; x.length; i++ ) {\n    s += x[i];\n  }\n  return s;\n}\n\nfunction mean(x) {\n  let s = 0;\n  for (let i = 0; i &lt; x.length; i++ ) {\n    s += x[i];\n  }\n  return s / x.length;\n}\n\nfunction cross_ent(x, y) {\n  return y ? -Math.log(sigmoid(x)) : -Math.log(sigmoid(-x));\n}\n\nfunction se(x, y) {\n  return (x - y) * (x - y);\n}\n\nfunction shuffle(array) {\n  let currentIndex = array.length,  randomIndex;\n\n  // While there remain elements to shuffle.\n  while (currentIndex &gt; 0) {\n\n    // Pick a remaining element.\n    randomIndex = Math.floor(Math.random() * currentIndex);\n    currentIndex--;\n\n    // And swap it with the current element.\n    [array[currentIndex], array[randomIndex]] = [\n      array[randomIndex], array[currentIndex]];\n  }\n\n  return array;\n}\n\nfunction acc(x, y) {\n  return Number(y == (x  &gt; 0));\n}\n\nfunction grid_func(f, width, height, x1, y1, x2, y2) {\n  let values = new Array(width * height);\n  const xstride = (x2 - x1) / width;\n  const ystride = (y2 - y1) / height;\n\n  \n  let y = 0;\n  let x = 0;\n  let ind = 0;\n  for (let i = 0; i &lt; height; i++ ) {\n    for (let j = 0; j &lt; width; j++, ind++) {\n      x = x1 + j * xstride;\n      y = y1 + i * ystride;\n      values[ind] = f(x, y);\n    }\n  }\n  return {width: width, height: height, x1: x1, y1: y1, x2: x2, y2: y2, values: values};\n}\n\nfunction get_accessors(keys, byindex=false) {\n  let isString = value =&gt; typeof value === 'string';\n  \n  let index = 0;\n  let indexmap = {};\n  let accessors = [];\n  for (let i = 0; i &lt; keys.length; i++){\n    let k = keys[i];\n    if (Array.isArray(k)) {\n      let access = isString(k[0]) ? (x =&gt; x[k[0]]) : k[0];\n      \n      if (byindex) {\n        if (isString(k[0]) && !(k[0] in indexmap)) {\n          indexmap[k[0]] = index;\n          index++;\n        }\n        let accessindex = indexmap[k[0]];\n        access = x =&gt; x[accessindex];\n        let process = k[1];\n        let final_access = x =&gt; process(access(x));\n        accessors.push(final_access);\n      }\n      else {\n        let process = k[1];\n        let final_access = x =&gt; process(access(x));\n        accessors.push(final_access);\n      }\n      \n    }\n    else {\n      let access = isString(k) ? (x =&gt; x[k]) : k;\n      if (byindex) { \n        if (isString(k) && !(k in indexmap)) {\n          indexmap[k] = index;\n          index++;\n        }\n        let accessindex = indexmap[k];\n        access = x =&gt; x[accessindex];\n      }\n      accessors.push(access); \n    }\n  }\n  return accessors;\n}\n\nfunction predict(obs, weights, keys=[\"0\", \"1\", \"2\", \"3\"], byindex=false) {\n  let isString = value =&gt; typeof value === 'string';\n  let accessors = get_accessors(keys, byindex);\n  \n  let output = weights[0];\n  let wi = 1;\n  for (let i = 0; (i &lt; keys.length) && (wi &lt; weights.length); i++, wi++){\n    output += weights[wi] * accessors[i](obs);\n  }\n  return output;\n}\n\nfunction mean_loss(f, data, weights, keys, label, l2=0) {\n  let reg = 0;\n  if (l2 &gt; 0){\n    for (let i = 1; i &lt; weights.length; i++) {\n      reg += weights[i] * weights[i];\n    }\n  }\n  \n  const isString = value =&gt; typeof value === 'string';\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n  return mean(data.map(x =&gt; f(predict(x, weights, keys), get_label(x)))) + l2 * reg;\n}\n\nfunction get_domains(data, accessors, margin=0.1) {\n  let domains = [];\n  for (let i = 0; i &lt; accessors.length; i++){\n    let xdomain = d3.extent(data, accessors[i]);\n    let xdsize = (xdomain[1] - xdomain[0]);\n    let xmin = xdomain[0] - xdsize * margin;\n    let xmax = xdomain[1] + xdsize * margin;\n    domains.push([xmin, xmax]);\n  }\n  return domains;\n}\n\nfunction logisticPlot2d(data, weights, keys, label, interval=0.05) {\n  const accuracy = mean_loss(acc, data, weights, keys, label);\n  \n  let isString = value =&gt; typeof value === 'string';\n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, accessors);\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n  \n  return Plot.plot({\n    x: {tickSpacing: 80, label: \"x\"},\n    y: {tickSpacing: 80, label: \"y\"},\n    title: \"Accuracy: \" + accuracy.toFixed(3),\n    color: {type: \"linear\", legend: true, scheme: \"BuRd\", domain: [-0.5, 1.5]},\n    marks: [\n      Plot.contour({\n        fill: (x, y) =&gt; sigmoid(predict([x, y], weights, index_accessors)),\n        x1: domains[0][0], y1: domains[1][0], x2: domains[0][1], y2: domains[1][1], interval: interval,\n      }),\n      Plot.dot(data, {x: accessors[0], y: accessors[1], stroke: x=&gt; (get_label(x) ? 1.35 : -0.35)})\n    ]\n  });\n}\n\nfunction logisticLossPlot2d(data, weights, keys, label) {\n  const loss = mean_loss(cross_ent, data, weights, keys, label);\n  \n  let isString = value =&gt; typeof value === 'string';\n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, accessors);\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n  \n  return Plot.plot({\n    x: {tickSpacing: 80, label: \"x\"},\n    y: {tickSpacing: 80, label: \"y\"},\n    title: \"Loss: \" + loss.toFixed(3),\n    color: {type: \"linear\", legend: true, scheme: \"BuRd\", domain: [0, 5]},\n    marks: [\n      Plot.contour({\n        value: (x, y) =&gt; predict([x, y], weights, index_accessors),\n        fillOpacity: 0.2,\n        stroke: \"black\", x1: domains[0][0], y1: domains[1][0], x2: domains[0][1], y2: domains[1][1],\n        thresholds: [-1e6,  0, 0.00001]\n      }),\n      Plot.dot(data, {x: accessors[0], y: accessors[1], stroke: x=&gt; cross_ent(predict(x, weights, keys), get_label(x)), \n                      strokeOpacity: 0.5 })\n    ]\n  });\n}\n\nfunction lossPlot2d(f, data, keys, label, l2=0, res=100, x1=-40, y1=-0.015, x2=40,  y2=0.015, vmax=50, nlines=25, ctype=\"sqrt\", scale=(x =&gt; x)) {\n  let grid = 0;\n  function lossFunc(w, b) {\n    return scale(mean_loss(f, data, [w, b], keys, label, l2));\n  }\n\n  grid = grid_func(lossFunc,\n                 res, res, x1, y1, x2, y2\n                );\n\n  function plot2d(weights) {\n    let w = weights;\n    if (!(Array.isArray(w[0]))){\n      w = [w];\n    }\n\n    var arrows = w.slice(0, w.length - 1).map(function(e, i) {\n      return e.concat(w[i+1]);\n    });\n\n    let interval= vmax / nlines; \n    let thresholds = [];\n    for (let i = 0; i &lt; nlines; i++) {\n      thresholds.push(i * interval);\n    }\n    let loss = mean_loss(f, data, w[w.length - 1], keys, label, l2)\n    return Plot.plot({\n      title: \"Loss: \" + loss.toFixed(3),\n      color: {type: \"linear\", legend: true, label: \"Loss\", scheme: \"BuRd\", domain: [0, vmax], type: ctype},\n      marks: [\n        Plot.contour(grid.values, {width: grid.width, height: grid.height, x1: grid.x1, x2:grid.x2, y1: grid.y1, y2: grid.y2,\n          stroke: Plot.identity, thresholds: thresholds}),\n        Plot.dot(w),\n        Plot.arrow(arrows, {x1: \"0\", y1: \"1\", x2: \"2\", y2: \"3\", stroke: \"black\"})\n      ]\n    })\n  }\n  return plot2d;\n}\n\n\n\nfunction regressionPlot(data, weights, keys, label, l2, f=se, stroke=\"\") {\n  let loss = mean_loss(f, data, weights, keys, label, l2);\n  let isString = value =&gt; typeof value === 'string';\n  \n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, get_accessors([label].concat(keys)));\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n\n  let stroke_shade = stroke;\n  if (stroke == \"\") {\n    stroke_shade = (x =&gt; f(predict(x, weights, keys), get_label(x)))\n  }\n  \n  return Plot.plot({\n    y: {domain: domains[0]},\n    title: \"Loss: \" + loss.toFixed(3),\n    color: {type: \"linear\", legend: true, label: \"Loss\", scheme: \"BuRd\", domain: [0, 100]},\n    marks: [\n      Plot.line(sample((x) =&gt; predict([x], weights, index_accessors), domains[1][0], domains[1][1]), {stroke: 'black'}),\n      Plot.dot(data, {x: accessors[0], y: get_label, stroke: stroke_shade })\n    ]\n  })\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nform_inputs = [\n  Inputs.range([0, 255], {step: 1, label: \"r\"}),\n  Inputs.range([0, 255], {step: 1, label: \"g\"}),\n  Inputs.range([0, 255], {step: 1, label: \"b\"})\n]\n\n\n\n\n\n\n\nviewof rgb = Inputs.form(form_inputs)\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof replay = Inputs.button(\"Replay\")\n\n\n\n\n\n\n\nprogress = {\n  replay;\n  console.log(form_inputs)\n  if (replay &gt; 0){\n    form_inputs.push(Inputs.range([0, 255], {step: 1, label: \"r\"}));\n    (viewof rgb).dispatchEvent(new Event(\"input\", {bubbles: true}))\n    //document.getElementById('notebook').src = \"https://nytimes.com\";}\n  }\n}"
  },
  {
    "objectID": "assignments/homework1-background/solutions.html",
    "href": "assignments/homework1-background/solutions.html",
    "title": "Homework 1: Introduction to Numpy",
    "section": "",
    "text": "# Run me first!\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')"
  },
  {
    "objectID": "assignments/homework1-background/solutions.html#part-1-numpy-basics",
    "href": "assignments/homework1-background/solutions.html#part-1-numpy-basics",
    "title": "Homework 1: Introduction to Numpy",
    "section": "Part 1: Numpy basics",
    "text": "Part 1: Numpy basics\nAs discussed in class, a square matrix \\(A\\) defines a linear mapping: \\(\\mathbb{R}^n\\rightarrow \\mathbb{R}^n\\). Given a vector \\(\\textbf{x}\\), we can find the corresponding output of this mapping \\(\\textbf{b}\\) using matrix-vector multiplication: \\(\\textbf{b}=A \\textbf{x}\\). We can write an example matrix-multiplication using matrix notation as:\n\\[\\begin{bmatrix}\n4 & -3 & 2 \\\\\n6 & 5  & 1 \\\\\n-4 & -1 & 2\n\\end{bmatrix} \\cdot \\begin{bmatrix}\n1 \\\\\n-2 \\\\\n1\n\\end{bmatrix}\n= \\begin{bmatrix}\n? \\\\\n? \\\\\n?\n\\end{bmatrix}\n\\]\nQ1: Perform this matrix-vector multiplication by hand and write the answer in the cell below.\n$$\n\\[\\begin{bmatrix}\n12 \\\\\n-3 \\\\\n0\n\\end{bmatrix}\\]\nQ2: In the code cell below, create the matrix \\(A\\) and the vector \\(\\textbf{x}\\) shown above, using Numpy. Then use the np.dot function to find the output of the mapping \\(\\textbf{b} = A\\textbf{x}\\). Verify that the answer matches what you derived above.\n\n# Fill answers here\nA = np.array([[4, -3, 2],\n              [6, 5, 1],\n              [-4, -1, 2]\n              ])\nx = np.array([1, -2, 1])\nb = np.dot(A, x)\n\nprint(b)\n\n[12 -3  0]\n\n\nOften we will have access to the transformed vector \\(\\textbf{b}\\) and need to find the orginal vector \\(\\textbf{x}\\). To do this we need to solve the system of linear equations \\(A\\textbf{x}=\\textbf{b}\\) for \\(\\textbf{x}\\). \\[\\begin{bmatrix}\n4 & -3 & 2 \\\\\n6 & 5  & 1 \\\\\n-4 & -1 & 2\n\\end{bmatrix} \\cdot \\begin{bmatrix}\n? \\\\\n? \\\\\n?\n\\end{bmatrix} = \\begin{bmatrix}\n2 \\\\\n-1 \\\\\n3\n\\end{bmatrix}\n\\]\nQ3: Find the missing \\(\\textbf{x}\\) in the equation above using the np.linalg.solve function and verify that \\(A\\textbf{x}=\\textbf{b}\\).\n\n# Fill answer here (A is the same matrix from above)\nb = np.array([2, -1, 3])\nx = np.linalg.solve(A, b)\nprint(b, np.dot(A, x))\n\n[ 2 -1  3] [ 2. -1.  3.]\n\n\nIn linear algebra you may have learned how to solve a system of linear equations using Gaussian elimination. Here we will implement an alternative approach known as Richardson iteration. In this method we start with an inital guess for the solution: \\(\\textbf{x}^{(0)}\\), then we will iteratively update this guess until the solution is reached. Given a matrix \\(A\\), a target \\(\\textbf{b}\\) and a current guess \\(\\textbf{x}^{(k)}\\), we can compute the Richardson update as:\n\\[\\textbf{x}^{(k+1)} \\leftarrow \\textbf{x}^{(k)} + \\omega \\left(\\textbf{b} - A\\textbf{x}^{(k)}\\right)\\]\nHere \\(\\omega\\) is a constant that we can choose to adjust the algorithm. We will set \\(\\omega = 0.1\\).\nQ4: Fill in the Richardson iteration function below and apply it to the system of linear equations from above using 100 updates. Verify that if gives a similar answer to np.linalg.solve.\n\n# Fill in function below\ndef richardson_iter(x_guess, A, b, omega=0.1):\n    new_x_guess = x_guess + omega * (b - np.dot(A, x_guess))\n    return new_x_guess\n\nx_guess = np.zeros(3)\nfor i in range(100):\n    x_guess = richardson_iter(x_guess, A, b)\n\nprint(x_guess, x)\n\n[-0.175 -0.2    1.05 ] [-0.175 -0.2    1.05 ]\n\n\nRecall that the length of a vector is given by it’s two-norm, which is defined as: \\[\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}.\\]\nCorrespondingly, the (Euclidian) distance between two points \\(\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^n\\) can be written as \\(\\|\\mathbf{a} - \\mathbf{b}\\|_2\\). As a convenient measure of error for our Richardson iteration algorithm, we will use the squared Euclidean distance. For a guess \\(\\mathbf{x}^{(k)}\\) we will compute the error \\(e^{(k)}\\) as: \\[e^{(k)} = \\|A\\mathbf{x}^{(k)} - \\mathbf{b}\\|_2^2\\]\nIn expanded form, this would be written as: \\[e^{(k)} = \\sum_{i=1}^n \\left(\\sum_{j=1}^n A_{ij}x^{(k)}_j - b_i\\right)^2\\]\nQ5: Write a function to compute the error of a given guess. Then run Richardson iteration again for 100 steps, computing the error at each step. Finally create a plot of the error for each step (error vs. step).\nHint: recall that basic operations in numpy (addition, subtraction, powers) are performed element-wise.\n\n# Fill in function below\ndef error(x_guess, A, b):\n    err = (np.dot(A, x_guess) - b) ** 2\n    err = err.sum()\n    return err\n\n# Add code to plot the error over time\nx_guess = np.zeros(3)\nall_errors = [error(x_guess, A, b)]\nfor step in range(100):\n    \n    x_guess = richardson_iter(x_guess, A, b)\n    all_errors.append(error(x_guess, A, b))\n\nplt.plot(all_errors)\n    \n\n\n\n\nQ6: Derive the partial derivative of the error with respect to a single entry of \\(\\mathbf{x}^{(k)}\\) (without loss of generality, we will say \\(x^{(k)}_1\\)). Work in the expanded form as in the equation above, writing your answer in the markdown cell below.\nHint: You may find it helpful to refer to the latex equation cheatsheet on the course website. You may show intermediate steps here or as handwritten work as a separate file in the repository. The final answer should be filled in here.\n\\[\\frac{\\partial e^{(k)}}{\\partial x^{(k)}_1}=  \\frac{\\partial}{\\partial x^{(k)}_1} \\sum_{i=1}^n \\left(\\sum_{j=1}^n A_{ij}x^{(k)}_j - b_i\\right)^2\\]\nApply sum and chain rules \\[=   2\\sum_{i=1}^n \\left(\\sum_{j=1}^n A_{ij}x^{(k)}_j - b_i\\right) \\frac{\\partial}{\\partial x^{(k)}_1}\\left(\\sum_{j=1}^n A_{ij}x^{(k)}_j - b_i\\right)\\] Apply sum rule noting that \\(\\frac{\\partial}{\\partial x^{(k)}_1}A_{ij}x_{j} = 0\\) for \\(j\\neq 1\\).\n\\[=2\\sum_{i=1}^n \\left(\\sum_{j=1}^n A_{ij}x^{(k)}_j - b_i\\right) A_{i1}\\]\nIn practice, we will likely want to compute the derivative with respect to all entries of \\(\\mathbf{x}\\): \\[\\frac{\\partial e^{(k)}}{\\partial \\mathbf{x}^{(k)}} = \\begin{bmatrix}\n\\frac{\\partial e^{(k)}}{\\partial x^{(k)}_1} \\\\\n\\vdots \\\\\n\\frac{\\partial e^{(k)}}{\\partial x^{(k)}_n}\n\\end{bmatrix}\\]\nQ7: Using the formula you just derived, write the formula for the vector of all partial derivatives in the compact matrix/vector notation (e.g. \\(A\\mathbf{x}=\\mathbf{b}\\)).\n\\(\\sum_{j=1}^n A_{ij}x^{(k)}_j\\) of the \\(i^{th}\\) entry of the matrix-vector product \\(\\mathbf{A}\\mathbf{x}^{(k)}\\) \\[=2\\sum_{i=1}^n \\left( (\\mathbf{A}\\mathbf{x}^{(k)})_i - b_i\\right) A_{i1}\\]\nSimilarly subtracting \\(b_i\\) corresponds to vector subtraction with the vector \\(\\mathbf{b}\\) \\[=2\\sum_{i=1}^n \\left( \\mathbf{A}\\mathbf{x}^{(k)} - \\mathbf{b}\\right)_i A_{i1}\\]\nFinally we’re left with the equivalent of a dot product for entry 1!\n\\[\\frac{\\partial e^{(k)}}{\\partial x_1^{(k)}}= 2 \\left( \\mathbf{A}\\mathbf{x}^{(k)} - \\mathbf{b}\\right)^T \\mathbf{A}_{1}\\]\nIf we remove the index into \\(\\mathbf{A}\\), we get an expression for the gradient! \\[\\frac{\\partial e^{(k)}}{\\partial \\mathbf{x}^{(k)}}= 2 \\left( \\mathbf{A}\\mathbf{x}^{(k)} - \\mathbf{b}\\right)^T \\mathbf{A}\\]\nQ8: In 1-2 sentences describe how this answer relates to the Richardson iteration algorithm above. We will discuss this more in class!\nWe see that this gradient is almost equivalent to the update we made in our Richardson iteration, but scaled by \\(-2\\mathbf{A}\\)!"
  },
  {
    "objectID": "assignments/homework1-background/solutions.html#part-2-working-with-batches-of-vectors",
    "href": "assignments/homework1-background/solutions.html#part-2-working-with-batches-of-vectors",
    "title": "Homework 1: Introduction to Numpy",
    "section": "Part 2: Working with batches of vectors",
    "text": "Part 2: Working with batches of vectors\nRecall that a vector can also be seen as either an \\(n \\times 1\\) matrix (column vector) or a \\(1 \\times n\\) matrix (row vector).\n\\[\\text{Column vector: } \\mathbf{x} = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3\n\\end{bmatrix}, \\quad \\text{Row vector: } \\mathbf{x} = \\begin{bmatrix}\nx_1 & x_2 & x_3\n\\end{bmatrix}\\]\nNote that we use the same notation for both as they refer to the same concept (a vector). The difference becomes relevant when we consider matrix-vector multiplication. We can write matrix-vector multiplication in two ways: \\[\\text{Matrix-vector: }A\\mathbf{x} = \\mathbf{b}, \\quad \\text{Vector-matrix: }\\mathbf{x}A^T= \\mathbf{b}\\] In matrix-vector multiplication we treat \\(\\textbf{x}\\) as a column vector (\\(n \\times 1\\) matrix), while in vector-matrix multiplication we treat it as a row vector (\\(n \\times 1\\) matrix). Transposing \\(A\\) for left multiplication ensures that the two forms give the same answer.\nQ9: Using the previously defined \\(\\mathbf{x}\\), create an explicit column vector and row vector. Then using the previously defined \\(A\\), verify that the matrix-vector and vector-matrix multiplications shown above do produce the same resultant vector \\(\\mathbf{b}\\).\nHint: Recall that np.dot is also used for matrix-matrix multiplication.\n\n# Fill in code here\nx_col = x[:, None] # Add new diminsion 1\nx_row = x[None, :] # Add new dimension 0\nprint(np.dot(A, x_col), np.dot(x_row, A.T))\n\n[[ 2.]\n [-1.]\n [ 3.]] [[ 2. -1.  3.]]\n\n\nThroughout this course we will typically use row vectors and vector-matrix multiplication, as this is more conventional in neural-network literature. The concept of row and column vectors becomes handy when transforming collections of vectors.\nRecall that a matrix can be seen as a collection of vectors. In numpy we can create a matrix from a list of (1- dimensional) vectors using the np.stack function. This function assumes that the vectors are row vectors creating the matrix as follows: \\[\\begin{bmatrix}\n3 & 1 & -2\n\\end{bmatrix},\\ \\begin{bmatrix}\n4 & 5 & 3\n\\end{bmatrix},\\ \\begin{bmatrix}\n-2 & -1 & 5\n\\end{bmatrix}\\quad \\overset{\\text{np.stack}}{\\longrightarrow} \\begin{bmatrix}\n3 & 1 & -2 \\\\\n4 & 5 & 3 \\\\\n-2 & -1 & 5\n\\end{bmatrix} \\]\nWe will call this matrix \\(X\\) to denote that it is a collection of vectors, rather than a single vector (\\(\\mathbf{x}\\)).\nQ10: Create this matrix in numpy using th np.stack function.\n\n\nX = np.stack([np.array([3, 1, -2]), \n              np.array([4, 5, 3]),\n              np.array([-2, -1, 5])])\nprint(X)\n\n[[ 3  1 -2]\n [ 4  5  3]\n [-2 -1  5]]\n\n\nWhen taken together as a matrix in this way, we can apply the linear mapping \\(A\\) to all vectors using matrix-matrix multiplication: \\[B=XA^T\\]\nLet’s put this into practice with a visual example.\nQ11: Create a \\(20 \\times 3\\) matrix, circle, in numpy of the following form\n\\[ \\begin{bmatrix} \\sin(\\theta_1) & \\cos(\\theta_1)  & 1 \\\\\n\\sin(\\theta_2) & \\cos(\\theta_2)  & 1 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n\\sin(\\theta_{20}) & \\cos(\\theta_{20})  & 1 \\\\\n\\end{bmatrix}\\] Where \\(\\theta_1...\\theta_{20}\\) are evenly spaced between \\(0\\) and \\(2\\pi\\).\n\ntheta = np.linspace(0, 2 * np.pi, 20) # Generates 20 evenly-spaced numbers between 0 and 2π\n\n# Fill in your code here\ncircle = np.stack([np.sin(theta), np.cos(theta), np.ones_like(theta)]).T\n\nThe code we just wrote creates a matrix corresponding to a collection of \\(20\\) row vectors of length 3. Each vector represents a point on the unit circle where the first entry is the x-coordinate, the second entry is the y-coordinate and the third entry is always \\(1\\): \\[ \\begin{bmatrix} x & y & 1 \\end{bmatrix}\\]\nQ12: Plot the set of 20 points in circle using the plt.plot function. Use only the x and y coordinates, ignoring the column of 1s.\n\nplt.figure(figsize=(4, 4))\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\n\n# Fill your code here\nplt.plot(circle[:, 0], circle[:, 1])\n\n\n\n\nQ13: Transform all the vectors in circle with the matrix \\(A\\) using a single call to np.dot. Then plot the original set of points in black and the transformed points in red using the plt.plot function.\nYou might also consider why we added the extra column of 1s! We will discuss the answer to that in class.\n\n# Fill your code here\ntransformed_circle = np.dot(circle, A)\n\n\nplt.figure(figsize=(4, 4))\nplt.xlim(-10, 10)\nplt.ylim(-10, 10)\n\n# Fill your code here\nplt.plot(circle[:, 0], circle[:, 1], c='k')\nplt.plot(transformed_circle[:, 0], transformed_circle[:, 1], c='r')"
  },
  {
    "objectID": "assignments/homework1-background/solutions.html#part-3-loading-and-visualizing-data",
    "href": "assignments/homework1-background/solutions.html#part-3-loading-and-visualizing-data",
    "title": "Homework 1: Introduction to Numpy",
    "section": "Part 3: Loading and visualizing data",
    "text": "Part 3: Loading and visualizing data\nFor most of this class we will be working with real-world data. A very well-known dataset in statistics is the Iris flower dataset collected by Edgar Anderson in 1929. The dataset consists of measurments of iris flowers. Each flower has 4 collected measurements: sepal length, sepal width, petal length, petal width, as well as a classification into one of 3 species: Iris setosa, Iris versicolor and Iris virginica. We will return to this dataset in the next homework.\nWe can load this dataset as Numpy objects using the Scikit-Learn library. Below we’ve extrated 4 relevant arrays: - features: a \\(150 \\times 4\\) matrix which has one row per observed flower and one column per measurement. - targets: a length \\(150\\) array that specifies the species of each flower as a number 0-2. - feature_names: a list of strings with the name of each measurement. - target_names: a list of strings with the name of each species.\nIn this homework, we will only visualize this dataset, which is typically a good first step in working with a new type of data. To get a convenient summary of the data we will create what is called a scatterplot matrix. This is a grid of plots where each plot contains a scatter plot with different features on the x and y axes. Because there are 4 features (measurements) in this data, we will create a 4-by-4 matrix to plot each pair of features.\nQ14: Fill in the code to create a scatterplot matrix for the Iris dataset below. - Each row of the matrix should use a different feature for the y-axis and each column should use a different feature for the x-axis. The plots on the diagonal where x and y would be the same feature can be ignored. - The x and y axis of each sub-plot should be labeled with the appropriate feature names. - The points in each scatterplot should be colored by the species label of that flower. Include a legend in at least 1 sub-plot.\nHint: The linked Wikipedia article shows an example of a scatterplot matrix for this dataset, feel free to use it as reference!\n\nimport sklearn.datasets as datasets\ndataset = datasets.load_iris()\nfeatures = dataset['data']\ntargets = dataset['target']\nfeature_names = dataset['feature_names']\ntarget_names = dataset['target_names']\n\n# Fill in the code below\nfig, ax = plt.subplots(4, 4, figsize=(20, 20))\nfor i in range(4):\n    for j in range(4):\n        # Skip sub-plots on the diagonal\n        if i == j: \n            continue\n\n        # Add subplot code here\n        for c, name in enumerate(target_names): # Iterate through the different classes\n            # Plot only the features of examples in that class, label specifies the name for the legend\n            ax[i, j].scatter(features[targets == c, i], features[targets == c, j], label=name)\n        ax[i, j].legend() # Create the legend\n\n        # Set the axis labels\n        ax[i, j].set_xlabel(feature_names[i])\n        ax[i, j].set_ylabel(feature_names[j])"
  },
  {
    "objectID": "assignments/homework2-linear-regression/solutions.html",
    "href": "assignments/homework2-linear-regression/solutions.html",
    "title": "Homework 2: Linear regression and maximum likelihood",
    "section": "",
    "text": "# Run me first!\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nIn this homework, we will use the following convention for dimentionality:\n\\(N:\\quad\\text{Number of observations in a dataset, so } \\mathcal{D} = \\{ (\\mathbf{x}_1, y_1),\\ (\\mathbf{x}_2, y_2),\\ ... \\,(\\mathbf{x}_N, y_N) \\}\\)\n\\(d:\\quad\\ \\text{Dimension of input (number of features), so } \\mathbf{x}_i \\in \\mathbb{R}^d\\)"
  },
  {
    "objectID": "assignments/homework2-linear-regression/solutions.html#part-1-linear-regression",
    "href": "assignments/homework2-linear-regression/solutions.html#part-1-linear-regression",
    "title": "Homework 2: Linear regression and maximum likelihood",
    "section": "Part 1: Linear regression",
    "text": "Part 1: Linear regression\nLet’s begin by reviewing the context of linear regression.\nRecall that the linear regression model makes predictions of the following form:\n\\[f(\\mathbf{x})=\\mathbf{x}^T\\mathbf{w} + b\\]\nOr if we consdier the augmented representation:\n\\[\\mathbf{x} \\rightarrow \\begin{bmatrix}\\mathbf{x} \\\\ 1 \\end{bmatrix} = \\begin{bmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\\\ 1 \\end{bmatrix},\\quad \\mathbf{w} \\rightarrow \\begin{bmatrix}\\mathbf{w} \\\\ b \\end{bmatrix} = \\begin{bmatrix}w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_d \\\\ b \\end{bmatrix}\\] The we can use the more convinient linear form: \\[f(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{w}\\]\nQ1: Write a function that takes in an input \\((\\mathbf{x})\\) and a set of weights \\((\\mathbf{w})\\) and makes a predicion using the function above. Your implementation should assume that the bias \\((b)\\) is included in the weights \\((\\mathbf{w})\\) and thus should augment the input to account for this.\n\n# Test inputs\nx = np.array([1, -3, 5])\nw = np.array([-1, 2, 0.5, 2])\ny = -2.5\n\ndef linear_regression(x, w):\n    x = np.pad(x, ((0, 1)), constant_values=1)\n    return np.dot(x, w)\n\n## Validate the function\nassert y == linear_regression(x, w)\n\nAs discussed in class, we can compactly refer to an entire dataset of inputs and outputs using the notation \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) respectively. Our convention will be that row \\(i\\) of \\(\\mathbf{X}\\) will correspond to the \\(i^{th}\\) observed input \\(\\mathbf{x}_i\\), while the corresponding entry \\(y_i\\) of \\(\\mathbf{y}\\) is the observed output. In other words \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) are defined as: \\[\n\\mathbf{X} =\n\\begin{bmatrix} X_{11} & X_{12} & \\dots  & X_{1d} & 1 \\\\\n                X_{21} & X_{22} & \\dots  & X_{2d} & 1 \\\\\n                \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n                X_{N1} & X_{N2} & \\dots  & X_{Nd} & 1 \\\\  \n                \\end{bmatrix} = \\begin{bmatrix} \\mathbf{x}_1^T \\\\ \\mathbf{x}_2^T \\\\ \\vdots \\\\ \\mathbf{x}_N^T \\end{bmatrix}, \\quad\n                \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{bmatrix}\n\\]\nWith this notation, we can make predictions for an entire set of inputs as: \\[f(\\mathbf{X}) = \\mathbf{X}\\mathbf{w}\\] Where the output is now a vector of predictions. We see that each entry corresponds to the prediction for input \\(\\mathbf{x}_i\\): \\[f(\\mathbf{X})_i = \\mathbf{x}_i^T\\mathbf{w} = f(\\mathbf{x}_i)\\]\nQ2: Modify (if needed) your linear regression prediction function to accept a set of inputs as matrix as described above and return a vector of predictions. Once again you should not assume the data includes the extra bias dimension. Then plot the prediction function given by the provided weights on the range \\([0, 5]\\). Recall that np.linspace(a, b, n) produces a vector of n equally spaced numbers between a and b.\n\n# Test inputs\nw = np.array([0.5, 2])\n\ndef linear_regression(x, w):\n    # Account for the extra dimension in pad\n    x = np.pad(x, ((0, 0), (0, 1)), constant_values=1)\n    return np.dot(x, w)\n\nX = np.linspace(0, 5, 200).reshape((-1, 1))\ny = linear_regression(X, w)\nplt.plot(X.flatten(), y)\n\n\n\n\nRecall that ordinary least squares finds the parameter \\(\\mathbf{w}\\) that minimizes the sum of squared errors between the true outputs \\(\\mathbf{y}\\) and predictions. \\[\\min_{\\mathbf{w}} \\sum_{i=1}^N \\left(\\mathbf{x}_i^T\\mathbf{w} - y_i\\right)^2\\]\nObserve that this is the same formula as the error we used for evaluating Richardson iteration in the last homework, we’ve simply renamed the variables from \\((\\mathbf{A}, \\mathbf{x}, \\mathbf{b})\\) to \\((\\mathbf{X}, \\mathbf{w}, \\mathbf{y})\\)!\nWe’ve now seen 3 equivalent ways to write the same formula. From most compact to least compact these are: \\[\\textbf{1: }  \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2 \\quad \\textbf{2: } \\sum_{i=1}^N \\left(\\mathbf{x}_i^T\\mathbf{w} - y_i\\right)^2 \\quad \\textbf{3: } \\sum_{i=1}^n \\left(\\sum_{j=1}^n X_{ij}w_j - y_i\\right)^2\\]\nNote: Notation can be confusing. If you are having trouble following, don’t hesitate to ask for help!\nQ3: Using the gradient formula you derived in the last homework, write a function that returns both the mean squared error and the gradient of the error with respect to \\(\\mathbf{w}\\), given \\(\\mathbf{w}\\), \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\). Hint: don’t forget about to augment X when computing the gradient!\n\ndef mse_and_grad(w, X, y):\n    error = linear_regression(X, w) - y\n    mse = np.mean(error ** 2)\n    grad_w = 2 * np.dot(error, np.pad(X, [(0,0), (0,1)], constant_values=1)) / X.shape[0]\n    return mse, grad_w\n\nRecall that we can use the gradient descent algorithm to find the input that minimizes a function, using the corresponding gradient function.\nGiven an initial guess of the optimal input, \\(\\mathbf{w}^{(0)}\\), gradient descent uses the following update to improve the guess: \\[\\mathbf{w}^{(i+1)} \\longleftarrow \\mathbf{w}^{(i)} - \\alpha \\nabla f(\\mathbf{w}),\\] where \\(\\alpha\\) is the learning rate or step size.\nQ4: Write a function to perform gradient descent. The function should take as input: - value_and_grad: A function that produces the output and gradient of the function to optimize (e.g. the mse_and_grad) - w0: An inital guess \\(\\mathbf{w}^{(0)}\\) - lr: The learning rate \\((\\alpha)\\) - niter: The number of updates to perform - *args: Any additional argumets to pass to value_and_grad (e.g. X and y)\nThe function should return: - w: The final guess - losses: A list (or array) that tracks the value of the function at each update\n\ndef gradient_descent(value_and_grad, w0, lr, niter, *args):\n    # Get the inital loss\n    initial_loss, grad = value_and_grad(w0, *args)\n    losses = [initial_loss]\n    w = w0\n\n    for i in range(niter):\n        w = w - lr * grad\n        loss, grad = value_and_grad(w, *args)\n        losses.append(loss)\n    return w, losses"
  },
  {
    "objectID": "assignments/homework2-linear-regression/solutions.html#part-2-applications-to-real-data",
    "href": "assignments/homework2-linear-regression/solutions.html#part-2-applications-to-real-data",
    "title": "Homework 2: Linear regression and maximum likelihood",
    "section": "Part 2: Applications to real data",
    "text": "Part 2: Applications to real data\nNow that we’ve setup a full implementation of linear regression, let’s test it on a real dataset. We’ll use the MPG dataset that we in class. The following code will load the data.\n\ndata = np.genfromtxt('auto-mpg.csv', delimiter=',', missing_values=['?'], filling_values=[0])\n\n# MPG is the output value\ntarget = 'MPG'\ny = data[:, 0]\n\n# The other variables are in inputs in the order listed\nfeatures = ['displacement', 'weight', 'acceleration', 'year']\nX = data[:, [2, 4, 5, 6]]\nX = (X - X.mean(axis=0)) / X.std(axis=0)\n\nLet’s start by fitting a model that just uses the feature weight.\nQ5: Use the gradient_descent and mse_and_grad functions you wrote above to fit a linear regression model that takes in a car’s weight and predicts its MPG rating. Start with the weights equal to 0 and perform 50 updates of gradient descent with a learning rate of 0.1. Plot the loss (MSE) as a function of the number of updates.\n\nXweight = X[:, 1:2]\nw0 = np.zeros((2,))\n\n(w, losses) = gradient_descent(mse_and_grad, w0, 0.1, 50, Xweight, y)\n\n### PLOTTING CODE HERE\nplt.figure(figsize=(6, 4))\nplt.plot(losses)\nprint(losses[-1])\n\n18.780939855850434\n\n\n\n\n\nQ6: Plot a scatterplot of weight vs. MPG. Then on the same plot, plot the prediction function for the linear regression model you just fit on the input range \\([-3, 3]]\\).\n\nplt.figure(figsize=(8, 6))\nplt.scatter(Xweight.flatten(), y)\n\nx = np.linspace(-3, 3, 100).reshape((-1, 1))\nplt.plot(x.flatten(), linear_regression(x, w), c='r')\n\n\n\n\nQ7: Repeat Q5 using all 5 features and compare the final loss to the final loss using only weight as an input. Describe any differences you observe.\n\nw0 = np.zeros((5,))\nw, losses = gradient_descent(mse_and_grad, w0, 0.1, 50, X, y)\n\nplt.figure(figsize=(6, 4))\nplt.plot(losses)\nprint(losses[-1])\n\n12.013140556897708\n\n\n\n\n\nWe see that in both cases the loss converges very quickly, but with 5 features, the final loss is significantly smaller (12 vs 18)\nIn machine learning we are typically less interested in how our model predicts the data we’ve already seen than we are in how well it makes predictions for new data. One way to estimate how well our model our model will generalize to new data is to hold out data while fitting our model. To do this we will split our dataset into two smaller datasets: a training dataset that we will use to fit our model, and a test or held-out dataset that we will only use to evaluate our model. By computing the loss on this test dataset, we can get a sense of how well our model will make prediction for new data.\n\\[\\mathcal{D} = \\{ (\\mathbf{x}_1, y_1),\\ (\\mathbf{x}_2, y_2),\\ ... \\,(\\mathbf{x}_N, y_N) \\}\\quad \\longrightarrow \\quad\n\\mathcal{D}_{train} = \\{ (\\mathbf{x}_1, y_1),\\ (\\mathbf{x}_2, y_2),\\ ... \\,(\\mathbf{x}_{Ntrain}, y_{Ntrain}) \\},\\  \n\\mathcal{D}_{test} = \\{ (\\mathbf{x}_1, y_1),\\ (\\mathbf{x}_2, y_2),\\ ... \\,(\\mathbf{x}_{Ntest}, y_{Ntest}) \\}\n\\]\nQ8: Split the MPG dataset into training and test datasets. Use 70% of the observations for training and 30% for test. Then repeat Q5 to fit a linear regression model on just the training dataset using only the weight feature (you don’t need to plot the loss for this question). Report the final loss (MSE) on the training dataset and on the test dataset.\n\nXweight_train = Xweight[:int(X.shape[0] * .7)]\ny_train = y[:int(X.shape[0] * .7)]\n\nXweight_test = Xweight[int(X.shape[0] * .7):]\ny_test = y[int(X.shape[0] * .7):]\n\nw0 = np.zeros((2,))\nw, losses = gradient_descent(mse_and_grad, w0, 0.1, 50, Xweight_train, y_train)\n\nmse_train, _ = mse_and_grad(w, Xweight_train, y_train)\nmse_test, _ = mse_and_grad(w, Xweight_test, y_test)\n\nprint('Loss on training data: %.4f, loss on test data: %.4f' % (mse_train, mse_test))\n\nLoss on training data: 8.4172, loss on test data: 60.1368\n\n\nQ9: Repeat Q8 using the all 5 features. Compare the results to the model using only weight.\n\nX_train = X[:int(X.shape[0] * .7)]\ny_train = y[:int(X.shape[0] * .7)]\n\nX_test = X[int(X.shape[0] * .7):]\ny_test = y[int(X.shape[0] * .7):]\n\nw0 = np.zeros((5,))\nw, losses = gradient_descent(mse_and_grad, w0, 0.1, 50, X_train, y_train)\n\nmse_train, _ = mse_and_grad(w, X_train, y_train)\nmse_test, _ = mse_and_grad(w, X_test, y_test)\n\nprint('Loss on training data: %.4f, loss on test data: %.4f' % (mse_train, mse_test))\n\nLoss on training data: 7.5947, loss on test data: 43.7603\n\n\nWe see that with all 5 features, both the training and the test loss are lower than with only weight!"
  },
  {
    "objectID": "assignments/homework2-linear-regression/solutions.html#part-3-maximum-likelihood-training",
    "href": "assignments/homework2-linear-regression/solutions.html#part-3-maximum-likelihood-training",
    "title": "Homework 2: Linear regression and maximum likelihood",
    "section": "Part 3: Maximum Likelihood Training",
    "text": "Part 3: Maximum Likelihood Training\n\nBackground:\nWe saw in lecture that we can view linear regression as the following probibalistic model: \\[\ny_i \\sim \\mathcal{N}\\big(\\mathbf{x}_i^T \\mathbf{w},\\ \\sigma^2\\big)\n\\] Where \\(\\mathcal{N}(\\mu, \\sigma)\\) is the Normal distribution, which has the following probability density function: \\[\np(y\\mid \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\text{exp}\\bigg(-\\frac{1}{2\\sigma^2} (y -\\mu)^2\\bigg)\n\\] We saw that a reasonable way to choose the optimal \\(\\mathbf{w}\\) is to maximize the likelihood of our observed dataset, which is equivalent to minimizing the negative log likelihood: \\[\\underset{\\mathbf{w}}{\\text{argmax}} \\prod_{i=1}^N p(y_i\\mid \\mathbf{x}_i, \\mathbf{w}, \\sigma) =\n\\underset{\\mathbf{w}}{\\text{argmin}} -\\sum_{i=1}^N \\log p(y_i\\mid \\mathbf{x}_i, \\mathbf{w}, \\sigma) = \\underset{\\mathbf{w}}{\\text{argmin}}\\ \\frac{1}{2\\sigma^2}\\sum_{i=1}^N  (y_i -\\mathbf{x}_i^T\\mathbf{w})^2 + C\n\\] Where \\(C\\) is a constant: \\(C = N \\log \\sigma \\sqrt{2\\pi}\\). We further saw that this is equivalent to minimizing the mean squared error for any choice of \\(\\sigma\\).\n\n\nLaplace Maximum Likelihood Estimation\nA natural question about the above model is: why choose the Normal distribution? In principal, we could define a linear model using any distribution over the real numbers \\(\\mathbb{R}\\). Let’s explore an alternative choice: the Laplace distribution (Wiki). The Laplace distribution \\(L(\\mu, a)\\) has the following PDF:\n\\[p(y\\mid \\mu, a) = \\frac{1}{2a} \\exp\\bigg(- \\frac{|y-\\mu|}{a} \\bigg) \\]\nAs for the normal distribution \\(\\mu\\) is the mean, while \\(a\\) defines the “width” of the distribution (analogous to variance for the Normal). We can compare the two distributions visually:\n\ny = np.linspace(-3, 3, 200)\nmu, sigma2_or_a = 0, 1\np_y_normal = (1 / np.sqrt(sigma2_or_a * 2 * np.pi)) * np.exp(- (0.5 / sigma2_or_a) * (y - mu) ** 2)\np_y_laplace = (1 / (2 * sigma2_or_a)) * np.exp(- (1 / sigma2_or_a) * np.abs(y - mu))\n\nplt.figure(figsize=(4,3))\nplt.plot(y, p_y_normal, label=r\"Normal PDF\")\nplt.plot(y, p_y_laplace, label=r\"Laplace PDF\")\nplt.xlabel('$y$')\nplt.ylabel('$p(y)$')\nplt.legend()\npass\n\n\n\n\nLet’s now consider the Laplace version of our probabilistic linear model: \\[\ny_i \\sim L\\big(\\mathbf{x}_i^T \\mathbf{w},\\ a\\big)\n\\] In this case that the negative log-likelihood is defined as: \\[\\mathbf{NLL}(\\mathbf{w}, b, \\mathbf{X}, \\mathbf{y}) = -\\sum_{i=1}^N \\log p(y_i\\mid \\mathbf{x}_i, \\mathbf{w}, a)\\]\nQ10: Write out the negative log-likelihood for this model in terms of \\(\\mathbf{w}, a, \\mathbf{X}, y\\) using the Laplace PDF shown above.\nWe can simply replace \\(p(y_i|\\mathbf{x}_i, a)\\) with the formula given above so we get: \\[\\mathbf{NLL}(\\mathbf{w}, a, \\mathbf{X}, \\mathbf{y}) = -\\sum_{i=1}^N \\log \\frac{1}{2a} \\exp \\bigg(-\\frac{|y_i-\\mathbf{x}_i^T\\mathbf{w}|}{a}\\bigg)\\]\n\\[ = \\frac{1}{a}\\sum_{i=1}^N |y_i-\\mathbf{x}_i^T\\mathbf{w}| + N\\log 2a\\]\nNote that if we drop the constants, we would call this loss the sum absolute error.\nQ11: Find the gradient with respect to \\(\\mathbf{w}\\) of the negative log-likelihood for this model. Hint: remember that the gradient is the vector of partial derivatives, so you may use the approach we used to find this vector for the squared error function in the previous homework!\nYou should use the following definition of the derivative of the absolute value \\(|\\cdot |\\) operator: \\[\\frac{d}{dx}|x| = sign(x), \\quad sign(x) = \\begin{cases} +1 \\quad \\text{ if  } x &gt; 0 \\\\ -1 \\quad \\text{ if  } x &lt; 0 \\\\ \\ \\ \\ 0 \\quad \\text{ if  } x = 0 \\end{cases}\\] (Technically \\(\\frac{d}{dx}|x|\\) is undefined at \\(x=0\\), but it is convinient to assume \\(\\frac{d}{dx}|0|=0\\) in practice.)\n\\[\\nabla_{\\mathbf{w}}\\mathbf{NLL}(\\mathbf{w}, b, \\mathbf{X}, \\mathbf{y}) =  \\frac{d}{d\\mathbf{w}} \\frac{1}{a}\\sum_{i=1}^N |y_i-\\mathbf{x}_i^T\\mathbf{w}| + N\\log 2a\\]\n\\[=   \\frac{1}{a}\\sum_{i=1}^N \\frac{d}{d\\mathbf{w}}|y_i-\\mathbf{x}_i^T\\mathbf{w}|\\]\n\\[=   \\frac{1}{a}\\sum_{i=1}^N sign(y_i-\\mathbf{x}_i^T\\mathbf{w})\\frac{d}{d\\mathbf{w}}(y_i-\\mathbf{x}_i^T\\mathbf{w})\\]\n\\[=   \\frac{1}{a}\\sum_{i=1}^N sign(y_i-\\mathbf{x}_i^T\\mathbf{w})\\mathbf{x}_i\\]\nQ12: Using the formula you just derived, write a function that returns both the negative log-likelihood and the gradient of the negative log-likelihood with respect to \\(\\mathbf{w}\\), given \\(\\mathbf{w}\\), \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) (assume that \\(a=1\\)). To make our loss more comparable to MSE, we’ll divide both outputs by \\(N\\).\n\ndef nll_and_grad(w, X, y):\n    N = X.shape[0]\n    error = linear_regression(X, w) - y\n    mae = np.sum(np.abs(error))\n    nll = mae + np.log(2) \n    Xaug = np.pad(X, [(0, 0), (0, 1)], constant_values=1)\n    \n    grad_w = np.dot(np.sign(error), Xaug)\n    return nll / N, grad_w / N\n\nQ13: Use the gradient_descent and nll_and_grad functions you wrote above to fit a linear regression model that takes in a car’s weight and predicts its MPG rating. Start with the weights equal to 0 and perform 50 updates of gradient descent with a learning rate of 1.. Plot the loss (NLL) as a function of the number of updates. Use the full dataset as in Q5.\n\nXweight = X[:, 1:2]\nw0 = np.zeros(2,)\n\n### YOUR CODE HERE\nw, losses = gradient_descent(nll_and_grad, w0, 0.1, 500, Xweight, y)\nwmse, mselosses = gradient_descent(mse_and_grad, w0, 0.1, 50, Xweight, y)\n\n### PLOTTING CODE HERE\nplt.figure(figsize=(6, 4))\nplt.plot(losses, label='Laplace NLL')\nplt.plot(mselosses, label='Mean squared error')\nplt.legend()\nprint(losses[-1])\n\n3.262693397826824\n\n\n\n\n\nQ14: Plot a scatterplot of weight vs. MPG. Then on the same plot, plot the prediction function for the linear regression model you just fit on the input range \\([-3, 3]\\). Describe any differences you see vs. the model you fit with MSE.\n\nplt.figure(figsize=(6, 4))\nplt.scatter(Xweight[:,0], y)\nplt.plot(np.linspace(-3, 3, 200), linear_regression(np.linspace(-3, 3, 200).reshape((-1, 1)), w), label='Laplace NLL weights')\nplt.plot(np.linspace(-3, 3, 200), linear_regression(np.linspace(-3, 3, 200).reshape((-1, 1)), wmse), label='MSE weights')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x17f7c2b20&gt;\n\n\n\n\n\nQ15: Using the parameters that you just found, plot the Laplace NLL loss as a function of the first entry in w (w1 below), holding the other (the bias) constant. Then, using the same plot and parameters, plot the MSE loss as a function of the first entry in w.\n\n# Example of deconstructing and reconstructing the parameter vector.\nw1, b = w\nwi = np.array([w1, b])\n\nlosses = [nll_and_grad(np.array([wi, b]), Xweight, y)[0] for wi in np.linspace(-10, 0)]\nmse_losses = [mse_and_grad(np.array([wi, b]), Xweight, y)[0] for wi in np.linspace(-10, 0)]\n\nplt.figure(figsize=(6, 4))\nplt.plot(mse_losses, label='Mean squared error')\nplt.plot(losses, label='Laplace NLL')\n\nplt.legend()\nplt.ylabel('$w_1$')\nplt.xlabel('Loss')\n\nText(0.5, 0, 'Loss')\n\n\n\n\n\nIn the cells below, copy your code for Q5 and Q13. Try out different learning rates and and numbers of iterations. Observe what happens to the loss plot in each case.\nQ16: Based on what you’ve obsevered above and in the previous questions, why might you choose the Normal distribution or the Laplace distribution for linear regression?\nWe see that the MSE weights are more sensitive to outliers than the Laplace NLL weights."
  },
  {
    "objectID": "assignments/homework2-linear-regression/main.html",
    "href": "assignments/homework2-linear-regression/main.html",
    "title": "Homework 2: Linear regression and maximum likelihood",
    "section": "",
    "text": "Please list anyone you discussed or collaborated on this assignment with below.\nLIST COLLABORATORS HERE"
  },
  {
    "objectID": "assignments/homework2-linear-regression/main.html#part-1-linear-regression",
    "href": "assignments/homework2-linear-regression/main.html#part-1-linear-regression",
    "title": "Homework 2: Linear regression and maximum likelihood",
    "section": "Part 1: Linear regression",
    "text": "Part 1: Linear regression\nLet’s begin by reviewing the context of linear regression.\nRecall that the linear regression model makes predictions of the following form:\n\\[f(\\mathbf{x})=\\mathbf{x}^T\\mathbf{w} + b\\]\nOr if we consdier the augmented representation:\n\\[\\mathbf{x} \\rightarrow \\begin{bmatrix}\\mathbf{x} \\\\ 1 \\end{bmatrix} = \\begin{bmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\\\ 1 \\end{bmatrix},\\quad \\mathbf{w} \\rightarrow \\begin{bmatrix}\\mathbf{w} \\\\ b \\end{bmatrix} = \\begin{bmatrix}w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_d \\\\ b \\end{bmatrix}\\] Then we can use the more convenient linear form: \\[f(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{w}\\]\nQ1: Write a function that takes in an input \\((\\mathbf{x})\\) and a set of weights \\((\\mathbf{w})\\) and makes a prediction using the function above. Your implementation should assume that the bias \\((b)\\) is included in the weights \\((\\mathbf{w})\\) and thus should augment the input to account for this (or separate \\(b\\) from \\(\\mathbf{w}\\)).\n\n# Test inputs\nx = np.array([1, -3, 5])\nw = np.array([-1, 2, 0.5, 2])\ny = -2.5\n\ndef linear_regression(x, w):\n    ### YOUR CODE HERE\n    pass\n\n## Validate the function\nassert y == linear_regression(x, w)\n\nAs discussed in class, we can compactly refer to an entire dataset of inputs and outputs using the notation \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) respectively. Our convention will be that row \\(i\\) of \\(\\mathbf{X}\\) will correspond to the \\(i^{th}\\) observed input \\(\\mathbf{x}_i\\), while the corresponding entry \\(y_i\\) of \\(\\mathbf{y}\\) is the observed output. In other words \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) are defined as: \\[\n\\mathbf{X} =\n\\begin{bmatrix} X_{11} & X_{12} & \\dots  & X_{1d} & 1 \\\\\n                X_{21} & X_{22} & \\dots  & X_{2d} & 1 \\\\\n                \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n                X_{N1} & X_{N2} & \\dots  & X_{Nd} & 1 \\\\  \n                \\end{bmatrix} = \\begin{bmatrix} \\mathbf{x}_1^T \\\\ \\mathbf{x}_2^T \\\\ \\vdots \\\\ \\mathbf{x}_N^T \\end{bmatrix}, \\quad\n                \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{bmatrix}\n\\]\nWith this notation, we can make predictions for an entire set of inputs as: \\[f(\\mathbf{X}) = \\mathbf{X}\\mathbf{w}\\] Where the output is now a vector of predictions. We see that each entry corresponds to the prediction for input \\(\\mathbf{x}_i\\): \\[f(\\mathbf{X})_i = \\mathbf{x}_i^T\\mathbf{w} = f(\\mathbf{x}_i)\\]\nQ2: Modify (if needed) your linear regression prediction function to accept a set of inputs as matrix as described above and return a vector of predictions. Once again you should not assume the data includes the extra bias dimension. Then plot the prediction function given by the provided weights on the range \\([0, 5]\\). Recall that np.linspace(a, b, n) produces a vector of n equally spaced numbers between a and b.\n\n# Test inputs\nw = np.array([0.5, 2])\n\ndef linear_regression(x, w):\n    ### YOUR CODE HERE\n    pass\n\n## CODE TO PLOT THE FUNCTION HERE\nX = \ny = linear_regression(X, w)\n\n\nRecall that ordinary least squares finds the parameter \\(\\mathbf{w}\\) that minimizes the mean of squared error between the true outputs \\(\\mathbf{y}\\) and predictions. \\[\\underset{\\mathbf{w}}{\\text{argmin}} \\frac{1}{N} \\sum_{i=1}^N \\left(\\mathbf{x}_i^T\\mathbf{w} - y_i\\right)^2\\]\nObserve that this is the same formula as the error we used for evaluating Richardson iteration in the last homework, we’ve simply renamed the variables from \\((\\mathbf{A}, \\mathbf{x}, \\mathbf{b})\\) to \\((\\mathbf{X}, \\mathbf{w}, \\mathbf{y})\\)! (We’ve also added the constant \\(\\frac{1}{N}\\), but this doesn’t change the optimal \\(\\mathbf{w}\\))\nWe’ve now seen 3 equivalent ways to write the same formula. From most compact to least compact these are: \\[\\textbf{1: } \\frac{1}{N} \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2 \\quad \\textbf{2: } \\frac{1}{N} \\sum_{i=1}^N \\left(\\mathbf{x}_i^T\\mathbf{w} - y_i\\right)^2 \\quad \\textbf{3: } \\frac{1}{N} \\sum_{i=1}^n \\left(\\sum_{j=1}^n X_{ij}w_j - y_i\\right)^2\\]\nQ3: Using the gradient formula we derived in class, write a function that returns both the mean squared error and the gradient of the error with respect to \\(\\mathbf{w}\\), given \\(\\mathbf{w}\\), \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\). Hint: don’t forget to augment X when computing the gradient!\n\ndef mse_and_grad(w, X, y):\n    ### YOUR CODE HERE\n    return mse, grad_w\n\nRecall that we can use the gradient descent algorithm to find the input that minimizes a function, using the corresponding gradient function.\nGiven an initial guess of the optimal input, \\(\\mathbf{w}^{(0)}\\), gradient descent uses the following update to improve the guess: \\[\\mathbf{w}^{(i+1)} \\longleftarrow \\mathbf{w}^{(i)} - \\alpha \\nabla f(\\mathbf{w}),\\] where \\(\\alpha\\) is the learning rate or step size.\nQ4: Write a function to perform gradient descent. The function should take as input: - value_and_grad: A function that produces the output and gradient of the function to optimize (e.g. the mse_and_grad) - w0: An inital guess \\(\\mathbf{w}^{(0)}\\) - lr: The learning rate \\((\\alpha)\\) - niter: The number of updates to perform - *args: Any additional argumets to pass to value_and_grad (e.g. X and y)\nThe function should return: - w: The final guess - losses: A list (or array) that tracks the value of the function at each update\n\ndef gradient_descent(value_and_grad, w0, lr, niter, *args):\n    # Get the inital loss\n    initial_loss, _ = value_and_grad(w0, *args)\n    losses = []\n\n    ### YOUR CODE HERE\n\n    return w, losses"
  },
  {
    "objectID": "assignments/homework2-linear-regression/main.html#part-2-applications-to-real-data",
    "href": "assignments/homework2-linear-regression/main.html#part-2-applications-to-real-data",
    "title": "Homework 2: Linear regression and maximum likelihood",
    "section": "Part 2: Applications to real data",
    "text": "Part 2: Applications to real data\nNow that we’ve setup a full implementation of linear regression, let’s test it on a real dataset. We’ll use the MPG dataset that we saw in class. The following code will load the data and rescale it.\n\ndata = np.genfromtxt('auto-mpg.csv', delimiter=',', missing_values=['?'], filling_values=[0])\n\n# MPG is the output value\ntarget = 'MPG'\ny = data[:, 0]\n\n# The other variables are inputs in the order listed\nfeatures = ['displacement', 'weight', 'acceleration', 'year']\nX = data[:, [2, 4, 5, 6]]\nX = (X - X.mean(axis=0)) / X.std(axis=0)\n\nLet’s start by fitting a model that just uses the feature weight.\nQ5: Use the gradient_descent and mse_and_grad functions you wrote above to fit a linear regression model that takes in a car’s weight and predicts its MPG rating. Start with the weights equal to 0 and perform 100 updates of gradient descent with a learning rate of 0.1. Plot the loss (MSE) as a function of the number of updates.\n\nXweight = X[:, 1:2]\nw0 = np.zeros((2,))\n\n### YOUR CODE HERE\nw, losses = \n\n### PLOTTING CODE HERE\nplt.figure(figsize=(6, 4))\n\nQ6: Plot a scatterplot of weight vs. MPG. Then on the same plot, plot the prediction function for the linear regression model you just fit on the input range \\([-3, 3]\\).\n\n### PLOTTING CODE HERE\nplt.figure(figsize=(6, 4))\n\nQ7: Repeat Q5 using all 4 features and compare the final loss to the final loss using only weight as an input. Describe any differences you observe.\n\nw0 = np.zeros((5,))\n\n### YOUR CODE HERE\nw, losses = \n\n### PLOTTING CODE HERE\nplt.figure(figsize=(6, 4))\n\nDESCRIBE RESULTS HERE\nIn machine learning we are typically less interested in how our model predicts the data we’ve already seen than we are in how well it makes predictions for new data. One way to estimate how well our model our model will generalize to new data is to hold out data while fitting our model. To do this we will split our dataset into two smaller datasets: a training dataset that we will use to fit our model, and a test or held-out dataset that we will only use to evaluate our model. By computing the loss on this test dataset, we can get a sense of how well our model will make prediction for new data.\n\\[\\mathcal{D} = \\{ (\\mathbf{x}_1, y_1),\\ (\\mathbf{x}_2, y_2),\\ ... \\,(\\mathbf{x}_N, y_N) \\}\\quad \\longrightarrow \\quad\n\\mathcal{D}_{train} = \\{ (\\mathbf{x}_1, y_1),\\ (\\mathbf{x}_2, y_2),\\ ... \\,(\\mathbf{x}_{Ntrain}, y_{Ntrain}) \\},\\  \n\\mathcal{D}_{test} = \\{ (\\mathbf{x}_1, y_1),\\ (\\mathbf{x}_2, y_2),\\ ... \\,(\\mathbf{x}_{Ntest}, y_{Ntest}) \\}\n\\]\nQ8: Split the MPG dataset into training and test datasets. Use 70% of the observations for training and 30% for test. Then repeat Q5 to fit a linear regression model on just the training dataset using only the weight feature (you don’t need to plot the loss for this question). Report the final loss (MSE) on the training dataset and on the test dataset.\n\n### CODE TO SPLIT DATASET HERE\nXweight_train = \ny_train = \n\nXweight_test = \ny_test = \n\n### CODE TO FIT MODEL HERE\nw0 = np.zeros((2,))\nw, losses = \n\n### CODE TO EVALUATE MODEL HERE\nmse_train = \nmse_test =\n\nprint('Loss on training data: %.4f, loss on test data: %.4f' % (mse_train, mse_test))\n\nQ9: Repeat Q8 using the all 4 features. Compare the results to the model using only weight.\n\n### CODE TO SPLIT DATASET HERE\nX_train = \ny_train = \n\nX_test = \ny_test = \n\n### CODE TO FIT MODEL HERE\nw0 = np.zeros((5,))\nw, losses = \n\n### CODE TO EVALUATE MODEL HERE\nmse_train = \nmse_test =\n\nprint('Loss on training data: %.4f, loss on test data: %.4f' % (mse_train, mse_test))\n\nDESCRIBE RESULTS HERE"
  },
  {
    "objectID": "assignments/homework2-linear-regression/main.html#part-3-maximum-likelihood-training",
    "href": "assignments/homework2-linear-regression/main.html#part-3-maximum-likelihood-training",
    "title": "Homework 2: Linear regression and maximum likelihood",
    "section": "Part 3: Maximum Likelihood Training",
    "text": "Part 3: Maximum Likelihood Training\n\nBackground:\nWe saw in lecture that we can view linear regression as the following probibalistic model: \\[\ny_i \\sim \\mathcal{N}\\big(\\mathbf{x}_i^T \\mathbf{w},\\ \\sigma^2\\big)\n\\] Where \\(\\mathcal{N}(\\mu, \\sigma)\\) is the Normal distribution, which has the following probability density function: \\[\np(y\\mid \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\text{exp}\\bigg(-\\frac{1}{2\\sigma^2} (y -\\mu)^2\\bigg)\n\\] We saw that a reasonable way to choose the optimal \\(\\mathbf{w}\\) is to maximize the likelihood of our observed dataset, which is equivalent to minimizing the negative log likelihood: \\[\\underset{\\mathbf{w}}{\\text{argmax}} \\prod_{i=1}^N p(y_i\\mid \\mathbf{x}_i, \\mathbf{w}, \\sigma) =\n\\underset{\\mathbf{w}}{\\text{argmin}} -\\sum_{i=1}^N \\log p(y_i\\mid \\mathbf{x}_i, \\mathbf{w}, \\sigma) = \\underset{\\mathbf{w}}{\\text{argmin}}\\ \\frac{1}{2\\sigma^2}\\sum_{i=1}^N  (y_i -\\mathbf{x}_i^T\\mathbf{w})^2 + C\n\\] Where \\(C\\) is a constant: \\(C = N \\log \\sigma \\sqrt{2\\pi}\\). We further saw that this is equivalent to minimizing the mean squared error for any choice of \\(\\sigma\\).\n\n\nLaplace Maximum Likelihood Estimation\nA natural question about the above model is: why choose the Normal distribution? In principal, we could define a linear model using any distribution over the real numbers \\(\\mathbb{R}\\). Let’s explore an alternative choice: the Laplace distribution (Wiki). The Laplace distribution \\(L(\\mu, a)\\) has the following PDF:\n\\[p(y\\mid \\mu, a) = \\frac{1}{2a} \\exp\\bigg(- \\frac{|y-\\mu|}{a} \\bigg) \\]\nAs for the normal distribution \\(\\mu\\) is the mean, while \\(a\\) defines the “width” of the distribution (analogous to variance for the Normal). We can compare the two distributions visually:\n\ny = np.linspace(-3, 3, 200)\nmu, sigma2_or_a = 0, 1\np_y_normal = (1 / np.sqrt(sigma2_or_a * 2 * np.pi)) * np.exp(- (0.5 / sigma2_or_a) * (y - mu) ** 2)\np_y_laplace = (1 / (2 * sigma2_or_a)) * np.exp(- (1 / sigma2_or_a) * np.abs(y - mu))\n\nplt.figure(figsize=(4,3))\nplt.plot(y, p_y_normal, label=r\"Normal PDF\")\nplt.plot(y, p_y_laplace, label=r\"Laplace PDF\")\nplt.xlabel('$y$')\nplt.ylabel('$p(y)$')\nplt.legend()\npass\n\n\n\n\nLet’s now consider the Laplace version of our probabilistic linear model: \\[\ny_i \\sim L\\big(\\mathbf{x}_i^T \\mathbf{w},\\ a\\big)\n\\] Recall that the negative log-likelihood is defined as: \\[\\mathbf{NLL}(\\mathbf{w}, a, \\mathbf{X}, \\mathbf{y}) = -\\sum_{i=1}^N \\log p(y_i\\mid \\mathbf{x}_i, \\mathbf{w}, a)\\]\nQ10: Write out the negative log-likelihood for this model in terms of \\(\\mathbf{w}, a, \\mathbf{X}, y\\) using the Laplace PDF shown above.\nYOUR ANSWER HERE \\[\\mathbf{NLL}(\\mathbf{w}, a, \\mathbf{X}, \\mathbf{y}) = \\]\nNote that if we drop the constants, we would call this loss the sum absolute error.\nQ11: Find the gradient with respect to \\(\\mathbf{w}\\) of the negative log-likelihood for this model. Hint: remember that the gradient is the vector of partial derivatives!\nYou should use the following definition of the derivative of the absolute value \\(|\\cdot |\\) operator: \\[\\frac{d}{dx}|x| = sign(x), \\quad sign(x) = \\begin{cases} +1 \\quad \\text{ if  } x &gt; 0 \\\\ -1 \\quad \\text{ if  } x &lt; 0 \\\\ \\ \\ \\ 0 \\quad \\text{ if  } x = 0 \\end{cases}\\] (Technically \\(\\frac{d}{dx}|x|\\) is undefined at \\(x=0\\), but it is convinient to assume \\(\\frac{d}{dx}|0|=0\\) in practice.)\nYOUR ANSWER HERE \\[\\nabla_{\\mathbf{w}}\\mathbf{NLL}(\\mathbf{w}, b, \\mathbf{X}, \\mathbf{y}) = \\]\nQ12: Using the formula you just derived, write a function that returns both the negative log-likelihood and the gradient of the negative log-likelihood with respect to \\(\\mathbf{w}\\), given \\(\\mathbf{w}\\), \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) (assume that \\(a=1\\)). We’ll divide both outputs by \\(N\\) to make our results more comparable to MSE. Hint: np.sign will compute the sign function descibed above.\n\ndef nll_and_grad(w, X, y):\n    N = X.shape[0]\n    ### YOUR CODE HERE\n\n    \n    return nll / N, grad_w / N\n\nQ13: Use the gradient_descent and nll_and_grad functions you wrote above to fit a linear regression model that takes in a car’s weight and predicts its MPG rating. Start with the weights equal to 0 and perform 100 updates of gradient descent with a learning rate of 1.0. Plot the loss (NLL) as a function of the number of updates. Use the full dataset as in Q5 and note the change in learning rate!\n\nXweight = X[:, 1:2]\nw0 = np.zeros((2,))\n\n### YOUR CODE HERE\nw_laplace, losses_laplace = \n\n### PLOTTING CODE HERE\nplt.figure(figsize=(6, 4))\n\nQ14: Plot a scatterplot of weight vs. MPG. Then on the same plot, plot the prediction function for the linear regression model you just fit on the input range \\([-3, 3]\\). Finally, copy your code from Q5 to once again find the optimal \\(\\mathbf{w}\\) using the MSE loss and plot this function on the same plot in a different color. Describe any differences you see between the two models\n\n### CODE FROM Q5 HERE\nw_mse, losses_mse = \n\n### PLOTTING CODE HERE\nplt.figure(figsize=(6, 4))\n\nWRITTEN ANSWER HERE\nQ15: Using the parameters that you just found, plot the Laplace NLL loss as a function of the first entry in w (w1 below), holding the other (the bias) constant. Plot this loss on the range \\([-10, 0]\\). Then, in a separate cell, plot the MSE loss as a function of the first entry in w on the same range.\n\n# Example of deconstructing and reconstructing the parameter vector.\nw1, b = w\nwi = np.array([w1, b])\n\n### PLOTTING CODE FOR NLL HERE\nplt.figure(figsize=(6, 4))\n\n\n### PLOTTING CODE FOR MSE HERE\nplt.figure(figsize=(6, 4))\n\nIn the cells below, copy your code for Q5 and Q13. Try out different learning rates and and numbers of iterations. Observe what happens to the loss plot in each case.\n\n### COPY Q5 CODE HERE\n\n\n### COPY Q13 CODE HERE\n\nQ16: Based on what you’ve obsevered above and in the previous questions, why might you choose the Normal distribution or the Laplace distribution for linear regression? What effect does the learning rate have (what happens if it’s set too high or too low)?\nYOUR ANSWER HERE"
  },
  {
    "objectID": "assignments/homework1-background/main.html",
    "href": "assignments/homework1-background/main.html",
    "title": "Homework 1: Introduction to Numpy",
    "section": "",
    "text": "Please list anyone you discussed or collaborated on this assignment with below.\nLIST COLLABORATORS HERE"
  },
  {
    "objectID": "assignments/homework1-background/main.html#part-1-numpy-basics",
    "href": "assignments/homework1-background/main.html#part-1-numpy-basics",
    "title": "Homework 1: Introduction to Numpy",
    "section": "Part 1: Numpy basics",
    "text": "Part 1: Numpy basics\nAs discussed in class, a square matrix \\(A\\) defines a linear mapping: \\(\\mathbb{R}^n\\rightarrow \\mathbb{R}^n\\). Given a vector \\(\\textbf{x}\\), we can find the corresponding output of this mapping \\(\\textbf{b}\\) using matrix-vector multiplication: \\(\\textbf{b}=A \\textbf{x}\\). We can write an example matrix-multiplication using matrix notation as:\n\\[\\begin{bmatrix}\n4 & -3 & 2 \\\\\n6 & 5  & 1 \\\\\n-4 & -1 & 2\n\\end{bmatrix} \\cdot \\begin{bmatrix}\n1 \\\\\n-2 \\\\\n1\n\\end{bmatrix}\n= \\begin{bmatrix}\n? \\\\\n? \\\\\n?\n\\end{bmatrix}\n\\]\nQ1: Perform this matrix-vector multiplication by hand and write the answer in the cell below.\nWRITE ANSWER HERE\nQ2: In the code cell below, create the matrix \\(A\\) and the vector \\(\\textbf{x}\\) shown above, using Numpy. Then use the np.dot function to find the output of the mapping \\(\\textbf{b} = A\\textbf{x}\\). Verify that the answer matches what you derived above.\n\n# Fill answers here\nA = \nx =\nb =\n\nprint(b)\n\nOften we will have access to the transformed vector \\(\\textbf{b}\\) and need to find the orginal vector \\(\\textbf{x}\\). To do this we need to solve the system of linear equations \\(A\\textbf{x}=\\textbf{b}\\) for \\(\\textbf{x}\\). \\[\\begin{bmatrix}\n4 & -3 & 2 \\\\\n6 & 5  & 1 \\\\\n-4 & -1 & 2\n\\end{bmatrix} \\cdot \\begin{bmatrix}\n? \\\\\n? \\\\\n?\n\\end{bmatrix} = \\begin{bmatrix}\n2 \\\\\n-1 \\\\\n3\n\\end{bmatrix}\n\\]\nQ3: Find the missing \\(\\textbf{x}\\) in the equation above using the np.linalg.solve function and verify that \\(A\\textbf{x}=\\textbf{b}\\).\n\n# Fill answer here (A is the same matrix from above)\nb = \nx = \n\nIn linear algebra you may have learned how to solve a system of linear equations using Gaussian elimination. Here we will implement an alternative approach known as Richardson iteration. In this method we start with an inital guess for the solution: \\(\\textbf{x}^{(0)}\\), then we will iteratively update this guess until the solution is reached. Given a matrix \\(A\\), a target \\(\\textbf{b}\\) and a current guess \\(\\textbf{x}^{(k)}\\), we can compute the Richardson update as:\n\\[\\textbf{x}^{(k+1)} \\leftarrow \\textbf{x}^{(k)} + \\omega \\left(\\textbf{b} - A\\textbf{x}^{(k)}\\right)\\]\nHere \\(\\omega\\) is a constant that we can choose to adjust the algorithm. We will set \\(\\omega = 0.1\\).\nQ4: Fill in the Richardson iteration function below and apply it to the system of linear equations from above using 100 updates. Verify that if gives a similar answer to np.linalg.solve.\n\n# Fill in function below\ndef richardson_iter(x_guess, A, b, omega=0.1):\n    \n    return new_x_guess\n\nx_guess = np.zeros(3)\nfor i in range(100):\n    x_guess = richardson_iter(x_guess, A, b)\n\nprint(x_guess, x)\n\nRecall that the length of a vector is given by it’s two-norm, which is defined as: \\[\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}.\\]\nCorrespondingly, the (Euclidian) distance between two points \\(\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^n\\) can be written as \\(\\|\\mathbf{a} - \\mathbf{b}\\|_2\\). As a convenient measure of error for our Richardson iteration algorithm, we will use the squared Euclidean distance. For a guess \\(\\mathbf{x}^{(k)}\\) we will compute the error \\(e^{(k)}\\) as: \\[e^{(k)} = \\|A\\mathbf{x}^{(k)} - \\mathbf{b}\\|_2^2\\]\nIn expanded form, this would be written as: \\[e^{(k)} = \\sum_{i=1}^n \\left(\\sum_{j=1}^n A_{ij}x^{(k)}_j - b_i\\right)^2\\]\nQ5: Write a function to compute the error of a given guess. Then run Richardson iteration again for 100 steps, computing the error at each step. Finally create a plot of the error for each step (error vs. step).\nHint: recall that basic operations in numpy (addition, subtraction, powers) are performed element-wise.\n\\[\\mathbf{c} = \\mathbf{A}\\mathbf{x}\\]\n\\[||c-b||^2_2\\]\n\\[\\sum_{i=1}^N (c_i - b_i)^2\\]\n$$c_i = {j=1}^n A{ij}x_j\n\n# Fill in function below\ndef error(x_guess, A, b):\n\n    return err\n\n# Add code to plot the error over time\n\nx_guess = np.zeros(3)\nfor step in range(100):\n\n    x_guess = richardson_iter(x_guess, A, b)\n\n    \n\nQ6: Derive the partial derivative of the error with respect to a single entry of \\(\\mathbf{x}^{(k)}\\) (without loss of generality, we will say \\(x^{(k)}_1\\)). Work in the expanded form as in the equation above, writing your answer in the markdown cell below.\nHint: You may find it helpful to refer to the latex equation cheatsheet on the course website. You may show intermediate steps here or as handwritten work as a separate file in the repository. The final answer should be filled in here.\nEDIT THIS CELL WITH YOUR ANSWER\n\\[\\frac{\\partial e^{(k)}}{\\partial x^{(k)}_1}= \\]\nYOU MAY ADD WORK HERE\nIn practice, we will likely want to compute the derivative with respect to all entries of \\(\\mathbf{x}\\): \\[\\frac{\\partial e^{(k)}}{\\partial \\mathbf{x}^{(k)}} = \\begin{bmatrix}\n\\frac{\\partial e^{(k)}}{\\partial x^{(k)}_1} \\\\\n\\vdots \\\\\n\\frac{\\partial e^{(k)}}{\\partial x^{(k)}_n}\n\\end{bmatrix}\\]\nQ7: Using the formula you just derived, write the formula for the vector of all partial derivatives in the compact matrix/vector notation (e.g. \\(A\\mathbf{x}=\\mathbf{b}\\)).\nEDIT THIS CELL WITH YOUR ANSWER\n\\[\\frac{\\partial e^{(k)}}{\\partial \\mathbf{x}^{(k)}}= \\]\nQ8: In 1-2 sentences describe how this answer relates to the Richardson iteration algorithm above. We will discuss this more in class!\nWRITE YOUR ANSWER HERE"
  },
  {
    "objectID": "assignments/homework1-background/main.html#part-2-working-with-batches-of-vectors",
    "href": "assignments/homework1-background/main.html#part-2-working-with-batches-of-vectors",
    "title": "Homework 1: Introduction to Numpy",
    "section": "Part 2: Working with batches of vectors",
    "text": "Part 2: Working with batches of vectors\nRecall that a vector can also be seen as either an \\(n \\times 1\\) matrix (column vector) or a \\(1 \\times n\\) matrix (row vector).\n\\[\\text{Column vector: } \\mathbf{x} = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3\n\\end{bmatrix}, \\quad \\text{Row vector: } \\mathbf{x} = \\begin{bmatrix}\nx_1 & x_2 & x_3\n\\end{bmatrix}\\]\nNote that we use the same notation for both as they refer to the same concept (a vector). The difference becomes relevant when we consider matrix-vector multiplication. We can write matrix-vector multiplication in two ways: \\[\\text{Matrix-vector: }A\\mathbf{x} = \\mathbf{b}, \\quad \\text{Vector-matrix: }\\mathbf{x}A^T= \\mathbf{b}\\] In matrix-vector multiplication we treat \\(\\textbf{x}\\) as a column vector (\\(n \\times 1\\) matrix), while in vector-matrix multiplication we treat it as a row vector (\\(n \\times 1\\) matrix). Transposing \\(A\\) for left multiplication ensures that the two forms give the same answer.\nQ9: Using the previously defined \\(\\mathbf{x}\\), create an explicit column vector and row vector. Then using the previously defined \\(A\\), verify that the matrix-vector and vector-matrix multiplications shown above do produce the same resultant vector \\(\\mathbf{b}\\).\nHint: Recall that np.dot is also used for matrix-matrix multiplication.\n\n# Fill in code here\nx_col =\nx_row = \n\nThroughout this course we will typically use row vectors and vector-matrix multiplication, as this is more conventional in neural-network literature. The concept of row and column vectors becomes handy when transforming collections of vectors.\nRecall that a matrix can be seen as a collection of vectors. In numpy we can create a matrix from a list of (1- dimensional) vectors using the np.stack function. This function assumes that the vectors are row vectors creating the matrix as follows: \\[\\begin{bmatrix}\n3 & 1 & -2\n\\end{bmatrix},\\ \\begin{bmatrix}\n4 & 5 & 3\n\\end{bmatrix},\\ \\begin{bmatrix}\n-2 & -1 & 5\n\\end{bmatrix}\\quad \\overset{\\text{np.stack}}{\\longrightarrow} \\begin{bmatrix}\n3 & 1 & -2 \\\\\n4 & 5 & 3 \\\\\n-2 & -1 & 5\n\\end{bmatrix} \\]\nWe will call this matrix \\(X\\) to denote that it is a collection of vectors, rather than a single vector (\\(\\mathbf{x}\\)).\nQ10: Create this matrix in numpy using th np.stack function.\n\n# Fill code here\n\nX = \nprint(X)\n\nWhen taken together as a matrix in this way, we can apply the linear mapping \\(A\\) to all vectors using matrix-matrix multiplication: \\[B=XA^T\\]\nLet’s put this into practice with a visual example.\nQ11: Create a \\(20 \\times 3\\) matrix, circle, in numpy of the following form\n\\[ \\begin{bmatrix} \\sin(\\theta_1) & \\cos(\\theta_1)  & 1 \\\\\n\\sin(\\theta_2) & \\cos(\\theta_2)  & 1 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n\\sin(\\theta_{20}) & \\cos(\\theta_{20})  & 1 \\\\\n\\end{bmatrix}\\] Where \\(\\theta_1...\\theta_{20}\\) are evenly spaced between \\(0\\) and \\(2\\pi\\).\n\ntheta = np.linspace(0, 2 * np.pi, 20) # Generates 20 evenly-spaced numbers between 0 and 2π\n\n# Fill in your code here\ncircle = \n\nThe code we just wrote creates a matrix corresponding to a collection of \\(20\\) row vectors of length 3. Each vector represents a point on the unit circle where the first entry is the x-coordinate, the second entry is the y-coordinate and the third entry is always \\(1\\): \\[ \\begin{bmatrix} x & y & 1 \\end{bmatrix}\\]\nQ12: Plot the set of 20 points in circle using the plt.plot function. Use only the x and y coordinates, ignoring the column of 1s.\n\nplt.figure(figsize=(4, 4))\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\n\n# Fill your code here\n\nQ13: Transform all the vectors in circle with the matrix \\(A\\) using a single call to np.dot. Then plot the original set of points in black and the transformed points in red using the plt.plot function.\nYou might also consider why we added the extra column of 1s! We will discuss the answer to that in class.\n\n# Fill your code here\ntransformed_circle = \n\n\nplt.figure(figsize=(4, 4))\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\n\n# Fill your code here"
  },
  {
    "objectID": "assignments/homework1-background/main.html#part-3-loading-and-visualizing-data",
    "href": "assignments/homework1-background/main.html#part-3-loading-and-visualizing-data",
    "title": "Homework 1: Introduction to Numpy",
    "section": "Part 3: Loading and visualizing data",
    "text": "Part 3: Loading and visualizing data\nFor most of this class we will be working with real-world data. A very well-known dataset in statistics is the Iris flower dataset collected by Edgar Anderson in 1929. The dataset consists of measurments of iris flowers. Each flower has 4 collected measurements: sepal length, sepal width, petal length, petal width, as well as a classification into one of 3 species: Iris setosa, Iris versicolor and Iris virginica. We will return to this dataset in the next homework.\nWe can load this dataset as Numpy objects using the Scikit-Learn library. Below we’ve extrated 4 relevant arrays: - features: a \\(150 \\times 4\\) matrix which has one row per observed flower and one column per measurement. - targets: a length \\(150\\) array that specifies the species of each flower as a number 0-2. - feature_names: a list of strings with the name of each measurement. - target_names: a list of strings with the name of each species.\nIn this homework, we will only visualize this dataset, which is typically a good first step in working with a new type of data. To get a convenient summary of the data we will create what is called a scatterplot matrix. This is a grid of plots where each plot contains a scatter plot with different features on the x and y axes. Because there are 4 features (measurements) in this data, we will create a 4-by-4 matrix to plot each pair of features.\nQ14: Fill in the code to create a scatterplot matrix for the Iris dataset below. - Each row of the matrix should use a different feature for the y-axis and each column should use a different feature for the x-axis. The plots on the diagonal where x and y would be the same feature can be ignored. - The x and y axis of each sub-plot should be labeled with the appropriate feature names. - The points in each scatterplot should be colored by the species label of that flower. Include a legend in at least 1 sub-plot.\nHint: The linked Wikipedia article shows an example of a scatterplot matrix for this dataset, feel free to use it as reference!\n\nimport sklearn.datasets as datasets\ndataset = datasets.load_iris()\nfeatures = dataset['data']\ntargets = dataset['target']\nfeature_names = dataset['feature_names']\ntarget_names = dataset['target_names']\n\n# Fill in the code below\nfig, ax = plt.subplots(4, 4, figsize=(12, 12))\nfor i in range(4):\n    for j in range(4):\n        # Skip sub-plots on the diagonal\n        if i == j: \n            continue\n\n        # Add subplot code here"
  },
  {
    "objectID": "lecture5-neural-networks-intro/notes.html#a-new-visualization",
    "href": "lecture5-neural-networks-intro/notes.html#a-new-visualization",
    "title": "Lecture 5: Neural networks",
    "section": "A new visualization",
    "text": "A new visualization\nSo far in this class we have seen how to make predictions of some output \\(y\\) given an input \\(\\mathbf{x}\\) using linear models. We saw that a reasonable model for continuous outputs \\((y\\in\\mathbb{R})\\) is linear regression.\n\\[\n\\textbf{Predict } y\\in \\textbf{ as } \\begin{cases} y = \\mathbf{x}^T\\mathbf{w}\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad (\\text{prediction function}) \\\\\n\\\\ p(y\\mid \\mathbf{x}, \\mathbf{w}, \\sigma^2) = \\mathcal{N}\\big(y\\mid \\mathbf{x}^T\\mathbf{w}, \\sigma^2) \\quad (\\text{probabilistic view})  \\end{cases}\n\\]\nA reasonable model for binary outputs \\((y\\in\\{0,1\\})\\) is logistic regression:\n\\[\n\\textbf{Predict } y\\in \\textbf{ as } \\begin{cases} y = \\mathbb{I}(\\mathbf{x}^T\\mathbf{w} &gt; 0)\\ \\quad\\quad\\quad\\quad\\quad (\\text{prediction function}) \\\\\n\\\\ p(y=1 \\mid \\mathbf{x}, \\mathbf{w}) = \\sigma(\\mathbf{x}^T\\mathbf{w}) \\quad (\\text{probabilistic view})  \\end{cases}\n\\]\nA reasonable model for categorical outputs \\((y\\in\\{0,1,…,C\\})\\) is multinomial logistic regression:\n\\[\n\\textbf{Predict } y\\in \\textbf{ as } \\begin{cases} y = \\underset{c}{\\text{argmax}} \\ \\mathbf{x}^T\\mathbf{w}_c \\ \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\ \\  (\\text{prediction function}) \\\\\n\\\\ p(y=c \\mid \\mathbf{x}, \\mathbf{w}) = \\text{softmax}(\\mathbf{x}^T\\mathbf{W})_c \\quad (\\text{probabilistic view})  \\end{cases}\n\\]\nIn each of these cases, the core of our prediction is a linear function \\((\\mathbf{x}^T\\mathbf{w})\\) parameterized by a set of weights \\(\\mathbf{w}\\), with possibly some nonlinear function (e.g. \\(\\sigma(\\cdot)\\)), applied to the result. This type of function is commonly depicted using a diagram like the one shown below.\n\n\n\n\n\nEach node corresponds to a scalar value: the nodes on the left correspond to each input dimension and the node on the right corresponds to the prediction. Each edge represents multiplying the value on the left with a corresponding weight."
  },
  {
    "objectID": "lecture5-neural-networks-intro/notes.html#so-far",
    "href": "lecture5-neural-networks-intro/notes.html#so-far",
    "title": "Lecture 5: Neural networks",
    "section": "So far",
    "text": "So far"
  },
  {
    "objectID": "lecture5-neural-networks-intro/notes.html#non-linear-logistic-regression",
    "href": "lecture5-neural-networks-intro/notes.html#non-linear-logistic-regression",
    "title": "Lecture 5: Neural networks",
    "section": "Non-linear logistic regression",
    "text": "Non-linear logistic regression\nWe can create a non-linear logistic regression model using the feature-transfor approach as:\n\\[\np(y=1\\mid \\mathbf{x}, \\mathbf{w}) = \\sigma\\big( \\phi(\\mathbf{x})^T\\mathbf{w} \\big)\n\\]\nPictorally, we can represent this using the diagram we just introduced as:\n\n\n\n\n\nThis demo application allows us to learn logistic regression models with different feature transforms. Hit the play button to start gradient descent!\nThis approach raises a big question though: how do we actually choose what transforms of our inputs to use?"
  },
  {
    "objectID": "lecture5-neural-networks-intro/notes.html#neural-networks-with-matrix-notation",
    "href": "lecture5-neural-networks-intro/notes.html#neural-networks-with-matrix-notation",
    "title": "Lecture 5: Neural networks",
    "section": "Neural networks with matrix notation",
    "text": "Neural networks with matrix notation\nIt is often more convenient to write all of the weights that are used to create our hidden layer as a single large matrix:\n\\[\\mathbf{W}_1 = \\begin{bmatrix} \\mathbf{w}_1^T \\\\ \\mathbf{w}_2^T \\\\ \\mathbf{w}_3^T \\\\ \\vdots \\end{bmatrix}\\] With this, we can write our general neural network more compactly as: \\[f(\\mathbf{x})= \\sigma( \\mathbf{W}_1 \\mathbf{x})^T \\mathbf{w_0} \\] Or for a whole dataset: \\[\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1^T \\\\ \\mathbf{x}_2^T \\\\ \\mathbf{x}_3^T \\\\ \\vdots \\end{bmatrix}\\]\n\\[f(\\mathbf{x})= \\sigma( \\mathbf{X}\\mathbf{W}_1 )\\mathbf{w_0}\\]"
  },
  {
    "objectID": "lecture5-neural-networks-intro/notes.html#optimizing-neural-networks",
    "href": "lecture5-neural-networks-intro/notes.html#optimizing-neural-networks",
    "title": "Lecture 5: Neural networks",
    "section": "Optimizing neural networks",
    "text": "Optimizing neural networks\nWe can still define a loss function for a neural network in the same way we did with our simpler linear models. The only difference is that now we have more parameters to choose:\n\\[\n\\mathbf{Loss}(\\mathbf{w}_0,\\mathbf{w}_1,\\mathbf{w}_2,…)\n\\]\nLet’s look at the logistic regression negative log-likelihood loss for the simple neural network we saw above:\n\\[\np(y=1\\mid \\mathbf{x}, \\mathbf{w}_0,\\mathbf{w}_1,\\mathbf{w}_2, \\mathbf{w}_3)=\\sigma(\\phi(\\mathbf{x})^T \\mathbf{w}_0),\\quad \\phi(\\mathbf{x}) = \\begin{bmatrix}  \\sigma(\\mathbf{x}^T \\mathbf{w}_1) \\\\ \\sigma(\\mathbf{x}^T \\mathbf{w}_2) \\\\ \\sigma(\\mathbf{x}^T \\mathbf{w}_3) \\end{bmatrix}\n\\] \\[ = \\sigma\\big(w_{01} \\cdot\\sigma(x_1 w_{11} + x_2 w_{12}) + w_{02} \\cdot\\sigma(x_1 w_{21} + x_2 w_{22})+ w_{03} \\cdot\\sigma(x_1 w_{31} + x_2 w_{32}) \\big)\\]\n\\[\n\\mathbf{NLL}(\\mathbf{w}_0,..., \\mathbf{X}, \\mathbf{y}) = -\\sum_{i=1}^N \\bigg[ y_i\\log p(y=1\\mid \\mathbf{x}, \\mathbf{w}_0,...) + (1-y_i)\\log p(y=0\\mid \\mathbf{x}, \\mathbf{w}_0,...) \\bigg]\n\\]\nWe see that we can write out a full expression for this loss in term of all the inputs and weights. We can even define the gradient of this loss with respect to all the weights:\n\\[\n\\nabla_{\\mathbf{w}_0...} = \\begin{bmatrix} \\frac{\\partial \\mathbf{NLL}}{\\partial w_{01}} \\\\ \\frac{\\partial \\mathbf{NLL}}{\\partial w_{02}} \\\\ \\frac{\\partial \\mathbf{NLL}}{\\partial w_{03}} \\\\ \\vdots\\end{bmatrix}\n\\]\nWhile computing this gradient by hand would be tedious, this does mean we can update all of these weights as before using gradient descent! In future classes, we’ll look at how to automate the process of computing this gradient.\nWe can see this in action for this network here."
  },
  {
    "objectID": "lecture5-neural-networks-intro/notes.html#activation-functions",
    "href": "lecture5-neural-networks-intro/notes.html#activation-functions",
    "title": "Lecture 5: Neural networks",
    "section": "Activation functions",
    "text": "Activation functions\nWhile we need a non-linear function as part of our neural network feature transform, it does not need to be the sigmoid function. A few other common choices are:\n\\[\n\\textbf{Sigmoid:}\\quad \\sigma(x) = \\frac{1}{1+e^{-x}}\n\\]\n\\[\n\\textbf{Hyperbolic tangent:}\\quad \\tanh(x) = \\frac{e^{2x}-1}{e^{2x}+1}\n\\]\n\\[\n\\textbf{Rectifed linear:}\\quad \\text{ReLU}(x) = \\max(x,\\ 0)\n\\]\n\n\n\n\n\nWe call these non-linear functions activation functions when used in neural networks. We’ll see other examples over the course of this class\nTry out different activation functions here."
  },
  {
    "objectID": "lecture5-neural-networks-intro/notes.html#background-and-a-new-visualization",
    "href": "lecture5-neural-networks-intro/notes.html#background-and-a-new-visualization",
    "title": "Lecture 5: Neural networks",
    "section": "Background and a new visualization",
    "text": "Background and a new visualization\nSo far in this class we have seen how to make predictions of some output \\(y\\) given an input \\(\\mathbf{x}\\) using linear models. We saw that a reasonable model for continuous outputs \\((y\\in\\mathbb{R})\\) is linear regression.\n\\[\n\\textbf{Predict } y\\in \\textbf{ as } \\begin{cases} y = \\mathbf{x}^T\\mathbf{w}\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad (\\text{prediction function}) \\\\\n\\\\ p(y\\mid \\mathbf{x}, \\mathbf{w}, \\sigma^2) = \\mathcal{N}\\big(y\\mid \\mathbf{x}^T\\mathbf{w}, \\sigma^2) \\quad (\\text{probabilistic view})  \\end{cases}\n\\]\nA reasonable model for binary outputs \\((y\\in\\{0,1\\})\\) is logistic regression:\n\\[\n\\textbf{Predict } y\\in \\textbf{ as } \\begin{cases} y = \\mathbb{I}(\\mathbf{x}^T\\mathbf{w} &gt; 0)\\ \\quad\\quad\\quad\\quad\\quad (\\text{prediction function}) \\\\\n\\\\ p(y=1 \\mid \\mathbf{x}, \\mathbf{w}) = \\sigma(\\mathbf{x}^T\\mathbf{w}) \\quad (\\text{probabilistic view})  \\end{cases}\n\\]\nA reasonable model for categorical outputs \\((y\\in\\{0,1,…,C\\})\\) is multinomial logistic regression:\n\\[\n\\textbf{Predict } y\\in \\textbf{ as } \\begin{cases} y = \\underset{c}{\\text{argmax}} \\ \\mathbf{x}^T\\mathbf{w}_c \\ \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\ \\  (\\text{prediction function}) \\\\\n\\\\ p(y=c \\mid \\mathbf{x}, \\mathbf{w}) = \\text{softmax}(\\mathbf{x}^T\\mathbf{W})_c \\quad (\\text{probabilistic view})  \\end{cases}\n\\]\nIn each of these cases, the core of our prediction is a linear function \\((\\mathbf{x}^T\\mathbf{w})\\) parameterized by a set of weights \\(\\mathbf{w}\\), with possibly some nonlinear function (e.g. \\(\\sigma(\\cdot)\\)), applied to the result. This type of function is commonly depicted using a diagram like the one shown below.\n\n\n\n\n\nEach node corresponds to a scalar value: the nodes on the left correspond to each input dimension and the node on the right corresponds to the prediction. Each edge represents multiplying the value on the left with a corresponding weight."
  },
  {
    "objectID": "lecture5-neural-networks-intro/demo.html",
    "href": "lecture5-neural-networks-intro/demo.html",
    "title": "CS 152: Neural Networks",
    "section": "",
    "text": "from demo import *\n\nimages, labels = get_dataset('ones_and_zeros')\n\n\n\n\n\nX = images.reshape((images.shape[0], -1))\ny = labels\n\n\ndef linear_function(X, w):\n    w, b = w[:-1], w[-1]\n    return np.dot(X, w) + b\n\ndef predict(X, w):\n    return (linear_function(X, w) &gt; 0).astype(float)\n\n\nw = np.zeros((X.shape[1] + 1,))\npredict(X, w)\n\n(12665,)\n\n\n\ndef accuracy(X, y, w):\n    return np.mean(predict(X, w) == y)\n\n\naccuracy(X, y, w)\n\n0.46766679826292934\n\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef predict_probability(X, w):\n    return sigmoid(linear_function(X, w))\n\n\npredict_probability(X, w)\n\narray([0.5, 0.5, 0.5, ..., 0.5, 0.5, 0.5])\n\n\n\ndef nll(w, X, y):\n    xw = linear_function(X, w)\n    py = sigmoid((2 * y - 1) * xw)\n    return -np.sum(np.log(py))\n\n\nnll( w, X, y)\n\n8778.709041791706\n\n\n\nX.shape\n\n(12665, 784)\n\n\n\nxw = linear_function(X, w)\npy = sigmoid((2 * y - 1) * xw)\ngrad = ((1 - py) * (2 * y - 1)).reshape((-1, 1)) * X\ngrad.shape\n\n(12665, 784)\n\n\n\ndef gradient(w, X, y):\n    xw = linear_function(X, w)\n    py = sigmoid((2 * y - 1) * xw)\n    grad = ((1 - py) * (2 * y - 1)).reshape((-1, 1)) * np.pad(X, [(0,0), (0,1)], constant_values=1.)\n    return -np.sum(grad, axis=0)\n\n\nimport time\nfrom IPython import display\n\ndef nll_and_grad(w, X, y):\n    return nll(w, X, y), gradient(w, X, y)\n\ndef gradient_descent(value_and_grad, w0, lr, steps, X, y):\n    f, ax = plt.subplots(1, 3, figsize=(12,4))\n\n    losses = []\n    weights = w0\n    for i in range(steps):\n        loss, g = value_and_grad(weights, X, y)\n        weights = weights - lr * g\n        losses.append(loss)\n\n\n        # Plotting code\n        \n        [a.cla() for a in ax]\n        display.clear_output(wait =True)\n        \n        ax[1].axis('off')\n        ax[2].axis('off')\n        ax[0].plot(losses)\n        ax[1].imshow(weights[:-1].reshape(images[0].shape))\n        ax[2].imshow(g[:-1].reshape(images[0].shape))\n        ax[1].set_title('Loss: %.3f, accuracy: %.3f' % (loss, accuracy(X, y, weights)))\n        display.display(f)\n        time.sleep(0.001)\n        \n    return weights, losses\n\nw = np.zeros(X.shape[1] + 1)\nweights, losses = gradient_descent(nll_and_grad, np.zeros_like(w), 0.0000003, 500, X, y)"
  },
  {
    "objectID": "lecture5-neural-networks-intro/notes.html#multi-layer-neural-networks",
    "href": "lecture5-neural-networks-intro/notes.html#multi-layer-neural-networks",
    "title": "Lecture 5: Neural networks",
    "section": "Multi-layer neural networks",
    "text": "Multi-layer neural networks\nWhat we’ve seen so far is a single hidden-layer neural network. But there’s no reason we’re restricted to a single layer!"
  },
  {
    "objectID": "lecture3-logistic-regression/notes.html#optimizing-logistic-regression",
    "href": "lecture3-logistic-regression/notes.html#optimizing-logistic-regression",
    "title": "Lecture 3: Logistic regression",
    "section": "Optimizing logistic regression",
    "text": "Optimizing logistic regression\nAs we saw with linear regression, we can find the optimal paramters \\(\\mathbf{w}^*\\) under this loss function using gradient descent:\n\\[\n\\mathbf{w}^{(i+1)} \\leftarrow \\mathbf{w}^{(i)} - \\alpha \\nabla_{\\mathbf{w}} \\mathbf{NLL}(\\mathbf{w}^{(i)}, \\mathbf{X}, \\mathbf{y})\n\\]\nTo use this, we first need to derive the gradient of the negative log-likelihood with respect to \\(\\mathbf{w}\\). We’ll start by writing out the simplest version of the NLL that we saw above:\n\\[\n\\mathbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = -\\sum_{i=1}^N \\log\\sigma\\big((2y_i-1)\\mathbf{x}_i^T\\mathbf{w}\\big)\n\\]\n\\[\n\\nabla_{\\mathbf{w}}\\mathbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = \\frac{d}{d\\mathbf{w}}-\\sum_{i=1}^N \\log\\sigma\\big((2y_i-1)\\mathbf{x}_i^T\\mathbf{w}\\big)\n\\]\nAs a first step, recall that the addition rule tells us that the derivative of a sum is a sum of derivatives:\n\\[\n= -\\sum_{i=1}^N \\frac{d}{d\\mathbf{w}} \\log\\sigma\\big((2y_i-1)\\mathbf{x}_i^T\\mathbf{w}\\big)\n\\]\nNext we’ll apply the chain rule to the \\(\\log\\) function, remembering that \\(\\frac{d}{dx} \\log x = \\frac{1}{x}\\):\n\\[\n= -\\sum_{i=1}^N \\bigg(\\frac{1}{ \\sigma\\big((2y_i-1)\\mathbf{x}_i^T\\mathbf{w}\\big) }\\bigg)\\frac{d}{d\\mathbf{w}} \\sigma\\big((2y_i-1)\\mathbf{x}_i^T\\mathbf{w}\\big)\n\\]\nThen we can apply the chain rule to the sigmoid function, using the fact that \\(\\frac{d}{dx} \\sigma(x)=\\sigma(x)(1-\\sigma(x))\\):\n\\[\n= -\\sum_{i=1}^N \\bigg(\\frac{1}{ \\sigma\\big((2y_i-1)\\mathbf{x}_i^T\\mathbf{w}\\big) } \\bigg)\n\\bigg(\\sigma\\big((2y_i-1)\\mathbf{x}_i^T\\mathbf{w}\\big)\\bigg) \\bigg(1-\\sigma\\big((2y_i-1)\\mathbf{x}_i^T\\mathbf{w}\\big)  \\bigg)\n\\frac{d}{d\\mathbf{w}}\\big((2y_i-1)\\mathbf{x}_i^T\\mathbf{w}\\big)\n\\]\nWe now see that the first 2 terms cancel!\n\\[\n= -\\sum_{i=1}^N  \\bigg(1-\\sigma\\big((2y_i-1)\\mathbf{x}_i^T\\mathbf{w}\\big)  \\bigg)\n\\frac{d}{d\\mathbf{w}}\\big((2y_i-1)\\mathbf{x}_i^T\\mathbf{w}\\big)\n\\]\nFinally we’re left with the gradient of a linear function, which is just:\n\\[\\frac{d}{d\\mathbf{w}}\\big((2y_i-1)\\mathbf{x}_i^T\\mathbf{w}\\big)=(2y_i-1)\\mathbf{x}_i\\]\nNote that the transpose is irrelevant as we’re no longer signifying a dot-product and \\(\\mathbf{x}_i\\) is just a vector. So finally we’re left with\n\\[\n\\nabla_{\\mathbf{w}}\\mathbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = -\\sum_{i=1}^N  \\bigg(1-\\sigma\\big((2y_i-1)\\mathbf{x}_i^T\\mathbf{w}\\big)  \\bigg)\n\\bigg((2y_i-1)\\mathbf{x}_i \\bigg)\n\\]"
  },
  {
    "objectID": "lecture6-backpropagation/notes.html",
    "href": "lecture6-backpropagation/notes.html",
    "title": "Lecture 6: Deep neural networks and backpropagation",
    "section": "",
    "text": "Plot = import(\"https://esm.sh/@observablehq/plot\") \nd3 = require(\"d3@7\")\ntopojson = require(\"topojson\")\nMathJax = require(\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-svg.min.js\").catch(() =&gt; window.MathJax)\ntf = require(\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js\").catch(() =&gt; window.tf)\n\nTHREE = {\n  const THREE = window.THREE = await require(\"three@0.130.0/build/three.min.js\");\n  await require(\"three@0.130.0/examples/js/controls/OrbitControls.js\").catch(() =&gt; {});\n  await require(\"three@0.130.0/examples/js/loaders/SVGLoader.js\").catch(() =&gt; {});\n  return THREE;\n}\n\nfunction sample(f, start, end, nsamples=100) {\n  let arr = [...Array(nsamples).keys()]\n  let dist = end - start\n  function arrmap(ind) {\n    const x = (ind * dist) / nsamples + start;\n    return [x, f(x)];\n  }\n  return arr.map(arrmap)\n}\n\nfunction sigmoid(x){\n  return 1 / (1 + Math.exp(-x));\n}\n\nfunction sum(x) {\n  let s = 0;\n  for (let i = 0; i &lt; x.length; i++ ) {\n    s += x[i];\n  }\n  return s;\n}\n\nfunction mean(x) {\n  let s = 0;\n  for (let i = 0; i &lt; x.length; i++ ) {\n    s += x[i];\n  }\n  return s / x.length;\n}\n\nfunction cross_ent(x, y) {\n  return y ? -Math.log(sigmoid(x)) : -Math.log(sigmoid(-x));\n}\n\nfunction se(x, y) {\n  return (x - y) * (x - y);\n}\n\nfunction shuffle(array) {\n  let currentIndex = array.length,  randomIndex;\n\n  // While there remain elements to shuffle.\n  while (currentIndex &gt; 0) {\n\n    // Pick a remaining element.\n    randomIndex = Math.floor(Math.random() * currentIndex);\n    currentIndex--;\n\n    // And swap it with the current element.\n    [array[currentIndex], array[randomIndex]] = [\n      array[randomIndex], array[currentIndex]];\n  }\n\n  return array;\n}\n\nfunction acc(x, y) {\n  return Number(y == (x  &gt; 0));\n}\n\nfunction grid_func(f, width, height, x1, y1, x2, y2) {\n  let values = new Array(width * height);\n  const xstride = (x2 - x1) / width;\n  const ystride = (y2 - y1) / height;\n\n  \n  let y = 0;\n  let x = 0;\n  let ind = 0;\n  for (let i = 0; i &lt; height; i++ ) {\n    for (let j = 0; j &lt; width; j++, ind++) {\n      x = x1 + j * xstride;\n      y = y1 + i * ystride;\n      values[ind] = f(x, y);\n    }\n  }\n  return {width: width, height: height, x1: x1, y1: y1, x2: x2, y2: y2, values: values};\n}\n\nfunction get_accessors(keys, byindex=false) {\n  let isString = value =&gt; typeof value === 'string';\n  \n  let index = 0;\n  let indexmap = {};\n  let accessors = [];\n  for (let i = 0; i &lt; keys.length; i++){\n    let k = keys[i];\n    if (Array.isArray(k)) {\n      let access = isString(k[0]) ? (x =&gt; x[k[0]]) : k[0];\n      \n      if (byindex) {\n        if (isString(k[0]) && !(k[0] in indexmap)) {\n          indexmap[k[0]] = index;\n          index++;\n        }\n        let accessindex = indexmap[k[0]];\n        access = x =&gt; x[accessindex];\n        let process = k[1];\n        let final_access = x =&gt; process(access(x));\n        accessors.push(final_access);\n      }\n      else {\n        let process = k[1];\n        let final_access = x =&gt; process(access(x));\n        accessors.push(final_access);\n      }\n      \n    }\n    else {\n      let access = isString(k) ? (x =&gt; x[k]) : k;\n      if (byindex) { \n        if (isString(k) && !(k in indexmap)) {\n          indexmap[k] = index;\n          index++;\n        }\n        let accessindex = indexmap[k];\n        access = x =&gt; x[accessindex];\n      }\n      accessors.push(access); \n    }\n  }\n  return accessors;\n}\n\nfunction predict(obs, weights, keys=[\"0\", \"1\", \"2\", \"3\"], byindex=false) {\n  let isString = value =&gt; typeof value === 'string';\n  let accessors = get_accessors(keys, byindex);\n  \n  let output = weights[0];\n  let wi = 1;\n  for (let i = 0; (i &lt; keys.length) && (wi &lt; weights.length); i++, wi++){\n    output += weights[wi] * accessors[i](obs);\n  }\n  return output;\n}\n\nfunction mean_loss(f, data, weights, keys, label, l2=0) {\n  let reg = 0;\n  if (l2 &gt; 0){\n    for (let i = 1; i &lt; weights.length; i++) {\n      reg += weights[i] * weights[i];\n    }\n  }\n  \n  const isString = value =&gt; typeof value === 'string';\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n  return mean(data.map(x =&gt; f(predict(x, weights, keys), get_label(x)))) + l2 * reg;\n}\n\nfunction get_domains(data, accessors, margin=0.1) {\n  let domains = [];\n  for (let i = 0; i &lt; accessors.length; i++){\n    let xdomain = d3.extent(data, accessors[i]);\n    let xdsize = (xdomain[1] - xdomain[0]);\n    let xmin = xdomain[0] - xdsize * margin;\n    let xmax = xdomain[1] + xdsize * margin;\n    domains.push([xmin, xmax]);\n  }\n  return domains;\n}\n\nfunction logisticPlot2d(data, weights, keys, label, interval=0.05) {\n  const accuracy = mean_loss(acc, data, weights, keys, label);\n  \n  let isString = value =&gt; typeof value === 'string';\n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, accessors);\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n  \n  return Plot.plot({\n    x: {tickSpacing: 80, label: \"x\"},\n    y: {tickSpacing: 80, label: \"y\"},\n    title: \"Accuracy: \" + accuracy.toFixed(3),\n    color: {type: \"linear\", legend: true, scheme: \"BuRd\", domain: [-0.5, 1.5]},\n    marks: [\n      Plot.contour({\n        fill: (x, y) =&gt; sigmoid(predict([x, y], weights, index_accessors)),\n        x1: domains[0][0], y1: domains[1][0], x2: domains[0][1], y2: domains[1][1], interval: interval,\n      }),\n      Plot.dot(data, {x: accessors[0], y: accessors[1], stroke: x=&gt; (get_label(x) ? 1.35 : -0.35)})\n    ]\n  });\n}\n\nfunction logisticLossPlot2d(data, weights, keys, label) {\n  const loss = mean_loss(cross_ent, data, weights, keys, label);\n  \n  let isString = value =&gt; typeof value === 'string';\n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, accessors);\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n  \n  return Plot.plot({\n    x: {tickSpacing: 80, label: \"x\"},\n    y: {tickSpacing: 80, label: \"y\"},\n    title: \"Loss: \" + loss.toFixed(3),\n    color: {type: \"linear\", legend: true, scheme: \"BuRd\", domain: [0, 5]},\n    marks: [\n      Plot.contour({\n        value: (x, y) =&gt; predict([x, y], weights, index_accessors),\n        fillOpacity: 0.2,\n        stroke: \"black\", x1: domains[0][0], y1: domains[1][0], x2: domains[0][1], y2: domains[1][1],\n        thresholds: [-1e6,  0, 0.00001]\n      }),\n      Plot.dot(data, {x: accessors[0], y: accessors[1], stroke: x=&gt; cross_ent(predict(x, weights, keys), get_label(x)), \n                      strokeOpacity: 0.5 })\n    ]\n  });\n}\n\nfunction lossPlot2d(f, data, keys, label, l2=0, res=100, x1=-40, y1=-0.015, x2=40,  y2=0.015, vmax=50, nlines=25, ctype=\"sqrt\", scale=(x =&gt; x)) {\n  let grid = 0;\n  function lossFunc(w, b) {\n    return scale(mean_loss(f, data, [w, b], keys, label, l2));\n  }\n\n  grid = grid_func(lossFunc,\n                 res, res, x1, y1, x2, y2\n                );\n\n  function plot2d(weights) {\n    let w = weights;\n    if (!(Array.isArray(w[0]))){\n      w = [w];\n    }\n\n    var arrows = w.slice(0, w.length - 1).map(function(e, i) {\n      return e.concat(w[i+1]);\n    });\n\n    let interval= vmax / nlines; \n    let thresholds = [];\n    for (let i = 0; i &lt; nlines; i++) {\n      thresholds.push(i * interval);\n    }\n    let loss = mean_loss(f, data, w[w.length - 1], keys, label, l2)\n    return Plot.plot({\n      title: \"Loss: \" + loss.toFixed(3),\n      color: {type: \"linear\", legend: true, label: \"Loss\", scheme: \"BuRd\", domain: [0, vmax], type: ctype},\n      marks: [\n        Plot.contour(grid.values, {width: grid.width, height: grid.height, x1: grid.x1, x2:grid.x2, y1: grid.y1, y2: grid.y2,\n          stroke: Plot.identity, thresholds: thresholds}),\n        Plot.dot(w),\n        Plot.arrow(arrows, {x1: \"0\", y1: \"1\", x2: \"2\", y2: \"3\", stroke: \"black\"})\n      ]\n    })\n  }\n  return plot2d;\n}\n\n\n\nfunction regressionPlot(data, weights, keys, label, l2, f=se, stroke=\"\") {\n  let loss = mean_loss(f, data, weights, keys, label, l2);\n  let isString = value =&gt; typeof value === 'string';\n  \n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, get_accessors([label].concat(keys)));\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n\n  let stroke_shade = stroke;\n  if (stroke == \"\") {\n    stroke_shade = (x =&gt; f(predict(x, weights, keys), get_label(x)))\n  }\n  \n  return Plot.plot({\n    y: {domain: domains[0]},\n    title: \"Loss: \" + loss.toFixed(3),\n    color: {type: \"linear\", legend: true, label: \"Loss\", scheme: \"BuRd\", domain: [0, 100]},\n    marks: [\n      Plot.line(sample((x) =&gt; predict([x], weights, index_accessors), domains[1][0], domains[1][1]), {stroke: 'black'}),\n      Plot.dot(data, {x: accessors[0], y: get_label, stroke: stroke_shade })\n    ]\n  })\n}\n\nfunction errorPlot(data, weights, keys, label, f, options={}) {\n  const isString = value =&gt; typeof value === 'string';\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n  let errors = data.map(x =&gt; [predict(x, weights, keys) - get_label(x), f(predict(x, weights, keys), get_label(x))]);\n\n  \n  let sigma = (options['sigma'] || 1);\n  let plots = [];\n  const xdomain = (options['xdomain'] || [-30, 30]);\n  const ydomain = (options['ydomain'] || [0, 0.1]);\n  \n\n  if (options['plotnormal']){\n    let pdf = x =&gt; Math.exp(-0.5 * x * x / sigma) * ydomain[1];\n    let normal = Plot.line(sample(pdf, xdomain[0], xdomain[1]), {stroke: 'crimson'});    \n    plots.push(normal);\n  }\n  if (options['plotlaplace']){\n    let pdf = x =&gt; Math.exp(-0.5 * Math.abs(x) / sigma) * ydomain[1];\n    let normal = Plot.line(sample(pdf, xdomain[0], xdomain[1]), {stroke: 'green'});    \n    plots.push(normal);\n  }\n  \n  return Plot.plot({\n    y: {grid: true, domain: ydomain},\n    x: {domain: xdomain},\n    color: {type: \"linear\", legend: true, label: \"Loss\", scheme: \"BuRd\", domain: [0, 100]},\n    marks: [\n      //Plot.rectY(errors, Plot.binX({y: \"count\", fill: x =&gt; mean(x.map(v =&gt; v[1]))}, {x: \"0\"})),\n      \n      Plot.rectY(errors, Plot.binX({y: \"proportion\"}, {x: \"0\", fill: 'steelblue', interval: 1})),\n      Plot.ruleY([0])\n    ].concat(plots)\n  })\n}\n\nfunction nnPlot(data, weights, keys, label, l2, f=se, stroke=\"\", options=[]) {\n  let loss = mean_loss(f, data, weights, keys, label, l2);\n  let isString = value =&gt; typeof value === 'string';\n  \n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, get_accessors([label].concat(keys)));\n  const get_label = isString(label) ? (x =&gt; x[label]) : label;\n\n  let stroke_shade = stroke;\n  if (stroke == \"\") {\n    stroke_shade = (x =&gt; f(predict(x, weights, keys), get_label(x)))\n  }\n\n  let a = []\n  if (options.indexOf(\"Show feature transforms\") &gt;= 0){\n    a = [Plot.line(sample((x) =&gt;  keys[1][1](x), domains[1][0], domains[1][1]), {stroke: 'red'}),\n      Plot.line(sample((x) =&gt; keys[2][1](x), domains[1][0], domains[1][1]), {stroke: 'blue'})]\n  }\n  \n  return Plot.plot({\n    y: {domain: domains[0]},\n    title: \"Loss: \" + loss.toFixed(3),\n    color: {type: \"linear\", legend: true, label: \"Loss\", scheme: \"BuRd\", domain: [0, 100]},\n    marks: [\n      Plot.line(sample((x) =&gt; predict([x], weights, index_accessors), domains[1][0], domains[1][1]), {stroke: 'black'}),\n      Plot.dot(data, {x: accessors[0], y: get_label, stroke: stroke_shade })\n    ].concat(a)\n  })\n}\npyodide = {\n  const pyodide =\n    await require(\"https://cdn.jsdelivr.net/pyodide/v0.21.0/full/pyodide.js\");\n  return pyodide.loadPyodide();\n}\ntfbase = require('@tensorflow/tfjs@4.11.0')\n\npy = {\n  let py = async (strings, ...expressions) =&gt; {\n    let globals = {};\n    const code = strings.reduce((result, string, index) =&gt; {\n      if (expressions[index]) {\n        const name = `x${index}`;\n        globals[name] = expressions[index];\n        return result + string + name;\n      }\n      return result + string;\n    }, '');\n    await pyodide.loadPackagesFromImports(code);\n    pyodide.globals.update(pyodide.toPy(globals))\n    const result = await pyodide.pyodide_py.eval_code_async(\n      code,\n      pyodide.globals\n    );\n    if (result?.t2Js) return result.t2Js();\n    if (result?.toJs) return result.toJs();\n    return result;\n  };\n  \n  pyodide.globals.update(pyodide.toPy({Plotbase: Plot, tfbase: tfbase}))\n  \npy`\nfrom pyodide.ffi import create_once_callable\nfrom types import SimpleNamespace\nfrom pyodide import to_js\nfrom js import Object\nimport pandas\n\ntfbase = SimpleNamespace(**tfbase)\n\ndef convert(a):\n  if isinstance(a, Tensor):\n    a = a.value\n  return to_js(a, dict_converter=Object.fromEntries)\n\nclass Tensor:\n  def __init__(self, *args, value=None, **kwargs):\n    if not (value is None):\n      self.value = value\n    else:\n      args = [convert(a) for a in args]\n      kwargs = {k: convert(a) for (k, a) in kwargs.items()}\n      self.value = tfbase.tensor(*args, **kwargs)\n\n  def __getattr__(self, name):\n    attr = getattr(self.value, name)\n    if callable(attr):\n      def run(*args, **kwargs):\n        args = [convert(a) for a in args]\n        kwargs = {k: convert(a) for (k, a) in kwargs.items()}\n        output = attr(*args, **kwargs)\n        return Tensor(value=output)\n      return run\n    return attr\n\n  def __add__(a, b):\n    return Tensor(value=tfbase.add(convert(a), convert(b)))\n  def __radd__(a, b):\n    return Tensor(value=tfbase.add(convert(b), convert(a)))\n  def __sub__(a, b):\n    return Tensor(value=tfbase.sub(convert(a), convert(b)))\n  def __rsub__(a, b):\n    return Tensor(value=tfbase.sub(convert(b), convert(a)))\n  def __mul__(a, b):\n    return Tensor(value=tfbase.mul(convert(a), convert(b)))\n  def __rmul__(a, b):\n    return Tensor(value=tfbase.mul(convert(b), convert(a)))\n  def __truediv__(a, b):\n    return Tensor(value=tfbase.div(convert(a), convert(b)))\n  def __rtruediv__(a, b):\n    return Tensor(value=tfbase.div(convert(b), convert(a)))\n  def __floordiv__(a, b):\n    return Tensor(value=tfbase.floorDiv(convert(a), convert(b)))\n  def __rfloordiv__(a, b):\n    return Tensor(value=tfbase.floorDiv(convert(b), convert(a)))\n  def __pow__(a, b):\n    return Tensor(value=tfbase.pow(convert(a), convert(b)))\n  def __rpow__(a, b):\n    return Tensor(value=tfbase.pow(convert(b), convert(a)))\n  def __neg__(a):\n    return Tensor(value=tfbase.neg(convert(a)))\n  def __eq__(a, b):\n    return Tensor(value=tfbase.equal(convert(a), convert(b)))\n  def __neq__(a, b):\n    return Tensor(value=tfbase.notEqual(convert(a), convert(b)))\n  def __lt__(a, b):\n    return Tensor(value=tfbase.less(convert(a), convert(b)))\n  def __gt__(a, b):\n    return Tensor(value=tfbase.greater(convert(a), convert(b)))\n  def __leq__(a, b):\n    return Tensor(value=tfbase.lessEqual(convert(a), convert(b)))\n  def __geq__(a, b):\n    return Tensor(value=tfbase.greaterEqual(convert(a), convert(b)))\n\n  def t2Js(self):\n    return to_js(self.value.arraySync())\n\nclass wrapper:\n  def __init__(self, f):\n    self.f = f\n\n  def __call__(self, x, *args, **kwargs):\n    return convert(self.f(Tensor(value=x), *args, **kwargs))\n\nclass grad:\n  def __init__(self, f):\n    self.f = f\n    self.wrapper = wrapper(f)\n\n  def __call__(self, x, *args, **kwargs):\n    output = tfbase.grad(create_once_callable(self.wrapper))(x.value, *args, **kwargs)\n    return Tensor(value=output)\n\ntf = Tensor(value=tfbase)\nPlotbase = SimpleNamespace(**Plotbase)\n\ndef plotconvert(a):\n  a = a.to_dict('records') if isinstance(a, pandas.DataFrame) else a\n  a = a.t2Js() if isinstance(a, Tensor) else a\n  return to_js(a, dict_converter=Object.fromEntries)\n\nclass PlotWrapper:\n  def __init__(self):\n    pass\n    \n  def __getattr__(self, name):\n    attr = getattr(Plotbase, name)\n    if callable(attr):\n      def run(*args, **kwargs):\n        args = [plotconvert(a) for a in args]\n        kwargs = {k: plotconvert(a) for (k, a) in kwargs.items()}\n        return attr(*args, **kwargs)\n      return run\n    return attr\n\nPlot = PlotWrapper()\n`  \n  return py;\n}"
  },
  {
    "objectID": "lecture6-backpropagation/notes.html#neural-networks-1",
    "href": "lecture6-backpropagation/notes.html#neural-networks-1",
    "title": "Lecture 5: Neural networks",
    "section": "Neural networks",
    "text": "Neural networks\nWhat we’ve just seen is a neural network!\nTerminology-wise we call a single feature transform like \\(\\sigma(x_1 w_{11} + x_2 w_{12})\\) a neuron.\nWe call the whole set of transformed features the hidden layer: \\[\\begin{bmatrix}  \\sigma(\\mathbf{x}^T \\mathbf{w}_1) \\\\ \\sigma(\\mathbf{x}^T \\mathbf{w}_2) \\\\ \\sigma(\\mathbf{x}^T \\mathbf{w}_3) \\\\ \\sigma(\\mathbf{x}^T \\mathbf{w}_4) \\end{bmatrix} \\]\nWe call \\(\\mathbf{x}\\) the input and \\(f(\\mathbf{x})\\) the output."
  },
  {
    "objectID": "lecture6-backpropagation/notes.html#optimizing-neural-networks",
    "href": "lecture6-backpropagation/notes.html#optimizing-neural-networks",
    "title": "Lecture 6: Deep neural networks and backpropagation",
    "section": "Optimizing neural networks",
    "text": "Optimizing neural networks\nWe can still define a loss function for a neural network in the same way we did with our simpler linear models. The only difference is that now we have more parameters to choose:\n\\[\n\\mathbf{Loss}(\\mathbf{w}_0,\\mathbf{W}_1,...)\n\\]\nLet’s look at the logistic regression negative log-likelihood loss for the simple neural network we saw above (for simplicity we’ll just call the network weights \\(\\mathbf{W}\\)). The probability of class 1 is estimated as:\n\\[\np(y=1\\mid \\mathbf{x}, \\mathbf{w}_0,\\mathbf{W})=\\sigma(\\phi(\\mathbf{x})^T \\mathbf{w}_0) = \\sigma(\\sigma(\\mathbf{x}^T \\mathbf{W}^T) \\mathbf{w}_0),\\quad \\phi(\\mathbf{x}) = \\begin{bmatrix}  \\sigma(\\mathbf{x}^T \\mathbf{W}_{1}) \\\\ \\sigma(\\mathbf{x}^T \\mathbf{W}_{2}) \\\\ \\sigma(\\mathbf{x}^T \\mathbf{W}_{3}) \\end{bmatrix}\n\\] \\[ = \\sigma\\big(w_{01} \\cdot\\sigma(x_1 W_{11} + x_2 W_{12}) + w_{02} \\cdot\\sigma(x_1 W_{21} + x_2 W_{22})+ w_{03} \\cdot\\sigma(x_1 W_{31} + x_2 W_{32}) \\big)\\]\nTherefore the negative log-likelihood is:\n\\[\n\\mathbf{NLL}(\\mathbf{w}_0,\\mathbf{W}, \\mathbf{X}, \\mathbf{y}) = -\\sum_{i=1}^N \\bigg[ y_i\\log p(y=1\\mid \\mathbf{x}, \\mathbf{w}_0,\\mathbf{W}) + (1-y_i)\\log p(y=0\\mid \\mathbf{x}, \\mathbf{w}_0,\\mathbf{W}) \\bigg]\n\\]\n\\[\n= -\\sum_{i=1}^N \\log \\sigma\\big((2y_i-1) \\phi(\\mathbf{x}_i)^T \\mathbf{w}\\big)\n\\]\nWe see that we can write out a full expression for this loss in term of all the inputs and weights. We can even define the gradient of this loss with respect to all the weights:\n\\[\n\\nabla_{\\mathbf{w}_0} \\mathbf{NLL}(\\mathbf{w}_0,\\mathbf{W}, \\mathbf{X}, \\mathbf{y}) = \\begin{bmatrix} \\frac{\\partial \\mathbf{NLL}}{\\partial w_{01}} \\\\ \\frac{\\partial \\mathbf{NLL}}{\\partial w_{02}} \\\\ \\frac{\\partial \\mathbf{NLL}}{\\partial w_{03}} \\\\ \\vdots\\end{bmatrix}, \\quad \\nabla_{\\mathbf{W}}\\mathbf{NLL}(\\mathbf{w}_0,\\mathbf{W}, \\mathbf{X}, \\mathbf{y}) =\n\\begin{bmatrix} \\frac{\\partial \\mathbf{NLL}}{\\partial W_{11}} &  \\frac{\\partial \\mathbf{NLL}}{\\partial W_{12}} & \\dots & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{1d}}  \\\\\n\\frac{\\partial \\mathbf{NLL}}{\\partial W_{21}} &  \\frac{\\partial \\mathbf{NLL}}{\\partial W_{22}} & \\dots & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{2d}} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial \\mathbf{NLL}}{\\partial W_{h1}} &  \\frac{\\partial \\mathbf{NLL}}{\\partial W_{h2}} & \\dots & \\frac{\\partial \\mathbf{NLL}}{\\partial W_{hd}}\n\\end{bmatrix}\n\\]\nNote that as \\(\\mathbf{W}\\) is a matrix, the gradient with respect to \\(\\mathbf{W}\\) is also a matrix! Our gradient descent algorithm can proceed in the same way it did for our linear models, but here we now need to update both sets of parameters:\n\\[\n\\mathbf{w}_0^{(k+1)} \\longleftarrow \\mathbf{w}_0^{(k)} -\\alpha \\nabla_{\\mathbf{w}_0} \\mathbf{NLL}(\\mathbf{w}_0,\\mathbf{W}, \\mathbf{X}, \\mathbf{y}), \\quad \\mathbf{W}^{(k+1)} \\longleftarrow \\mathbf{W}^{(k)} -\\alpha \\nabla_{\\mathbf{W}} \\mathbf{NLL}(\\mathbf{w}_0,\\mathbf{W}, \\mathbf{X}, \\mathbf{y})\n\\]\nThe important question now becomes: how do we compute these gradients?"
  },
  {
    "objectID": "lecture6-backpropagation/notes.html#neural-networks-with-matrix-notation",
    "href": "lecture6-backpropagation/notes.html#neural-networks-with-matrix-notation",
    "title": "Lecture 5: Neural networks",
    "section": "Neural networks with matrix notation",
    "text": "Neural networks with matrix notation\nIt is often more convenient to write all of the weights that are used to create our hidden layer as a single large matrix:\n\\[\\mathbf{W}_1 = \\begin{bmatrix} \\mathbf{w}_1^T \\\\ \\mathbf{w}_2^T \\\\ \\mathbf{w}_3^T \\\\ \\vdots \\end{bmatrix}\\] With this, we can write our general neural network more compactly as: \\[f(\\mathbf{x})= \\sigma( \\mathbf{W}_1 \\mathbf{x})^T \\mathbf{w_0} \\] Or for a whole dataset: \\[\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1^T \\\\ \\mathbf{x}_2^T \\\\ \\mathbf{x}_3^T \\\\ \\vdots \\end{bmatrix}\\]\n\\[f(\\mathbf{x})= \\sigma( \\mathbf{X}\\mathbf{W}_1 )\\mathbf{w_0}\\]"
  },
  {
    "objectID": "lecture6-backpropagation/notes.html#linear-transforms",
    "href": "lecture6-backpropagation/notes.html#linear-transforms",
    "title": "Lecture 5: Neural networks",
    "section": "Linear transforms",
    "text": "Linear transforms\nThus far we’ve looked at a logistic regression feature transform as the basis of our neural network. Can we use linear regression as a feature transform?\nLet’s see what happens in our simple example: \\[\\mathbf{x} = \\begin{bmatrix} x_1\\\\ x_2 \\end{bmatrix}, \\quad \\mathbf{w}_0 = \\begin{bmatrix} w_{01} \\\\ w_{02} \\end{bmatrix}\\] \\[\nf(\\mathbf{x})=\\phi(\\mathbf{x})^T \\mathbf{w}_0,\\quad \\phi(\\mathbf{x}) = \\begin{bmatrix}  \\mathbf{x}^T \\mathbf{w}_1 \\\\ \\mathbf{x}^T \\mathbf{w}_2 \\\\ \\mathbf{x}^T \\mathbf{w}_3 \\\\ \\end{bmatrix} =\n\\begin{bmatrix}  x_1 w_{11} + x_2 w_{12} \\\\ x_1 w_{21} + x_2 w_{22} \\\\ x_1 w_{31} + x_2 w_{32} \\\\\\end{bmatrix}\n\\] In this case, we can write out our prediction function explicitly as: \\[f(\\mathbf{x}) = w_{01}\\cdot (x_1 w_{11} + x_2 w_{12}) + w_{02} \\cdot(x_1 w_{21} + x_2 w_{22})+ w_{03} \\cdot(x_1 w_{31} + x_2 w_{32}) \\] \\[= (w_{11}w_{01}) x_1 +(w_{12}w_{01}) x_2 +  (w_{21}w_{02}) x_1 +(w_{22}w_{02}) x_2  +(w_{31}w_{03}) x_1 +(w_{32}w_{03}) x_2 \\]\n\\[\n= (w_{11}w_{01} + w_{21}w_{02} + w_{31}w_{03}) x_1 + (w_{12}w_{01} + w_{22} w_{02} + w_{32} w_{03}) x_2\n\\]\nWe see that we ultimately just end up with another linear function of \\(\\mathbf{x}\\) and we’re no better off than in our orginal case. We can see this in practice here.\nIn general: \\[\nf(\\mathbf{x})=\\phi(\\mathbf{x})^T \\mathbf{w}_0,\\quad \\phi(\\mathbf{x}) = \\begin{bmatrix}  \\mathbf{x}^T \\mathbf{w}_1 \\\\ \\mathbf{x}^T \\mathbf{w}_2\\\\ \\mathbf{x}^T \\mathbf{w}_3 \\\\ \\mathbf{x}^T \\mathbf{w}_4\\end{bmatrix}\n\\]\n\\[\nf(\\mathbf{x})= w_{01} (\\mathbf{x}^T \\mathbf{w}_1) +  w_{02} (\\mathbf{x}^T \\mathbf{w}_2) +...\n\\] \\[= \\mathbf{x}^T ( w_{01}\\mathbf{w}_1) +  \\mathbf{x}^T (w_{02} \\mathbf{w}_2) +...\n\\] Which is again just a linear function. The motivates the need for using a non-linear function like \\(\\sigma(\\cdot)\\) in our neurons."
  },
  {
    "objectID": "lecture6-backpropagation/notes.html#activation-functions",
    "href": "lecture6-backpropagation/notes.html#activation-functions",
    "title": "Lecture 5: Neural networks",
    "section": "Activation functions",
    "text": "Activation functions\nWhile we need a non-linear function as part of our neural network feature transform, it does not need to be the sigmoid function. A few other common choices are:\n\\[\n\\textbf{Sigmoid:}\\quad \\sigma(x) = \\frac{1}{1+e^{-x}}\n\\]\n\\[\n\\textbf{Hyperbolic tangent:}\\quad \\tanh(x) = \\frac{e^{2x}-1}{e^{2x}+1}\n\\]\n\\[\n\\textbf{Rectifed linear:}\\quad \\text{ReLU}(x) = \\max(x,\\ 0)\n\\]\n\n\n\n\n\nWe call these non-linear functions activation functions when used in neural networks. We’ll see other examples over the course of this class\nTry out different activation functions here."
  },
  {
    "objectID": "lecture6-backpropagation/notes.html#multi-layer-neural-networks",
    "href": "lecture6-backpropagation/notes.html#multi-layer-neural-networks",
    "title": "Lecture 5: Neural networks",
    "section": "Multi-layer neural networks",
    "text": "Multi-layer neural networks\nWhat we’ve seen so far is a single hidden-layer neural network. But there’s no reason we’re restricted to a single layer!"
  },
  {
    "objectID": "lecture6-backpropagation/notes.html#benefits-of-neural-networks",
    "href": "lecture6-backpropagation/notes.html#benefits-of-neural-networks",
    "title": "Lecture 6: Deep neural networks and backpropagation",
    "section": "Benefits of neural networks",
    "text": "Benefits of neural networks\nWe’ve seen that the neural network transform is still fairly restrictive, with a limited number of neurons we can’t fit any arbitrary function. In fact, if we choose our feature transforms wisely we can do better than than a neural network.\nFor example, consider the simple 3-neuron network above. We can see that if we try to fit a circular dataset with it, it performs worse than an explicit transform with \\(x_1^2\\) and \\(x_2^2\\).\n\nCircle dataset with neural network\nCircle dataset with \\(x_1^2\\) and \\(x_2^2\\):\n\nSimilarly, for a cross dataset, we can do better with the feature transform that includes \\(x_1x_2\\) as a feature:\n\nCross dataset with neural network\nCross dataset with \\(x_1x_2\\)\n\nHowever, if we choose the wrong feature transform for a given dataset, we do far worse.\n\nCircle dataset with \\(x_1 x_2\\)\nCross dataset with \\(x_1^2\\) and \\(x_2^2\\)\n\nWe see that the real power of the neural network here is the ability to adapt the transform to the given dataset, without needing to carefully choose the correct transform!"
  },
  {
    "objectID": "lecture6-backpropagation/notes.html#deep-neural-networks",
    "href": "lecture6-backpropagation/notes.html#deep-neural-networks",
    "title": "Lecture 6: Deep neural networks and backpropagation",
    "section": "Deep Neural Networks",
    "text": "Deep Neural Networks\nWhat we’ve seen so far is a neural network with a single hidden layer, meaning that we create a feature transform for our data and then simply use that to make our prediction. We see that each individual feature transform is a bit limited, being just a logistic regression function.\n\\[\\phi(\\mathbf{x})_i = \\sigma(\\mathbf{x}^T \\mathbf{w}_i)\\]\nNo matter what we set \\(\\mathbf{w}_i\\) this transform would not be able to replicate a transform like \\(\\phi(\\mathbf{x})_i = x_i^2\\). However, we’ve already seen a way to make logistic regression more expressive: neural networks!\nThe idea behind a deep or multi-layer neural network is that we can apply this idea of neural network feature transforms recursively:\n\\[\\phi(\\mathbf{x})_i = \\sigma(\\sigma(\\mathbf{x}^T\\mathbf{W}^T) \\mathbf{w}_i)\\]\nHere we’ve transformed our input before computing our feature transform. In terms of a dataset we can write the full prediction function for this 2-layer network as:\n\\[\nf(\\mathbf{X}) = \\sigma(\\sigma(\\mathbf{X}\\mathbf{W}_1^T)\\mathbf{W}_2^T)\\mathbf{w}_0\n\\]\nWe’ve now defined a set of weight parameters for each of our 2 hidden layers \\(\\mathbf{W}_1\\) and \\(\\mathbf{W}_2\\). It’s a little easier to see what’s happening here if we look a our diagram for this case:\n\n\n\n\n\nWe can see that stacking these transforms allows us to fit even more complicated functions here. Note that we are still not limited to doing this twice! We can fit many layers of transforms:\n\nLater on in the semester we’ll talk in more depth about the effect of the number of layers and the number of neurons per layer!"
  },
  {
    "objectID": "lecture6-backpropagation/notes.html#neural-networks-with-matrices",
    "href": "lecture6-backpropagation/notes.html#neural-networks-with-matrices",
    "title": "Lecture 6: Deep neural networks and backpropagation",
    "section": "Neural networks with matrices",
    "text": "Neural networks with matrices\nLet’s return to our simple neural network example, where we have 2 inputs and 3 neurons (transforms): \\[\\mathbf{x} = \\begin{bmatrix} x_1\\\\ x_2 \\end{bmatrix}, \\quad \\mathbf{w}_0 = \\begin{bmatrix} w_{01} \\\\ w_{02} \\end{bmatrix}\\] \\[\nf(\\mathbf{x})=\\phi(\\mathbf{x})^T \\mathbf{w}_0,\\quad \\phi(\\mathbf{x}) = \\begin{bmatrix}  \\sigma(\\mathbf{x}^T \\mathbf{w}_1) \\\\ \\sigma(\\mathbf{x}^T \\mathbf{w}_2) \\\\ \\sigma(\\mathbf{x}^T \\mathbf{w}_3) \\end{bmatrix} =\n\\begin{bmatrix}  \\sigma(x_1 w_{11} + x_2 w_{12}) \\\\ \\sigma(x_1 w_{21} + x_2 w_{22}) \\\\ \\sigma(x_1 w_{31} + x_2 w_{32}) \\end{bmatrix}\n\\]\nAgain, we can represent this pictorially again as a node-link diagram:\n\n\n\n\n\nLet’s look at a more compact way to write this, using a weight matrix for the neural network layer. Let’s look at the transform before we apply the sigmoid function:\n\\[\n\\begin{bmatrix}  \\mathbf{x}^T \\mathbf{w}_1 \\\\ \\mathbf{x}^T \\mathbf{w}_2 \\\\ \\mathbf{x}^T \\mathbf{w}_3 \\end{bmatrix} =\n\\begin{bmatrix}  x_1 w_{11} + x_2 w_{12} \\\\ x_1 w_{21} + x_2 w_{22} \\\\ x_1 w_{31} + x_2 w_{32} \\end{bmatrix} = \\begin{bmatrix} w_{11} & w_{12} \\\\w_{21} & w_{22} \\\\ w_{31} & w_{32} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\n\\]\nIf we define a matrix \\(\\mathbf{W}\\) for all of the weights as:\n\\[\n\\mathbf{W} = \\begin{bmatrix} w_{11} & w_{12} \\\\w_{21} & w_{22} \\\\ w_{31} & w_{32} \\end{bmatrix}\n\\]\nwe get:\n\\[\n\\begin{bmatrix}  \\mathbf{x}^T \\mathbf{w}_1 \\\\ \\mathbf{x}^T \\mathbf{w}_2 \\\\ \\mathbf{x}^T \\mathbf{w}_3 \\end{bmatrix} = \\mathbf{W}\\mathbf{x} = (\\mathbf{x}^T\\mathbf{W}^T)^T\n\\]\nIf we let \\(h\\) be the number of neuron (or hidden layer units) then this is a \\(h \\times d\\) matrix. Therefore, we can write our transform as:\n\\[\n\\phi(\\mathbf{x}) = \\sigma(\\mathbf{x}^T\\mathbf{W}^T)^T, \\quad f(\\mathbf{x}) = \\sigma(\\mathbf{x}^T\\mathbf{W}^T) \\mathbf{w}_0\n\\]\nRecall that if we have multiple observations, as in a dataset, we define them together as an \\(N \\times d\\) matrix \\(\\mathbf{X}\\) such that each row is an observation:\n\\[\n\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1^T \\\\ \\mathbf{x}_2^T \\\\ \\mathbf{x}_3^T  \\\\ \\vdots  \\end{bmatrix}\n\\]\nTherefore, we can transform all of these observations at once by multiplying this matrix by \\(\\mathbf{W}^T\\).\n\\[\n\\phi(\\mathbf{X}) = \\sigma(\\mathbf{X}\\mathbf{W}^T)^T = \\begin{bmatrix} \\sigma(\\mathbf{x}_1^T\\mathbf{w}_1) & \\sigma(\\mathbf{x}_1^T\\mathbf{w}_2) & \\dots  & \\sigma(\\mathbf{x}_1^T\\mathbf{w}_h \\\\\n\\sigma(\\mathbf{x}_2^T\\mathbf{w}_1) & \\sigma(\\mathbf{x}_2^T\\mathbf{w}_2) & \\dots  & \\sigma(\\mathbf{x}_2^T\\mathbf{w}_h) \\\\\n\\vdots & \\vdots & \\ddots  & \\vdots \\\\\n\\sigma(\\mathbf{x}_N^T\\mathbf{w}_1) & \\sigma(\\mathbf{x}_N^T\\mathbf{w}_2) & \\dots  & \\sigma(\\mathbf{x}_N^T\\mathbf{w}_h)\n\\end{bmatrix}\n\\]\nWe see that this is an \\(N \\times h\\) matrix where each row is a transformed observation! We can then write our full prediction function as\n\\[\n\\quad f(\\mathbf{x}) = \\sigma(\\mathbf{X}\\mathbf{W}^T) \\mathbf{w}_0\n\\]\nTo summarize:\n\n\\(\\mathbf{X}: \\quad N \\times d\\) matrix of observations\n\\(\\mathbf{W}: \\quad h \\times d\\) matrix of network weights\n\\(\\mathbf{w}_0: \\quad h\\ (\\times 1)\\) vector of linear regression weights\n\nIf we check that our dimensions work for matrix multiplication we see that we get the \\(N\\times 1\\) vector of predictions we are looking for!\n\\[\n(N \\times d) (h \\times d)^T (h \\times 1) \\rightarrow (N \\times d) (d \\times h) (h \\times 1) \\rightarrow (N \\times h) (h \\times 1)\n\\]\n\\[\n\\longrightarrow (N \\times1)\n\\]"
  },
  {
    "objectID": "lecture6-backpropagation/notes.html#section",
    "href": "lecture6-backpropagation/notes.html#section",
    "title": "Lecture 5: Neural networks",
    "section": "",
    "text": "Later on in the semester we’ll talk in more depth about the effect of the number of layers and the number of neurons per layer!"
  },
  {
    "objectID": "lecture6-backpropagation/notes.html#motivation",
    "href": "lecture6-backpropagation/notes.html#motivation",
    "title": "Lecture 6: Deep neural networks and backpropagation",
    "section": "Motivation",
    "text": "Motivation\nWe saw above that the NLL for logistic regression with a neural network is:\n\\[\n\\mathbf{NLL}(\\mathbf{w}_0,\\mathbf{W}, \\mathbf{X}, \\mathbf{y}) = -\\sum_{i=1}^N \\log \\sigma\\big((2y_i-1) \\phi(\\mathbf{x}_i)^T \\mathbf{w}\\big)\n\\]\nIf we write this out in terms of the individual values we get:\n\\[\n= -\\sum_{i=1}^N \\log \\sigma\\big((2y_i-1)\\sigma\\big(w_{01} \\cdot\\sigma(x_1 W_{11} + x_2 W_{12}) + w_{02} \\cdot\\sigma(x_1 W_{21} + x_2 W_{22})+ w_{03} \\cdot\\sigma(x_1 W_{31} + x_2 W_{32}) \\big)\\big)\n\\]\nWe could use the same approach as usual to find the derivative of this loss with respect to each individual weight parameter, but it would be very tedious and this is only a single-layer network! Things would only get more complicated with more layers. Furthermore if we changed some aspect of the network, like the activation function, we’d have to do it all over again.\nIdeally we’d like a programmatic way to compute derivatives. Knowing that we compute derivatives using a fixed set of known rules, this should be possible!"
  },
  {
    "objectID": "lecture6-backpropagation/notes.html#the-chain-rule-revisited",
    "href": "lecture6-backpropagation/notes.html#the-chain-rule-revisited",
    "title": "Lecture 6: Deep neural networks and backpropagation",
    "section": "The chain rule revisited",
    "text": "The chain rule revisited\nWhile we often think about the chain rule in terms of functions:\n\\[\n\\frac{d}{dx}f(g(x)) = f'(g(x))g'(x)\n\\]\nIt’s often easier to view it imperatively, in terms of individual values. For example we might say:\n\\[\nb = g(x)\n\\]\n\\[\na = f(b)\n\\]\nIn this case we can write the chain rule as:\n\\[\n\\frac{da}{dx} = \\frac{da}{db}\\frac{db}{dx}\n\\]\nThis corresponds with how we might think about this in code. For example we might have the code:\n\nb = x ** 2\na = log(b)\n\nIn this case we have:\n\\[\na = \\log(b), \\quad b = x^2\n\\]\nWe can compute the derivative of \\(a\\) with respect to \\(x\\) using the chain rule as:\n\\[\n\\frac{da}{db} = \\frac{1}{b}, \\quad \\frac{db}{dx} = 2x\n\\]\n\\[\n\\frac{da}{dx} = \\bigg(\\frac{1}{b}\\bigg)(2x) = \\frac{2x}{x^2} = \\frac{2}{x}\n\\]"
  },
  {
    "objectID": "lecture6-backpropagation/notes.html#composing-many-operations",
    "href": "lecture6-backpropagation/notes.html#composing-many-operations",
    "title": "Lecture 6: Deep neural networks and backpropagation",
    "section": "Composing many operations",
    "text": "Composing many operations\nFor more complex functions, we might be composing many more operations, but we can break down derivative computations in the same way. For example, if we want the derivative with respect to \\(x\\) of some simple loss:\n\\[\nL=-\\log \\sigma\\big(w x^2\\big)\n\\]\nWe can break this down into each individual operation that we apply:\n\\[\na = x^2\n\\]\n\\[\nb=wa\n\\]\n\\[\nc=\\sigma(b)\n\\]\n\\[\ng= \\log c\n\\]\n\\[\nL=-g\n\\]\nThe chain rule tells us that:\n\\[\n\\frac{dL}{dx} = \\frac{dL}{dg}\\frac{dg}{dc}\\frac{dc}{db}\\frac{db}{da}\\frac{da}{dx}\n\\]\nSince each step is a single operation with a known derivative, we can easily compute every term above! Thus, we begin to see a recipe for computing derivatives programatically. Every time we perform some operation, we will also compute the derivative with respect to the input (we can’t just compute the derivatives because each derivative needs the preceding value, e.g. \\(\\frac{dg}{dc}=\\frac{1}{c}\\), so we need to first compute \\(c\\)).\nWe can visually look at the chain of computation that we’re performing as a diagram that shows each step and the result.\n\n\n\n\n\nWe call this structure the computational graph."
  },
  {
    "objectID": "lecture6-backpropagation/notes.html#forward-and-reverse-mode-automatic-differentiation",
    "href": "lecture6-backpropagation/notes.html#forward-and-reverse-mode-automatic-differentiation",
    "title": "Lecture 6: Deep neural networks and backpropagation",
    "section": "Forward and reverse mode automatic differentiation",
    "text": "Forward and reverse mode automatic differentiation\nWe are not actually interested in all of the intermediate derivatives ( \\(\\frac{db}{da}, \\frac{dc}{db}\\) etc.), so it doesn’t make much sense to compute all of them and then multiply them together. Instead, we’d rather just incrementally compute the value we’re interested in \\(\\frac{dL}{dx}\\), as we go.\nThere are 2 ways we could consider doing this. One way is to always keep track of the derivative of the current value with respect to \\(x\\). So in the diagram above, each time we perform a new operation we will also compute the derivative of the operation and then update our knowledge of the derivative with respect to \\(x\\). For example for the operation going from \\(b\\) to \\(c\\):\n\\[\nc \\leftarrow \\sigma(b), \\quad \\frac{dc}{dx} \\leftarrow \\frac{dc}{db}\\cdot\\frac{db}{dx}\n\\]\nWe call this approach forward-mode automatic differentiation.\nThe alternative approach is to work backwards, first compute \\(L\\) and \\(\\frac{dL}{dg}\\) and then go backwards through the chain updating the derivative of the final output with respect to each input for the \\(b\\) to \\(c\\) operation this looks like:\n\\[\nc \\leftarrow \\sigma(b), \\quad \\frac{dL}{db} \\leftarrow \\frac{dc}{db}\\cdot\\frac{dL}{dc}\n\\]\nThis means we need to do our computation in 2 passes. First we need to go through the chain of operations to compute \\(L\\), then we need to go backwards through the chain to compute \\(\\frac{dL}{dx}\\). Note that computing each intermediate derivative requires the a corresponding intermediate value (e.g. \\(\\frac{dc}{db}\\) requires \\(b\\) to compute). So we need to store all the intermediate values as we go. The approach is called reverse-mode automatic differentiation or more commonly: backpropagation. We can summarize both approaches below:"
  },
  {
    "objectID": "lecture6-backpropagation/notes.html#automatic-differentiation-with-multiple-inputs",
    "href": "lecture6-backpropagation/notes.html#automatic-differentiation-with-multiple-inputs",
    "title": "Lecture 6: Deep neural networks and backpropagation",
    "section": "Automatic differentiation with multiple inputs",
    "text": "Automatic differentiation with multiple inputs\nYou might wonder why we’d ever use reverse-mode when it seems to require much more complication in keeping track of all the intermediate values. To see why it is useful, lets’s consider the common case where we would like to take derivatives with respect to multiple inputs at the same time. For example we might have an expression like:\n\\[\n-\\log \\sigma (w_1 x_1+w_2x_2 +w_3x_3)\n\\]\nIn this case we want to find the gradient:\n\\[\n\\frac{dL}{d\\mathbf{x}} = \\begin{bmatrix}\\frac{dL}{dx_1} \\\\ \\frac{dL}{dx_2} \\\\ \\frac{dL}{dx_3} \\end{bmatrix}\n\\]\nWe see that in forward mode, we now need to keep a vector of gradients at many steps if we want to compute the derivative with respect to every input!\n\n\n\n\n\nIn reverse mode, however we only ever need to keep the derivative of the loss with respect to the current value. If we assume that the loss is always a single value, this is much more efficient!"
  },
  {
    "objectID": "lecture6-backpropagation/notes.html#reusing-values",
    "href": "lecture6-backpropagation/notes.html#reusing-values",
    "title": "Lecture 6: Deep neural networks and backpropagation",
    "section": "Reusing values",
    "text": "Reusing values\nOne thing we need to consider is the fact that values can be used in multiple different operations. For example, consider the code below.\n\ndef loss(x):\n    a = x ** 2\n    b = 5 * a\n    c = log(a)\n    g = b * c\n    L = -g\n    return L\n\nThis corresponds to the following sequence of operations:\n\\[\na = x^2\n\\]\n\\[\nb=5a\n\\]\n\\[\nc=\\log a\n\\]\n\\[\ng = bc\n\\]\n\\[\nL=-b\n\\]\nWe see that both \\(b\\) and \\(c\\) depend on \\(a\\). Leading to the following computational graph:\n\n\n\n\n\nIn forward mode this means that we compute 2 different values for \\(\\frac{dg}{dx}\\), one from \\(b\\) \\((\\frac{dg}{db}\\cdot\\frac{db}{dx})\\) and one from \\(c\\) \\((\\frac{dg}{dc}\\cdot\\frac{dc}{dx})\\).\n\n\n\n\n\nIn reverse mode this means that we compute 2 different values for \\(\\frac{dL}{da}\\), one from \\(b\\) \\((\\frac{dL}{db}\\cdot\\frac{db}{da})\\) and one from \\(c\\) \\((\\frac{dL}{dc}\\cdot\\frac{dc}{da})\\).\n\n\n\n\n\nThe resolution in both cases is simple! Just add the two terms. So in forward mode:\n\\[\\frac{dg}{dx} = \\frac{dg}{db}\\cdot \\frac{db}{dx} +\\frac{dg}{dc}\\cdot \\frac{dc}{dx}\\]\nIn reverse mode:\n\\[\\frac{dL}{da} = \\frac{dL}{db}\\cdot \\frac{db}{da} + \\frac{dL}{dc}\\cdot \\frac{dc}{da} \\]\nThe forward case for this example is just an application of the product rule:\n\\[\ng =bc\n\\]\n\\[\n\\frac{dg}{dx} =\\frac{dg}{db}\\cdot \\frac{db}{dx} +\\frac{dg}{dc}\\cdot \\frac{dc}{dx} = c\\cdot \\frac{db}{dx} +b \\cdot \\frac{dc}{dx}\n\\]\nFor reverse mode we need to expand an rearrange a bit:\n\\[\n\\frac{dL}{da} =\\frac{dL}{dg}\\cdot \\frac{dg}{da}\n\\]\n\\[\n\\frac{dg}{da} =\\frac{dg}{db}\\cdot \\frac{db}{da} +\\frac{dg}{dc}\\cdot \\frac{dc}{da}\n\\]\n\\[\n\\frac{dL}{da} =\\frac{dL}{dg}\\bigg( \\frac{dg}{db}\\cdot \\frac{db}{da} +\\frac{dg}{dc}\\cdot \\frac{dc}{da} \\bigg)\n\\]\n\\[\n=\\frac{dL}{dg} \\frac{dg}{db} \\frac{db}{da} + \\frac{dL}{dg}\\frac{dg}{dc} \\frac{dc}{da}\n\\]\n\\[\n=\\frac{dL}{db}\\cdot \\frac{db}{da} + \\frac{dL}{dc}\\cdot \\frac{dc}{da}\n\\]\nThis also works for addition:\n\\[\ng =b + c\n\\]\n\\[\n\\frac{dg}{dx} =\\frac{dg}{db}\\cdot \\frac{db}{dx} +\\frac{dg}{dc}\\cdot \\frac{dc}{dx}\n\\]\n\\[\n\\frac{dg}{db}=1,\\ \\frac{dg}{dc}=1\n\\]\n\\[\n\\frac{dg}{dx} =\\frac{db}{dx} + \\frac{dc}{dx}\n\\]\nAnd in general any binary operation! (Division, powers etc.)."
  },
  {
    "objectID": "lecture6-backpropagation/notes.html#implementing-automatic-differentiation",
    "href": "lecture6-backpropagation/notes.html#implementing-automatic-differentiation",
    "title": "Lecture 6: Deep neural networks and backpropagation",
    "section": "Implementing automatic differentiation",
    "text": "Implementing automatic differentiation\nWe’ll start by developing an automatic differentiation class that uses reverse-mode automatic differentiation, as this is what will be most useful for neural networks.\nRecall that for reverse-mode AD to work, everytime we perform an operation on one or more numbers we need to store the result of that operation as well as the parent values (the inputs to the operation). We also need to be able to compute the derivative of that operation. Since for every operation we need to store several pieces of data and several functions, it makes sense to define a class to represent the result of an operation.\nFor example, if we want to make a class that represents the operation c=a+b our class needs several properties:\n\nvalue: The value of the operation (c)\nparents: The parent operations (a and b)\ngrad: The derivative of the final loss with respect to c (\\(\\frac{dL}{dc}\\))\nfunc: A function that computes the operation (a+b)\ngrads: A function that computes the derivatives of the operation (\\(\\frac{dc}{da}\\) and \\(\\frac{dc}{db}\\))\n\nFor this example, we’ll call our class AutogradValue. This will be the base class for all of our possible operations and represents declaring a variable with a value (a = 5). This is useful because it lets us define values that we might want to find derivatives with respect to.\nLet’s see how this will work in practice. If we want to take derivatives we will first define the inputs using AutogradValue.\n\na = AutogradValue(5)\nb = AutogradValue(2)\n\nThen we can perform whatever operations we want on these inputs:\n\nc = a + b\nL = log(c)\n\nEach of these operations will produce a new AutogradValue object representing the result of that operation.\nFinally we can run the backward pass by running a method backward() (that we will write) on the outout L. This will compute the gradients of L with respect to each input that we defined (\\(\\frac{dL}{da}\\) and \\(\\frac{dL}{db}\\)). Rather than returning these derivatives, the backward() method will update the grad property of a and b, making it easy to access the correct derivative.\n\nL.backward()\ndL_da = a.grad\n\nWe’ll also be able to compute operations with non-AutogradValue numbers, but obviously won’t be able to compute derivaitives with respect to these values.\n\ns = 4\nL = s * a\ndL_da = a.grad # Will work because a is an AutogradValue\ndL_ds = s.grad # Will give an error because s is not an AutogradValue\n\nLet’s look at one possible implementation for AutogradValue:\n\nclass AutogradValue:\n    '''\n    Base class for automatic differentiation operations. \n    Represents variable delcaration. Subclasses will overwrite \n    func and grads to define new operations.\n\n    Properties:\n        parents (list):  A list of the inputs to the operation, \n                         may be AutogradValue or float\n        args    (list):  A list of raw values of each \n                         input (as floats)\n        grad    (float): The derivative of the final loss with \n                         respect to this value (dL/da)\n        value   (float): The value of the result of this operation\n    '''\n\n    def __init__(self, *args):\n        self.parents = list(args)\n        self.args = [arg.value if isinstance(arg, AutogradValue) \n                     else arg \n                     for arg in self.parents]\n        self.grad = 0.\n        self.value = self.forward_pass()\n\n    def forward_pass(self):\n        # Calls func to compute the value of this operation \n        return self.func(*self.args)\n\nFor convenience, in this implementation we’ve also defined a property args which simply stores the value of each parent. Note that we can also allow parents to be either be AutogradValue or a primitive data type like float. This will allow us to do things like multiply an AutogradValue variable with a float, e.g. a * 5.\nThe forward_pass function computes the actual value of the node given it’s parents. This will depend on what kind of operation we’re doing (addition, subtraction, multiplication, etc.), so we’ll define a func method that we can override that does the actual calculation. In the base case we’ll just directly assign the value:\n\nclass AutogradValue:\n    def func(self, input):\n        '''\n        Compute the value of the operation given the inputs.\n        For declaring a variable, this is just the identity \n        function (return the input).\n\n        Args:\n            input (float): The input to the operation\n        Returns:\n            value (float): The result of the operation\n        '''\n        return input\n\nIn subclasses we’ll override func:\n\nclass _square(AutogradValue):\n    # Square operator (a ** 2)\n    def func(self, a):\n        return a ** 2\n\nclass _mul(AutogradValue):\n    # Multiply operator (a * b)\n    def func(self, a, b):\n        return a + b\n\nLet’s consider what the computational graph will look like for the following code:\n\nx = AutogradValue(2)\na = x ** 2\nb = 5 * a\nc = b * a\nL = -c\n\n\nHere the blue nodes represent constants (of type float), while the black nodes are AutogradValue objects. We see that every AutogradValue is populated with the value, parents and args, but that after this first pass grad is still 0 for each object, so we have not computed \\(\\frac{dL}{dx}\\). To do so, we need to run the backward pass by calling L.backward().\n\nL.backward()\nprint('dL_dx', x.grad)\n\nIn the backward pass, each node needs to update the grad property of its parents."
  },
  {
    "objectID": "lecture6-backpropagation/notes.html#partial-and-total-derivatives",
    "href": "lecture6-backpropagation/notes.html#partial-and-total-derivatives",
    "title": "Lecture 6: Deep neural networks and backpropagation",
    "section": "Partial and total derivatives",
    "text": "Partial and total derivatives\nSo far we’ve been a bit sloppy in our discussion of derivatives. To see why, let’s consider one more case:\n\\[\na = x^2\n\\]\n\\[\nb=5a\n\\]\n\\[\nc = a b\n\\]\n\\[\nL=-c\n\\]\n\n\n\n\n\nSaying that \\(\\frac{dc}{da}=b\\) isn’t quite correct, because \\(b\\) also depends on \\(a\\), really \\(\\frac{dc}{da} =\\frac{dc}{da}+\\frac{dc}{db}\\frac{db}{da}\\). We already account for this in our automatic differentiation though, so we want a way to talk about the derivative of an operation with respect to it’s inputs ignoring how the inputs may depend on each other.\nThis is where the notion of a partial derivative comes in, the partial derivative of function with respect to an input is the derivative ignoring any dependencies between inputs. We’ve already seen how we denote this:\n\\[\n\\frac{\\partial c}{\\partial a} = b =5a\n\\]\nThe total derivative is the derivative where we do account for this. In our example:\n\\[\n\\frac{dc}{da} =\\frac{\\partial c}{\\partial a}+\\frac{\\partial c}{\\partial b}\\frac{\\partial b}{\\partial a} = 5a + 5a = 10a\n\\]\nIn our earlier examples, we typically had partial derivatives equal to total derivatives, so the distinction wasn’t really important. This example shows why it is.\nLet’s see our earlier example, but this time we’ll make the distinction between partial and total derivatives explicit\n\n\n\n\n\n\n\n\n\n\nThis is also why we specify gradients in terms of partial derivatives! If we’re taking the gradient of a function with respect to multiple inputs, we don’t know where these inputs come from. They might depend on each other! By specifying gradients at partial derivatives, we make it clear that we’re not accounting for that."
  },
  {
    "objectID": "lecture6-backpropagation/notes.html#computational-graphs-of-vectors",
    "href": "lecture6-backpropagation/notes.html#computational-graphs-of-vectors",
    "title": "Lecture 6: Deep neural networks and backpropagation",
    "section": "Computational graphs of vectors",
    "text": "Computational graphs of vectors\nAs we’ve seen, in the context of neural networks we typically perform operations on large collections of values such as vectors. For example, we might perform an element-wise square on a large vector:\n\nx = np.ones((500,))\na = x ** 2\n\nIn this case we are performing 500 individual square operations, so our computational graph would look like:\n\nRemember for our automatic differentiation implementation we would need an object for each of these values.\n\nx = np.array([AutogradValue(1), AutogradValue(1), ...])\na = x ** 2\n\nEach of these object introduces some overhead as each object needs to be constructed individually and needs to store not just the value and gradients, but also the parents. Furthermore our backward pass needs to determine the order of nodes to visit. For a large neural network, having a node for every single value would be computationally very complex.\nSince we’re performing the same operation on each entry of the vector, there’s really no need to have a separate node for each entry. Therefore we might instead prefer each node in our computational graph to represent a vector or matrix and for our operations to correspond to vector or matrix operations:\n\nx = AutogradValue(np.ones((500,)))\na = x ** 2\n\nIn this case the computational graph for this operation would be:\n\nOur derivative calculations can similarly be performed as element-wise operations:\n\\[\n\\begin{bmatrix} \\frac{da_1}{dx_1} \\\\ \\frac{da_2}{dx_2} \\\\ \\vdots \\\\ \\frac{da_{500}}{dx_{500}} \\end{bmatrix} =\n\\begin{bmatrix} 2x_1 \\\\ 2x_2 \\\\ \\vdots \\\\ 2x_{500} \\end{bmatrix} = 2\\mathbf{x}\n\\]\nSo in fact, for this case, we don’t actually need to change our AutogradValue implementation for square at all!\n\nclass _square(AutogradValue):\n    # Square operator (a ** 2)\n    def func(self, a):\n        return a ** 2\n\n    # Returns a vector of the element-wise derivatives!\n    def grads(self, a):\n        return (a ** 2,)"
  },
  {
    "objectID": "lecture6-backpropagation/notes.html#automatic-differentiation-for-linear-regression",
    "href": "lecture6-backpropagation/notes.html#automatic-differentiation-for-linear-regression",
    "title": "Lecture 6: Deep neural networks and backpropagation",
    "section": "Automatic differentiation for linear regression",
    "text": "Automatic differentiation for linear regression\nLet’s take a look at a concrete example how reverse-mode automatic differentiation with vectors would work. Specifically let’s look at taking the gradient of the mean squared error loss we used for linear regression.\n\\[\nL = \\frac{1}{N}\\sum_{i=1}^N (y_i - \\mathbf{x}_i^T\\mathbf{w})^2 = \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{y} - \\mathbf{X}\\mathbf{w})_i^2\n\\]\nHere the right-hand side of the expression corresponds to how we might write this formula using numpy:\n\nse = (y - np.dot(X, w)) ** 2\nmse = np.sum(se) / X.shape[0] \n\nWe are interested in the gradient with respect to \\(\\mathbf{w}\\), or \\(\\frac{dL}{d\\mathbf{w}}\\) for optimizing our model. Let’s start by looking at the simplest sequence of operations:\n\\[\n\\mathbf{a} = \\mathbf{X}\\mathbf{w}\n\\]\n\\[\n\\mathbf{b} = \\mathbf{y} - \\mathbf{a}\n\\]\n\\[\n\\mathbf{c} = \\mathbf{b}^2\n\\]\n\\[\ng = \\sum_{i=1}^N c_i\n\\]\n\\[\nL = \\frac{1}{N}g\n\\]\nWe can setup our computational graph and reverse mode algorithm exactly as we did before, only in this case some of the operations will be on vectors! (\\(g\\) and \\(L\\) are still scalars, but \\(\\mathbf{w}\\), \\(\\mathbf{a}\\), \\(\\mathbf{b}\\) and \\(\\mathbf{c}\\) are vectors)\n\n\n\n\n\nLet’s walk through the computation of \\(\\frac{dL}{d\\mathbf{w}}\\).\nComputing \\(\\frac{dL}{dg}\\): The first step in the backward pass is straightforward:\n\\[\n\\frac{dL}{dg} = \\frac{1}{N}\n\\]\nComputing \\(\\frac{dL}{d\\mathbf{c}}\\):\n\\[\n\\frac{dL}{d\\mathbf{c}}= \\frac{dL}{dg}\\frac{dg}{d\\mathbf{c}}\n\\]\nSince \\(\\mathbf{c}\\) is a vector, \\(\\frac{dg}{d\\mathbf{c}}\\) and \\(\\frac{dL}{d\\mathbf{c}}\\) must be vectors of the derivative of \\(L\\) or \\(g\\) with respect to each entry of \\(\\mathbf{c}\\). In other words \\(\\frac{dg}{d\\mathbf{c}}\\) and \\(\\frac{dL}{d\\mathbf{c}}\\) are gradients!\n\\[\n\\frac{dL}{d\\mathbf{c}}= \\begin{bmatrix} \\frac{dL}{dc_1} \\\\ \\frac{dL}{dc_2} \\\\ \\vdots \\\\ \\frac{dL}{dc_N}  \\end{bmatrix} = \\frac{dL}{dg} \\begin{bmatrix} \\frac{dg}{dc_1} \\\\ \\frac{dg}{dc_2} \\\\ \\vdots \\\\ \\frac{dg}{dc_N}  \\end{bmatrix} = \\frac{dL}{dg}\\frac{dg}{d\\mathbf{c}}\n\\]\nWe know that the derivative of a sum with respect to a single element is \\(1\\), as in: \\(\\frac{d}{dc_1}(c_1+c_2)=1\\), so it follows that the gradient of our summation is simply a vector of 1s.\n\\[\n\\frac{dg}{d\\mathbf{c}} =\\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}, \\quad \\frac{dL}{d\\mathbf{c}} =\\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}\\frac{1}{N} = \\begin{bmatrix} \\frac{1}{N} \\\\ \\frac{1}{N} \\\\ \\vdots \\\\ \\frac{1}{N} \\end{bmatrix}\n\\]\nComputing \\(\\frac{dL}{d\\mathbf{b}}\\):\nSince \\(\\mathbf{b}^2\\) is an element-wise operation \\((c_i=b_i^2)\\) we know that:\n\\[\n\\frac{dL}{db_i}=\\frac{dL}{dc_i}\\frac{dc_i}{db_i} = \\frac{1}{N}(2b_i)\n\\]\nTherefore:\n\\[\n\\frac{dL}{d\\mathbf{b}} = \\begin{bmatrix} \\frac{dL}{dc_1}\\frac{dc_1}{db_1}  \\\\ \\frac{dL}{dc_2}\\frac{dc_2}{db_2}  \\\\ \\vdots \\\\ \\frac{dL}{dc_N}\\frac{dc_N}{db_N}   \\end{bmatrix} = \\begin{bmatrix} \\frac{2}{N}b_1  \\\\ \\frac{2}{N}b_2  \\\\ \\vdots \\\\ \\frac{2}{N}b_N   \\end{bmatrix}\n\\]\nComputing \\(\\frac{dL}{d\\mathbf{a}}\\):\nSince \\(\\mathbf{y}-\\mathbf{a}\\) is also an element-wise operation \\((b_i=y_i-a_i)\\) we similarly see that:\n\\[\n\\frac{dL}{da_i}=\\frac{dL}{db_i}\\frac{db_i}{da_i} = \\frac{2b_i}{N}(-1)\n\\]\n\\[\n\\frac{dL}{d\\mathbf{a}} = \\begin{bmatrix} \\frac{dL}{db_1}\\frac{db_1}{da_1}  \\\\ \\frac{dL}{dc_2}\\frac{db_2}{da_2}  \\\\ \\vdots \\\\ \\frac{dL}{dc_N}\\frac{db_N}{da_N}   \\end{bmatrix} = \\begin{bmatrix} \\frac{-2}{N}b_1  \\\\ \\frac{-2}{N}b_2  \\\\ \\vdots \\\\ \\frac{-2}{N}b_N   \\end{bmatrix}\n\\]\nComputing \\(\\frac{dL}{d\\mathbf{w}}\\):\nThis last calculation is less straightforward. \\(\\mathbf{X}\\mathbf{w}\\) is not an element-wise operation, so we can’t just apply the chain rule element-wise. We need a new approach! Let’s break down the general problem that we see here."
  },
  {
    "objectID": "lecture6-backpropagation/notes.html#vector-valued-functions",
    "href": "lecture6-backpropagation/notes.html#vector-valued-functions",
    "title": "Lecture 6: Deep neural networks and backpropagation",
    "section": "Vector-valued functions",
    "text": "Vector-valued functions\nA vector-valued function is a function that takes in a vector and returns a vector:\n\\[\n\\mathbf{y} = f(\\mathbf{x}), \\quad f: \\mathbb{R}^n\\rightarrow\\mathbb{R}^m\n\\]\nFor example the matrix-vector product we’ve just seen is a simple vector-valued function:\n\\[\nf(\\mathbf{w}) =\\mathbf{X}\\mathbf{w}\n\\]\nIf \\(\\mathbf{X}\\) is an \\((N\\times d)\\) matrix and \\(\\mathbf{w}\\) is a length- \\(d\\) vector, then \\(f(\\mathbf{w})=\\mathbf{X}\\mathbf{w}\\) is a mapping \\(\\mathbb{R}^d\\rightarrow \\mathbb{R}^N\\)"
  },
  {
    "objectID": "lecture6-backpropagation/notes.html#jacobians",
    "href": "lecture6-backpropagation/notes.html#jacobians",
    "title": "Lecture 6: Deep neural networks and backpropagation",
    "section": "Jacobians",
    "text": "Jacobians\nA the Jacobian \\((\\mathbf{J})\\) of a vector-valued function is the matrix of partial derivatives of every output with respect to every input. We can think of it as an extension of the gradient for vector-valued functions. If we have a vector-valued function \\(f\\) and \\(\\mathbf{a}\\) is the result of applying \\(f\\) to \\(\\mathbf{w}\\):\n\\[\n\\mathbf{a}=f(\\mathbf{w}), \\quad f:\\mathbb{R}^d\\rightarrow \\mathbb{R}^N\n\\]\nThe corresponding Jacobian is:\n\\[\\frac{d\\mathbf{a}}{d\\mathbf{w}} = \\begin{bmatrix} \\frac{\\partial a_1}{\\partial w_1} & \\frac{\\partial a_1}{\\partial w_2}& \\dots& \\frac{\\partial a_1}{\\partial w_d} \\\\\n\\frac{\\partial a_2}{\\partial w_1} & \\frac{\\partial a_2}{\\partial w_2}& \\dots& \\frac{\\partial a_2}{\\partial w_d} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial a_N}{\\partial w_1} & \\frac{\\partial a_N}{\\partial w_2}& \\dots& \\frac{\\partial a_N}{\\partial w_d} \\\\\n\\end{bmatrix}\\]\nIn general:\n\\[\n\\bigg(\\frac{d\\mathbf{a}}{d\\mathbf{w}}\\bigg)_{ij} = \\frac{\\partial a_i}{\\partial w_j}\n\\]\nLet’s consider the Jacobian of a matrix-vector product:\n\\[\n\\mathbf{a} = \\mathbf{X}\\mathbf{w}\n\\]\nWe can find each entry in the Jacobian by taking the corresponding partial derivative.\n\\[\na_i =\\sum_{j=1}^d X_{ij}w_j, \\quad \\frac{\\partial a_i}{\\partial w_j}=X_{ij}\n\\]\nIn this case we see that the Jacobian is just \\(\\mathbf{X}\\)!"
  },
  {
    "objectID": "lecture6-backpropagation/notes.html#jacobian-vector-products",
    "href": "lecture6-backpropagation/notes.html#jacobian-vector-products",
    "title": "Lecture 6: Deep neural networks and backpropagation",
    "section": "Jacobian-vector products",
    "text": "Jacobian-vector products"
  },
  {
    "objectID": "lecture6-backpropagation/notes.html#vector-jacobian-products",
    "href": "lecture6-backpropagation/notes.html#vector-jacobian-products",
    "title": "Lecture 6: Deep neural networks and backpropagation",
    "section": "Vector-Jacobian products",
    "text": "Vector-Jacobian products\nLet’s return to our linear regression example:\n\n\n\n\n\nWe see that \\(\\frac{d\\mathbf{a}}{d\\mathbf{w}}\\) is actually a Jacobian that we now know how to compute, so the remaining question is how to combine it with \\(\\frac{dL}{d\\mathbf{a}}\\) in order to get the gradient we’re looking for \\(\\frac{dL}{d\\mathbf{w}}\\)? The answer turns out to be simple: use the vector-matrix product:\n\\[\n\\frac{dL}{d\\mathbf{w}} = \\frac{dL}{d\\mathbf{a}}^T\\frac{d\\mathbf{a}}{d\\mathbf{w}}\n\\]\nWe might be curious why this is the right thing to do as opposed to a matrix-vector product or some other function. To see why, let’s consider what an entry of our final gradient \\(\\frac{dL}{d\\mathbf{w}}\\) should be. We’ve previously seen that when a value is use by more than one child operation, we need to sum the contribution of each child to the total derivative. So in this case, for a given entry of \\(\\mathbf{w}\\) we need to sum the gradient contribution from every element of \\(\\mathbf{a}\\):\n\\[\n\\frac{dL}{dw_j} = \\frac{dL}{da_1} \\frac{da_1}{dw_j}+\\frac{dL}{da_2} \\frac{da_2}{dw_j}+...\\frac{dL}{da_N} \\frac{da_N}{dw_j} = \\sum_{i=1}^N \\frac{dL}{da_i} \\frac{da_i}{dw_j}\n\\]\nWhich we can see is equivalent to an entry in the vector matrix product:\n\\[\n\\frac{dL}{d\\mathbf{w}} = \\begin{bmatrix} \\frac{dL}{da_1} & \\frac{dL}{da_2} & ... & \\frac{dL}{da_N} \\end{bmatrix}\\begin{bmatrix} \\frac{\\partial a_1}{\\partial w_1} & \\frac{\\partial a_1}{\\partial w_2}& \\dots& \\frac{\\partial a_1}{\\partial w_d} \\\\\n\\frac{\\partial a_2}{\\partial w_1} & \\frac{\\partial a_2}{\\partial w_2}& \\dots& \\frac{\\partial a_2}{\\partial w_d} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial a_N}{\\partial w_1} & \\frac{\\partial a_N}{\\partial w_2}& \\dots& \\frac{\\partial a_N}{\\partial w_d} \\\\\n\\end{bmatrix} = \\frac{dL}{d\\mathbf{a}}^T\\frac{d\\mathbf{a}}{d\\mathbf{w}}\n\\]\nWe call this a vector-Jacobian product or VJP for short. We see that because it’s simply derived from our basic gradient rules, it’s valid for any vector-valued operation, as long as we can compute the Jacobian! We can use this to perform the final step in the backward pass for the MSE.\nComputing \\(\\frac{dL}{d\\mathbf{w}}\\):\nFrom our vector-Jacobian product rule we know that:\n\\[\n\\frac{dL}{d\\mathbf{w}} = \\frac{dL}{d\\mathbf{a}}^T\\frac{d\\mathbf{a}}{d\\mathbf{w}}= \\frac{-2}{N}\\mathbf{b}^T\\mathbf{X}\n\\]\nIf we substitute back in \\(\\mathbf{b}=\\mathbf{y}-\\mathbf{X}\\mathbf{w}\\) we see that this is equivalent to the gradient we derived in a previous class!\n\\[ \\frac{dL}{d\\mathbf{w}} = \\frac{-2}{N}(\\mathbf{y}-\\mathbf{X}\\mathbf{w})^T\\mathbf{X} \\]"
  },
  {
    "objectID": "lecture6-backpropagation/notes.html#vjps-for-element-wise-operations",
    "href": "lecture6-backpropagation/notes.html#vjps-for-element-wise-operations",
    "title": "Lecture 6: Deep neural networks and backpropagation",
    "section": "VJPs for element-wise operations",
    "text": "VJPs for element-wise operations\nIt’s worth noting that element-wise operations are still vector-valued functions.\n\\[\n\\mathbf{c}=\\mathbf{b}^2, \\quad \\mathbb{R}^n\\rightarrow\\mathbb{R}^n\n\\]\nSo why didn’t we need to do a vector-Jaobian product for that operation? The answer is that we did, just implicitly! If we consider the Jacobian for this operation we see that because each entry of \\(\\mathbf{c}\\) only depends on the corresponding entry of \\(\\mathbf{b}\\), the Jacobian for this operation is \\(0\\) everywhere except the main diagonal:\n\\[\n\\frac{d\\mathbf{c}}{d\\mathbf{b}} = \\begin{bmatrix} \\frac{\\partial c_1}{\\partial b_1} & 0 & \\dots& 0 \\\\\n0 & \\frac{\\partial c_2}{\\partial b_2}& \\dots& 0\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0& 0& \\dots& \\frac{\\partial c_N}{\\partial b_d} \\\\\n\\end{bmatrix}\n\\]\nThis means that we can write the vector-Jacobian product as we did before:\n\\[\n\\frac{dL}{d\\mathbf{c}}\\frac{d\\mathbf{c}}{d\\mathbf{b}}=\\begin{bmatrix} \\frac{dL}{dc_1}\\frac{dc_1}{db_1}  \\\\ \\frac{dL}{dc_2}\\frac{dc_2}{db_2}  \\\\ \\vdots \\\\ \\frac{dL}{dc_N}\\frac{dc_N}{db_N}   \\end{bmatrix}\n\\]\nThis is a big computational savings over explicitly constructing the full Jacobian and performing a vector-matrix multiplication."
  },
  {
    "objectID": "lecture6-backpropagation/notes.html#vjps-for-matrices",
    "href": "lecture6-backpropagation/notes.html#vjps-for-matrices",
    "title": "Lecture 6: Deep neural networks and backpropagation",
    "section": "VJPs for matrices",
    "text": "VJPs for matrices\nSo far we’ve seen VJPs with respect to vectors. What if in the formulation above, we were to take the derivative with respect to \\(\\mathbf{X}\\) instead of \\(\\mathbf{w}\\)?\n\\[\n\\mathbf{a}=\\mathbf{X}\\mathbf{w},\\quad \\frac{d\\mathbf{a}}{d\\mathbf{X}}\n\\]\nIn this case \\(\\mathbf{X}\\) is an \\(N\\times d\\) matrix, so if we were to consider all the partial derivatives making up the Jacobian \\(\\frac{d\\mathbf{a}}{d\\mathbf{X}}\\) there would be \\(N\\times N\\times d\\) values, while the gradient \\(\\frac{dL}{d\\mathbf{X}}\\) is itself an \\(N\\times d\\) matrix of the derivative of \\(L\\) with respect to each value of \\(\\mathbf{X}\\).\nLet’s look at how we can formulate the vector-Jacobian product:\n\\[\n\\frac{dL}{d\\mathbf{X}} = \\frac{dL}{d\\mathbf{a}}^T\\frac{d\\mathbf{a}}{d\\mathbf{X}}\n\\]\nWe know that for a given entry of \\(\\frac{dL}{d\\mathbf{X}}\\), we can again compute the derivative by summing the contribution from each child, in this case every entry of \\(\\mathbf{a}\\)\n\\[\n\\frac{dL}{dX_{jk}} = \\sum_{i=1}^N \\frac{\\partial L}{\\partial a_{i}} \\frac{\\partial a_i}{\\partial X_{jk}}\n\\]\nThis suggests that we can compute the VJP by flattening the Jacobian from an \\(N\\times N \\times d\\) structure into a \\(N \\times Nd\\) matrix. Therefore performing the vector-matrix product will give us a length \\(Nd\\) vector with the appropriate values for us to reshape into our \\(N \\times d\\) Jacobian \\(\\frac{dL}{d\\mathbf{X}}\\).\nIn code this could look like:\n\ndL_da, da_dx  # The computed gradients/jacobians\nda_dx = da_dx.reshape((da_dx.shape[0], -1))\ndL_dx = np.dot(dL_da, da_dx)\ndL_dx = dL_dx.reshape(x.shape)\n\nLet’s return to our example:\n\\[\\mathbf{a}=\\mathbf{X}\\mathbf{w}\\]\nDo we need to instantiate the full Jacobian here? No! In our original operation:\n\\[\na_i = \\sum_{k=1}^d X_{ik}w_k\n\\]\nThus \\(\\frac{\\partial a_i}{\\partial X_{jk}}\\) is only nonzero when \\(j=i\\). Taking the derivative we get:\n\\[\n\\frac{\\partial a_i}{\\partial X_{jk}} = \\frac{\\partial}{\\partial X_{jk}}\\sum_{k=1}^d X_{ik}w_k = \\mathbb{I}(i=j)w_k\n\\]\nWe can therefore write the vector-Jacobian product as:\n\\[\n\\frac{\\partial L}{\\partial X_{ik}}=\\frac{\\partial L}{\\partial a_i}\\frac{\\partial a_i}{\\partial X_{ik}} = \\frac{\\partial L}{\\partial a_i}w_k\n\\]\nIn code we could write this as:\n\nw_mat = w.reshape((1, -1)) # reshape w to 1 x d\ndL_da_mat = dL_da.reshape((-1, 1)) # reshape dL_da to N x 1\ndL_dx = w_mat * dL_da_mat # dL_dx becomes N x d, entry ik = dL_da_i * w_k"
  },
  {
    "objectID": "lecture6-backpropagation/notes.html#forward-mode-ad-with-vectors",
    "href": "lecture6-backpropagation/notes.html#forward-mode-ad-with-vectors",
    "title": "Lecture 6: Deep neural networks and backpropagation",
    "section": "Forward mode AD with vectors",
    "text": "Forward mode AD with vectors\nWhat about forward mode for our linear regression example?\n\\[\n\\mathbf{a} = \\mathbf{X}\\mathbf{w}\n\\]\n\\[\n\\mathbf{b} = \\mathbf{y} - \\mathbf{a}\n\\]\n\\[\n\\mathbf{c} = \\mathbf{b}^2\n\\]\n\\[\ng = \\sum_{i=1}^N c_i\n\\]\n\\[\nL = \\frac{1}{N}g\n\\]\n\n\n\n\n\nAs we’ve seen \\(\\frac{d\\mathbf{a}}{d\\mathbf{w}}\\) is an \\(N\\times d\\) Jacobian matrix. After computing this, the next step will be to compute:\n\\[\\frac{d\\mathbf{b}}{d\\mathbf{w}}=\\frac{d\\mathbf{b}}{d\\mathbf{a}}\\frac{d\\mathbf{a}}{d\\mathbf{w}}\\]\nHere we are multiplying the \\(N\\times N\\) Jacobian \\(\\frac{d\\mathbf{b}}{d\\mathbf{a}}\\) with the \\(N\\times d\\) gradient \\(\\frac{d\\mathbf{a}}{d\\mathbf{w}}\\) to get the \\(N\\times d\\) gradient \\(\\frac{d\\mathbf{b}}{d\\mathbf{w}}\\). We can again verify that this is the correct computation by checking an individual element of \\(\\frac{d\\mathbf{b}}{d\\mathbf{w}}\\):\n\\[\n\\frac{db_i}{dw_j}=\\sum_{k=1}^N \\frac{db_i}{da_k}\\frac{da_k}{dw_j}\n\\]\nWe call this operation a Jacobian-vector product or JVP."
  },
  {
    "objectID": "assignments/homework3-logistic-regression/main.html",
    "href": "assignments/homework3-logistic-regression/main.html",
    "title": "Homework 4: Automatic Differentiation and Neural Networks",
    "section": "",
    "text": "Please list anyone you discussed or collaborated on this assignment with below.\nLIST COLLABORATORS HERE"
  },
  {
    "objectID": "assignments/homework3-logistic-regression/main.html#overview",
    "href": "assignments/homework3-logistic-regression/main.html#overview",
    "title": "Homework 4: Automatic Differentiation and Neural Networks",
    "section": "Overview",
    "text": "Overview\nIn this homework we will build a tiny automatic differentiation, matrix and neural network library from scratch!\n\n# Run me first!\nfrom hw4_support import *\n\n\nPython features\nThis homework makes use of a few fancy features in Python that are worth knowing about if you are unfamiliar. - Variable length arguments (e.g. *args) - List comprehensions (e.g. [a**2 for a in range(5)]) - Magic methods (e.g. __add__)"
  },
  {
    "objectID": "assignments/homework3-logistic-regression/main.html#part-1-reverse-mode-automatic-differentiation",
    "href": "assignments/homework3-logistic-regression/main.html#part-1-reverse-mode-automatic-differentiation",
    "title": "Homework 4: Automatic Differentiation and Neural Networks",
    "section": "Part 1: Reverse-mode automatic differentiation",
    "text": "Part 1: Reverse-mode automatic differentiation\nWe’ll start by developing an automatic differentiation class that uses reverse-mode automatic differentiation, as this is what will be most useful for neural networks.\nRecall that for reverse-mode AD to work, everytime we perform an operation on one or more numbers we need to store the result of that operation as well as the parent values (the inputs to the operation). We also need to be able to compute the derivative of that operation. Since for every operation we need to store several pieces of data and several functions, it makes sense to define a class to represent the result of an operation.\nFor example, if we want to make a class that represents the operation c=a+b our class needs several properties: - value: The value of the operation (c) - parents: The parent operations (a and b) - grad: The derivative of the final loss with respect to c (\\(\\frac{dL}{dc}\\)) - func: A function that computes the operation (a+b) - grads: A function that computes the derivatives of the operation (\\(\\frac{dc}{da}\\) and \\(\\frac{dc}{db}\\))\nFor this homework, we’ve provided the outline of such a class, called AutogradValue. This will be the base class for all of our possible operations and represents declaring a variable with a value (a = 5). This is useful because it lets us define values that we might want to find derivatives with respect to.\nLet’s see how this will work in practice. If we want to take derivatives we will first define the inputs using AutogradValue.\na = AutogradValue(5)\nb = AutogradValue(2)\nThen we can perform whatever operations we want on these inputs:\nc = a + b\nL = log(c)\nEach of these operations will produce a new AutogradValue object representing the result of that operation.\nFinally we can run the backward pass by running a method backward() (that we will write) on the outout L. This will compute the gradients of L with respect to each input that we defined (\\(\\frac{dL}{da}\\) and \\(\\frac{dL}{db}\\)). Rather than returning these derivatives, the backward() method will update the grad property of a and b, making it easy to access the correct derivative.\nL.backward()\ndL_da = a.grad\nWe’ll also be able to compute operations with non-AutogradValue numbers, but obviously won’t be able to compute derivaitives with respect to these values.\ns = 4\nL = s * a\ndL_da = a.grad # Will work because a is an AutogradValue\ndL_ds = s.grad # Will give an error because s is not an AutogradValue\nNow that we’ve seen what our final produce will look like, let’s define our AutogradValue class.\n\nclass AutogradValue:\n    '''\n    Base class for automatic differentiation operations. Represents variable delcaration.\n    Subclasses will overwrite func and grads to define new operations.\n\n    Properties:\n        parents (list): A list of the inputs to the operation, may be AutogradValue or float\n        args    (list): A list of raw values of each input (as floats)\n        grad    (float): The derivative of the final loss with respect to this value (dL/da)\n        value   (float): The value of the result of this operation\n    '''\n\n    def __init__(self, *args):\n        self.parents = list(args)\n        self.args = [arg.value if isinstance(arg, AutogradValue) else arg for arg in self.parents]\n        self.grad = 0.\n        self.value = self.forward_pass()\n\n    def func(self, input):\n        '''\n        Compute the value of the operation given the inputs.\n        For declaring a variable, this is just the identity function (return the input).\n\n        Args:\n            input (float): The input to the operation\n        Returns:\n            value (float): The result of the operation\n        '''\n        return input\n\n    def grads(self, *args):\n        '''\n        Compute the derivative of the operation with respect to each input.\n        In the base case the derivative of the identity function is just 1. (da/da = 1).\n\n        Args:\n            input (float): The input to the operation\n        Returns:\n            grads (tuple): The derivative of the operation with respect to each input\n                            Here there is only a single input, so we return a length-1 tuple.\n        '''\n        return (1,)\n    \n    def forward_pass(self):\n        # Calls func to compute the value of this operation \n        return self.func(*self.args)\n    \n    def __repr__(self):\n        # Python magic function for string representation.\n        return str(self.value)\n\n \n\nNow that we’ve defined the framework for an operation that can be used in automatic differentiation, we need to define some actual useful operations by subclassing AutogradValue\n\nQ1\nFill out the func and grads methods of each subclass below. Recall that func should always return the result of the operation and grads should always return a tuple of the derivative with respect to each input.\n\nclass _add(AutogradValue):\n    # Addition operator (a + b)\n    def func(self, a, b):\n        return a + b\n    \n    def grads(self, a, b):\n        return 1., 1.\n    \nclass _neg(AutogradValue):\n    # Negation operator (-a)\n    def func(self, a):\n        return -a\n    \n    def grads(self, a):\n        return (-1.,)\n\nclass _sub(AutogradValue):\n    # Subtraction operator (a - b)\n    def func(self, a, b):\n        # Your code here\n    \n    def grads(self, a, b):\n        # Your code here\n\nclass _mul(AutogradValue):\n    # Multiplication operator (a * b)\n    def func(self, a, b):\n        # Your code here\n    \n    def grads(self, a, b):\n        # Your code here\n\nclass _div(AutogradValue):\n    # Division operator (a / b)\n    def func(self, a, b):\n        # Your code here\n    \n    def grads(self, a, b):\n        # Your code here\n    \nclass _exp(AutogradValue):\n    # Exponent operator (e^a, or exp(a))\n    def func(self, a):\n        # Your code here\n    \n    def grads(self, a):\n        # Your code here\n\nclass _log(AutogradValue):\n    # (Natural) log operator (log(a))\n    def func(self, a):\n        # Your code here\n    \n    def grads(self, a):\n        # Your code here\n\nBelow, we’ll define our basic functions and operators in terms of the operator classes we just wrote.\n\ndef exp(a):\n    return _exp(a) if isinstance(a, AutogradValue) else math.exp(a)\ndef log(a):\n    return _log(a) if isinstance(a, AutogradValue) else math.log(a)\n\n# Note: Remember that above we defined a class for each type of operation\n# so in this code we are overriding the basic operators for AutogradValue\n# such that they construct a new object of the class corresponding to the\n# given operation and return it. \n# (You don't need to everything that's happening here to do the HW)\nAutogradValue.exp = lambda a: _exp(a)\nAutogradValue.log = lambda a: _log(a)\nAutogradValue.__add__ = lambda a, b: _add(a, b)\nAutogradValue.__radd__ = lambda a, b: _add(b, a)\nAutogradValue.__sub__ = lambda a, b: _sub(a, b)\nAutogradValue.__rsub__ = lambda a, b: _sub(b, a)\nAutogradValue.__neg__ = lambda a: _neg(a)\nAutogradValue.__mul__ = lambda a, b: _mul(a, b)\nAutogradValue.__rmul__ = lambda a, b: _mul(b, a)\nAutogradValue.__truediv__ = lambda a, b: _div(a, b)\nAutogradValue.__rtruediv__ = lambda a, b: _div(b, a)\n\nWe should now be able to use our AutogradValue objects as if they are numbers!\n\na = AutogradValue(5)\nb = AutogradValue(2)\n\nprint((a + 5) * b)\nprint(log(b))\n\ntest_operators(AutogradValue)\n\nLet confirm that we do keep the entire compuational graph for operations defined in this way.\n\n\nQ2\nWrite a function graph_print that takes a single argument. If the argument is an AutogradValue (or one of its subclasses), print its value property and then call graph_print on each of its parents. If the argument is not an AutogradValue, just print it. The format of printing is not important.\nHint: You can use the built-in Python function isinstance to determine if something is an AutogradValue or one of its subclasses. e.g. isinstance(a, AutogradValue)\n\ndef graph_print(a):\n    ## YOUR CODE HERE\n\na = AutogradValue(5.)\nb = AutogradValue(2.)\nc = log((a + 5) * b)\ngraph_print(c)\n\nThe function should print (it’s ok if the numbers or order aren’t exact):\n2.995732273553991\n20.0\n10.0\n5.0\n5.0\n5\n2.0\n2.0\nNow in order to do automatic differentiation, we need to define how to do the backward pass. We’ll start with the backward_step for a single operation.\n\n\nQ3\nFill in the method backward_pass which computes a single step of the reverse pass through the computational graph (assume self is an AutogradValue instance). If backward_pass is called on a value c, the method should: - Assume that self.grad contains the derivaive of the final loss with respect to c (\\(\\frac{dL}{dc}\\)). - Check if each parent of c is an AutogradValue. If it is, update that parent’s grad property to account for c (e.g. for parent a, update the value of \\(\\frac{dL}{da}\\))\nFor example: if c represents the result of an addition so c = a + b, calling backward_pass on c will update the grad property of both a and b. (a.grad represents \\(\\frac{dL}{da}\\) and is initialized to 0).\nHint: grads will be one of the methods we wrote in Q1. Recall that if c has parents a and b then grads method will give \\(\\frac{dc}{da}\\) and \\(\\frac{dc}{db}\\).\n\ndef backward_pass(self):\n    ## YOUR CODE HERE\n\nAutogradValue.backward_pass = backward_pass\n\n# Test our implementation\ntest_backward_pass(AutogradValue)\n\nFinally we need to define the backward method itself. We will call this on the loss value to find the derivatives of the loss with respect to each input. This means working our way backward through the sequence of operations. Remember that if c=a+b, then if c.grad is \\(\\frac{dL}{dc}\\), calling backward_pass on c will update \\(\\frac{dL}{da}\\) (a.grad) and \\(\\frac{dL}{db}\\) (b.grad).\nThe complication is that c may be used in multiple operations, so we can’t call backward_pass on c until we’ve called backward_pass on each child operation of c otherwise c.grad won’t have the correct value of \\(\\frac{dL}{dc}\\), as in this example:\nc = a + b\ng = c * 2\nh = c + 4\nL = g * h\n\nL.backward_pass() # Updates dL/dg and dL/dh\nh.backward_pass() # Updates dL/dc\n\n##WRONG ORDER\nc.backward_pass() # Incorrect because dL/dc hasn't accounted for dL/dg\ng.backward_pass()\n\n## CORRECT ORDER\ng.backward_pass() # Updates dL/dc\nc.backward_pass() # Updates dL/da and dL/db\n\n\nQ4\nFill in the backward method for AutogradValue. Your backward method should call backward_pass on each operation used to compute the loss (self is the loss value). Some important things to keep in mind: - backward_pass should only be called once on each operation - backward_pass must be called on every child of an operation before it can be called on the operation. - You should not try to call backward_pass on values that aren’t instances of AutogradValue, even though this might be stored in operation.parents\nHint: The efficiency of this method will have a large impact on the running time of later problems! We won’t score efficiency in grading, but it still might be worth optimizing this function a bit.\n\ndef backward(self):\n    # We call backward on the loss, so dL/dL = 1\n    self.grad = 1.\n    \n    ## YOUR CODE HERE\n\nAutogradValue.backward = backward\n# Test our implementation\ntest_backward(AutogradValue)\n\nNow we can use our AutogradValue class to compute derivatives!\n\na = AutogradValue(5)\nb = AutogradValue(2)\n\nL = -log(5 *b + a)\nL.backward()\nprint(a.grad, b.grad)\n\nIf we want to train a neural network using our automatic differentiation implementation, we’re going to want to be able to use numpy to do matrix operations. Fortunately, the our AutogradValue class is (mostly) compatible with numpy!\nWe can create arrays of AutogradValue and take derivatives as shown below:\n\na = np.array([AutogradValue(5), AutogradValue(2)])\nL = np.dot(a, a)\nL.backward()\nprint('Gradient for a', a[0].grad, a[1].grad)\n\nIt would be a bit tedious to define every AutogradValue array in this way, so let’s write some convinience functions to make doing automatic differentiation with numpy easier.\n\n\nQ5\nComplete the following two functions wrap_array and unwrap_gradient.\nwrap_array should take a numpy array of floats and return a new array where every element has been made into an AutogradValue.\nunwrap_gradient should take a numpy array of AutogradValue and return a new array of floats, where every element is the extracted grad property of the corresponding element from the original array.\nHint: You can create an array from nested lists as shown above.\n\ndef wrap_array(a):\n    '''\n    Wraps the elements of an array with AutogradValue\n\n    Args:\n        a (array of float): The array to wrap\n    Returns:\n        g (array of AutogradValue): An array g, such that g[i,j] = AutogradValue(a[i,j])\n    '''\n    ## YOUR CODE HERE\n\ndef unwrap_gradient(a):\n    '''\n    Unwraps the gradient of an array with AutogradValues\n\n    Args:\n        a (array of AutogradValue): The array to unwrap\n    Returns:\n        g (array of float): An array g, such that g[i,j] = a[i,j].grad\n    '''\n    ## YOUR CODE HERE"
  },
  {
    "objectID": "assignments/homework3-logistic-regression/main.html#part-2-implementing-a-neural-network",
    "href": "assignments/homework3-logistic-regression/main.html#part-2-implementing-a-neural-network",
    "title": "Homework 4: Automatic Differentiation and Neural Networks",
    "section": "Part 2: Implementing a neural network",
    "text": "Part 2: Implementing a neural network\nNow that we have everything we need to apply our automatic differentiation to train a neural network!\nBefore we do that though, let’s try out our automatic differentiation for logistic regression. Below is a slight modification of LogisticRegression implementation we saw in the last homework.\n\n\ndef sigmoid(x):\n    # Computes the sigmoid function\n    return 1. / (1. + np.exp(-x))\n\nclass LogisticRegression:\n    def __init__(self, dims):\n        '''\n        Args:\n            dims (int): d, the dimension of each input\n        '''\n        self.weights = np.zeros((dims + 1, 1))\n\n    def prediction_function(self, X, w):\n        '''\n        Get the result of our base function for prediction (i.e. x^t w)\n\n        Args:\n            X (array): An N x d matrix of observations.\n            w (array): A (d+1) vector of weights.\n        Returns:\n            pred (array): A length N vector of f(X).\n        '''\n        X = np.pad(X, ((0,0), (0,1)), constant_values=1.)\n        return np.dot(X, w)\n\n    def predict(self, X):\n        '''\n        Predict labels given a set of inputs.\n\n        Args:\n            X (array): An N x d matrix of observations.\n        Returns:\n            pred (array): A length N vector of predictions in {0, 1}\n        '''\n        return (self.prediction_function(X, self.weights) &gt; 0)\n    \n    def predict_probability(self, X):\n        '''\n        Predict the probability of each class given a set of inputs\n\n        Args:\n            X (array): An N x d matrix of observations.\n        Returns:\n            probs (array): A length N vector of predicted class probabilities\n        '''\n        return sigmoid(self.prediction_function(X, self.weights))\n\n    def accuracy(self, X, y):\n        '''\n        Compute the accuracy of the model's predictions on a dataset\n\n        Args:\n            X (array): An N x d matrix of observations.\n            y (array): A length N vector of labels.\n        Returns:\n            acc (float): The accuracy of the classifier\n        '''\n        return (self.predict(X) == y).mean()\n\n    def nll(self, X, y, w=None):\n        '''\n        Compute the negative log-likelihood loss.\n\n        Args:\n            X (array): An N x d matrix of observations.\n            y (array): A length N vector of labels.\n            w (array, optional): A (d+1) x 1 matrix of weights.\n        Returns:\n            nll (float): The NLL loss\n        '''\n        if w is None:\n            w = self.weights\n\n        xw = self.prediction_function(X, w)\n        py = sigmoid((2 * y - 1) * xw)\n        return -(np.log(py)).sum()\n    \n    def nll_gradient(self, X, y):\n        '''\n        Compute the gradient of the negative log-likelihood loss.\n\n        Args:\n            X (array): An N x d matrix of observations.\n            y (array): A length N vector of labels.\n        Returns:\n            grad (array): A length (d + 1) vector with the gradient\n        '''\n        xw = self.prediction_function(X, self.weights)\n        py = sigmoid((2 * y - 1) * xw)\n        grad = ((1 - py) * (2 * y - 1)).reshape((-1, 1)) * np.pad(X, [(0,0), (0,1)], constant_values=1.)\n        return -np.sum(grad, axis=0)\n    \n    def nll_and_grad_no_autodiff(self, X, y):\n        # Compute nll_and_grad without automatic diferentiation\n        return self.nll(X, y), self.nll_gradient(X, y)\n\n\nQ6\nWrite the method nll_and_grad for the LogisticRegression class using the automatic differentiation tools we built above. Verify that it gives a similar answer to nll_and_grad_no_autodiff.\n\ndef nll_and_grad(self, X, y):\n    ## YOUR CODE HERE\n\nLogisticRegression.nll_and_grad = nll_and_grad\n\nOur automatic differentiation is very inefficient (we’ll fix this in the next homework!), so we’ll test our model on a very small dataset.\n\nX, y = make_moons(100, noise=0.1)\nmodel = LogisticRegression(2)\ngradient_descent(model, X, y, lr=3e-2, steps=250)\n\nprint('Model accuracy: %.3f' % model.accuracy(X, y))\nplot_boundary(model, X, y)\n\nNow let’s extend our model to be a neural network! We’ll create a neural network class that extends our logistic regression class. First we’ll setup the needed weight matrices.\n\n\nQ7\nFill in the Neural Network __init__ method below. The method should take in the input data dimension and a list of integers specifying the size of each hidden layer (the number of neurons in each layer). The function should create a list of numpy arrays of the appropriate shapes for the weight matrices.\nFor example if dims is 2 and hidden_sizes is [4, 4], then self.weights should have 3 entries of shapes [(4x2), (4x4), (4,)]. This network is shown below.\n\n\nInput Layer ∈ ℝ²Hidden Layer ∈ ℝ⁴Hidden Layer ∈ ℝ⁴Output Layer ∈ ℝ¹\n\nIf you find it easier you could also define the weights in terms of \\(W^T\\) instead, in which case the shapes would be: [(2x4), (4x4), (4,)].\nThe values in each array should be drawn from a normal distribution with standard deviation 0.01. You can create such a matrix in numpy using:\nnp.random.normal(scale=0.01, size=shape)\n\nclass NeuralNetwork(LogisticRegression):\n    def __init__(self, dims, hidden_sizes=[]):\n        ## YOUR CODE HERE\n        self.weights = \n\nRecall that for logistic regression the prediction function (before threholding or sigmoid) was \\(\\mathbf{X}\\mathbf{w}\\). We now want to implement the prediction function for our neural network class. This function should perform the appropriate feature transforms and multiply by the regression weights. For a neural network with a single hidden layer this will look like:\n\\(f(\\mathbf{X}) = \\sigma(\\mathbf{X}\\mathbf{W}_1^T)\\mathbf{w}_0\\)\nUse the sigmoid activation function for this problem.\nFor multiple layers we can also think of this a a chain of feature transforms: \\[\\Phi_1 = \\sigma(\\mathbf{X}\\mathbf{W}_1^T)\\] \\[\\Phi_2 = \\sigma(\\Phi_1 \\mathbf{W}_2^T)\\] \\[...\\] \\[\\Phi_l = \\sigma(\\Phi_{l-1} \\mathbf{W}_l^T)\\] \\[f(\\mathbf{X}) = \\Phi_l\\mathbf{w}_0\\] Where \\(\\Phi_i\\) is just the variable that represents the neurons at layer \\(i\\) (the result of the first \\(i\\) transforms applied to \\(\\mathbf{X}\\)).\n\n\nQ8\nImplement the prediction function as described above. Note that the prediction function should use the weights passed into the w argument rather than self.weights, this will make it easier to implement the next question.\n\ndef prediction_function(self, X, w):\n    '''\n    Get the result of our base function for prediction (i.e. x^t w)\n\n    Args:\n        X (array): An N x d matrix of observations.\n        w (list of arrays): A list of weight matrices\n    Returns:\n        pred (array): An N x 1 matrix of f(X).\n    '''\n    ## YOUR CODE HERE\n\n\nNeuralNetwork.prediction_function = prediction_function\n\n\n\nQ9\nImplement an nll_and_grad method for the NeuralNetwork class using your automatic differentiation implmentation to compute the gradient with respect to each weight matrix.\n\ndef nll_and_grad(self, X, y):\n    '''\n    Get the negative log-likelihood loss and its gradient\n\n    Args:\n        X (array): An N x d matrix of observations.\n        y (array): A length N vector of labels\n    Returns:\n        nll (float): The negative log-likelihood\n        grads (list of arrays): A list of the gradient of the nll with respect\n                                to each value in self.weights.\n    '''\n    ## YOUR CODE HERE\n\nNeuralNetwork.nll_and_grad = nll_and_grad\n\nWe now have everything in place to train a neural network from scratch! Let’s try it on our tiny dataset. Feel free to change the inputs.\nHint: If this give very poor results and/or runs very slowly, make sure to carefully check the shape of each operation in your code to make sure it matches your expectation.\n\nX, y = make_moons(100, noise=0.1)\nmodel = NeuralNetwork(2, [5, 5])\ngradient_descent(model, X, y, lr=3e-2, steps=250)\n\nprint('Model accuracy: %.3f' % model.accuracy(X, y))\nplot_boundary(model, X, y)"
  },
  {
    "objectID": "assignments/homework3-logistic-regression/main.html#part-4-forward-mode-automatic-differentiation",
    "href": "assignments/homework3-logistic-regression/main.html#part-4-forward-mode-automatic-differentiation",
    "title": "Homework 4: Automatic Differentiation and Neural Networks",
    "section": "Part 4: Forward-mode automatic differentiation",
    "text": "Part 4: Forward-mode automatic differentiation\nLet’s now try out the other type of automatic differentiation that we learned about: forward-mode. To do this we’ll create a subclass of our AutogradValue class called ForwardValue.\nRecall that for this version of automatic differentiaion each operation needs to keep track of the derivative of it’s value with respect each original input. For example, in the code below the g object needs to store both \\(\\frac{dg}{da}\\) and \\(\\frac{dg}{db}\\).\na = ForwardValue(5)\nb = ForwardValue(2)\nc = a + b\ng = c * 2\nOur ForwardValue class will maintain a dict called forward_grads that maps each original input object to the derivative of the current value with respect to that input (so g will have a dict with keys that are the a and b objects).\nIn the base case, when we declare a variable the derivative with respect to itself is just 1 (\\(\\frac{da}{da}=1\\))\n\nclass ForwardValue(AutogradValue):\n    def __init__(self, *args):\n        super().__init__(*args)\n        self.forward_grads = {self: 1}\n\nWe see now that our forward_pass method needs to update forward_grads (e.g. to compute \\(\\frac{dg}{da}\\) and \\(\\frac{dg}{db}\\)) using the forward_grads values of its parents (e.g. \\(\\frac{dc}{da}\\) and \\(\\frac{dc}{db}\\))\n\nQ10\nImplement the forward_pass method below for forward-mode automatic differentiation. This method should update the foward_grads property of the operation such that: - foward_grads has an entry for every input that appears in foward_grads of any parent operation. - If an input appears in more than 1 parent, make sure to add the gradients appropritately (if g has parents b and c then \\(\\frac{dg}{da} = \\frac{dg}{db}\\frac{db}{da} + \\frac{dg}{dc}\\frac{dc}{da}\\) ) - Parents that are not AutogradValue objects are ignored\n\ndef forward_pass(self):\n    # Clear forward_grads if it exists\n    self.forward_grads = {} \n    ## YOUR CODE HERE\n    \n\n    # Make sure to still return the operation's value\n    return self.func(*self.args)\n\n# Overwrite the AutogradValue method so that operators still work\nAutogradValue.forward_pass = forward_pass\n\nWe can now take derivates of functions much like with our reverse-mode implementation!\n\na = ForwardValue(5)\nb = ForwardValue(2)\n\nL = -log(5 *b + a)\nprint(L.forward_grads[a], L.forward_grads[b])\n\n\n\nQ11\nComplete the ForwardModeNeuralNetwork class that inherits from NeuralNetwork by implementing nll_and_grad to use the forward-mode implementation you just wrote!\n\nclass ForwardModeNeuralNetwork(NeuralNetwork):\n    def nll_and_grad(self, X, y):\n        ## YOUR CODE HERE\n\nWe can again test it on our tiny dataset.\n\nX, y = make_moons(100, noise=0.1)\nmodel = ForwardModeNeuralNetwork(2, [5, 5])\ngradient_descent(model, X, y, lr=3e-2, steps=250)\n\nprint('Model accuracy: %.3f' % model.accuracy(X, y))\nplot_boundary(model, X, y)\n\n\n\nQ12\nBased on what we’ve learned and your experience here, why would we choose forward or reverse-mode automatic differentiation?\nYOUR ANSWER HERE"
  },
  {
    "objectID": "lecture6-backpropagation/notes.html#implementing-reverse-mode-automatic-differentiation",
    "href": "lecture6-backpropagation/notes.html#implementing-reverse-mode-automatic-differentiation",
    "title": "Lecture 6: Deep neural networks and backpropagation",
    "section": "Implementing reverse-mode automatic differentiation",
    "text": "Implementing reverse-mode automatic differentiation\nWe’ll start by developing an automatic differentiation class that uses reverse-mode automatic differentiation, as this is what will be most useful for neural networks.\nRecall that for reverse-mode AD to work, everytime we perform an operation on one or more numbers we need to store the result of that operation as well as the parent values (the inputs to the operation). We also need to be able to compute the derivative of that operation. Since for every operation we need to store several pieces of data and several functions, it makes sense to define a class to represent the result of an operation.\nFor example, if we want to make a class that represents the operation c=a+b our class needs several properties:\n\nvalue: The value of the operation (c)\nparents: The parent operations (a and b)\ngrad: The derivative of the final loss with respect to c (\\(\\frac{dL}{dc}\\))\nfunc: A function that computes the operation (a+b)\ngrads: A function that computes the derivatives of the operation (\\(\\frac{dc}{da}\\) and \\(\\frac{dc}{db}\\))\n\nFor this example, we’ll call our class AutogradValue. This will be the base class for all of our possible operations and represents declaring a variable with a value (a = 5). This is useful because it lets us define values that we might want to find derivatives with respect to.\nLet’s see how this will work in practice. If we want to take derivatives we will first define the inputs using AutogradValue.\n\na = AutogradValue(5)\nb = AutogradValue(2)\n\nThen we can perform whatever operations we want on these inputs:\n\nc = a + b\nL = log(c)\n\nEach of these operations will produce a new AutogradValue object representing the result of that operation.\nFinally we can run the backward pass by running a method backward() (that we will write) on the outout L. This will compute the gradients of L with respect to each input that we defined (\\(\\frac{dL}{da}\\) and \\(\\frac{dL}{db}\\)). Rather than returning these derivatives, the backward() method will update the grad property of a and b, making it easy to access the correct derivative.\n\nL.backward()\ndL_da = a.grad\n\nWe’ll also be able to compute operations with non-AutogradValue numbers, but obviously won’t be able to compute derivaitives with respect to these values.\n\ns = 4\nL = s * a\ndL_da = a.grad # Will work because a is an AutogradValue\ndL_ds = s.grad # Will give an error because s is not an AutogradValue\n\nLet’s look at one possible implementation for AutogradValue:\n\nclass AutogradValue:\n    '''\n    Base class for automatic differentiation operations. \n    Represents variable delcaration. Subclasses will overwrite \n    func and grads to define new operations.\n\n    Properties:\n        parents (list):  A list of the inputs to the operation, \n                         may be AutogradValue or float\n        args    (list):  A list of raw values of each \n                         input (as floats)\n        grad    (float): The derivative of the final loss with \n                         respect to this value (dL/da)\n        value   (float): The value of the result of this operation\n    '''\n\n    def __init__(self, *args):\n        self.parents = list(args)\n        self.args = [arg.value if isinstance(arg, AutogradValue) \n                     else arg \n                     for arg in self.parents]\n        self.grad = 0.\n        self.value = self.forward_pass()\n\n    def forward_pass(self):\n        # Calls func to compute the value of this operation \n        return self.func(*self.args)\n\nFor convenience, in this implementation we’ve also defined a property args which simply stores the value of each parent. Note that we can also allow parents to be either be AutogradValue or a primitive data type like float. This will allow us to do things like multiply an AutogradValue variable with a float, e.g. a * 5.\nThe forward_pass function computes the actual value of the node given it’s parents. This will depend on what kind of operation we’re doing (addition, subtraction, multiplication, etc.), so we’ll define a func method that we can override that does the actual calculation. In the base case we’ll just directly assign the value:\n\nclass AutogradValue:\n    def func(self, input):\n        '''\n        Compute the value of the operation given the inputs.\n        For declaring a variable, this is just the identity \n        function (return the input).\n\n        Args:\n            input (float): The input to the operation\n        Returns:\n            value (float): The result of the operation\n        '''\n        return input\n\nIn subclasses we’ll override func:\n\nclass _square(AutogradValue):\n    # Square operator (a ** 2)\n    def func(self, a):\n        return a ** 2\n\nclass _mul(AutogradValue):\n    # Multiply operator (a * b)\n    def func(self, a, b):\n        return a * b\n\nLet’s consider what the computational graph will look like for the following code:\n\nx = AutogradValue(2)\na = x ** 2\nb = 5 * a\nc = b * a\nL = -c\n\n\nHere the blue nodes represent constants (of type float), while the black nodes are AutogradValue objects. We see that every AutogradValue is populated with the value, parents and args, but that after this first pass grad is still 0 for each object, so we have not computed \\(\\frac{dL}{dx}\\). To do so, we need to run the backward pass by calling L.backward().\n\nL.backward()\nprint('dL_dx', x.grad)\n\nIn the backward pass, each node needs to update the grad property of its parents.\n\nSo first \\(L\\) needs to update the grad property for \\(c\\), which represents \\(\\frac{dL}{dc}\\). Then \\(c\\) is able to update the grad property of \\(a\\) and \\(b\\) (\\(\\frac{dL}{da}\\) and \\(\\frac{dL}{db}\\)).\nNote that the order we perform these updates matters! \\(\\frac{dL}{da}\\) will not be correct until both \\(b\\) and \\(c\\) have updated \\(a\\), thus \\(a\\) cannot perform the update to \\(\\frac{dL}{dx}\\) until both \\(b\\) and \\(c\\) have updated \\(a\\).\nFor each operation, we see that we also need to be able to compute the appropriate local derivatives of the value with respect to each input. For instance \\(a\\) needs to be able to compute \\(\\frac{da}{dx}\\) , \\(c\\) needs to be able to compute \\(\\frac{dc}{da}\\) and \\(\\frac{dc}{db}\\). We will definite another method grads that can compute these values for a given operation and override it for each subclass. Since grads might need to compute multiple derivatives (as for multiplication or addition) we’ll have it return a tuple.\n\nclass AutogradValue:\n        def grads(self, *args):\n        '''\n        Compute the derivative of the operation with respect to each input.\n        In the base case the derivative of the identity function is just 1. (da/da = 1).\n\n        Args:\n            input (float): The input to the operation\n        Returns:\n            grads (tuple): The derivative of the operation with respect to each input\n                            Here there is only a single input, so we return a length-1 tuple.\n        '''\n        return (1,)\n      \nclass _square(AutogradValue):\n    # Square operator (a ** 2)\n    def grads(self, a):\n        return (2 * a,)\n\nclass _mul(AutogradValue):\n    # Multiply operator (a * b)\n    def grads(self, a, b):\n        return (b, a)\n\nWith this in hand we can write a function that performs the backward update for a given operation. We’ll call this method backward_pass."
  },
  {
    "objectID": "assignments/homework3-logistic-regression/solutions.html",
    "href": "assignments/homework3-logistic-regression/solutions.html",
    "title": "Homework 3: Logistic regression and feature transforms",
    "section": "",
    "text": "In this homework, we will use the following convention for dimentionality:\n\\(N:\\quad\\text{Number of observations in a dataset, so } \\mathcal{D} = \\{ (\\mathbf{x}_1, y_1),\\ (\\mathbf{x}_2, y_2),\\ ... \\,(\\mathbf{x}_N, y_N) \\}\\)\n\\(d:\\quad\\ \\text{Dimension of input (number of features), so } \\mathbf{x}_i \\in \\mathbb{R}^d\\)\n\\(C: \\quad\\ \\text{Number of classes, so } y_i \\in \\{1,...,C\\}\\)\n\n# Run me first!\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom hw2_code_backup import get_dataset, gradient_descent, test_nll, test_nll_grad, test_predict, test_predict_probability, test_softmax, test_split\n\n# UNCOMMENT IF NEEDED, SEE Q10\nimport autograd.numpy as np\n\n\nBackground\nIn class we derived the logistic regression model for making predictions on binary data. Recall the the prediction function for logistic regression can be written as:\n\\[f(\\mathbf{x}) = \\mathbb{I}(\\mathbf{x}^T\\mathbf{w} \\geq 0)\\]\nThe estimated probability of \\(y=1\\) as: \\[p(y=1\\mid \\mathbf{x}, \\mathbf{w}) = \\sigma(\\mathbf{x}^T\\mathbf{w})\\]\nAlso recall that the negative log-likelihood loss for logistic regression can be written as:\n\\[\\mathbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})=-\\sum_{i=1}^N \\log \\sigma\\big((2 y_i - 1)\\mathbf{x}_i^T\\mathbf{w}\\big)\\]\nand it’s gradient with respect to \\(\\mathbf{w}\\) is: \\[\\nabla_{\\mathbf{w}}\\mathbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})=-\\sum_{i=1}^N \\big(1 - \\sigma((2 y_i - 1)\\mathbf{x}_i^T\\mathbf{w})\\big) \\big(2 y_i - 1\\big)\\mathbf{x}_i\\]\nBelow is an implementation of logistic regression using the functions we derived in class. In this example, we’ve created a logistic regression class that encapsulates the weights along with all of the functions we need to train and make predictions with the model.\n\ndef linear_function(X, w):\n    # Returns a linear function of X (and adds bias)\n    X = np.pad(X, ((0,0), (0,1)), constant_values=1.)\n    return np.dot(X, w)\n\ndef sigmoid(x):\n    # Computes the sigmoid function\n    return 1 / (1 + np.exp(-x))\n\nclass LogisticRegression:\n    def __init__(self, dims):\n        '''\n        Args:\n            dims (int): d, the dimension of each input\n        '''\n        self.weights = np.zeros((dims + 1,))\n\n    def predict(self, X):\n        '''\n        Predict labels given a set of inputs.\n\n        Args:\n            X (array): An N x d matrix of observations.\n        Returns:\n            pred (int array): A length N array of predictions in {0, 1}\n        '''\n        return (linear_function(X, self.weights) &gt; 0).astype(int)\n    \n    def predict_probability(self, X):\n        '''\n        Predict the probability of each class given a set of inputs\n\n        Args:\n            X (array): An N x d matrix of observations.\n        Returns:\n            probs (array): A length N vector of predicted class probabilities\n        '''\n        return sigmoid(linear_function(X, self.weights))\n\n    def accuracy(self, X, y):\n        '''\n        Compute the accuracy of the model's predictions on a dataset\n\n        Args:\n            X (array): An N x d matrix of observations.\n            y (array): A length N vector of labels.\n        Returns:\n            acc (float): The accuracy of the classifier\n        '''\n        return np.mean(self.predict(X) == y)\n\n    def nll(self, X, y):\n        '''\n        Compute the negative log-likelihood loss.\n\n        Args:\n            X (array): An N x d matrix of observations.\n            y (int array): A length N vector of labels.\n        Returns:\n            nll (float): The NLL loss\n        '''\n        xw = linear_function(X, self.weights)\n        py = sigmoid((2 * y - 1) * xw)\n        return -np.sum(np.log(py))\n    \n    def nll_gradient(self, X, y):\n        '''\n        Compute the gradient of the negative log-likelihood loss.\n\n        Args:\n            X (array): An N x d matrix of observations.\n            y (array): A length N vector of labels.\n        Returns:\n            grad (array): A length (d + 1) vector with the gradient\n        '''\n        xw = linear_function(X, self.weights)\n        py = sigmoid((2 * y - 1) * xw)\n        grad = ((1 - py) * (2 * y - 1)).reshape((-1, 1)) * np.pad(X, [(0,0), (0,1)], constant_values=1.)\n        return -np.sum(grad, axis=0)\n    \n    def nll_and_grad(self, X, y):\n        '''\n        Compute both the NLL and it's gradient\n\n        Args:\n            X (array): An N x d matrix of observations.\n            y (array): A length N vector of labels.\n        Returns:\n            nll (float): The NLL loss\n            grad (array): A length (d + 1) vector with the gradient\n        '''\n        return self.nll(X, y), self.nll_gradient(X, y)\n\n\nLet’s take a look at how to use this class. The provided code includes a function get_dataset that downloads and loads one of serval different datasets. For this first example, we will use the humans and horses dataset, a dataset of images of humans and horses. We can load the dataset as follows:\n\nimages, labels, label_names = get_dataset('horses_and_humans')\n\n\n\n\nAs we saw in class, before we can use logistic regression on image data, we first need to reshape it from a 3-dimensional array into a 2-dimensional matrix:\n\nimage_shape = images[0].shape                # Keep track of the original image shape\nX = images.reshape((images.shape[0], -1))    # Reshape into an N x d matrix X\ny = labels\nprint('Image shape: ', images.shape, ', X shape: ', X.shape)\n\nImage shape:  (1027, 48, 48) , X shape:  (1027, 2304)\n\n\nWe can create a model using the LogisticRegression class, specifying the number of features (\\(d\\)):\n\nmodel = LogisticRegression(X.shape[1])\n\nWe can train the model using the gradient_descent function provided in the support code:\n\nlosses = gradient_descent(model, X, y, lr=1e-6, steps=2500, image_shape=image_shape, watch=False)\n\n# Uncomment to run with a live visualization\n# losses = gradient_descent(model, X, y, lr=1e-6, steps=500, image_shape=images[0].shape, watch=True)\n\nLoss 711.86, accuracy: 0.57:   0%|          | 0/2500 [00:00&lt;?, ?it/s]Loss 358.28, accuracy: 0.86: 100%|██████████| 2500/2500 [00:48&lt;00:00, 51.11it/s]\n\n\nWe can make predictions using the built-in methods:\n\nprediction = model.predict(X)\nprobabilities = model.predict_probability(X)\n\n# Get the probability of the prediction [p(y=1) if prediction is 1 otherwise p(y=0)]\nprobability_of_prediction = np.where(prediction, probabilities, 1 - probabilities)\n\n# Show an image and the corresponding prediction\nplt.imshow(X[0].reshape(image_shape), cmap='gray')\nprint('Prediction: %s, probability: %.3f' % (label_names[prediction[0]], probability_of_prediction[0]))\n\nPrediction: human, probability: 0.641\n\n\n\n\n\n\n\nPart 1: Logistic regression and feature transforms\nWe’ll first evaluate the performance of the logistic regression model above.\n\nQ1: Train and test splits\nWrite a function to split the provided dataset into a train set and a test set. The train set should include 70% of the observations and the test set should include the remaining 30%. The data should be randomly shuffled to make sure there is no bias in the ordering.\n\ndef split_data(X, y):\n    inds = np.arange(X.shape[0])\n    np.random.shuffle(inds)\n    Xtrain, Xtest = X[inds[:700]], X[inds[700:]]\n    ytrain, ytest = y[inds[:700]], y[inds[700:]]\n\n    return Xtrain, ytrain, Xtest, ytest\n\n# Test the function\nXtrain, ytrain, Xtest, ytest = split_data(X, y)\ntest_split(X, y, Xtrain, ytrain, Xtest, ytest)\n\nPassed!\n\n\n\n\nQ2: Model evaluation\nUsing the function you just wrote, train a new logistic regression model on just the training data. Evaluate the accuracy and loss of the trained model on both the training data and the test data.\n\n## YOUR CODE HERE\nnp.random.seed(10)\nXtrain, ytrain, Xtest, ytest = split_data(X, y)\nmodel = LogisticRegression(X.shape[1])\nlosses = gradient_descent(model, Xtrain, ytrain, lr=1e-6, steps=2500, image_shape=image_shape, watch=False)\ntrain_acc, train_loss = model.accuracy(Xtrain, ytrain), model.nll(Xtrain, ytrain)\ntest_acc, test_loss = model.accuracy(Xtest, ytest), model.nll(Xtest, ytest)\n\nprint('Training accuracy: %.3f, loss: %.3f' % (train_acc, train_loss))\nprint('Test accuracy: %.3f, loss: %.3f' % (test_acc, test_loss))\n\nLoss 254.93, accuracy: 0.86: 100%|██████████| 2500/2500 [00:36&lt;00:00, 67.96it/s]\n\n\nTraining accuracy: 0.863, loss: 254.908\nTest accuracy: 0.813, loss: 141.683\n\n\nRecall that in class we dicussed feature transforms an easy way to get more expressive models, using our linear model tools. Here we’ll try applying some basic feature transforms to this problem and see if we can improve the performance.\n\n\nQ3: Quadratic feature transforms\nCreate a transformed versions of the training and test datasets by adding quadratic features. Only add the unary quadratic terms (\\(x_i^2\\)) not the cross terms (\\(x_i x_j\\)). For a single dimension the transform would look like: \\[\\phi(x_i) = \\begin{bmatrix} x_i \\\\ x_i^2 \\end{bmatrix}\\]\nIn general, the transform should look like:\n\\[\\textbf{Single observation: }\\phi(\\mathbf{x}) = \\begin{bmatrix}x_1 \\\\ \\vdots \\\\ x_d \\\\ x_1^2 \\\\ \\vdots \\\\ x_d^2 \\end{bmatrix}, \\quad \\textbf{Dataset: } \\phi(\\mathbf{X}) = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1d} & x_{11}^2 & \\dots & x_{1d}^2 \\\\ x_{21} & x_{22} & \\dots & x_{2d} & x_{21}^2 & \\dots & x_{2d}^2 \\\\  \\vdots & \\vdots & & \\vdots & \\vdots & & \\vdots \\\\ x_{N1} & x_{N2} & \\dots & x_{Nd} & x_{N1}^2 & \\dots & x_{Nd}^2 \\\\  \\end{bmatrix} \\]\n\n## YOUR CODE HERE\nXtrain_quad = np.concatenate([Xtrain, Xtrain ** 2], axis=1)\nXtest_quad = np.concatenate([Xtest, Xtest ** 2], axis=1)\n\nassert Xtrain_quad.shape == (Xtrain.shape[0], 2 * Xtrain.shape[1])\n\n\n\nQ4: Evaluating quadratic transforms\nTrain a new logistic regression model and evaluate the training and test accuracy and loss as you did in question 2.\n\n## YOUR CODE HERE\nmodel = LogisticRegression(Xtrain_quad.shape[1])\nlosses = gradient_descent(model, Xtrain_quad, ytrain, lr=1e-6, steps=2500, image_shape=image_shape, watch=False)\ntrain_acc, train_loss = model.accuracy(Xtrain_quad, ytrain), model.nll(Xtrain_quad, ytrain)\ntest_acc, test_loss = model.accuracy(Xtest_quad, ytest), model.nll(Xtest_quad, ytest)\n\n\nprint('Training accuracy: %.3f, loss: %.3f' % (train_acc, train_loss))\nprint('Test accuracy: %.3f, loss: %.3f' % (test_acc, test_loss))\n\nLoss 190.14, accuracy: 0.92: 100%|██████████| 2500/2500 [01:00&lt;00:00, 41.39it/s]\n\n\nTraining accuracy: 0.920, loss: 190.113\nTest accuracy: 0.853, loss: 109.164\n\n\n\n\nQ5: Evaluating sin transforms\nRepeat questions 3 & 4, but using a different transform, defined as:\n\\[\\phi(x_i) = \\begin{bmatrix} x_i \\\\ \\sin(10 x_i) \\end{bmatrix}\\]\n\n## YOUR CODE HERE\nXtrain_sin = np.concatenate([Xtrain, np.sin(10 * Xtrain)], axis=1)\nXtest_sin = np.concatenate([Xtest, np.sin(10 * Xtest)], axis=1)\n\nassert Xtrain_sin.shape == (Xtrain.shape[0], 2 * Xtrain.shape[1])\n\nmodel = LogisticRegression(Xtrain_sin.shape[1])\nlosses = gradient_descent(model, Xtrain_sin, ytrain, lr=1e-6, steps=2500, image_shape=image_shape, watch=False)\ntrain_acc, train_loss = model.accuracy(Xtrain_sin, ytrain), model.nll(Xtrain_sin, ytrain)\ntest_acc, test_loss = model.accuracy(Xtest_sin, ytest), model.nll(Xtest_sin, ytest)\n\n\nprint('Training accuracy: %.3f, loss: %.3f' % (train_acc, train_loss))\nprint('Test accuracy: %.3f, loss: %.3f' % (test_acc, test_loss))\n\n  0%|          | 0/2500 [00:00&lt;?, ?it/s]Loss 136.40, accuracy: 0.98: 100%|██████████| 2500/2500 [00:59&lt;00:00, 42.21it/s]\n\n\nTraining accuracy: 0.979, loss: 136.363\nTest accuracy: 0.844, loss: 121.323\n\n\n\n\nQ6: Comparing feature transforms\nBased on the results, would you use any feature transform for this problem? If so, which one?\nYOUR ANSWER HERE\n\n\n\nPart 2: Multinomial logistic regression\nIn this part, we will look at implementing multinomial logistic regression. Recall that this model extends logistic regression to the cases where there may be more than 2 possible labels, so \\(y\\in\\{1,...,C\\}\\), where \\(C\\) is the number of classes (possible outputs).\nWe saw that rather than having a single weight vector, this model has a weight vector for each class, \\(\\mathbf{w}_1,...,\\mathbf{w}_C\\). We can view these together as the rows of a weight matrix \\(\\mathbf{W}\\): \\[\\mathbf{W} = \\begin{bmatrix} \\mathbf{w}_1^T \\\\ \\mathbf{w}_2^T \\\\ \\vdots \\\\ \\mathbf{w}_C^T \\end{bmatrix}\\]\nWe saw that the prediction function for this model was: \\[f(\\mathbf{x}) = \\underset{c\\in \\{1,...,C\\}}{\\text{argmax}}\\ \\mathbf{x}^T\\mathbf{w}\\]\nThe probabilistic model was defined as: \\[\np(y_i=c \\mid \\mathbf{x}, \\mathbf{W}) = \\text{softmax}(\\mathbf{x}^T\\mathbf{W})_c=\\frac{e^{\\mathbf{x}^T\\mathbf{w}_c}}{\\sum_{j=1}^Ce^{\\mathbf{x}^T\\mathbf{w}_j}}\n\\]\nThe negative log-likelihood loss was defined as: \\[\n\\textbf{NLL}(\\mathbf{W}, \\mathbf{X}, \\mathbf{y})=-\\sum_{i=1}^N \\bigg(\\mathbf{x}_i^T\\mathbf{w}_{y_i}- \\log\\sum_{j=1}^Ce^{\\mathbf{x}_i^T\\mathbf{w}_{j}}\\bigg)\n\\]\nIn the next few questions we will create a modified version of our logistic regression class that supports multinomal logistic regression. The class definition is below. A few things to note:\n1: We will still assume y is an array of int in numpy. We can convert an array y to an array of int with y = y.astype(int) and back with y = y.astype(float)\n2: Remeber that numpy is 0-indexed, so our classes will actually be \\(0\\) to \\(C-1\\), (\\(y\\in \\{0,...,C-1\\}\\))\n2: We will assume our weight matrix is an \\(C \\times d\\) matrix as shown below\n\nclass MultinomialLogisticRegression(LogisticRegression):\n    def __init__(self, classes, dims):\n        '''\n        Args:\n            classes (int): C, the number of possible outputs\n            dims (int): d, the dimension of each input\n        '''\n        self.classes = classes\n        self.weights = np.zeros((classes, dims + 1,))\n\n\nQ7: Prediction\nWrite a function to make a prediction using the multinomial logistic regression prediction rule above. (assume self is the MultinomialLogisticRegression object)\n\ndef multiclass_predict(self, X):\n    '''\n    Predict labels given a set of inputs.\n\n    Args:\n        X (array): An N x d matrix of observations.\n    Returns:\n        pred (int array): A length N array of predictions in {0,...,(C-1)}\n    '''\n    W = self.weights\n\n    ## YOUR CODE HERE\n    pred = linear_function(X, self.weights.T).argmax(axis=-1)\n    return pred\n\n## Test the function\ntest_predict(multiclass_predict)\n\n## Add it to our class\nMultinomialLogisticRegression.predict = multiclass_predict\n\nPassed!\n\n\n\n\nQ7: Softmax\nImplement the softmax function. \\[\n\\text{softmax}(\\mathbf{x})_c = \\frac{e^{x_c}}{\\sum_{j=1}^Ce^{x_j}}, \\quad\n\\text{softmax}(\\mathbf{x}) = \\begin{bmatrix}\\frac{e^{x_1}}{\\sum_{j=1}^Ce^{x_j}} \\\\ \\frac{e^{x_2}}{\\sum_{j=1}^Ce^{x_j}} \\\\ \\vdots \\\\ \\frac{e^{x_C}}{\\sum_{j=1}^Ce^{x_j}} \\end{bmatrix}\n\\]\nYou function should accept inputs as either a length \\(C\\) vector or as an \\(N\\times C\\) matrix. If the input is a matrix, the softmax function should be applied to each row of the matrix.\n\ndef softmax(x):\n    '''\n    Apply the softmax function to a vector or matrix\n\n    Args:\n        X (array): An N x C matrix of transformed inputs (or a length C vector)\n    Returns:\n        probs (array):  An N x C matrix with the softmax function applied to each row\n    ''' \n    ex = np.exp(x)\n    return ex / np.sum(ex, axis=-1, keepdims=True)\n\n\ntest_softmax(softmax)\n\nPassed!\n\n\n\n\nQ7: Multinomial logistic regression NLL\nImplement a function to compute the multinomial logistic regression negative log-likelihood. \\[\n\\textbf{NLL}(\\mathbf{W}, \\mathbf{X}, \\mathbf{y})=-\\sum_{i=1}^N \\bigg(\\mathbf{x}_i^T\\mathbf{w}_{y_i}- \\log\\sum_{j=1}^Ce^{\\mathbf{x}_i^T\\mathbf{w}_{j}}\\bigg)\n\\]\nHint: Recall that \\(y_i\\) is an integer, so \\(\\mathbf{w}_{j}\\) refers to the row of the weight matrix at index \\(y_i\\) (you could access this as W[y[i]]). It’s possible to answer this question without loops, but you may find it easier to loop over each possible class/observation.\n\ndef nll(self, X, y):\n    '''\n    Compute the negative log-likelihood loss.\n\n    Args:\n        X (array): An N x d matrix of observations.\n        y (int array): A length N vector of labels.\n    Returns:\n        nll (float): The NLL loss\n    '''\n    xw = linear_function(X, self.weights.T)\n    loss = xw[np.arange(X.shape[0]), y].sum() - np.log(np.sum(np.exp(xw), axis=1)).sum()\n    return -loss\n\ntest_nll(nll)\nMultinomialLogisticRegression.nll = nll\n\nPassed!\n\n\n\n\nQ8: Gradient of NLL\nDerive the gradient of the negative log-likelihood with respect to \\(\\mathbf{w}_c\\), the weight vector for a single class.\nHint: Again note that \\(\\mathbf{w}_{y_i}\\) refers to the weight vector corresponding to the true class of observation \\(i\\), and so only depend on \\(\\mathbf{w}_c\\) if \\(y_i=c\\). This means that: \\[\\frac{d}{d\\mathbf{w}_c} \\mathbf{x_i}^T \\mathbf{w}_{y_i} = \\begin{cases}\\mathbf{x}_i \\quad \\text{ if } y_i = c \\\\ 0 \\quad \\ \\text{ otherwise}  \\end{cases}\\] We can write this more compactly using an indicator function: \\[\\frac{d}{d\\mathbf{w}_c} \\mathbf{x_i}^T \\mathbf{w}_{y_i} = \\mathbb{I}(y_i=c)\\ \\mathbf{x}_i \\]\n\\[\\nabla_{\\mathbf{w}_c} \\textbf{NLL}(\\mathbf{W}, \\mathbf{X}, \\mathbf{y}) = \\frac{d}{d\\mathbf{w}_c}-\\sum_{i=1}^N \\bigg(\\mathbf{x}_i^T\\mathbf{w}_{y_i}- \\log\\sum_{j=1}^Ce^{\\mathbf{x}_i^T\\mathbf{w}_{j}}\\bigg)\\]\nThe first step is to apply the addition/subtraction rule:\n\\[= -\\sum_{i=1}^N \\bigg(\\frac{d}{d\\mathbf{w}_c}\\big(\\mathbf{x}_i^T\\mathbf{w}_{y_i}\\big)- \\frac{d}{d\\mathbf{w}_c}\\log\\sum_{j=1}^Ce^{\\mathbf{x}_i^T\\mathbf{w}_{j}}\\bigg)\\]\nThen we can apply the rule we see above: \\[= -\\sum_{i=1}^N \\bigg(\\mathbb{I}(y_i=c)\\ \\mathbf{x}_i- \\frac{d}{d\\mathbf{w}_c}\\log\\sum_{j=1}^Ce^{\\mathbf{x}_i^T\\mathbf{w}_{j}}\\bigg)\\] Next we’ll apply the chain rule to the \\(\\log\\) function: \\[= -\\sum_{i=1}^N \\bigg(\\mathbb{I}(y_i=c)\\ \\mathbf{x}_i- \\bigg(\\frac{1}{\\sum_{j=1}^Ce^{\\mathbf{x}_i^T\\mathbf{w}_{j}}}\\bigg)\\ \\frac{d}{d\\mathbf{w}_c} \\bigg(\\sum_{j=1}^Ce^{\\mathbf{x}_i^T\\mathbf{w}_{j}}\\bigg) \\bigg)\\] We see that only one term in the inner summation depends on \\(\\mathbf{w}_c\\) \\[= -\\sum_{i=1}^N \\bigg(\\mathbb{I}(y_i=c)\\ \\mathbf{x}_i- \\bigg(\\frac{1}{\\sum_{j=1}^Ce^{\\mathbf{x}_i^T\\mathbf{w}_{j}}}\\bigg)\\  \\bigg(\\frac{d}{d\\mathbf{w}_c}e^{\\mathbf{x}_i^T\\mathbf{w}_{c}}\\bigg) \\bigg)\\] Applying the chain rule to the exponential we get: \\[= -\\sum_{i=1}^N \\bigg(\\mathbb{I}(y_i=c)\\ \\mathbf{x}_i- \\bigg(\\frac{1}{\\sum_{j=1}^Ce^{\\mathbf{x}_i^T\\mathbf{w}_{j}}}\\bigg)\\  \\bigg(e^{\\mathbf{x}_i^T\\mathbf{w}_{c}} \\frac{d}{d\\mathbf{w}_c} \\mathbf{x}_i^T\\mathbf{w}_{c}\\bigg) \\bigg)\\] \\[= -\\sum_{i=1}^N \\bigg(\\mathbb{I}(y_i=c)\\ \\mathbf{x}_i- \\bigg(\\frac{1}{\\sum_{j=1}^Ce^{\\mathbf{x}_i^T\\mathbf{w}_{j}}}\\bigg)\\  \\bigg(e^{\\mathbf{x}_i^T\\mathbf{w}_{c}}  \\mathbf{x}_i\\bigg) \\bigg)\\] Factoring out \\(\\mathbf{x}_i\\) we get: \\[= -\\sum_{i=1}^N  \\mathbf{x}_i\\bigg(\\mathbb{I}(y_i=c) - \\frac{e^{\\mathbf{x}_i^T\\mathbf{w}_{c}}  }{\\sum_{j=1}^Ce^{\\mathbf{x}_i^T\\mathbf{w}_{j}}} \\bigg)\\]\n\n\nQ9: Implementing the gradient\nWrite a function that computes the gradient of the negative log-likelihood with repect to the weight vector for a given class, using the results of the derivation above.\n\ndef nll_gradient_c(W, X, y, c):\n    '''\n    Compute the negative log-likelihood loss.\n\n    Args:\n        W (array): The C x d weight matrix.\n        X (array): An N x d matrix of observations.\n        y (int array): A length N vector of labels.\n        c (int): The class to compute the gradient for\n    Returns:\n        grad (array): A length d vector representing the gradient with respect to w_c\n    '''\n    X = np.pad(X, [(0, 0), (0, 1)], constant_values=1.)\n    grad = 1 / np.exp(np.dot(X, W.T)).sum(axis=-1)\n\n    gradc = - X[y == c].sum(axis=0)\n    wc = W[:, c]\n    exwc = np.exp(np.dot(X, wc))\n    gradc = gradc + (grad * exwc).dot(X)\n    return gradc\n\n\n\n\nQ10: Implementing the full gradient\nUsing the function you just wrote, write a function to compute the full gradient with respect to the \\(C \\times d\\) weight matrix. Hint: The output should be a matrix!\n\ndef nll_gradient(self, X, y):\n    '''\n    Compute the negative log-likelihood loss.\n\n    Args:\n        X (array): An N x d matrix of observations.\n        y (int array): A length N vector of labels.\n    Returns:\n        grad (array): A C x d matrix representing the gradient with respect to W\n    '''\n    X = np.pad(X, [(0, 0), (0, 1)], constant_values=1.)\n    grad = 1 / np.exp(np.dot(X, self.weights.T)).sum(axis=-1)\n\n    grads = []\n    for c in range(self.classes):\n        gradc = - X[y == c].sum(axis=0)\n        wc = self.weights[c]\n        exwc = np.exp(np.dot(X, wc))\n        gradc = gradc + (grad * exwc).dot(X)\n        grads.append(gradc)\n    return np.stack(grads)\n\n\ntest_nll_grad(nll_gradient)\nMultinomialLogisticRegression.nll_gradient = nll_gradient\n\nPassed!\n\n\nNote if you are struggling with this problem, you can uncomment the following cell and the import statement at the top of this notebook to get a valid gradient function based on you nll function.\n\n'''\ndef autograd_nll_gradient(self, X, y):\n    import autograd\n    def autograd_nll(W):\n        temp_model = MultinomialLogisticRegression(self.classes, X.shape[1])\n        temp_model.weights = W\n        return temp_model.nll(X, y)\n    return autograd.grad(autograd_nll)(self.weights)\n#test_nll_grad(nll_gradient)\ntest_nll_grad(autograd_nll_gradient)\nMultinomialLogisticRegression.nll_gradient = autograd_nll_gradient\n'''\n\n'\\ndef autograd_nll_gradient(self, X, y):\\n    import autograd\\n    def autograd_nll(W):\\n        temp_model = MultinomialLogisticRegression(self.classes, X.shape[1])\\n        temp_model.weights = W\\n        return temp_model.nll(X, y)\\n    return autograd.grad(autograd_nll)(self.weights)\\n#test_nll_grad(nll_gradient)\\ntest_nll_grad(autograd_nll_gradient)\\nMultinomialLogisticRegression.nll_gradient = autograd_nll_gradient\\n'\n\n\nFinally, we will test out our multinomial logistic regression classifier on the MNIST dataset (https://en.wikipedia.org/wiki/MNIST_database), one of the most popular datasets in machine learning! We’ll start by loading it as before.\n\nimages, labels, label_names = get_dataset('mnist')\n\nimage_shape = images[0].shape                # Keep track of the original image shape\nX = images.reshape((images.shape[0], -1))    # Reshape into an N x d matrix X\ny = labels\nprint('Image shape: ', images.shape, ', X shape: ', X.shape)\n\n# Create the initial model\nmodel = MultinomialLogisticRegression(classes=len(label_names), dims=X.shape[1])\n\n\n\n\nImage shape:  (5000, 28, 28) , X shape:  (5000, 784)\n\n\n\n\nQ11: Repeat question 2 using the MNIST dataset\n\nXtrain, ytrain, Xtest, ytest = split_data(X, y)\n\nmodel = MultinomialLogisticRegression(classes=len(label_names), dims=X.shape[1])\nlosses = gradient_descent(model, Xtrain, ytrain, lr=1e-6, steps=2500, image_shape=image_shape, watch=False)\ntrain_acc, train_loss = model.accuracy(Xtrain, ytrain), model.nll(Xtrain, ytrain)\ntest_acc, test_loss = model.accuracy(Xtest, ytest), model.nll(Xtest, ytest)\n\nprint('Training accuracy: %.3f, loss: %.3f' % (train_acc, train_loss))\nprint('Test accuracy: %.3f, loss: %.3f' % (test_acc, test_loss))\n\nprint('Training accuracy: %.3f, loss: %.3f' % (train_acc, train_loss))\nprint('Test accuracy: %.3f, loss: %.3f' % (test_acc, test_loss))\n\nLoss 442.68, accuracy: 0.87: 100%|██████████| 2500/2500 [01:07&lt;00:00, 37.21it/s]\n\n\nTraining accuracy: 0.870, loss: 442.595\nTest accuracy: 0.845, loss: 3025.243\nTraining accuracy: 0.870, loss: 442.595\nTest accuracy: 0.845, loss: 3025.243\n\n\n\nfor i in range(10):\n    plt.imshow(model.weights[i, :-1].reshape((28, 28)))\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQ12: Repeat question 3 using the MNIST dataset\n\n## YOUR CODE HERE\nXtrain_quad = np.concatenate([Xtrain, Xtrain ** 2], axis=1)\nXtest_quad = np.concatenate([Xtest, Xtest ** 2], axis=1)\n\nassert Xtrain_quad.shape == (Xtrain.shape[0], 2 * Xtrain.shape[1])\nassert Xtrain_quad.shape == (Xtrain.shape[0], 2 * Xtrain.shape[1])\n\n\n\nQ13: Repeat question 4 using the MNIST dataset\n\n## YOUR CODE HERE\n\nmodel = MultinomialLogisticRegression(classes=len(label_names), dims=Xtrain_quad.shape[1])\n\nlosses = gradient_descent(model, Xtrain_quad, ytrain, lr=1e-6, steps=2500, image_shape=image_shape, watch=False)\ntrain_acc, train_loss = model.accuracy(Xtrain_quad, ytrain), model.nll(Xtrain_quad, ytrain)\ntest_acc, test_loss = model.accuracy(Xtest_quad, ytest), model.nll(Xtest_quad, ytest)\n\n\nprint('Training accuracy: %.3f, loss: %.3f' % (train_acc, train_loss))\nprint('Test accuracy: %.3f, loss: %.3f' % (test_acc, test_loss))\n\nLoss 414.28, accuracy: 0.89: 100%|██████████| 2500/2500 [01:16&lt;00:00, 32.50it/s]\n\n\nTraining accuracy: 0.887, loss: 414.195\nTest accuracy: 0.850, loss: 2944.114\n\n\n\n\nQ14: Repeat question 5 using the MNIST dataset\n\n## YOUR CODE HERE\nXtrain_sin = np.concatenate([Xtrain, np.sin(10 * Xtrain)], axis=1)\nXtest_sin = np.concatenate([Xtest, np.sin(10 * Xtest)], axis=1)\n\nassert Xtrain_sin.shape == (Xtrain.shape[0], 2 * Xtrain.shape[1])\n\nmodel = MultinomialLogisticRegression(len(label_names), Xtrain_sin.shape[1])\nlosses = gradient_descent(model, Xtrain_sin, ytrain, lr=1e-6, steps=2500, image_shape=image_shape, watch=False)\ntrain_acc, train_loss = model.accuracy(Xtrain_sin, ytrain), model.nll(Xtrain_sin, ytrain)\ntest_acc, test_loss = model.accuracy(Xtest_sin, ytest), model.nll(Xtest_sin, ytest)\n\n\nprint('Training accuracy: %.3f, loss: %.3f' % (train_acc, train_loss))\nprint('Test accuracy: %.3f, loss: %.3f' % (test_acc, test_loss))\n\nLoss 388.06, accuracy: 0.89: 100%|██████████| 2500/2500 [01:12&lt;00:00, 34.38it/s]\n\n\nTraining accuracy: 0.890, loss: 387.972\nTest accuracy: 0.850, loss: 2865.387\n\n\n\n\nQ15: Final evaluation\nBased on the results, would you use any feature transform for this problem? If so, which one?\nIn this case we see that the feature transform makes very little difference."
  },
  {
    "objectID": "lecture7-pytorch/notebook.html",
    "href": "lecture7-pytorch/notebook.html",
    "title": "Lecture 7: PyTorch",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons\nimport tqdm\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\ndef plot_boundary(model, X, y, alpha=1, title=''):\n    xrange = (-X[:, 0].min() + X[:, 0].max()) / 10\n    yrange = (-X[:, y].min() + X[:, y].max()) / 10\n    feature_1, feature_2 = np.meshgrid(\n        np.linspace(X[:, 0].min() - xrange, X[:, 0].max() + xrange, 250),\n        np.linspace(X[:, 1].min() - yrange, X[:, 1].max() + yrange, 250)\n    )\n    grid = np.vstack([feature_1.ravel(), feature_2.ravel()]).T\n    y_pred = np.reshape(model.predict(torch.tensor(grid).float()).detach().numpy(), feature_1.shape)\n    display = DecisionBoundaryDisplay(\n        xx0=feature_1, xx1=feature_2, response=y_pred\n    )\n    display.plot()\n    display.ax_.scatter(\n        X[:, 0], X[:, 1], c=y, alpha=alpha, edgecolor=\"black\"\n    )\n    plt.title(title)\n    plt.show()"
  },
  {
    "objectID": "lecture7-pytorch/notebook.html#introduction-to-pytorch",
    "href": "lecture7-pytorch/notebook.html#introduction-to-pytorch",
    "title": "Lecture 7: PyTorch",
    "section": "Introduction to PyTorch",
    "text": "Introduction to PyTorch\nThe most basic object in PyTorch is a tensor. Tensor objects behave much like the AutogradValue objects we are creating in the homework! We can create a tensor object with a given value as follows\n\nx = torch.tensor(4.)\nx\n\ntensor(4.)\n\n\nPerforming basic operations on tensor objects gives tensor objects.\n\na = x ** 2 + 5\na\n\ntensor(21.)\n\n\ntensor objects also support reverse-mode automatic differentiation! To use this, we must specify that we will want to compute the derivative with respect to a given tensor. We can do this with the requires_grad argument.\n\nx = torch.tensor(4., requires_grad=True)\n\nOnce we have a tensor that requires_grad, we can perform operations on it to compute a loss.\n\na = x ** 2 + 5\nL = torch.log(a) # Functions like log must be called through torch\nL\n\ntensor(3.0445, grad_fn=&lt;LogBackward0&gt;)\n\n\nOnce we have a loss running the backward pass is done exactly as in the homework. First we call backward() on the loss tensor object, then we can access the derivative through the grad property of x.\n\nL.backward()\nx.grad\n\ntensor(0.3810)\n\n\nWe can also create tensor objects that wrap arrays.\n\nx = torch.tensor(np.array([3, 4, 5]))\nx\n\ntensor([3, 4, 5])\n\n\nWe can also just directly create tensors as we would numpy arrays\n\nx = torch.tensor([3, 4, 5])\nx\n\ntensor([3, 4, 5])\n\n\nIncluding convienience constructors.\n\nprint(torch.ones((5,)))\nprint(torch.zeros((2, 3)))\n\ntensor([1., 1., 1., 1., 1.])\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n\nAutomatic differentiation still works for arrays. In this case it gives use the gradient of the loss (hence the grad property).\n\nx = torch.tensor([3., 4., 5.], requires_grad=True)\nL = torch.sum(x ** 2)\nL\n\ntensor(50., grad_fn=&lt;SumBackward0&gt;)\n\n\n\nL.grad\n\n/var/folders/t9/fjsr8h1x7c7cg4vqw_xhjq2w0000gn/T/ipykernel_79454/323164392.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:491.)\n  L.grad\n\n\n\nL.backward()\nx.grad\n\ntensor([ 6.,  8., 10.])\n\n\nWe can convert tensor objects back to numpy by calling x.detach().numpy(). (detach removes the variable from any automatic differentiation computations)\n\nx.detach().numpy()\n\narray([3., 4., 5.], dtype=float32)\n\n\nAt this point it’s probably worth remarking on where the name tensor comes from.\nSo far we’ve discussed 3 kinds of array objects - Scalars: which are just single values (0-dimensional) - Vectors: 1-dimensional arrays of numbers - Matrices: 2-dimensional arrays of numbers\n\n\n\nAlt text\n\n\nA tensor is the generalization of a vector or matrix to any number of dimensions. For example, a 3-dimensional tensor can be seen in multiple ways.\n\n\n\nAlt text\n\n\nA tensor object can be created with any number of dimensions. For example, we could create a 2x2x2 tensor as:\n\nt = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\nt\n\ntensor([[[1, 2],\n         [3, 4]],\n\n        [[5, 6],\n         [7, 8]]])\n\n\nOr we could create the tensor in the image using arange and reshape.\n\nt = torch.arange(30).reshape((3, 2, 5))\nt\n\ntensor([[[ 0,  1,  2,  3,  4],\n         [ 5,  6,  7,  8,  9]],\n\n        [[10, 11, 12, 13, 14],\n         [15, 16, 17, 18, 19]],\n\n        [[20, 21, 22, 23, 24],\n         [25, 26, 27, 28, 29]]])\n\n\n4-dimensional tensors can also be visualized\n\n\n\nAlt text\n\n\n\nt = torch.ones((3, 2, 4, 5))\nt.shape\n\ntorch.Size([3, 2, 4, 5])\n\n\nThere are some notable differences between torch and numpy when it comes to operations. The important one to watch out for at this point is matrix multiplation. In numpy we accomplished with with np.dot:\n\nx = np.ones((4, 5))\nw = np.ones((5, 2))\nnp.dot(x, w)\n\narray([[5., 5.],\n       [5., 5.],\n       [5., 5.],\n       [5., 5.]])\n\n\nIn PyTorch torch.dot only does vector dot products and thus only applies to 1-dimensional tensor objects:\n\nx = torch.ones((4, 5))\nw = torch.ones((5, 2))\ntorch.dot(x, w)\n\nRuntimeError: 1D tensors expected, but got 2D and 2D tensors\n\n\nInstead we use the torch.matmul function for this purpose\n\ntorch.matmul(x, w)\n\ntensor([[5., 5.],\n        [5., 5.],\n        [5., 5.],\n        [5., 5.]])\n\n\nPyTorch also has many handy built-in functions that numpy doesn’t have, such as sigmoid.\n\nx = torch.linspace(-5, 5, 50)\ns = torch.sigmoid(x)\nplt.plot(x, s)\n\n\n\n\nThis makes it very easy to implement something like logistic regression.\n\nclass LogisticRegression:\n    def __init__(self, dims):\n        self.weights = torch.ones((dims,), requires_grad=True)\n        self.bias = torch.zeros((), requires_grad=True)\n\n    def predict_probability(self, X):\n        f_X = torch.matmul(X, self.weights) + self.bias\n        return torch.sigmoid(f_X)\n\n    def predict(self, X):\n        return self.predict_probability(X) &gt; 0.5\n\nLet’s try loading a dataset, converting it to tensor and making predictions\n\nX, y = make_moons(noise=0.1)\nX, y = torch.tensor(X).float(), torch.tensor(y)\n\nmodel = LogisticRegression(2)\nplot_boundary(model, X, y)\n\n\n\n\nWhen working with PyTorch, it is convention to separate the loss function from the model, where the loss function will just take predictions and labels.\n\ndef NLL(pred, y):\n    LL = y * torch.log(pred) + (1. - y) * torch.log(1. - pred)\n    return -LL.sum()"
  },
  {
    "objectID": "lecture7-pytorch/notebook.html#gradient-descent",
    "href": "lecture7-pytorch/notebook.html#gradient-descent",
    "title": "Lecture 7: PyTorch",
    "section": "Gradient descent",
    "text": "Gradient descent\nGradient descent is also implemented in PyTorch in the optim module.\n\nfrom torch import optim\n\nGradient descent works a bit differently in PyTorch than what we’ve seen. We first need to construct a gradient descent object which specifies which values we’re optimizing and what the learning rate will be. We specify the values to optimize by simply passing a list of weights/parameters to the constructor.\nIn PyTorch, basic gradient descent is encapsulated in the optim.SGD class (SGD stands for stochastic gradient descent, we’ll talk about what stochastic means in this context next week.)\n\noptimizer = optim.SGD([model.weights, model.bias], lr=0.1)\n\nNotice that this object doesn’t even take in the function we’re trying to optimize, only the inputs. We need to call the function ourselves and run backward() to compute the gradients.\n\npredictions = model.predict_probability(X)\nloss = NLL(predictions, y)\nloss.backward()\n\nLet’s look at our model weights\n\nmodel.weights\nmodel.weights.grad\n\ntensor([-5.6526, 24.1355])\n\n\nWe can take a single step of gradient descent using the step method of the optimizer.\n\noptimizer.step()\nmodel.weights\n\ntensor([ 1.5653, -1.4135], requires_grad=True)\n\n\nWe see that this actually updates the weights themselves!\nIt’s important to note that in PyTorch, calling backward does not clear the value stored in grad. So computing the gradient multiple times will result in updates to the gradient.\n\nprint(model.weights.grad)\nNLL(model.predict_probability(X), y).backward()\nprint(model.weights.grad)\nNLL(model.predict_probability(X), y).backward()\nprint(model.weights.grad)\n\ntensor([-5.6526, 24.1355])\ntensor([-14.5242,  28.7704])\ntensor([-23.3958,  33.4053])\n\n\nWe can clear the stored gradients using the optimizer.\n\noptimizer.zero_grad()\nprint(model.weights.grad)\n\nNone\n\n\nSo far we’ve only taking a single step of gradient descent. In order to run many steps, we need to write a loop to do everything we just saw.\n\nfor i in range(10):\n    predictions = model.predict_probability(X)\n    loss = NLL(predictions, y)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    print(loss.item())\n\n45.934814453125\n40.60100555419922\n32.82542037963867\n30.3773136138916\n28.918834686279297\n28.118915557861328\n27.602989196777344\n27.248106002807617\n26.989551544189453\n26.794662475585938\n\n\nWe should now see that our model has been optimized!\n\nplot_boundary(model, X, y)"
  },
  {
    "objectID": "lecture7-pytorch/notebook.html#torch.nn",
    "href": "lecture7-pytorch/notebook.html#torch.nn",
    "title": "Lecture 7: PyTorch",
    "section": "torch.nn",
    "text": "torch.nn\nWhile PyTorch as a tool for automatic differentiation and optimization would be useful by itself. It actually gives us a lot more than that!\nOn of the most important features of PyTorch is its model-building tools in the torch.nn module. This gives us a lot of powerful features that we can use to build complex neural networks!\n\nfrom torch import nn\n\nLet’s start by building out logistic regression model in the torch.nn framwork. In order for a model to benefit from torch.nn our model class needs to inheret from nn.Module\n\nclass LogisticRegression(nn.Module):\n    def __init__(self, dims):\n        super().__init__()\n        self.weights = nn.Parameter(torch.ones((dims,)))\n        self.bias = nn.Parameter(torch.zeros(()))\n\n    def forward(self, X):\n        return torch.sigmoid(torch.matmul(X, self.weights) + self.bias)\n\n    def predict_probability(self, X):\n        return self.forward(X)\n\n    def predict(self, X):\n        return self.forward(X) &gt; 0.5\n\nThere are 2 changes to note here. The first is that we wrapped our weights and bias terms in nn.Parameter. This tells PyTorch that these are the parameters we will want to optimize. We don’t need to specify requires_grad for parameters, PyTorch will take care of that for us.\nThe second is that we moved the implmentation of predict_probability to forward. In PyTorch models the forward method is special, it defines the model as a function. If we call the model as a function forward will be called internally.\n\nmodel = LogisticRegression(2)\nmodel(X)\n\ntensor([0.5652, 0.7431, 0.8238, 0.5468, 0.8991, 0.5719, 0.8651, 0.7784, 0.6153,\n        0.7053, 0.7419, 0.3129, 0.4502, 0.5031, 0.4508, 0.7075, 0.7626, 0.4756,\n        0.4158, 0.4693, 0.7904, 0.7241, 0.2430, 0.5297, 0.7944, 0.8496, 0.6825,\n        0.6149, 0.6730, 0.5231, 0.6151, 0.6108, 0.5509, 0.7476, 0.6634, 0.8512,\n        0.8117, 0.7527, 0.5092, 0.7742, 0.8012, 0.7604, 0.6411, 0.3242, 0.2805,\n        0.4016, 0.6296, 0.3000, 0.7045, 0.7307, 0.7442, 0.8091, 0.5104, 0.8006,\n        0.6166, 0.5173, 0.6404, 0.5779, 0.8199, 0.9030, 0.7872, 0.6059, 0.8091,\n        0.9367, 0.5806, 0.7576, 0.8079, 0.3847, 0.4926, 0.4909, 0.8605, 0.6233,\n        0.5605, 0.5980, 0.3629, 0.6874, 0.8761, 0.7593, 0.8267, 0.7912, 0.7147,\n        0.4980, 0.4396, 0.9295, 0.8956, 0.5096, 0.7148, 0.7788, 0.7961, 0.2600,\n        0.4964, 0.6008, 0.5710, 0.7882, 0.9379, 0.8573, 0.7131, 0.8105, 0.6835,\n        0.5392], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\nThis means that we can use instances of nn.Module as parameterized functions. For example, we might create a general linear (technically affine) function in the same way.\n\\[f(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{W}^T + \\mathbf{b},  \\quad f: \\mathbb{R}^i \\rightarrow \\mathbb{R}^o\\]\nNote that here we are not assuming an augmented representation of \\(\\mathbf{x}\\).\n\nclass Linear(nn.Module):\n    def __init__(self, inputs, outputs):\n        super().__init__()\n        self.weightsT = nn.Parameter(torch.ones((inputs, outputs)))\n        self.bias = nn.Parameter(torch.zeros((outputs,)))\n\n    def forward(self, X):\n        return torch.matmul(X, self.weightsT) + self.bias\n\nWe can use this module to implement out logistic regression model above.\n\nclass LogisticRegression(nn.Module):\n    def __init__(self, dims):\n        super().__init__()\n        self.linear = Linear(dims, 1)                       # Dims input 1 output\n\n    def forward(self, X):\n        return torch.sigmoid(self.linear(X)).reshape((-1,)) # Turn output into a vector\n\n    def predict_probability(self, X):\n        return self.forward(X)\n\n    def predict(self, X):\n        return self.forward(X) &gt; 0.5\n\n\nmodel = LogisticRegression(2)\nmodel(X)\n\ntensor([0.5652, 0.7431, 0.8238, 0.5468, 0.8991, 0.5719, 0.8651, 0.7784, 0.6153,\n        0.7053, 0.7419, 0.3129, 0.4502, 0.5031, 0.4508, 0.7075, 0.7626, 0.4756,\n        0.4158, 0.4693, 0.7904, 0.7241, 0.2430, 0.5297, 0.7944, 0.8496, 0.6825,\n        0.6149, 0.6730, 0.5231, 0.6151, 0.6108, 0.5509, 0.7476, 0.6634, 0.8512,\n        0.8117, 0.7527, 0.5092, 0.7742, 0.8012, 0.7604, 0.6411, 0.3242, 0.2805,\n        0.4016, 0.6296, 0.3000, 0.7045, 0.7307, 0.7442, 0.8091, 0.5104, 0.8006,\n        0.6166, 0.5173, 0.6404, 0.5779, 0.8199, 0.9030, 0.7872, 0.6059, 0.8091,\n        0.9367, 0.5806, 0.7576, 0.8079, 0.3847, 0.4926, 0.4909, 0.8605, 0.6233,\n        0.5605, 0.5980, 0.3629, 0.6874, 0.8761, 0.7593, 0.8267, 0.7912, 0.7147,\n        0.4980, 0.4396, 0.9295, 0.8956, 0.5096, 0.7148, 0.7788, 0.7961, 0.2600,\n        0.4964, 0.6008, 0.5710, 0.7882, 0.9379, 0.8573, 0.7131, 0.8105, 0.6835,\n        0.5392], grad_fn=&lt;ReshapeAliasBackward0&gt;)\n\n\nThe power here is that because Linear is also an instance of nn.Module, PyTorch knows that it’s weights should also be considered part of our models weights. We can access the weights of a model using the parameters() method.\n\nlist(model.parameters())\n\n[Parameter containing:\n tensor([[1.],\n         [1.]], requires_grad=True),\n Parameter containing:\n tensor([0.], requires_grad=True)]\n\n\nThis lets us easily apply gradient descent:\n\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\nfor i in range(10):\n    predictions = model(X)        # Now we can just call model!\n    loss = NLL(predictions, y)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    print(loss.item())\n\n76.53839111328125\n45.934814453125\n40.60100555419922\n32.82542037963867\n30.3773136138916\n28.918834686279297\n28.118915557861328\n27.602989196777344\n27.248106002807617\n26.989551544189453\n\n\n\nplot_boundary(model, X, y)\n\n\n\n\nPyTorch unsurprisingly also provides a built-in Linear module. As nn.Linear.\n\nnn.Linear(2, 1)\n\nLinear(in_features=2, out_features=1, bias=True)\n\n\nKnowing how to make a parameterized function in PyTorch, let’s consider making a neural network layer with a sigmoid activation function.\n\\[f(\\mathbf{x}) = \\sigma(\\mathbf{x}^T\\mathbf{W}^T + \\mathbf{b})\\]\n\nclass SigmoidLayer(nn.Module):\n    def __init__(self, inputs, outputs):\n        super().__init__()\n        self.linear = nn.Linear(inputs, outputs)\n\n    def forward(self, X):\n        return torch.sigmoid(self.linear(X))\n        \n\nLet’s create a layer with 10 neurons. (So \\(\\mathbf{W}:\\ (10 \\times 2)\\))\n\nlayer = SigmoidLayer(2, 10)\nprint(X.shape)\nlayer(X).shape\n\ntorch.Size([100, 2])\n\n\ntorch.Size([100, 10])\n\n\nLet’s use this to create a neural network class for binary classification!\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self, dims, hidden_size):\n        super().__init__()\n        self.layer = SigmoidLayer(dims, hidden_size)\n        self.linear = Linear(hidden_size, 1)                       \n\n    def forward(self, X):\n        hidden_neurons = self.layer(X)\n        output = self.linear(hidden_neurons)\n        return torch.sigmoid(output).reshape((-1,))\n\n    def predict_probability(self, X):\n        return self.forward(X)\n\n    def predict(self, X):\n        return self.forward(X) &gt; 0.5\n\nWe see that PyTorch recognizes both the parameters of the logistic regression and the parameters of our neural network feature transform:\n\nmodel = NeuralNetwork(2, 10)\nlist(model.parameters())\n\n[Parameter containing:\n tensor([[-0.0632, -0.0324],\n         [-0.4227, -0.0076],\n         [ 0.5793, -0.4920],\n         [ 0.1640, -0.5442],\n         [ 0.2326,  0.3651],\n         [-0.0184,  0.6859],\n         [ 0.0893,  0.1236],\n         [-0.4774, -0.3702],\n         [-0.0048,  0.4830],\n         [-0.4720,  0.5458]], requires_grad=True),\n Parameter containing:\n tensor([-0.2695, -0.1038, -0.7001,  0.3398, -0.0591,  0.6680,  0.3601, -0.3093,\n          0.0831, -0.4315], requires_grad=True),\n Parameter containing:\n tensor([[1.],\n         [1.],\n         [1.],\n         [1.],\n         [1.],\n         [1.],\n         [1.],\n         [1.],\n         [1.],\n         [1.]], requires_grad=True),\n Parameter containing:\n tensor([0.], requires_grad=True)]\n\n\nThis means that we can easily run our optimization as before.\n\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\nfor i in range(100):\n    predictions = model(X)        # Now we can just call model!\n    loss = NLL(predictions, y)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    print(loss.item())\n\n251.74571228027344\n461.0118103027344\n62.689453125\n58.230892181396484\n51.833839416503906\n45.31672668457031\n42.269630432128906\n41.090309143066406\n42.59092712402344\n44.23348617553711\n51.76810073852539\n45.51247024536133\n48.5499267578125\n37.86709976196289\n36.245426177978516\n32.07354736328125\n30.741405487060547\n29.285472869873047\n28.51435089111328\n27.839073181152344\n27.37657928466797\n26.992008209228516\n26.69622802734375\n26.455289840698242\n26.262691497802734\n26.106895446777344\n25.978981018066406\n25.872283935546875\n25.78037452697754\n25.699132919311523\n25.625377655029297\n25.55675506591797\n25.492244720458984\n25.430410385131836\n25.371490478515625\n25.31437873840332\n25.260303497314453\n25.208097457885742\n25.160297393798828\n25.115331649780273\n25.07819175720215\n25.046260833740234\n25.029979705810547\n25.02396011352539\n25.0517578125\n25.099485397338867\n25.22385025024414\n25.381305694580078\n25.71344757080078\n26.064964294433594\n26.785503387451172\n27.31593894958496\n28.478464126586914\n28.659645080566406\n29.6827335357666\n28.770679473876953\n28.902992248535156\n27.449777603149414\n27.00601577758789\n25.92099380493164\n25.47977066040039\n24.875839233398438\n24.58094596862793\n24.25676918029785\n24.065155029296875\n23.871578216552734\n23.734432220458984\n23.599082946777344\n23.48827362060547\n23.378538131713867\n23.279327392578125\n23.17983627319336\n23.08414077758789\n22.986806869506836\n22.889585494995117\n22.789363861083984\n22.686796188354492\n22.579761505126953\n22.468294143676758\n22.35063362121582\n22.226394653320312\n22.093841552734375\n21.952234268188477\n21.799684524536133\n21.6351318359375\n21.456491470336914\n21.26246452331543\n21.05084991455078\n20.82036781311035\n20.568992614746094\n20.29586410522461\n19.999610900878906\n19.680295944213867\n19.337650299072266\n18.972978591918945\n18.587278366088867\n18.183042526245117\n17.762325286865234\n17.328487396240234\n16.884227752685547\n\n\n\nplot_boundary(model, X, y)\n\n\n\n\nPyTorch also gives us an easier (but less flexible) way to define a composition of modules like this. In PyTorch we can define this simple network using nn.Sequential\n\nmodel = nn.Sequential(\n    nn.Linear(2, 10),\n    nn.Sigmoid(),\n    nn.Linear(10, 1),\n    nn.Sigmoid(),\n)\nmodel(X).reshape((-1,))\n\ntensor([0.3653, 0.3638, 0.3685, 0.3575, 0.3788, 0.3567, 0.3749, 0.3824, 0.3716,\n        0.3777, 0.3612, 0.3494, 0.3608, 0.3496, 0.3610, 0.3802, 0.3737, 0.3504,\n        0.3581, 0.3621, 0.3859, 0.3814, 0.3410, 0.3605, 0.3768, 0.3890, 0.3599,\n        0.3586, 0.3567, 0.3529, 0.3723, 0.3711, 0.3682, 0.3644, 0.3774, 0.3742,\n        0.3804, 0.3808, 0.3526, 0.3757, 0.3669, 0.3735, 0.3731, 0.3516, 0.3461,\n        0.3566, 0.3558, 0.3496, 0.3594, 0.3775, 0.3755, 0.3808, 0.3586, 0.3792,\n        0.3543, 0.3657, 0.3559, 0.3642, 0.3848, 0.3816, 0.3824, 0.3546, 0.3808,\n        0.3906, 0.3661, 0.3723, 0.3823, 0.3552, 0.3561, 0.3543, 0.3730, 0.3723,\n        0.3523, 0.3669, 0.3531, 0.3590, 0.3764, 0.3702, 0.3680, 0.3841, 0.3798,\n        0.3555, 0.3480, 0.3861, 0.3814, 0.3495, 0.3793, 0.3814, 0.3839, 0.3427,\n        0.3540, 0.3623, 0.3700, 0.3661, 0.3915, 0.3718, 0.3660, 0.3685, 0.3601,\n        0.3597], grad_fn=&lt;ReshapeAliasBackward0&gt;)\n\n\nHere nn.Sigmoid is a built-in module that just applies the sigmoid function. Its implementation would look like:\n\nclass SigmoidLayer(nn.Module):\n    def forward(self, X):\n        return torch.sigmoid(X)\n\nWe could use this to create a network with several hidden layers:\n\nmodel = nn.Sequential(\n    nn.Linear(2, 10),\n    nn.Sigmoid(),\n    nn.Linear(10, 10),\n    nn.Sigmoid(),\n    nn.Linear(10, 10),\n    nn.Sigmoid(),\n    nn.Linear(10, 1),\n    nn.Sigmoid(),\n)\nmodel(X)\n\ntensor([[0.4041],\n        [0.4038],\n        [0.4038],\n        [0.4040],\n        [0.4038],\n        [0.4039],\n        [0.4038],\n        [0.4041],\n        [0.4042],\n        [0.4041],\n        [0.4038],\n        [0.4042],\n        [0.4042],\n        [0.4039],\n        [0.4042],\n        [0.4042],\n        [0.4040],\n        [0.4039],\n        [0.4042],\n        [0.4042],\n        [0.4042],\n        [0.4042],\n        [0.4041],\n        [0.4041],\n        [0.4040],\n        [0.4041],\n        [0.4039],\n        [0.4039],\n        [0.4038],\n        [0.4039],\n        [0.4042],\n        [0.4042],\n        [0.4042],\n        [0.4039],\n        [0.4042],\n        [0.4038],\n        [0.4040],\n        [0.4041],\n        [0.4039],\n        [0.4040],\n        [0.4038],\n        [0.4040],\n        [0.4042],\n        [0.4042],\n        [0.4041],\n        [0.4042],\n        [0.4039],\n        [0.4042],\n        [0.4038],\n        [0.4041],\n        [0.4040],\n        [0.4040],\n        [0.4041],\n        [0.4040],\n        [0.4038],\n        [0.4042],\n        [0.4038],\n        [0.4041],\n        [0.4041],\n        [0.4038],\n        [0.4041],\n        [0.4039],\n        [0.4040],\n        [0.4039],\n        [0.4041],\n        [0.4040],\n        [0.4041],\n        [0.4042],\n        [0.4040],\n        [0.4040],\n        [0.4038],\n        [0.4042],\n        [0.4039],\n        [0.4041],\n        [0.4042],\n        [0.4038],\n        [0.4038],\n        [0.4039],\n        [0.4038],\n        [0.4041],\n        [0.4042],\n        [0.4040],\n        [0.4039],\n        [0.4038],\n        [0.4039],\n        [0.4039],\n        [0.4042],\n        [0.4041],\n        [0.4041],\n        [0.4041],\n        [0.4040],\n        [0.4040],\n        [0.4042],\n        [0.4038],\n        [0.4039],\n        [0.4038],\n        [0.4039],\n        [0.4038],\n        [0.4039],\n        [0.4040]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\nPyTorch also provides built-in loss functions. The PyTorch function for the negative log-likelihood for logistic regression is called nn.functional.binary_cross_entropy. It has some sharp edges though.\nFor one, it expects y to be a float type. We can convert a PyTorch int tensor into a float one by calling the float method.\nWe also see that our sequential model returns a column vector, so y should match that as well.\n\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\nyfloat = y.float().reshape((-1, 1))\n\nfor i in range(100):\n    predictions = model(X)        # Now we can just call model!\n    loss = nn.functional.binary_cross_entropy(predictions, yfloat)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    print(loss.item())\n\n0.712141752243042\n0.7091044783592224\n0.7065528631210327\n0.7044107913970947\n0.7026132941246033\n0.7011056542396545\n0.6998415589332581\n0.6987819671630859\n0.6978939771652222\n0.6971497535705566\n0.696526288986206\n0.6960040330886841\n0.6955663561820984\n0.6951996684074402\n0.694892406463623\n0.6946350336074829\n0.6944191455841064\n0.6942383646965027\n0.6940867900848389\n0.6939594745635986\n0.6938527822494507\n0.6937631964683533\n0.6936879754066467\n0.69362473487854\n0.6935714483261108\n0.6935266852378845\n0.6934890151023865\n0.6934571266174316\n0.6934301853179932\n0.6934073567390442\n0.693388044834137\n0.693371593952179\n0.693357527256012\n0.6933454871177673\n0.6933351755142212\n0.6933264136314392\n0.6933186054229736\n0.6933119297027588\n0.693306028842926\n0.6933008432388306\n0.6932962536811829\n0.6932921409606934\n0.6932885050773621\n0.6932851672172546\n0.6932820081710815\n0.6932792067527771\n0.6932765245437622\n0.6932740807533264\n0.693271815776825\n0.6932694911956787\n0.6932673454284668\n0.6932653784751892\n0.6932634115219116\n0.6932615637779236\n0.6932596564292908\n0.6932578086853027\n0.6932560801506042\n0.693254292011261\n0.6932525634765625\n0.693250834941864\n0.6932492256164551\n0.6932475566864014\n0.6932458281517029\n0.6932441592216492\n0.6932425498962402\n0.6932408809661865\n0.6932392120361328\n0.6932376027107239\n0.6932359933853149\n0.693234384059906\n0.6932327747344971\n0.6932311058044434\n0.6932294964790344\n0.6932278275489807\n0.6932263374328613\n0.6932246685028076\n0.6932230591773987\n0.6932214498519897\n0.693219780921936\n0.6932182312011719\n0.6932166218757629\n0.693215012550354\n0.6932134032249451\n0.6932117938995361\n0.693210244178772\n0.6932085156440735\n0.6932069659233093\n0.6932052373886108\n0.6932037472724915\n0.6932021379470825\n0.6932004690170288\n0.6931988596916199\n0.6931973099708557\n0.693195641040802\n0.6931940317153931\n0.6931924223899841\n0.6931908130645752\n0.693189263343811\n0.6931875348091125\n0.6931860446929932\n\n\nFor convinience, let’s definie a wrapper class for our model.\n\nclass LogisticRegressionNeuralNetwork(nn.Module):\n    def __init__(self, network):\n        super().__init__()\n        self.network = network                   \n\n    def forward(self, X):\n        return self.network(X).reshape((-1,))\n\n    def predict_probability(self, X):\n        return self.forward(X)\n\n    def predict(self, X):\n        return self.forward(X) &gt; 0.5"
  },
  {
    "objectID": "lecture7-pytorch/notebook.html#evaluating-models",
    "href": "lecture7-pytorch/notebook.html#evaluating-models",
    "title": "Lecture 7: PyTorch",
    "section": "Evaluating models",
    "text": "Evaluating models\nWe see that we have a lot of options when designing a neural network. So far the choices we’ve seen are: - The number of layers - The number of neurons in each layer - The activation function - The learning rate for gradient descent\nAnd this is just the beginning! As we go on, we’ll learn about many more options that we have.\nLet’s take a look at how to make some of these choices. In many real cases, our data will not be a cleanly separated into 2 classes as we’ve seen. For instance, we can look at a noisier version of the dataset we saw before.\n\nX, y = make_moons(300, noise=0.5)\nX, y = torch.tensor(X).float(), torch.tensor(y)\nplt.scatter(*X.T, c=y, edgecolor=\"black\")\n\n&lt;matplotlib.collections.PathCollection at 0x2e043f070&gt;\n\n\n\n\n\nIn reality, this dataset was drawn from the distribution shown below! The optimal classifier would still have an “s-shaped” decision boundary\n\nX, y = make_moons(50000, noise=0.5)\nplt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.1)\npass\n\n\n\n\nLet’s split this into training and test sets as we’ve seen.\n\ninds = np.arange(X.shape[0])\nnp.random.shuffle(inds)\nXtrain, ytrain = X[inds[:150]], y[inds[:150]]\nXtest, ytest = X[inds[150:]], y[inds[150:]]\n\n# We can plot training and test data on the same plot\nplt.scatter(*Xtrain.T, c=ytrain, edgecolor=\"black\", marker='o', label='training data')\nplt.scatter(*Xtest.T, c=ytest, edgecolor=\"black\", marker='^', label='test data')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x2e0a7c670&gt;"
  },
  {
    "objectID": "lecture8-optimization/notes.html",
    "href": "lecture8-optimization/notes.html",
    "title": "Lecture 8: Regularization",
    "section": "",
    "text": "Try out the concepts from this lecture in the Neural Network Playground!"
  },
  {
    "objectID": "lecture7-pytorch/notebook2.html",
    "href": "lecture7-pytorch/notebook2.html",
    "title": "CS 152: Neural Networks",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport autograd.numpy as np\nfrom autograd import grad, jacobian, hessian, elementwise_grad\n\n\nx = np.linspace(-3, 3, 300)\n\ndef sigmoid(x):\n    #return (2 - x) ** 2 - 0.5 * x + 1\n    return 1 / (1 + np.exp(-x))\n\ndef tangent(f, x):\n    y = f(x)\n    slope = grad(f)(x)\n\n    def tf(a):\n        return (a - x) * slope + y\n\n    return tf\n\ndef tangent2(f, x):\n    y = f(x)\n    slope = grad(f)(x)\n    curve = 0.5 * grad(grad(f))(x)\n\n    def tf(a):\n        return curve * (a - x) ** 2 + (a - x) * slope + y\n    return tf\n\n\ndsigmoid = elementwise_grad(sigmoid)\nd2sigmoid = elementwise_grad(dsigmoid)\n\npoint = 1.\ntpoint = tangent2(sigmoid, point)\n\n\nplt.scatter(point, sigmoid(point), zorder=10)\nplt.plot(x, sigmoid(x))\nplt.plot(x, tpoint(x))\n\n#plt.plot(x, d2sigmoid(x))"
  },
  {
    "objectID": "lecture7-pytorch/notebook copy.html",
    "href": "lecture7-pytorch/notebook copy.html",
    "title": "Lecture 7: PyTorch",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons\nimport tqdm\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\ndef plot_boundary(model, X, y, alpha=1, title=''):\n    xrange = (-X[:, 0].min() + X[:, 0].max()) / 10\n    yrange = (-X[:, y].min() + X[:, y].max()) / 10\n    feature_1, feature_2 = np.meshgrid(\n        np.linspace(X[:, 0].min() - xrange, X[:, 0].max() + xrange, 250),\n        np.linspace(X[:, 1].min() - yrange, X[:, 1].max() + yrange, 250)\n    )\n    grid = np.vstack([feature_1.ravel(), feature_2.ravel()]).T\n    y_pred = np.reshape(model.predict(torch.tensor(grid).float()).detach().numpy(), feature_1.shape)\n    display = DecisionBoundaryDisplay(\n        xx0=feature_1, xx1=feature_2, response=y_pred\n    )\n    display.plot()\n    display.ax_.scatter(\n        X[:, 0], X[:, 1], c=y, alpha=alpha, edgecolor=\"black\"\n    )\n    plt.title(title)\n    plt.show()"
  },
  {
    "objectID": "lecture7-pytorch/notebook copy.html#introduction-to-pytorch",
    "href": "lecture7-pytorch/notebook copy.html#introduction-to-pytorch",
    "title": "Lecture 7: PyTorch",
    "section": "Introduction to PyTorch",
    "text": "Introduction to PyTorch\nThe most basic object in PyTorch is a tensor. Tensor objects behave much like the AutogradValue objects we are creating in the homework! We can create a tensor object with a given value as follows\n\nx = torch.tensor(4.)\nx\n\ntensor(4.)\n\n\nPerforming basic operations on tensor objects gives tensor objects.\n\na = x ** 2 + 5\na\n\ntensor(21.)\n\n\ntensor objects also support reverse-mode automatic differentiation! To use this, we must specify that we will want to compute the derivative with respect to a given tensor. We can do this with the requires_grad argument.\n\nx = torch.tensor(4., requires_grad=True)\n\nOnce we have a tensor that requires_grad, we can perform operations on it to compute a loss.\n\na = x ** 2 + 5\nL = torch.log(a) # Functions like log must be called through torch\nL\n\ntensor(3.0445, grad_fn=&lt;LogBackward0&gt;)\n\n\nOnce we have a loss running the backward pass is done exactly as in the homework. First we call backward() on the loss tensor object, then we can access the derivative through the grad property of x.\n\nL.backward()\nx.grad\n\ntensor(0.3810)\n\n\nWe can also create tensor objects that wrap arrays.\n\nx = torch.tensor(np.array([3, 4, 5]))\nx\n\ntensor([3, 4, 5])\n\n\nWe can also just directly create tensors as we would numpy arrays\n\nx = torch.tensor([3, 4, 5])\nx\n\ntensor([3, 4, 5])\n\n\nIncluding convienience constructors.\n\nprint(torch.ones((5,)))\nprint(torch.zeros((2, 3)))\n\ntensor([1., 1., 1., 1., 1.])\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n\nAutomatic differentiation still works for arrays. In this case it gives use the gradient of the loss (hence the grad property).\n\nx = torch.tensor([3., 4., 5.], requires_grad=True)\nL = torch.sum(x ** 2)\nL\n\ntensor(50., grad_fn=&lt;SumBackward0&gt;)\n\n\n\nL.grad\n\n/var/folders/t9/fjsr8h1x7c7cg4vqw_xhjq2w0000gn/T/ipykernel_79454/323164392.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:491.)\n  L.grad\n\n\n\nL.backward()\nx.grad\n\ntensor([ 6.,  8., 10.])\n\n\nWe can convert tensor objects back to numpy by calling x.detach().numpy(). (detach removes the variable from any automatic differentiation computations)\n\nx.detach().numpy()\n\narray([3., 4., 5.], dtype=float32)\n\n\nAt this point it’s probably worth remarking on where the name tensor comes from.\nSo far we’ve discussed 3 kinds of array objects - Scalars: which are just single values (0-dimensional) - Vectors: 1-dimensional arrays of numbers - Matrices: 2-dimensional arrays of numbers\n\n\n\nAlt text\n\n\nA tensor is the generalization of a vector or matrix to any number of dimensions. For example, a 3-dimensional tensor can be seen in multiple ways.\n\n\n\nAlt text\n\n\nA tensor object can be created with any number of dimensions. For example, we could create a 2x2x2 tensor as:\n\nt = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\nt\n\ntensor([[[1, 2],\n         [3, 4]],\n\n        [[5, 6],\n         [7, 8]]])\n\n\nOr we could create the tensor in the image using arange and reshape.\n\nt = torch.arange(30).reshape((3, 2, 5))\nt\n\ntensor([[[ 0,  1,  2,  3,  4],\n         [ 5,  6,  7,  8,  9]],\n\n        [[10, 11, 12, 13, 14],\n         [15, 16, 17, 18, 19]],\n\n        [[20, 21, 22, 23, 24],\n         [25, 26, 27, 28, 29]]])\n\n\n4-dimensional tensors can also be visualized\n\n\n\nAlt text\n\n\n\nt = torch.ones((3, 2, 4, 5))\nt.shape\n\ntorch.Size([3, 2, 4, 5])\n\n\nThere are some notable differences between torch and numpy when it comes to operations. The important one to watch out for at this point is matrix multiplation. In numpy we accomplished with with np.dot:\n\nx = np.ones((4, 5))\nw = np.ones((5, 2))\nnp.dot(x, w)\n\narray([[5., 5.],\n       [5., 5.],\n       [5., 5.],\n       [5., 5.]])\n\n\nIn PyTorch torch.dot only does vector dot products and thus only applies to 1-dimensional tensor objects:\n\nx = torch.ones((4, 5))\nw = torch.ones((5, 2))\ntorch.dot(x, w)\n\nRuntimeError: 1D tensors expected, but got 2D and 2D tensors\n\n\nInstead we use the torch.matmul function for this purpose\n\ntorch.matmul(x, w)\n\ntensor([[5., 5.],\n        [5., 5.],\n        [5., 5.],\n        [5., 5.]])\n\n\nPyTorch also has many handy built-in functions that numpy doesn’t have, such as sigmoid.\n\nx = torch.linspace(-5, 5, 50)\ns = torch.sigmoid(x)\nplt.plot(x, s)\n\n\n\n\nThis makes it very easy to implement something like logistic regression.\n\nclass LogisticRegression:\n    def __init__(self, dims):\n        self.weights = torch.ones((dims,), requires_grad=True)\n        self.bias = torch.zeros((), requires_grad=True)\n\n    def predict_probability(self, X):\n        f_X = torch.matmul(X, self.weights) + self.bias\n        return torch.sigmoid(f_X)\n\n    def predict(self, X):\n        return self.predict_probability(X) &gt; 0.5\n\nLet’s try loading a dataset, converting it to tensor and making predictions\n\nX, y = make_moons(noise=0.1)\nX, y = torch.tensor(X).float(), torch.tensor(y)\n\nmodel = LogisticRegression(2)\nplot_boundary(model, X, y)\n\n\n\n\nWhen working with PyTorch, it is convention to separate the loss function from the model, where the loss function will just take predictions and labels.\n\ndef NLL(pred, y):\n    LL = y * torch.log(pred) + (1. - y) * torch.log(1. - pred)\n    return -LL.sum()"
  },
  {
    "objectID": "lecture7-pytorch/notebook copy.html#gradient-descent",
    "href": "lecture7-pytorch/notebook copy.html#gradient-descent",
    "title": "Lecture 7: PyTorch",
    "section": "Gradient descent",
    "text": "Gradient descent\nGradient descent is also implemented in PyTorch in the optim module.\n\nfrom torch import optim\n\nGradient descent works a bit differently in PyTorch than what we’ve seen. We first need to construct a gradient descent object which specifies which values we’re optimizing and what the learning rate will be. We specify the values to optimize by simply passing a list of weights/parameters to the constructor.\nIn PyTorch, basic gradient descent is encapsulated in the optim.SGD class (SGD stands for stochastic gradient descent, we’ll talk about what stochastic means in this context next week.)\n\noptimizer = optim.SGD([model.weights, model.bias], lr=0.1)\n\nNotice that this object doesn’t even take in the function we’re trying to optimize, only the inputs. We need to call the function ourselves and run backward() to compute the gradients.\n\npredictions = model.predict_probability(X)\nloss = NLL(predictions, y)\nloss.backward()\n\nLet’s look at our model weights\n\nmodel.weights\nmodel.weights.grad\n\ntensor([-5.6526, 24.1355])\n\n\nWe can take a single step of gradient descent using the step method of the optimizer.\n\noptimizer.step()\nmodel.weights\n\ntensor([ 1.5653, -1.4135], requires_grad=True)\n\n\nWe see that this actually updates the weights themselves!\nIt’s important to note that in PyTorch, calling backward does not clear the value stored in grad. So computing the gradient multiple times will result in updates to the gradient.\n\nprint(model.weights.grad)\nNLL(model.predict_probability(X), y).backward()\nprint(model.weights.grad)\nNLL(model.predict_probability(X), y).backward()\nprint(model.weights.grad)\n\ntensor([-5.6526, 24.1355])\ntensor([-14.5242,  28.7704])\ntensor([-23.3958,  33.4053])\n\n\nWe can clear the stored gradients using the optimizer.\n\noptimizer.zero_grad()\nprint(model.weights.grad)\n\nNone\n\n\nSo far we’ve only taking a single step of gradient descent. In order to run many steps, we need to write a loop to do everything we just saw.\n\nfor i in range(10):\n    predictions = model.predict_probability(X)\n    loss = NLL(predictions, y)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    print(loss.item())\n\n45.934814453125\n40.60100555419922\n32.82542037963867\n30.3773136138916\n28.918834686279297\n28.118915557861328\n27.602989196777344\n27.248106002807617\n26.989551544189453\n26.794662475585938\n\n\nWe should now see that our model has been optimized!\n\nplot_boundary(model, X, y)"
  },
  {
    "objectID": "lecture7-pytorch/notebook copy.html#torch.nn",
    "href": "lecture7-pytorch/notebook copy.html#torch.nn",
    "title": "Lecture 7: PyTorch",
    "section": "torch.nn",
    "text": "torch.nn\nWhile PyTorch as a tool for automatic differentiation and optimization would be useful by itself. It actually gives us a lot more than that!\nOn of the most important features of PyTorch is its model-building tools in the torch.nn module. This gives us a lot of powerful features that we can use to build complex neural networks!\n\nfrom torch import nn\n\nLet’s start by building out logistic regression model in the torch.nn framwork. In order for a model to benefit from torch.nn our model class needs to inheret from nn.Module\n\nclass LogisticRegression(nn.Module):\n    def __init__(self, dims):\n        super().__init__()\n        self.weights = nn.Parameter(torch.ones((dims,)))\n        self.bias = nn.Parameter(torch.zeros(()))\n\n    def forward(self, X):\n        return torch.sigmoid(torch.matmul(X, self.weights) + self.bias)\n\n    def predict_probability(self, X):\n        return self.forward(X)\n\n    def predict(self, X):\n        return self.forward(X) &gt; 0.5\n\nThere are 2 changes to note here. The first is that we wrapped our weights and bias terms in nn.Parameter. This tells PyTorch that these are the parameters we will want to optimize. We don’t need to specify requires_grad for parameters, PyTorch will take care of that for us.\nThe second is that we moved the implmentation of predict_probability to forward. In PyTorch models the forward method is special, it defines the model as a function. If we call the model as a function forward will be called internally.\n\nmodel = LogisticRegression(2)\nmodel(X)\n\ntensor([0.5652, 0.7431, 0.8238, 0.5468, 0.8991, 0.5719, 0.8651, 0.7784, 0.6153,\n        0.7053, 0.7419, 0.3129, 0.4502, 0.5031, 0.4508, 0.7075, 0.7626, 0.4756,\n        0.4158, 0.4693, 0.7904, 0.7241, 0.2430, 0.5297, 0.7944, 0.8496, 0.6825,\n        0.6149, 0.6730, 0.5231, 0.6151, 0.6108, 0.5509, 0.7476, 0.6634, 0.8512,\n        0.8117, 0.7527, 0.5092, 0.7742, 0.8012, 0.7604, 0.6411, 0.3242, 0.2805,\n        0.4016, 0.6296, 0.3000, 0.7045, 0.7307, 0.7442, 0.8091, 0.5104, 0.8006,\n        0.6166, 0.5173, 0.6404, 0.5779, 0.8199, 0.9030, 0.7872, 0.6059, 0.8091,\n        0.9367, 0.5806, 0.7576, 0.8079, 0.3847, 0.4926, 0.4909, 0.8605, 0.6233,\n        0.5605, 0.5980, 0.3629, 0.6874, 0.8761, 0.7593, 0.8267, 0.7912, 0.7147,\n        0.4980, 0.4396, 0.9295, 0.8956, 0.5096, 0.7148, 0.7788, 0.7961, 0.2600,\n        0.4964, 0.6008, 0.5710, 0.7882, 0.9379, 0.8573, 0.7131, 0.8105, 0.6835,\n        0.5392], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\nThis means that we can use instances of nn.Module as parameterized functions. For example, we might create a general linear (technically affine) function in the same way.\n\\[f(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{W}^T + \\mathbf{b},  \\quad f: \\mathbb{R}^i \\rightarrow \\mathbb{R}^o\\]\nNote that here we are not assuming an augmented representation of \\(\\mathbf{x}\\).\n\nclass Linear(nn.Module):\n    def __init__(self, inputs, outputs):\n        super().__init__()\n        self.weightsT = nn.Parameter(torch.ones((inputs, outputs)))\n        self.bias = nn.Parameter(torch.zeros((outputs,)))\n\n    def forward(self, X):\n        return torch.matmul(X, self.weightsT) + self.bias\n\nWe can use this module to implement out logistic regression model above.\n\nclass LogisticRegression(nn.Module):\n    def __init__(self, dims):\n        super().__init__()\n        self.linear = Linear(dims, 1)                       # Dims input 1 output\n\n    def forward(self, X):\n        return torch.sigmoid(self.linear(X)).reshape((-1,)) # Turn output into a vector\n\n    def predict_probability(self, X):\n        return self.forward(X)\n\n    def predict(self, X):\n        return self.forward(X) &gt; 0.5\n\n\nmodel = LogisticRegression(2)\nmodel(X)\n\ntensor([0.5652, 0.7431, 0.8238, 0.5468, 0.8991, 0.5719, 0.8651, 0.7784, 0.6153,\n        0.7053, 0.7419, 0.3129, 0.4502, 0.5031, 0.4508, 0.7075, 0.7626, 0.4756,\n        0.4158, 0.4693, 0.7904, 0.7241, 0.2430, 0.5297, 0.7944, 0.8496, 0.6825,\n        0.6149, 0.6730, 0.5231, 0.6151, 0.6108, 0.5509, 0.7476, 0.6634, 0.8512,\n        0.8117, 0.7527, 0.5092, 0.7742, 0.8012, 0.7604, 0.6411, 0.3242, 0.2805,\n        0.4016, 0.6296, 0.3000, 0.7045, 0.7307, 0.7442, 0.8091, 0.5104, 0.8006,\n        0.6166, 0.5173, 0.6404, 0.5779, 0.8199, 0.9030, 0.7872, 0.6059, 0.8091,\n        0.9367, 0.5806, 0.7576, 0.8079, 0.3847, 0.4926, 0.4909, 0.8605, 0.6233,\n        0.5605, 0.5980, 0.3629, 0.6874, 0.8761, 0.7593, 0.8267, 0.7912, 0.7147,\n        0.4980, 0.4396, 0.9295, 0.8956, 0.5096, 0.7148, 0.7788, 0.7961, 0.2600,\n        0.4964, 0.6008, 0.5710, 0.7882, 0.9379, 0.8573, 0.7131, 0.8105, 0.6835,\n        0.5392], grad_fn=&lt;ReshapeAliasBackward0&gt;)\n\n\nThe power here is that because Linear is also an instance of nn.Module, PyTorch knows that it’s weights should also be considered part of our models weights. We can access the weights of a model using the parameters() method.\n\nlist(model.parameters())\n\n[Parameter containing:\n tensor([[1.],\n         [1.]], requires_grad=True),\n Parameter containing:\n tensor([0.], requires_grad=True)]\n\n\nThis lets us easily apply gradient descent:\n\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\nfor i in range(10):\n    predictions = model(X)        # Now we can just call model!\n    loss = NLL(predictions, y)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    print(loss.item())\n\n76.53839111328125\n45.934814453125\n40.60100555419922\n32.82542037963867\n30.3773136138916\n28.918834686279297\n28.118915557861328\n27.602989196777344\n27.248106002807617\n26.989551544189453\n\n\n\nplot_boundary(model, X, y)\n\n\n\n\nPyTorch unsurprisingly also provides a built-in Linear module. As nn.Linear.\n\nnn.Linear(2, 1)\n\nLinear(in_features=2, out_features=1, bias=True)\n\n\nKnowing how to make a parameterized function in PyTorch, let’s consider making a neural network layer with a sigmoid activation function.\n\\[f(\\mathbf{x}) = \\sigma(\\mathbf{x}^T\\mathbf{W}^T + \\mathbf{b})\\]\n\nclass SigmoidLayer(nn.Module):\n    def __init__(self, inputs, outputs):\n        super().__init__()\n        self.linear = nn.Linear(inputs, outputs)\n\n    def forward(self, X):\n        return torch.sigmoid(self.linear(X))\n        \n\nLet’s create a layer with 10 neurons. (So \\(\\mathbf{W}:\\ (10 \\times 2)\\))\n\nlayer = SigmoidLayer(2, 10)\nprint(X.shape)\nlayer(X).shape\n\ntorch.Size([100, 2])\n\n\ntorch.Size([100, 10])\n\n\nLet’s use this to create a neural network class for binary classification!\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self, dims, hidden_size):\n        super().__init__()\n        self.layer = SigmoidLayer(dims, hidden_size)\n        self.linear = Linear(hidden_size, 1)                       \n\n    def forward(self, X):\n        hidden_neurons = self.layer(X)\n        output = self.linear(hidden_neurons)\n        return torch.sigmoid(output).reshape((-1,))\n\n    def predict_probability(self, X):\n        return self.forward(X)\n\n    def predict(self, X):\n        return self.forward(X) &gt; 0.5\n\nWe see that PyTorch recognizes both the parameters of the logistic regression and the parameters of our neural network feature transform:\n\nmodel = NeuralNetwork(2, 10)\nlist(model.parameters())\n\n[Parameter containing:\n tensor([[-0.0632, -0.0324],\n         [-0.4227, -0.0076],\n         [ 0.5793, -0.4920],\n         [ 0.1640, -0.5442],\n         [ 0.2326,  0.3651],\n         [-0.0184,  0.6859],\n         [ 0.0893,  0.1236],\n         [-0.4774, -0.3702],\n         [-0.0048,  0.4830],\n         [-0.4720,  0.5458]], requires_grad=True),\n Parameter containing:\n tensor([-0.2695, -0.1038, -0.7001,  0.3398, -0.0591,  0.6680,  0.3601, -0.3093,\n          0.0831, -0.4315], requires_grad=True),\n Parameter containing:\n tensor([[1.],\n         [1.],\n         [1.],\n         [1.],\n         [1.],\n         [1.],\n         [1.],\n         [1.],\n         [1.],\n         [1.]], requires_grad=True),\n Parameter containing:\n tensor([0.], requires_grad=True)]\n\n\nThis means that we can easily run our optimization as before.\n\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\nfor i in range(100):\n    predictions = model(X)        # Now we can just call model!\n    loss = NLL(predictions, y)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    print(loss.item())\n\n251.74571228027344\n461.0118103027344\n62.689453125\n58.230892181396484\n51.833839416503906\n45.31672668457031\n42.269630432128906\n41.090309143066406\n42.59092712402344\n44.23348617553711\n51.76810073852539\n45.51247024536133\n48.5499267578125\n37.86709976196289\n36.245426177978516\n32.07354736328125\n30.741405487060547\n29.285472869873047\n28.51435089111328\n27.839073181152344\n27.37657928466797\n26.992008209228516\n26.69622802734375\n26.455289840698242\n26.262691497802734\n26.106895446777344\n25.978981018066406\n25.872283935546875\n25.78037452697754\n25.699132919311523\n25.625377655029297\n25.55675506591797\n25.492244720458984\n25.430410385131836\n25.371490478515625\n25.31437873840332\n25.260303497314453\n25.208097457885742\n25.160297393798828\n25.115331649780273\n25.07819175720215\n25.046260833740234\n25.029979705810547\n25.02396011352539\n25.0517578125\n25.099485397338867\n25.22385025024414\n25.381305694580078\n25.71344757080078\n26.064964294433594\n26.785503387451172\n27.31593894958496\n28.478464126586914\n28.659645080566406\n29.6827335357666\n28.770679473876953\n28.902992248535156\n27.449777603149414\n27.00601577758789\n25.92099380493164\n25.47977066040039\n24.875839233398438\n24.58094596862793\n24.25676918029785\n24.065155029296875\n23.871578216552734\n23.734432220458984\n23.599082946777344\n23.48827362060547\n23.378538131713867\n23.279327392578125\n23.17983627319336\n23.08414077758789\n22.986806869506836\n22.889585494995117\n22.789363861083984\n22.686796188354492\n22.579761505126953\n22.468294143676758\n22.35063362121582\n22.226394653320312\n22.093841552734375\n21.952234268188477\n21.799684524536133\n21.6351318359375\n21.456491470336914\n21.26246452331543\n21.05084991455078\n20.82036781311035\n20.568992614746094\n20.29586410522461\n19.999610900878906\n19.680295944213867\n19.337650299072266\n18.972978591918945\n18.587278366088867\n18.183042526245117\n17.762325286865234\n17.328487396240234\n16.884227752685547\n\n\n\nplot_boundary(model, X, y)\n\n\n\n\nPyTorch also gives us an easier (but less flexible) way to define a composition of modules like this. In PyTorch we can define this simple network using nn.Sequential\n\nmodel = nn.Sequential(\n    nn.Linear(2, 10),\n    nn.Sigmoid(),\n    nn.Linear(10, 1),\n    nn.Sigmoid(),\n)\nmodel(X).reshape((-1,))\n\ntensor([0.3653, 0.3638, 0.3685, 0.3575, 0.3788, 0.3567, 0.3749, 0.3824, 0.3716,\n        0.3777, 0.3612, 0.3494, 0.3608, 0.3496, 0.3610, 0.3802, 0.3737, 0.3504,\n        0.3581, 0.3621, 0.3859, 0.3814, 0.3410, 0.3605, 0.3768, 0.3890, 0.3599,\n        0.3586, 0.3567, 0.3529, 0.3723, 0.3711, 0.3682, 0.3644, 0.3774, 0.3742,\n        0.3804, 0.3808, 0.3526, 0.3757, 0.3669, 0.3735, 0.3731, 0.3516, 0.3461,\n        0.3566, 0.3558, 0.3496, 0.3594, 0.3775, 0.3755, 0.3808, 0.3586, 0.3792,\n        0.3543, 0.3657, 0.3559, 0.3642, 0.3848, 0.3816, 0.3824, 0.3546, 0.3808,\n        0.3906, 0.3661, 0.3723, 0.3823, 0.3552, 0.3561, 0.3543, 0.3730, 0.3723,\n        0.3523, 0.3669, 0.3531, 0.3590, 0.3764, 0.3702, 0.3680, 0.3841, 0.3798,\n        0.3555, 0.3480, 0.3861, 0.3814, 0.3495, 0.3793, 0.3814, 0.3839, 0.3427,\n        0.3540, 0.3623, 0.3700, 0.3661, 0.3915, 0.3718, 0.3660, 0.3685, 0.3601,\n        0.3597], grad_fn=&lt;ReshapeAliasBackward0&gt;)\n\n\nHere nn.Sigmoid is a built-in module that just applies the sigmoid function. Its implementation would look like:\n\nclass SigmoidLayer(nn.Module):\n    def forward(self, X):\n        return torch.sigmoid(X)\n\nWe could use this to create a network with several hidden layers:\n\nmodel = nn.Sequential(\n    nn.Linear(2, 10),\n    nn.Sigmoid(),\n    nn.Linear(10, 10),\n    nn.Sigmoid(),\n    nn.Linear(10, 10),\n    nn.Sigmoid(),\n    nn.Linear(10, 1),\n    nn.Sigmoid(),\n)\nmodel(X)\n\ntensor([[0.4041],\n        [0.4038],\n        [0.4038],\n        [0.4040],\n        [0.4038],\n        [0.4039],\n        [0.4038],\n        [0.4041],\n        [0.4042],\n        [0.4041],\n        [0.4038],\n        [0.4042],\n        [0.4042],\n        [0.4039],\n        [0.4042],\n        [0.4042],\n        [0.4040],\n        [0.4039],\n        [0.4042],\n        [0.4042],\n        [0.4042],\n        [0.4042],\n        [0.4041],\n        [0.4041],\n        [0.4040],\n        [0.4041],\n        [0.4039],\n        [0.4039],\n        [0.4038],\n        [0.4039],\n        [0.4042],\n        [0.4042],\n        [0.4042],\n        [0.4039],\n        [0.4042],\n        [0.4038],\n        [0.4040],\n        [0.4041],\n        [0.4039],\n        [0.4040],\n        [0.4038],\n        [0.4040],\n        [0.4042],\n        [0.4042],\n        [0.4041],\n        [0.4042],\n        [0.4039],\n        [0.4042],\n        [0.4038],\n        [0.4041],\n        [0.4040],\n        [0.4040],\n        [0.4041],\n        [0.4040],\n        [0.4038],\n        [0.4042],\n        [0.4038],\n        [0.4041],\n        [0.4041],\n        [0.4038],\n        [0.4041],\n        [0.4039],\n        [0.4040],\n        [0.4039],\n        [0.4041],\n        [0.4040],\n        [0.4041],\n        [0.4042],\n        [0.4040],\n        [0.4040],\n        [0.4038],\n        [0.4042],\n        [0.4039],\n        [0.4041],\n        [0.4042],\n        [0.4038],\n        [0.4038],\n        [0.4039],\n        [0.4038],\n        [0.4041],\n        [0.4042],\n        [0.4040],\n        [0.4039],\n        [0.4038],\n        [0.4039],\n        [0.4039],\n        [0.4042],\n        [0.4041],\n        [0.4041],\n        [0.4041],\n        [0.4040],\n        [0.4040],\n        [0.4042],\n        [0.4038],\n        [0.4039],\n        [0.4038],\n        [0.4039],\n        [0.4038],\n        [0.4039],\n        [0.4040]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\nPyTorch also provides built-in loss functions. The PyTorch function for the negative log-likelihood for logistic regression is called nn.functional.binary_cross_entropy. It has some sharp edges though.\nFor one, it expects y to be a float type. We can convert a PyTorch int tensor into a float one by calling the float method.\nWe also see that our sequential model returns a column vector, so y should match that as well.\n\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\nyfloat = y.float().reshape((-1, 1))\n\nfor i in range(100):\n    predictions = model(X)        # Now we can just call model!\n    loss = nn.functional.binary_cross_entropy(predictions, yfloat)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    print(loss.item())\n\n0.712141752243042\n0.7091044783592224\n0.7065528631210327\n0.7044107913970947\n0.7026132941246033\n0.7011056542396545\n0.6998415589332581\n0.6987819671630859\n0.6978939771652222\n0.6971497535705566\n0.696526288986206\n0.6960040330886841\n0.6955663561820984\n0.6951996684074402\n0.694892406463623\n0.6946350336074829\n0.6944191455841064\n0.6942383646965027\n0.6940867900848389\n0.6939594745635986\n0.6938527822494507\n0.6937631964683533\n0.6936879754066467\n0.69362473487854\n0.6935714483261108\n0.6935266852378845\n0.6934890151023865\n0.6934571266174316\n0.6934301853179932\n0.6934073567390442\n0.693388044834137\n0.693371593952179\n0.693357527256012\n0.6933454871177673\n0.6933351755142212\n0.6933264136314392\n0.6933186054229736\n0.6933119297027588\n0.693306028842926\n0.6933008432388306\n0.6932962536811829\n0.6932921409606934\n0.6932885050773621\n0.6932851672172546\n0.6932820081710815\n0.6932792067527771\n0.6932765245437622\n0.6932740807533264\n0.693271815776825\n0.6932694911956787\n0.6932673454284668\n0.6932653784751892\n0.6932634115219116\n0.6932615637779236\n0.6932596564292908\n0.6932578086853027\n0.6932560801506042\n0.693254292011261\n0.6932525634765625\n0.693250834941864\n0.6932492256164551\n0.6932475566864014\n0.6932458281517029\n0.6932441592216492\n0.6932425498962402\n0.6932408809661865\n0.6932392120361328\n0.6932376027107239\n0.6932359933853149\n0.693234384059906\n0.6932327747344971\n0.6932311058044434\n0.6932294964790344\n0.6932278275489807\n0.6932263374328613\n0.6932246685028076\n0.6932230591773987\n0.6932214498519897\n0.693219780921936\n0.6932182312011719\n0.6932166218757629\n0.693215012550354\n0.6932134032249451\n0.6932117938995361\n0.693210244178772\n0.6932085156440735\n0.6932069659233093\n0.6932052373886108\n0.6932037472724915\n0.6932021379470825\n0.6932004690170288\n0.6931988596916199\n0.6931973099708557\n0.693195641040802\n0.6931940317153931\n0.6931924223899841\n0.6931908130645752\n0.693189263343811\n0.6931875348091125\n0.6931860446929932\n\n\nFor convinience, let’s definie a wrapper class for our model.\n\nclass LogisticRegressionNeuralNetwork(nn.Module):\n    def __init__(self, network):\n        super().__init__()\n        self.network = network                   \n\n    def forward(self, X):\n        return self.network(X).reshape((-1,))\n\n    def predict_probability(self, X):\n        return self.forward(X)\n\n    def predict(self, X):\n        return self.forward(X) &gt; 0.5"
  },
  {
    "objectID": "lecture7-pytorch/notebook copy.html#evaluating-models",
    "href": "lecture7-pytorch/notebook copy.html#evaluating-models",
    "title": "Lecture 7: PyTorch",
    "section": "Evaluating models",
    "text": "Evaluating models\nWe see that we have a lot of options when designing a neural network. So far the choices we’ve seen are: - The number of layers - The number of neurons in each layer - The activation function - The learning rate for gradient descent\nAnd this is just the beginning! As we go on, we’ll learn about many more options that we have.\nLet’s take a look at how to make some of these choices. In many real cases, our data will not be a cleanly separated into 2 classes as we’ve seen. For instance, we can look at a noisier version of the dataset we saw before.\n\nX, y = make_moons(300, noise=0.5)\nX, y = torch.tensor(X).float(), torch.tensor(y)\nplt.scatter(*X.T, c=y, edgecolor=\"black\")\n\n&lt;matplotlib.collections.PathCollection at 0x2e043f070&gt;\n\n\n\n\n\nIn reality, this dataset was drawn from the distribution shown below! The optimal classifier would still have an “s-shaped” decision boundary\n\nX, y = make_moons(50000, noise=0.5)\nplt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.1)\npass\n\n\n\n\nLet’s split this into training and test sets as we’ve seen.\n\ninds = np.arange(X.shape[0])\nnp.random.shuffle(inds)\nXtrain, ytrain = X[inds[:150]], y[inds[:150]]\nXtest, ytest = X[inds[150:]], y[inds[150:]]\n\n# We can plot training and test data on the same plot\nplt.scatter(*Xtrain.T, c=ytrain, edgecolor=\"black\", marker='o', label='training data')\nplt.scatter(*Xtest.T, c=ytest, edgecolor=\"black\", marker='^', label='test data')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x2e0a7c670&gt;"
  },
  {
    "objectID": "lecture7-pytorch/notebook copy.html#underfittting",
    "href": "lecture7-pytorch/notebook copy.html#underfittting",
    "title": "Lecture 7: PyTorch",
    "section": "Underfittting",
    "text": "Underfittting\nWe’ll start by fitting a logistic regression model as we’ve seen. This time we’ll keep track of the loss on both the training data and the test data.\n\nnetwork = nn.Sequential(\n    nn.Linear(2, 1),\n    nn.Sigmoid(),\n)\nmodel = LogisticRegressionNeuralNetwork(network)   \n\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\ntrain_losses, test_losses = [], []\npbar = tqdm.trange(2500)\nfor i in pbar:\n    predictions = model(Xtrain)        # Now we can just call model!\n    loss = torch.nn.functional.binary_cross_entropy(predictions, ytrain.flatten().float())\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    pbar.set_description('Loss: %.3f' % loss.item())\n\n    train_losses.append(loss.item())\n    test_loss = torch.nn.functional.binary_cross_entropy(model(Xtest), ytest.flatten().float())\n    test_losses.append(test_loss.item())\n\nLoss: 0.501: 100%|██████████| 2500/2500 [00:02&lt;00:00, 892.89it/s] \n\n\n\nplot_boundary(model, Xtrain, ytrain)\n\n\n\n\nLet’s compute the accuracy on both the training and the test data\n\ntrain_acc = (model.predict(Xtrain) == ytrain).float().mean()\ntest_acc = (model.predict(Xtest) == ytest).float().mean()\nprint('Training accuracy: %.3f, Test accuracy: %.3f' % (train_acc, test_acc))\n\nTraining accuracy: 0.793, Test accuracy: 0.793\n\n\nWe can also look at how the loss on both the training and test data changes as we run gradient descent.\n\nplt.plot(train_losses[:15000:100], label='Training loss')\nplt.plot(test_losses[:15000:100], label='Test loss')\nplt.legend()\nplt.title('Loss vs. training iteration')\n\nText(0.5, 1.0, 'Loss vs. training iteration')\n\n\n\n\n\nHere wee see that both the training loss/accuracy and the test loss/accuracy are quite poor! From our descision boundary plot we can see quite clearly that this is a consequence of our choice of a linear model for this classification problem. We call this problem underfitting, meaning that our model is not expressive enough to capture all the intricacies of our data. As we’ve already seen we can address this by adding neural network layers to increase the expressivity of our model."
  },
  {
    "objectID": "lecture7-pytorch/notebook copy.html#overfitting",
    "href": "lecture7-pytorch/notebook copy.html#overfitting",
    "title": "Lecture 7: PyTorch",
    "section": "Overfitting",
    "text": "Overfitting\nLet’s try creating a much more complex model; one with several large neural network layers and fitting it to our data.\n\nnetwork = nn.Sequential(\n    nn.Linear(2, 200),\n    nn.ReLU(),\n    nn.Linear(200, 200),\n    nn.ReLU(),\n    nn.Linear(200, 200),\n    nn.ReLU(),\n    nn.Linear(200, 1),\n    nn.Sigmoid(),\n)\nmodel = LogisticRegressionNeuralNetwork(network)   \n\noptimizer = optim.SGD(model.parameters(), lr=0.03)\n\ntrain_losses, test_losses = [], []\npbar = tqdm.trange(25000)\nfor i in pbar:\n    predictions = model(Xtrain)        # Now we can just call model!\n    loss = torch.nn.functional.binary_cross_entropy(predictions, ytrain.flatten().float())\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    pbar.set_description('Loss: %.3f' % loss.item())\n\n    train_losses.append(loss.item())\n    test_loss = torch.nn.functional.binary_cross_entropy(model(Xtest), ytest.flatten().float())\n    test_losses.append(test_loss.item())\n\nLoss: 0.073: 100%|██████████| 25000/25000 [00:41&lt;00:00, 602.24it/s]\n\n\nWe can view the descision boundary and accuracy for this classifier.\n\ntrain_acc = (model.predict(Xtrain) == ytrain).float().mean()\nplot_boundary(model, Xtrain, ytrain, title='Training accuracy: %.3f' % train_acc)\n\n\n\n\nWe see that our model is much more expressive and basically correctly classifies every observation in our training dataset. This is great! However the boundary is quite complex. Let’s see what happens when we evaluate on our test set.\n\ntest_acc = (model.predict(Xtest) == ytest).float().mean()\nplot_boundary(model, Xtest, ytest, title='Test accuracy: %.3f' % test_acc)\n\n\n\n\nThe accuracy is much worse. Rather than capturing the true distribution of classes, our model has captured the training set we happened to draw. This means if we draw a new dataset from the same distribution (like our test set), performance is poor. We call this issue overfitting.\nLet’s see how the training and test loss change over gradient descent.\n\nplt.plot(train_losses[:15000:100], label='Training loss')\nplt.plot(test_losses[:15000:100], label='Test loss')\nplt.legend()\nplt.title('Loss vs. training iteration')\n\nText(0.5, 1.0, 'Loss vs. training iteration')\n\n\n\n\n\nMuch of the rest of this class will focus on how to meet the deicate balance of overfitting vs. underfitting!\nIt’s worth noting the the best solution to overfitting is to get more data. If we train with enough data we can avoid overfitting entirely.\n\nX, y = make_moons(2000, noise=0.5)\nplt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.1)\n\n&lt;matplotlib.collections.PathCollection at 0x2b7bd5460&gt;\n\n\n\n\n\n\nX, y = torch.tensor(X).float(), torch.tensor(y)\nnetwork = nn.Sequential(\n    nn.Linear(2, 200),\n    nn.ReLU(),\n    nn.Linear(200, 200),\n    nn.ReLU(),\n    nn.Linear(200, 200),\n    nn.ReLU(),\n    nn.Linear(200, 1),\n    nn.Sigmoid(),\n)\nmodel = LogisticRegressionNeuralNetwork(network)   \n\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\ntrain_losses, test_losses = [], []\npbar = tqdm.trange(2500)\nfor i in pbar:\n    predictions = model(X)        # Now we can just call model!\n    loss = torch.nn.functional.binary_cross_entropy(predictions, y.flatten().float())\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    pbar.set_description('Loss: %.3f' % loss.item())\n\n    train_losses.append(loss.item())\n    test_loss = torch.nn.functional.binary_cross_entropy(model(Xtest), ytest.flatten().float())\n    test_losses.append(test_loss.item())\n\n/var/folders/t9/fjsr8h1x7c7cg4vqw_xhjq2w0000gn/T/ipykernel_79454/2923296796.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X, y = torch.tensor(X).float(), torch.tensor(y)\nLoss: 0.390: 100%|██████████| 2500/2500 [00:16&lt;00:00, 151.68it/s]\n\n\n\ntrain_acc = (model.predict(Xtrain) == ytrain).float().mean()\nplot_boundary(model, Xtrain, ytrain, title='Training accuracy: %.3f' % train_acc)\n\n\n\n\n\ntest_acc = (model.predict(Xtest) == ytest).float().mean()\nplot_boundary(model, Xtest, ytest, title='Test accuracy: %.3f' % test_acc)\n\n\n\n\n\nplt.plot(train_losses[:15000:100], label='Training loss')\nplt.plot(test_losses[:15000:100], label='Test loss')\nplt.legend()\nplt.title('Loss vs. training iteration')\n\nText(0.5, 1.0, 'Loss vs. training iteration')\n\n\n\n\n\nUnfortunately, this often isn’t realistic. Data is hard to collect and more data means our model is slower and more expensive to train."
  },
  {
    "objectID": "lecture7-pytorch/notebook copy.html#early-stopping",
    "href": "lecture7-pytorch/notebook copy.html#early-stopping",
    "title": "Lecture 7: PyTorch",
    "section": "Early stopping",
    "text": "Early stopping\nLet’s return to the original case\n\nX, y = make_moons(300, noise=0.5)\nX, y = torch.tensor(X).float(), torch.tensor(y)\n\ninds = np.arange(X.shape[0])\nnp.random.shuffle(inds)\nXtrain, ytrain = X[inds[:150]], y[inds[:150]]\nXtest, ytest = X[inds[150:]], y[inds[150:]]\n\n# We can plot training and test data on the same plot\nplt.scatter(*Xtrain.T, c=ytrain, edgecolor=\"black\", marker='o', label='training data')\nplt.scatter(*Xtest.T, c=ytest, edgecolor=\"black\", marker='^', label='test data')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x2e0cedf40&gt;\n\n\n\n\n\nLet’s try a network somewhere in-between, with just a single hidden layer.\n\nnetwork = nn.Sequential(\n    nn.Linear(2, 100),\n    nn.ReLU(),\n    nn.Linear(100, 1),\n    nn.Sigmoid(),\n)\nmodel = LogisticRegressionNeuralNetwork(network)   \n\noptimizer = optim.SGD(model.parameters(), lr=0.03)\n\ntrain_losses, test_losses = [], []\npbar = tqdm.trange(25000)\nfor i in pbar:\n    predictions = model(Xtrain)        # Now we can just call model!\n    loss = torch.nn.functional.binary_cross_entropy(predictions, ytrain.flatten().float())\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    pbar.set_description('Loss: %.3f' % loss.item())\n\n    train_losses.append(loss.item())\n    test_loss = torch.nn.functional.binary_cross_entropy(model(Xtest), ytest.flatten().float())\n    test_losses.append(test_loss.item())\n\nLoss: 0.366: 100%|██████████| 25000/25000 [00:28&lt;00:00, 870.32it/s] \n\n\n\ntrain_acc = (model.predict(Xtrain) == ytrain).float().mean()\nplot_boundary(model, Xtrain, ytrain, title='Training accuracy: %.3f' % train_acc)\n\n\n\n\n\ntest_acc = (model.predict(Xtest) == ytest).float().mean()\nplot_boundary(model, Xtest, ytest, title='Test accuracy: %.3f' % test_acc)\n\n\n\n\nWe see that this network is more consistent between train and test, and now performs better on test data! Let’s take a look at the plot of training and test loss.\n\nplt.plot(train_losses[:15000:100], label='Training loss')\nplt.plot(test_losses[:15000:100], label='Test loss')\nplt.legend()\nplt.title('Loss vs. training iteration')\n\nText(0.5, 1.0, 'Loss vs. training iteration')\n\n\n\n\n\nWe still see that both drop quickly, but that test loss increases after a point. How might we use this to pick a better model?\nOne option would be to just use the model where the test loss is lowest. After all, that is our ultimate goal. There are different ways we can think about implementing this. One way to to have gradient descent stop when the test loss begins to increase. We call this approach early stopping\n\nnetwork = nn.Sequential(\n    nn.Linear(2, 100),\n    nn.ReLU(),\n    nn.Linear(100, 1),\n    nn.Sigmoid(),\n)\nmodel = LogisticRegressionNeuralNetwork(network)   \n\noptimizer = optim.SGD(model.parameters(), lr=0.03)\n\ntrain_losses, test_losses = [], []\npbar = tqdm.trange(25000)\nfor i in pbar:\n    predictions = model(Xtrain)        # Now we can just call model!\n    loss = torch.nn.functional.binary_cross_entropy(predictions, ytrain.flatten().float())\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    pbar.set_description('Loss: %.3f' % loss.item())\n\n    train_losses.append(loss.item())\n    test_loss = torch.nn.functional.binary_cross_entropy(model(Xtest), ytest.flatten().float())\n    test_losses.append(test_loss.item())\n\n    if i &gt; 50 and test_loss.item() &gt; test_losses[-2]:\n        break\n\nLoss: 0.481:   1%|▏         | 346/25000 [00:00&lt;00:30, 810.31it/s]\n\n\n\nplt.plot(train_losses[:15000:100], label='Training loss')\nplt.plot(test_losses[:15000:100], label='Test loss')\nplt.legend()\nplt.title('Loss vs. training iteration')\n\nText(0.5, 1.0, 'Loss vs. training iteration')\n\n\n\n\n\n\ntrain_acc = (model.predict(Xtrain) == ytrain).float().mean()\nplot_boundary(model, Xtrain, ytrain, title='Training accuracy: %.3f' % train_acc)\n\n\n\n\n\ntest_acc = (model.predict(Xtest) == ytest).float().mean()\nplot_boundary(model, Xtest, ytest, title='Test accuracy: %.3f' % test_acc)\n\n\n\n\nHere we see that actually we do the best so far with this approach!\nWe could also try our early-stopping approach with our more complex network.\n\nnetwork = nn.Sequential(\n    nn.Linear(2, 200),\n    nn.ReLU(),\n    nn.Linear(200, 200),\n    nn.ReLU(),\n    nn.Linear(200, 200),\n    nn.ReLU(),\n    nn.Linear(200, 1),\n    nn.Sigmoid(),\n)\nmodel = LogisticRegressionNeuralNetwork(network)   \n\noptimizer = optim.SGD(model.parameters(), lr=0.03)\n\ntrain_losses, test_losses = [], []\npbar = tqdm.trange(25000)\nfor i in pbar:\n    predictions = model(Xtrain)        # Now we can just call model!\n    loss = torch.nn.functional.binary_cross_entropy(predictions, ytrain.flatten().float())\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    pbar.set_description('Loss: %.3f' % loss.item())\n\n    train_losses.append(loss.item())\n    test_loss = torch.nn.functional.binary_cross_entropy(model(Xtest), ytest.flatten().float())\n    test_losses.append(test_loss.item())\n\n    if i &gt; 50 and test_loss.item() &gt; test_losses[-2]:\n        break\n\nLoss: 0.465:   2%|▏         | 382/25000 [00:00&lt;00:42, 574.88it/s]\n\n\n\ntrain_acc = (model.predict(Xtrain) == ytrain).float().mean()\nplot_boundary(model, Xtrain, ytrain, title='Training accuracy: %.3f' % train_acc)\n\n\n\n\n\ntest_acc = (model.predict(Xtest) == ytest).float().mean()\nplot_boundary(model, Xtest, ytest, title='Test accuracy: %.3f' % test_acc)\n\n\n\n\n\nplt.plot(train_losses[:15000:100], label='Training loss')\nplt.plot(test_losses[:15000:100], label='Test loss')\nplt.legend()\nplt.title('Loss vs. training iteration')\n\nText(0.5, 1.0, 'Loss vs. training iteration')"
  },
  {
    "objectID": "lecture7-pytorch/notebook copy.html#train-validation-and-test",
    "href": "lecture7-pytorch/notebook copy.html#train-validation-and-test",
    "title": "Lecture 7: PyTorch",
    "section": "Train, validation and test",
    "text": "Train, validation and test\nThere is an issue here though! We’ve now used out test set to (indirectly) train our model. Both by using it to choose the number of layers and by using it to determine how long to run our optimization! This means that our model choice will be biased by our choice of test set, so how can we trust that our test loss or accuracy will actually be a good measure of the real-world performance of our model?\nTo deal with this issue we will typically split our data into 3 parts that we’ll call training, validation and test. We’ll use the validation portion as the portion to fit the model and the test set as the portion we use to estimate how well it will do in practice.\n\nX, y = make_moons(300, noise=0.5)\nX, y = torch.tensor(X).float(), torch.tensor(y)\n\ninds = np.arange(X.shape[0])\nnp.random.shuffle(inds)\nXtrain, ytrain = X[inds[:150]], y[inds[:150]]\nXvalid, yvalid = X[inds[150:225]], y[inds[150:225]]\nXtest, ytest = X[inds[225:]], y[inds[225:]]\n\n# We can plot training and test data on the same plot\nplt.scatter(*Xtrain.T, c=ytrain, edgecolor=\"black\", marker='o', label='training data')\nplt.scatter(*Xtest.T, c=ytest, edgecolor=\"black\", marker='^', label='test data')\nplt.scatter(*Xvalid.T, c=yvalid, edgecolor=\"black\", marker='*', label='valid data')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x2e0c664c0&gt;\n\n\n\n\n\n\nnetwork = nn.Sequential(\n    nn.Linear(2, 200),\n    nn.ReLU(),\n    nn.Linear(200, 200),\n    nn.ReLU(),\n    nn.Linear(200, 200),\n    nn.ReLU(),\n    nn.Linear(200, 1),\n    nn.Sigmoid(),\n)\nmodel = LogisticRegressionNeuralNetwork(network)   \n\noptimizer = optim.SGD(model.parameters(), lr=0.03)\n\ntrain_losses, valid_losses = [], []\npbar = tqdm.trange(25000)\nfor i in pbar:\n    predictions = model(Xtrain)        # Now we can just call model!\n    loss = torch.nn.functional.binary_cross_entropy(predictions, ytrain.flatten().float())\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    pbar.set_description('Loss: %.3f' % loss.item())\n\n    train_losses.append(loss.item())\n    valid_loss = torch.nn.functional.binary_cross_entropy(model(Xvalid), yvalid.flatten().float())\n    valid_losses.append(valid_loss.item())\n\n    if i &gt; 50 and valid_loss.item() &gt; valid_losses[-2]:\n        break\n\nLoss: 0.406:   4%|▍         | 1060/25000 [00:02&lt;00:46, 513.32it/s]\n\n\n\ntrain_acc = (model.predict(Xtrain) == ytrain).float().mean()\nplot_boundary(model, Xtrain, ytrain, title='Training accuracy: %.3f' % train_acc)\n\n\n\n\n\ntest_acc = (model.predict(Xvalid) == yvalid).float().mean()\nplot_boundary(model, Xvalid, yvalid, title='Validation accuracy: %.3f' % test_acc)\n\n\n\n\n\ntest_acc = (model.predict(Xtest) == ytest).float().mean()\nplot_boundary(model, Xtest, ytest, title='Test accuracy: %.3f' % test_acc)"
  },
  {
    "objectID": "lecture7-pytorch/notebook copy.html#cross-validation",
    "href": "lecture7-pytorch/notebook copy.html#cross-validation",
    "title": "Lecture 7: PyTorch",
    "section": "Cross validation",
    "text": "Cross validation\nAnother important approach is cross validation. In this setting, rather than using a single validation set, we will split our training set many times!\n\n\n\nAlt text"
  },
  {
    "objectID": "lecture8-optimization/notes.html#l2-regularization",
    "href": "lecture8-optimization/notes.html#l2-regularization",
    "title": "Lecture 8: Regularization",
    "section": "L2 Regularization",
    "text": "L2 Regularization\nOne way that overfitting can manifest is as a prediction function that is overly sensitive to small changes in the input. We can see this in examples like the one below.\n\nHere in the overfit case, our prediction function is trying to capture the noise of the data rather than just the overall trend. This is explicitly encouraged by losses like the mean squared-error loss, as the loss says fit every observation as closely as possible:\n\\[\\textbf{MSE}(\\mathbf{X}, \\mathbf{y}, \\mathbf{w}) = \\frac{1}{N} \\sum_{i=1}^N ((f(\\mathbf{x}_i, \\mathbf{w}) - y_i)^2)\\]\nWhen our prediction function \\(f\\) is complex enough, it can exactly capture variations due to noise in the training data. As we can see, this means that the function must be very non-smooth, small changes in the input correspond to big changes in the output as we can clearly see in the marked region. This means that the function in this region has a very large slope.\nHow does this observation help us think about regularization? Well, we know that the weights of our model control the slope of the function; large weights correspond to large slopes. Therefore if we want to ensure our prediction function is smooth, we need to make sure that the weights are not too large.\n\n\n\nAn overfit network will have large weights to encode large slopes.\n\n\n\n\nA regularized network will have smaller weights encoding a smooth function.\n\n\nWe can account for this in our loss function by adding a loss that encourages our weights to be close to 0. One such loss is the L2 loss. For a given weight vector \\(\\mathbf{w}\\) the L2 loss is simply the squared 2-norm of the vector: \\[\\textbf{L}_2(\\mathbf{w}) = \\|\\mathbf{w}\\|_2^2 = \\sum_{i=1}^d w_i^2\\]\nIf we have a weight matrix \\(\\mathbf{W}\\) as in neural networks or multinomial logistic regression the L2 loss is just the squared matrix 2-norm, which is again simply the sum of every element squared. For a \\(d \\times e\\) weight matrix \\(\\mathbf{W}\\) the L2 loss is:\n\\[\\textbf{L}_2(\\mathbf{W}) = \\|\\mathbf{W}\\|_2^2 = \\sum_{i=1}^d\\sum_{j=1}^e w_{ij}^2\\]\nWe can then train our model with a combination of losses. For example, if we’re training a regression model we could use:\n\\[\\textbf{Loss}(\\mathbf{X}, \\mathbf{y}, \\mathbf{w}) = \\textbf{MSE}(\\mathbf{X}, \\mathbf{y}, \\mathbf{w}) + \\lambda \\textbf{L}_2(\\mathbf{w})\\]\nHere \\(\\lambda\\) is a value that we can choose to trade off between these two losses. Too high a value for \\(\\lambda\\) and we might end up with a value that is too smooth or just flat, too low and or L2 loss might not affect our result at all."
  },
  {
    "objectID": "lecture8-optimization/notes.html#bias-terms",
    "href": "lecture8-optimization/notes.html#bias-terms",
    "title": "Lecture 8: Regularization",
    "section": "Bias terms",
    "text": "Bias terms\nOne important detail of \\(\\ell^2\\) regularization is how we treat"
  },
  {
    "objectID": "playground/CONTRIBUTING.html",
    "href": "playground/CONTRIBUTING.html",
    "title": "CS 152: Neural Networks",
    "section": "",
    "text": "Want to contribute? Great! First, read this page (including the small print at the end).\n\nBefore you contribute\nBefore we can use your code, you must sign the [Google Individual Contributor License Agreement] (https://cla.developers.google.com/about/google-individual) (CLA), which you can do online. The CLA is necessary mainly because you own the copyright to your changes, even after your contribution becomes part of our codebase, so we need your permission to use and distribute your code. We also need to be sure of various other things—for instance that you’ll tell us if you know that your code infringes on other people’s patents. You don’t have to sign the CLA until after you’ve submitted your code for review and a member has approved it, but you must do it before we can put your code into our codebase. Before you start working on a larger contribution, you should get in touch with us first through the issue tracker with your idea so that we can help out and possibly guide you. Coordinating up front makes it much easier to avoid frustration later on.\n\n\nCode reviews\nAll submissions, including submissions by project members, require review. We use Github pull requests for this purpose.\n\n\nThe small print\nContributions made by corporations are covered by a different agreement than the one above, the [Software Grant and Corporate Contributor License Agreement] (https://cla.developers.google.com/about/google-corporate)."
  },
  {
    "objectID": "lecture9-optimization/notes.html",
    "href": "lecture9-optimization/notes.html",
    "title": "Lecture 9: Optimization",
    "section": "",
    "text": "Try out the concepts from this lecture in the Neural Network Playground!"
  },
  {
    "objectID": "lecture7-pytorch/notebook.html#underfittting",
    "href": "lecture7-pytorch/notebook.html#underfittting",
    "title": "Lecture 7: PyTorch",
    "section": "Underfittting",
    "text": "Underfittting\nWe’ll start by fitting a logistic regression model as we’ve seen. This time we’ll keep track of the loss on both the training data and the test data.\n\nnetwork = nn.Sequential(\n    nn.Linear(2, 1),\n    nn.Sigmoid(),\n)\nmodel = LogisticRegressionNeuralNetwork(network)   \n\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\ntrain_losses, test_losses = [], []\npbar = tqdm.trange(2500)\nfor i in pbar:\n    predictions = model(Xtrain)        # Now we can just call model!\n    loss = torch.nn.functional.binary_cross_entropy(predictions, ytrain.flatten().float())\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    pbar.set_description('Loss: %.3f' % loss.item())\n\n    train_losses.append(loss.item())\n    test_loss = torch.nn.functional.binary_cross_entropy(model(Xtest), ytest.flatten().float())\n    test_losses.append(test_loss.item())\n\nLoss: 0.501: 100%|██████████| 2500/2500 [00:02&lt;00:00, 892.89it/s] \n\n\n\nplot_boundary(model, Xtrain, ytrain)\n\n\n\n\nLet’s compute the accuracy on both the training and the test data\n\ntrain_acc = (model.predict(Xtrain) == ytrain).float().mean()\ntest_acc = (model.predict(Xtest) == ytest).float().mean()\nprint('Training accuracy: %.3f, Test accuracy: %.3f' % (train_acc, test_acc))\n\nTraining accuracy: 0.793, Test accuracy: 0.793\n\n\nWe can also look at how the loss on both the training and test data changes as we run gradient descent.\n\nplt.plot(train_losses[:15000:100], label='Training loss')\n#plt.plot(test_losses[:15000:100], label='Test loss')\nplt.legend()\nplt.title('Loss vs. training iteration')\n\nText(0.5, 1.0, 'Loss vs. training iteration')\n\n\n\n\n\nHere wee see that both the training loss/accuracy and the test loss/accuracy are quite poor! From our descision boundary plot we can see quite clearly that this is a consequence of our choice of a linear model for this classification problem. We call this problem underfitting, meaning that our model is not expressive enough to capture all the intricacies of our data. As we’ve already seen we can address this by adding neural network layers to increase the expressivity of our model."
  },
  {
    "objectID": "lecture7-pytorch/notebook.html#overfitting",
    "href": "lecture7-pytorch/notebook.html#overfitting",
    "title": "Lecture 7: PyTorch",
    "section": "Overfitting",
    "text": "Overfitting\nLet’s try creating a much more complex model; one with several large neural network layers and fitting it to our data.\n\nnetwork = nn.Sequential(\n    nn.Linear(2, 200),\n    nn.ReLU(),\n    nn.Linear(200, 200),\n    nn.ReLU(),\n    nn.Linear(200, 200),\n    nn.ReLU(),\n    nn.Linear(200, 1),\n    nn.Sigmoid(),\n)\nmodel = LogisticRegressionNeuralNetwork(network)   \n\noptimizer = optim.SGD(model.parameters(), lr=0.03)\n\ntrain_losses, test_losses = [], []\npbar = tqdm.trange(25000)\nfor i in pbar:\n    predictions = model(Xtrain)        # Now we can just call model!\n    loss = torch.nn.functional.binary_cross_entropy(predictions, ytrain.flatten().float())\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    pbar.set_description('Loss: %.3f' % loss.item())\n\n    train_losses.append(loss.item())\n    test_loss = torch.nn.functional.binary_cross_entropy(model(Xtest), ytest.flatten().float())\n    test_losses.append(test_loss.item())\n\nLoss: 0.073: 100%|██████████| 25000/25000 [00:41&lt;00:00, 602.24it/s]\n\n\nWe can view the descision boundary and accuracy for this classifier.\n\ntrain_acc = (model.predict(Xtrain) == ytrain).float().mean()\nplot_boundary(model, Xtrain, ytrain, title='Training accuracy: %.3f' % train_acc)\n\n\n\n\nWe see that our model is much more expressive and basically correctly classifies every observation in our training dataset. This is great! However the boundary is quite complex. Let’s see what happens when we evaluate on our test set.\n\ntest_acc = (model.predict(Xtest) == ytest).float().mean()\nplot_boundary(model, Xtest, ytest, title='Test accuracy: %.3f' % test_acc)\n\n\n\n\nThe accuracy is much worse. Rather than capturing the true distribution of classes, our model has captured the training set we happened to draw. This means if we draw a new dataset from the same distribution (like our test set), performance is poor. We call this issue overfitting.\nLet’s see how the training and test loss change over gradient descent.\n\nplt.plot(train_losses[:15000:100], label='Training loss')\nplt.plot(test_losses[:15000:100], label='Test loss')\nplt.legend()\nplt.title('Loss vs. training iteration')\n\nText(0.5, 1.0, 'Loss vs. training iteration')\n\n\n\n\n\nMuch of the rest of this class will focus on how to meet the deicate balance of overfitting vs. underfitting!\nIt’s worth noting the the best solution to overfitting is to get more data. If we train with enough data we can avoid overfitting entirely.\n\nX, y = make_moons(2000, noise=0.5)\nplt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.1)\n\n&lt;matplotlib.collections.PathCollection at 0x2b7bd5460&gt;\n\n\n\n\n\n\nX, y = torch.tensor(X).float(), torch.tensor(y)\nnetwork = nn.Sequential(\n    nn.Linear(2, 200),\n    nn.ReLU(),\n    nn.Linear(200, 200),\n    nn.ReLU(),\n    nn.Linear(200, 200),\n    nn.ReLU(),\n    nn.Linear(200, 1),\n    nn.Sigmoid(),\n)\nmodel = LogisticRegressionNeuralNetwork(network)   \n\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\ntrain_losses, test_losses = [], []\npbar = tqdm.trange(2500)\nfor i in pbar:\n    predictions = model(X)        # Now we can just call model!\n    loss = torch.nn.functional.binary_cross_entropy(predictions, y.flatten().float())\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    pbar.set_description('Loss: %.3f' % loss.item())\n\n    train_losses.append(loss.item())\n    test_loss = torch.nn.functional.binary_cross_entropy(model(Xtest), ytest.flatten().float())\n    test_losses.append(test_loss.item())\n\n/var/folders/t9/fjsr8h1x7c7cg4vqw_xhjq2w0000gn/T/ipykernel_79454/2923296796.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X, y = torch.tensor(X).float(), torch.tensor(y)\nLoss: 0.390: 100%|██████████| 2500/2500 [00:16&lt;00:00, 151.68it/s]\n\n\n\ntrain_acc = (model.predict(Xtrain) == ytrain).float().mean()\nplot_boundary(model, Xtrain, ytrain, title='Training accuracy: %.3f' % train_acc)\n\n\n\n\n\ntest_acc = (model.predict(Xtest) == ytest).float().mean()\nplot_boundary(model, Xtest, ytest, title='Test accuracy: %.3f' % test_acc)\n\n\n\n\n\nplt.plot(train_losses[:15000:100], label='Training loss')\nplt.plot(test_losses[:15000:100], label='Test loss')\nplt.legend()\nplt.title('Loss vs. training iteration')\n\nText(0.5, 1.0, 'Loss vs. training iteration')\n\n\n\n\n\nUnfortunately, this often isn’t realistic. Data is hard to collect and more data means our model is slower and more expensive to train."
  },
  {
    "objectID": "lecture7-pytorch/notebook.html#early-stopping",
    "href": "lecture7-pytorch/notebook.html#early-stopping",
    "title": "Lecture 7: PyTorch",
    "section": "Early stopping",
    "text": "Early stopping\nLet’s return to the original case\n\nX, y = make_moons(300, noise=0.5)\nX, y = torch.tensor(X).float(), torch.tensor(y)\n\ninds = np.arange(X.shape[0])\nnp.random.shuffle(inds)\nXtrain, ytrain = X[inds[:150]], y[inds[:150]]\nXtest, ytest = X[inds[150:]], y[inds[150:]]\n\n# We can plot training and test data on the same plot\nplt.scatter(*Xtrain.T, c=ytrain, edgecolor=\"black\", marker='o', label='training data')\nplt.scatter(*Xtest.T, c=ytest, edgecolor=\"black\", marker='^', label='test data')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x2e0cedf40&gt;\n\n\n\n\n\nLet’s try a network somewhere in-between, with just a single hidden layer.\n\nnetwork = nn.Sequential(\n    nn.Linear(2, 100),\n    nn.ReLU(),\n    nn.Linear(100, 1),\n    nn.Sigmoid(),\n)\nmodel = LogisticRegressionNeuralNetwork(network)   \n\noptimizer = optim.SGD(model.parameters(), lr=0.03)\n\ntrain_losses, test_losses = [], []\npbar = tqdm.trange(25000)\nfor i in pbar:\n    predictions = model(Xtrain)        # Now we can just call model!\n    loss = torch.nn.functional.binary_cross_entropy(predictions, ytrain.flatten().float())\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    pbar.set_description('Loss: %.3f' % loss.item())\n\n    train_losses.append(loss.item())\n    test_loss = torch.nn.functional.binary_cross_entropy(model(Xtest), ytest.flatten().float())\n    test_losses.append(test_loss.item())\n\nLoss: 0.366: 100%|██████████| 25000/25000 [00:28&lt;00:00, 870.32it/s] \n\n\n\ntrain_acc = (model.predict(Xtrain) == ytrain).float().mean()\nplot_boundary(model, Xtrain, ytrain, title='Training accuracy: %.3f' % train_acc)\n\n\n\n\n\ntest_acc = (model.predict(Xtest) == ytest).float().mean()\nplot_boundary(model, Xtest, ytest, title='Test accuracy: %.3f' % test_acc)\n\n\n\n\nWe see that this network is more consistent between train and test, and now performs better on test data! Let’s take a look at the plot of training and test loss.\n\nplt.plot(train_losses[:15000:100], label='Training loss')\nplt.plot(test_losses[:15000:100], label='Test loss')\nplt.legend()\nplt.title('Loss vs. training iteration')\n\nText(0.5, 1.0, 'Loss vs. training iteration')\n\n\n\n\n\nWe still see that both drop quickly, but that test loss increases after a point. How might we use this to pick a better model?\nOne option would be to just use the model where the test loss is lowest. After all, that is our ultimate goal. There are different ways we can think about implementing this. One way to to have gradient descent stop when the test loss begins to increase. We call this approach early stopping\n\nnetwork = nn.Sequential(\n    nn.Linear(2, 100),\n    nn.ReLU(),\n    nn.Linear(100, 1),\n    nn.Sigmoid(),\n)\nmodel = LogisticRegressionNeuralNetwork(network)   \n\noptimizer = optim.SGD(model.parameters(), lr=0.03)\n\ntrain_losses, test_losses = [], []\npbar = tqdm.trange(25000)\nfor i in pbar:\n    predictions = model(Xtrain)        # Now we can just call model!\n    loss = torch.nn.functional.binary_cross_entropy(predictions, ytrain.flatten().float())\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    pbar.set_description('Loss: %.3f' % loss.item())\n\n    train_losses.append(loss.item())\n    test_loss = torch.nn.functional.binary_cross_entropy(model(Xtest), ytest.flatten().float())\n    test_losses.append(test_loss.item())\n\n    if i &gt; 50 and test_loss.item() &gt; test_losses[-2]:\n        break\n\nLoss: 0.481:   1%|▏         | 346/25000 [00:00&lt;00:30, 810.31it/s]\n\n\n\nplt.plot(train_losses[:15000:100], label='Training loss')\nplt.plot(test_losses[:15000:100], label='Test loss')\nplt.legend()\nplt.title('Loss vs. training iteration')\n\nText(0.5, 1.0, 'Loss vs. training iteration')\n\n\n\n\n\n\ntrain_acc = (model.predict(Xtrain) == ytrain).float().mean()\nplot_boundary(model, Xtrain, ytrain, title='Training accuracy: %.3f' % train_acc)\n\n\n\n\n\ntest_acc = (model.predict(Xtest) == ytest).float().mean()\nplot_boundary(model, Xtest, ytest, title='Test accuracy: %.3f' % test_acc)\n\n\n\n\nHere we see that actually we do the best so far with this approach!\nWe could also try our early-stopping approach with our more complex network.\n\nnetwork = nn.Sequential(\n    nn.Linear(2, 200),\n    nn.ReLU(),\n    nn.Linear(200, 200),\n    nn.ReLU(),\n    nn.Linear(200, 200),\n    nn.ReLU(),\n    nn.Linear(200, 1),\n    nn.Sigmoid(),\n)\nmodel = LogisticRegressionNeuralNetwork(network)   \n\noptimizer = optim.SGD(model.parameters(), lr=0.03)\n\ntrain_losses, test_losses = [], []\npbar = tqdm.trange(25000)\nfor i in pbar:\n    predictions = model(Xtrain)        # Now we can just call model!\n    loss = torch.nn.functional.binary_cross_entropy(predictions, ytrain.flatten().float())\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    pbar.set_description('Loss: %.3f' % loss.item())\n\n    train_losses.append(loss.item())\n    test_loss = torch.nn.functional.binary_cross_entropy(model(Xtest), ytest.flatten().float())\n    test_losses.append(test_loss.item())\n\n    if i &gt; 50 and test_loss.item() &gt; test_losses[-2]:\n        break\n\nLoss: 0.465:   2%|▏         | 382/25000 [00:00&lt;00:42, 574.88it/s]\n\n\n\ntrain_acc = (model.predict(Xtrain) == ytrain).float().mean()\nplot_boundary(model, Xtrain, ytrain, title='Training accuracy: %.3f' % train_acc)\n\n\n\n\n\ntest_acc = (model.predict(Xtest) == ytest).float().mean()\nplot_boundary(model, Xtest, ytest, title='Test accuracy: %.3f' % test_acc)\n\n\n\n\n\nplt.plot(train_losses[:15000:100], label='Training loss')\nplt.plot(test_losses[:15000:100], label='Test loss')\nplt.legend()\nplt.title('Loss vs. training iteration')\n\nText(0.5, 1.0, 'Loss vs. training iteration')"
  },
  {
    "objectID": "lecture7-pytorch/notebook.html#train-validation-and-test",
    "href": "lecture7-pytorch/notebook.html#train-validation-and-test",
    "title": "Lecture 7: PyTorch",
    "section": "Train, validation and test",
    "text": "Train, validation and test\nThere is an issue here though! We’ve now used out test set to (indirectly) train our model. Both by using it to choose the number of layers and by using it to determine how long to run our optimization! This means that our model choice will be biased by our choice of test set, so how can we trust that our test loss or accuracy will actually be a good measure of the real-world performance of our model?\nTo deal with this issue we will typically split our data into 3 parts that we’ll call training, validation and test. We’ll use the validation portion as the portion to fit the model and the test set as the portion we use to estimate how well it will do in practice.\n\nX, y = make_moons(300, noise=0.5)\nX, y = torch.tensor(X).float(), torch.tensor(y)\n\ninds = np.arange(X.shape[0])\nnp.random.shuffle(inds)\nXtrain, ytrain = X[inds[:150]], y[inds[:150]]\nXvalid, yvalid = X[inds[150:225]], y[inds[150:225]]\nXtest, ytest = X[inds[225:]], y[inds[225:]]\n\n# We can plot training and test data on the same plot\nplt.scatter(*Xtrain.T, c=ytrain, edgecolor=\"black\", marker='o', label='training data')\nplt.scatter(*Xtest.T, c=ytest, edgecolor=\"black\", marker='^', label='test data')\nplt.scatter(*Xvalid.T, c=yvalid, edgecolor=\"black\", marker='*', label='valid data')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x2e0c664c0&gt;\n\n\n\n\n\n\nnetwork = nn.Sequential(\n    nn.Linear(2, 200),\n    nn.ReLU(),\n    nn.Linear(200, 200),\n    nn.ReLU(),\n    nn.Linear(200, 200),\n    nn.ReLU(),\n    nn.Linear(200, 1),\n    nn.Sigmoid(),\n)\nmodel = LogisticRegressionNeuralNetwork(network)   \n\noptimizer = optim.SGD(model.parameters(), lr=0.03)\n\ntrain_losses, valid_losses = [], []\npbar = tqdm.trange(25000)\nfor i in pbar:\n    predictions = model(Xtrain)        # Now we can just call model!\n    loss = torch.nn.functional.binary_cross_entropy(predictions, ytrain.flatten().float())\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    pbar.set_description('Loss: %.3f' % loss.item())\n\n    train_losses.append(loss.item())\n    valid_loss = torch.nn.functional.binary_cross_entropy(model(Xvalid), yvalid.flatten().float())\n    valid_losses.append(valid_loss.item())\n\n    if i &gt; 50 and valid_loss.item() &gt; valid_losses[-2]:\n        break\n\nLoss: 0.406:   4%|▍         | 1060/25000 [00:02&lt;00:46, 513.32it/s]\n\n\n\ntrain_acc = (model.predict(Xtrain) == ytrain).float().mean()\nplot_boundary(model, Xtrain, ytrain, title='Training accuracy: %.3f' % train_acc)\n\n\n\n\n\ntest_acc = (model.predict(Xvalid) == yvalid).float().mean()\nplot_boundary(model, Xvalid, yvalid, title='Validation accuracy: %.3f' % test_acc)\n\n\n\n\n\ntest_acc = (model.predict(Xtest) == ytest).float().mean()\nplot_boundary(model, Xtest, ytest, title='Test accuracy: %.3f' % test_acc)"
  },
  {
    "objectID": "lecture7-pytorch/notebook.html#cross-validation",
    "href": "lecture7-pytorch/notebook.html#cross-validation",
    "title": "Lecture 7: PyTorch",
    "section": "Cross validation",
    "text": "Cross validation\nAnother important approach is cross validation. In this setting, rather than using a single validation set, we will split our training set many times!\n\n\n\nAlt text"
  },
  {
    "objectID": "lecture7-pytorch/notebook.html#overfitting-with-regression",
    "href": "lecture7-pytorch/notebook.html#overfitting-with-regression",
    "title": "Lecture 7: PyTorch",
    "section": "Overfitting with regression",
    "text": "Overfitting with regression\n\nnp.random.seed(5)\nX = np.random.randn(50, 1) * 0.1 + np.linspace(-3, 3, 50).reshape((-1, 1))\ny = X ** 2 + np.random.randn(50, 1) * 1.\nX, y = torch.tensor(X).float(), torch.tensor(y).float() - y.mean()\n\nXtest = np.random.randn(30, 1) * 0.1 + np.linspace(-3, 3, 30).reshape((-1, 1))\nytest = Xtest ** 2 + np.random.randn(30, 1) * 1.\nXtest, ytest = torch.tensor(Xtest).float(), torch.tensor(ytest).float() - y.mean()\n\nXvalid = np.random.randn(30, 1) * 0.1 + np.linspace(-3, 3, 30).reshape((-1, 1))\nyvalid = Xvalid ** 2 + np.random.randn(30, 1) * 1.\nXvalid, yvalid = torch.tensor(Xvalid).float(), torch.tensor(yvalid).float() - y.mean()\nplt.scatter(X.flatten(), y)\n\n&lt;matplotlib.collections.PathCollection at 0x298928f10&gt;\n\n\n\n\n\n\nfrom torch import nn, optim\nnetwork = nn.Sequential(\n    nn.Linear(1, 200),\n    nn.ReLU(),\n    nn.Linear(200, 200),\n    nn.ReLU(),\n    nn.Linear(200, 200),\n    nn.ReLU(),\n    nn.Linear(200, 1),\n)\nmodel = network \n\noptimizer = optim.Adam(model.parameters(), lr=0.0003)\n\ntrain_losses, valid_losses = [], []\npbar = tqdm.trange(25000)\nfor i in pbar:\n    predictions = model(X)        # Now we can just call model!\n    loss = torch.sum((predictions - y) ** 2)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    pbar.set_description('Loss: %.3f' % loss.item())\n\n    train_losses.append(loss.item())\n    valid_loss = torch.sum((model(Xvalid) - yvalid) ** 2)\n    valid_losses.append(valid_loss.item())\n\n    \n\nLoss: 3.583: 100%|██████████| 25000/25000 [01:02&lt;00:00, 398.69it/s] \n\n\n\nXplot = torch.linspace(-3, 3, 200).reshape((-1, 1))\nyplot = model(Xplot)\nplt.plot(Xplot.flatten().detach().numpy(), yplot.detach().numpy(), c='red')\nplt.scatter(X.flatten(), y)\n\n&lt;matplotlib.collections.PathCollection at 0x2b776ee20&gt;\n\n\n\n\n\n\nfrom torch import nn, optim\nnetwork = nn.Sequential(\n    nn.Linear(1, 200),\n    nn.ReLU(),\n    nn.Linear(200, 200),\n    nn.ReLU(),\n    nn.Linear(200, 200),\n    nn.ReLU(),\n    nn.Linear(200, 1),\n)\nmodel = network \n\noptimizer = optim.Adam(model.parameters(), lr=0.0003, weight_decay=3.)\n\ntrain_losses, valid_losses = [], []\npbar = tqdm.trange(25000)\nfor i in pbar:\n    predictions = model(X)        # Now we can just call model!\n    loss = torch.sum((predictions - y) ** 2)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    pbar.set_description('Loss: %.3f' % loss.item())\n\n    train_losses.append(loss.item())\n    valid_loss = torch.sum((model(Xvalid) - yvalid) ** 2)\n    valid_losses.append(valid_loss.item())\n\nLoss: 45.709: 100%|██████████| 25000/25000 [01:10&lt;00:00, 355.28it/s]\n\n\n\nXplot = torch.linspace(-3, 3, 200).reshape((-1, 1))\nyplot = model(Xplot)\nplt.plot(Xplot.flatten().detach().numpy(), yplot.detach().numpy(), c='red')\nplt.scatter(X.flatten(), y)\n\n&lt;matplotlib.collections.PathCollection at 0x2b7acaa30&gt;\n\n\n\n\n\n\nfrom torch import nn, optim\nnetwork = nn.Sequential(\n    nn.Linear(1, 200),\n    nn.ReLU(),\n    nn.Linear(200, 200),\n    nn.ReLU(),\n    nn.Linear(200, 200),\n    nn.ReLU(),\n    nn.Linear(200, 1),\n)\nmodel = network \n\noptimizer = optim.Adam(model.parameters(), lr=0.0003, weight_decay=300.)\n\ntrain_losses, valid_losses = [], []\npbar = tqdm.trange(25000)\nfor i in pbar:\n    predictions = model(X)        # Now we can just call model!\n    loss = torch.sum((predictions - y) ** 2)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    pbar.set_description('Loss: %.3f' % loss.item())\n\n    train_losses.append(loss.item())\n    valid_loss = torch.sum((model(Xvalid) - yvalid) ** 2)\n    valid_losses.append(valid_loss.item())\n\nLoss: 443.216:  25%|██▍       | 6226/25000 [00:18&lt;00:55, 338.47it/s]\n\n\nKeyboardInterrupt: \n\n\n\nXplot = torch.linspace(-3, 3, 200).reshape((-1, 1))\nyplot = model(Xplot)\nplt.plot(Xplot.flatten().detach().numpy(), yplot.detach().numpy(), c='red')\nplt.scatter(X.flatten(), y)\n\n&lt;matplotlib.collections.PathCollection at 0x2b79517c0&gt;\n\n\n\n\n\n\nfrom torch import nn, optim\nnetwork = nn.Sequential(\n    nn.Linear(1, 200),\n    nn.Dropout(p=0.5),\n    nn.ReLU(),\n    nn.Linear(200, 200),\n    nn.Dropout(p=0.5),\n    nn.ReLU(),\n    nn.Linear(200, 200),\n    nn.Dropout(p=0.5),\n    nn.ReLU(),\n    nn.Linear(200, 1),\n)\nmodel = network \n\noptimizer = optim.Adam(model.parameters(), lr=0.0003, weight_decay=300.)\n\ntrain_losses, valid_losses = [], []\npbar = tqdm.trange(25000)\nfor i in pbar:\n    predictions = model(X)        # Now we can just call model!\n    loss = torch.sum((predictions - y) ** 2)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    pbar.set_description('Loss: %.3f' % loss.item())\n\n    train_losses.append(loss.item())\n    valid_loss = torch.sum((model(Xvalid) - yvalid) ** 2)\n    valid_losses.append(valid_loss.item())"
  },
  {
    "objectID": "lecture8-optimization/notes.html#dropout",
    "href": "lecture8-optimization/notes.html#dropout",
    "title": "Lecture 8: Regularization",
    "section": "Dropout",
    "text": "Dropout\nAnother way to think about overfitting is through Interdependency. In order to capture small scale variations in our data, our network needs to dedicate many complex connections to capturing these specific variations. Another effective and popular form of regularization is dropout which purposefully breaks connections at training training time in order to encourage the network to learn reduce reliance on single specialized neurons and create redundancy.\nAs shown in this figure from the original dropout paper, dropout randomly removes neurons from the network at each step of training, performing the update with respect to this new randomized network.\n\nThe probability that any given neuron is removed is called the dropout rate \\(r\\). Mathematically, we can view dropout as a randomized function that is applied to the input of each layer. This function performs an element-wise multiplication \\((\\odot)\\) of the input \\(\\mathbf{X}\\) with a random matrix \\(\\mathbf{D}\\) of 1’s and 0’s, where \\(p(d_{ij}=0)=r\\).\n\\[\\text{Dropout}(\\mathbf{X}, r) = \\mathbf{D} \\odot \\mathbf{X}, \\quad \\mathbf{D} =\n\\begin{bmatrix}\nd_{11} & d_{12} & \\dots & d_{1n} \\\\\nd_{21} & d_{22} & \\dots & d_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nd_{m1} & d_{m2} & \\dots &  d_{mn}\n\\end{bmatrix},\\ d_{ij} \\sim \\text{Bernoulli}(1-r)\\]\nWe can shorten \\(\\text{Dropout}(\\mathbf{X}, r)\\) to \\(\\text{DO}_r(\\mathbf{X})\\). With this we can write a network layer with sigmoid activation and dropout as:\n\\[\n\\phi(\\mathbf{x}) = \\sigma(\\text{DO}_r(\\mathbf{x})^T\\mathbf{W} + \\mathbf{b})\n\\]\nA network with several dropout layers would have a prediction function defined as:\n\\[f(\\mathbf{x}, \\mathbf{w}_0,...) = \\text{DO}_r(\\sigma( \\text{DO}_r(\\sigma( \\text{DO}_r(\\sigma(\\text{DO}_r( \\mathbf{x})^T  \\mathbf{W}_2 + \\mathbf{b}_2))^T \\mathbf{W}_1 + \\mathbf{b}_1))^T \\mathbf{w}_0 + \\mathbf{b}_0\\]\nOr more simply as a sequence of operations:\n\\[\n\\mathbf{a} = \\sigma(\\text{DO}_r(\\mathbf{x})^T\\mathbf{W}_2 + \\mathbf{b}_2)\n\\]\n\\[\n\\mathbf{b} = \\sigma(\\text{DO}_r(\\mathbf{a})^T\\mathbf{W}_1 + \\mathbf{b}_1)\n\\]\n\\[\n\\mathbf{f} = \\text{DO}_r(\\mathbf{b})^T\\mathbf{w}_0 + b_0\n\\]\nThe randomness introduced by dropout will cause our prediction function to be noisy. By dropping out different neurons at each step, we get different prediction functions from the same weights:\n\n\n\n\n\n\n\n\n\nIf we average all these networks however, we get something quite smooth, with redundancies in the predictions made by each neuron:\n\nWe can see from this example that unlike L2 and L1 regularization, dropout doesn’t enforce that weights should be small. Rather it encourages redundancy in the network, preventing neurons from becoming co-dependent."
  },
  {
    "objectID": "lecture8-optimization/notes.html#l2-regularization-1",
    "href": "lecture8-optimization/notes.html#l2-regularization-1",
    "title": "Lecture 8: Regularization",
    "section": "",
    "text": "We’ve seen that overfitting occurs when we do not have enough data and when the data we do have is noisy. In these cases we would expect that our predictions should have some degree of uncertainty. Do the losses we’ve looked at so far account for this?\nLet’s revisit the general negative log-likelihood framework that we’ve seen throughout this class:\n\\[\n\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = -\\sum_{i=1}^N \\log p(y_i \\mid \\mathbf{x}_i, \\mathbf{w})\n\\]\nRecall that in the logistic regression case this looked like:\n\\[\n\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = -\\sum_{i=1}^N \\log \\sigma( (2y_i-1)\\phi(\\mathbf{x}_i)^T\\mathbf{w})\n\\]\nAnd in the linear regression case, it looked like:\n\\[\n\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = \\sum_{i=1}^N (\\phi(\\mathbf{x}_i)^T\\mathbf{w}-y_i)^2\n\\]\nClearly this loss says that our model should aim to maximize \\(p(y_i\\mid \\mathbf{x}_i, \\mathbf{w})\\) for every observation \\(i\\) even for observations affected by noise. This can be bad! If we have a complex approximating function, like a deep neural network, it can make confident predictions on observations that should be uncertain.\n\n\n\n\n\nMathamatically, we see from our loss that confident predictions correspond to large values of \\(\\phi(\\mathbf{x}_i)^T\\mathbf{w}\\), if we can discourage this value from becoming too big, we can prevent our predictions from being too confident. We can accomplish this with a loss that encourages the entries of \\(\\mathbf{w}\\) to be close to \\(0\\). A natural choice would be the squared error, which we can write in a few different ways:\n\\[\n\\sum_{i=1}^d(w_i-0)^2=\\sum_{i=1}^dw_i^2=\\mathbf{w}^T\\mathbf{w} = \\|\\mathbf{w} \\|^2_2\n\\]\nWe call this loss an \\(\\ell^2\\) (L2) regularizer, as it minimizes the \\(\\ell^2\\) norm of \\(\\mathbf{w}\\). If we combine this with our negative log-likelihood, we get a loss that balances making confident, correct predictions, while preventing overfitting due to overconfidence in noisy predictions.\n\\[\n\\textbf{Loss}(\\mathbf{w}, \\mathbf{X},\\mathbf{y})= \\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) + \\lambda \\|\\mathbf{w}\\|^2_2\n\\]\nHere \\(\\lambda\\) is a value that we choose in order to trade-off these two goals. We can see what this looks like for a single observation in the case of logistic regression. Below we’ll plot the function \\(-\\log \\sigma(w) + \\lambda w^2\\), the loss in the 1-dimensional case. We see that in the case where \\(\\lambda=0\\), we can make \\(w\\) arbitrarily large and thus the function has no minimum! When \\(\\lambda &gt; 0\\), we get a clearly defined minimum."
  },
  {
    "objectID": "lecture8-optimization/notes.html#l2-regularization-for-neural-networks",
    "href": "lecture8-optimization/notes.html#l2-regularization-for-neural-networks",
    "title": "Lecture 8: Regularization",
    "section": "L2 Regularization for neural networks",
    "text": "L2 Regularization for neural networks\nIf we are dealing with a neural network model, we may actually have many weight vectors and matrices. For example in a 4 hidden-layer network with sigmoid activations we have a prediction function that looks like: \\[f(\\mathbf{x}, \\mathbf{w}_0,...) = \\sigma( \\sigma( \\sigma( \\sigma( \\mathbf{x^T} \\mathbf{W}_4)^T  \\mathbf{W}_3)^T  \\mathbf{W}_2)^T \\mathbf{W}_1)^T \\mathbf{w}_0\\]\n\nIn this case, we can simply add up the L2 loss for every weight. For a network with \\(L\\) hidden layers the L2 loss would simply be: \\[\\textbf{L}_2(\\mathbf{w}_0, \\mathbf{W}_1,...,\\mathbf{W}_L) = \\sum_{l=0}^L\\|\\mathbf{W}_l\\|_2^2\\]\nIn practice most networks also incorporate bias terms, so each linear function in our network can be written as:\n\\[\n\\mathbf{x}^T\\mathbf{W} + \\mathbf{b}\n\\]\nAnd the full prediction function for a sigmoid-activation network might be:\n\\[f(\\mathbf{x}, \\mathbf{w}_0,...) = \\sigma( \\sigma( \\sigma( \\sigma( \\mathbf{x^T} \\mathbf{W}_4 + \\mathbf{b}_4)^T  \\mathbf{W}_3 + \\mathbf{b}_3)^T  \\mathbf{W}_2 + \\mathbf{b}_2)^T \\mathbf{W}_1 + \\mathbf{b}_1)^T \\mathbf{w}_0 + \\mathbf{b}_0\\]\nEach of these biases is a constant offset and does not affect the slope of the function or how quickly the output changes with small changes in the input. This means that the bias terms do not contribute to overfitting, therefore we do not need to regularize them!"
  },
  {
    "objectID": "lecture8-optimization/notes.html#l2-regularization-in-pytorch",
    "href": "lecture8-optimization/notes.html#l2-regularization-in-pytorch",
    "title": "Lecture 8: Regularization",
    "section": "L2 Regularization in PyTorch",
    "text": "L2 Regularization in PyTorch\nIn PyTorch, L2 regularization is actually handled by the optimizer and is known as weight decay. This name comes from the fact that regularization encourages unimportant weights to decay to 0. When creating a PyTorch optimizer, we can specify how much L2 regularization to add to our loss by setting the weight_decay option to our desired L2 weight ( \\(\\lambda\\) in our notation).\n\nfrom torch import optim\noptimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=0.01)"
  },
  {
    "objectID": "lecture8-optimization/notes.html#l1-regularization",
    "href": "lecture8-optimization/notes.html#l1-regularization",
    "title": "Lecture 8: Regularization",
    "section": "L1 Regularization",
    "text": "L1 Regularization\nA natural alternative to L2 regularization, where we minimized the square of each weight is to simply minimize the absolute value of each weight, which should have a similar effect of encouraging our weights to be close to 0.\n\\[\\text{Vector: }\\textbf{L}_1(\\mathbf{w}) = \\|\\mathbf{w}\\|_1 = \\sum_{i=1}^d |w_i|, \\quad \\text{Matrix: }\\textbf{L}_1(\\mathbf{W}) = \\|\\mathbf{W}\\|_1 = \\sum_{i=1}^d\\sum_{j=1}^e |w_{ij}|\\]\nWe call this L1 regularization, as it is equivalent to minimizing the L1 norm \\((\\|\\cdot\\|_1)\\) of each weight vector/matrix.\nIf we plot the L2 and L1 losses for a single weight \\(w\\), we can get a sense of the differences between these two approaches.\n\n\n\n\n\n\n\n\nWe see that the L2 loss strongly penalizes weights far from 0 compared to the L1 loss. However, this penalty decays quadratically towards 0, so weights close to 0 incur very little loss. The L1 loss decays linearly and thus more strongly penalizes weights that are already close to 0. Intuitively, this means that the L1 loss focuses on decreasing weights already close to 0 just as much as weights that are far from 0. This has the effect of encouraging sparsity, as the L1 loss can trade-off allowing some weights to be large if others go to exactly 0. The L2 loss encourages all weights to be reasonably small.\nWe can see the same distinction if we plot the L2 and L2 losses as a function of 2 weights:\n\nIf we overlay a hypothetical MSE loss as a function of the two weights, we can get a sense of why the L1 loss encourages sparsity. For most curves of constant MSE, the point that minimizes the L1 loss falls at a point where one of the weights is exactly 0. If our L1 weight \\((\\lambda)\\) is high enough, our overall minimum would fall at one of these points.\n\nWe can see the effect these two forms of regularization have on a real network.\n\n\n\nL2 Regularization encourages all weights to be small.\n\n\n\n\nL1 Regularization encourages all but the most relevant weights to go to 0."
  },
  {
    "objectID": "lecture8-optimization/notes.html#train-test-splits",
    "href": "lecture8-optimization/notes.html#train-test-splits",
    "title": "Lecture 8: Regularization",
    "section": "Train-test splits",
    "text": "Train-test splits\nAs we’ve seen previously to get an unbiased estimate of how well our model will perform on new data, it is good practice to hold-out a test set of data that we will not use to train our model with gradient descent. Instead we will fit our model on the remaining data (the training set) and then compute the loss (or other metrics like accuracy) on the test set, using this as an estimate of the performance of our model."
  },
  {
    "objectID": "lecture8-optimization/notes.html#train-validation-test-splits",
    "href": "lecture8-optimization/notes.html#train-validation-test-splits",
    "title": "Lecture 8: Regularization",
    "section": "Train-validation-test splits",
    "text": "Train-validation-test splits\nIf our test loss isn’t very good we may decide that we want to change our hyperparameters and train the model again. We could keep doing this over and over until we get a test loss that we’re satisfied with. However, as soon as we use the test data to make choices about our model, the test loss we’ll no longer be an unbiased estimate of the performance of our model on new data! After all, we’ve now used it to fit our model. This isn’t ideal as we will no longer have a reliable estimate for the true performance of our model.\nA simple approach to addressing this is to split our data into 3 parts: a training set that we’ll use for gradient descent, a test set that we’ll use for evaluating our model, and a validation set that we’ll use for choosing hyperparameters. When we run gradient descent, we can hold out both the test and validation sets, but we’ll allow ourselves to use the performance on the validation set (the validation loss) to choose hyperparameters. The test set we’ll reserve for the very end, when we’ve definitively chosen our model and need to estimate how well it will do. At a high-level the process looks like this:"
  },
  {
    "objectID": "lecture8-optimization/notes.html#dropout-at-evaluation-time",
    "href": "lecture8-optimization/notes.html#dropout-at-evaluation-time",
    "title": "Lecture 8: Regularization",
    "section": "Dropout at evaluation time",
    "text": "Dropout at evaluation time\nWhen we’re evaluating our model or trying to make predictions on new data, we likely don’t want our prediction function to be noisy. As we can see in the examples above, applying dropout can lead to poor predictions if we’re unlucky. A simple approach might just remove the dropout functions at evaluation time:\n\\[\n\\phi(\\mathbf{x})_{train} = \\sigma(\\text{DO}_r(\\mathbf{x})^T\\mathbf{W} + \\mathbf{b}) \\quad \\rightarrow \\quad \\phi(\\mathbf{x})_{eval} = \\sigma(\\mathbf{x}^T\\mathbf{W} + \\mathbf{b})\n\\]\nHowever this has a problem! To see why consider the expected value of a linear function with dropout:\n\\[\n\\mathbb{E}[ \\text{DO}_r(\\mathbf{x})^T\\mathbf{w}] = \\sum_i d_i x_i w_i, \\quad d_i \\sim \\text{Bernoulli}(1-r)\n\\]\n\\[\n= \\sum_i p(d_i=1) x_i w_i = (1-r)\\sum_i  x_i w_i &lt;  \\sum_i  x_i w_i\n\\]\nIf \\(r=0.5\\) (the value suggested by the original dropout inventors), then on average the output of our function with dropout will only be half as large as the function without dropout! If we simply get rid of the dropout functions, the scale of our predictions will be way off.\nA simple solution is to simply define dropout at evaluation time to scale the output according to the dropout rate. So at evaluation time dropout is defined as:\n\\[\n\\text{Dropout}_{eval}(\\mathbf{X}, r) = (1-r) \\mathbf{X}\n\\]\nThis gives use the smooth prediction function we’re looking for:"
  },
  {
    "objectID": "lecture8-optimization/notes.html#dropout-in-pytorch",
    "href": "lecture8-optimization/notes.html#dropout-in-pytorch",
    "title": "Lecture 8: Regularization",
    "section": "Dropout in PyTorch",
    "text": "Dropout in PyTorch\nIn Pytorch, dropout is implemented as a module (or layer) as with the linear and activation layers we’ve seen previously. We can define a network with dropout very simply using the nn.Dropout module:\n\n# 2 Hidden-layer network with dropout\nmodel = nn.Sequential(nn.Dropout(0.5), nn.Linear(2, 10), nn.ReLU(), \n                      nn.Dropout(0.5), nn.Linear(10, 10), nn.ReLU(), \n                      nn.Dropout(0.5), nn.Linear(10, 1)\n                     )"
  },
  {
    "objectID": "lecture8-optimization/notes.html#cross-validation",
    "href": "lecture8-optimization/notes.html#cross-validation",
    "title": "Lecture 8: Regularization",
    "section": "Cross-validation",
    "text": "Cross-validation\nAn alternative approach is multiple splits of the same training data. Rather than partitioning our training set into distinct training and validation sets, we can divide our training set into \\(K\\) groups called folds.\nTo evaluate the performance of a given hyperparameter setting we can train our model multiple times, each time holding out a different group as the validation set. This gives us \\(K\\) estimates of the performance of a given choice of hyperparameters.\n\n\n\n\n\n\n\nCross-validation can be a more reliable way to choose hyperparameters at the expense of needing to retrain to model \\(K\\) times, which can be computationally expensive."
  },
  {
    "objectID": "lecture8-optimization/notes.html#overfitting-and-underfitting",
    "href": "lecture8-optimization/notes.html#overfitting-and-underfitting",
    "title": "Lecture 8: Regularization",
    "section": "Overfitting and underfitting",
    "text": "Overfitting and underfitting\nThere’s two possible reasons that a model might perform poorly on validation or test data (assuming gradient descent works well).\n\n\nUnderfitting occurs when our model is too simple to capture the data we’re trying to model. For example, if we try to fit a linear model to U-shaped data we see that the model can never fit the data well. We can identify underfitting when both the training and validation/test loss will be poor.\nOverfitting occurs when our prediction function is too complex. In this case the model may capture all of the small variations present in the training data even when these variations are simply due to noise or poor measurements, thus may not be reflected in held-out data. A better approach would be to model these variations as uncertainty rather than variations in the prediction function. We can identify overfitting when the training loss is good, but the validation/loss is poor.\n\nIf we think about plotting our training and validation loss as a function of the complexity of our model, we might see underfitting when the model is very simple and overfitting when the model is very complex. The idea model would be right in the middle when the validation loss is at its minimum.\n\nIn this case model complexity could mean several different things:\n\nNumber of layers\nNumber of neurons per layer\nActivation functions\nExplicit feature transforms applied\n\nLet’s look at a specific example where we’ll fit 3 neural networks of different levels of complexity on the same data:\n\nUnderfit model\n\n\n\nWell-fit model\n\n\n\nOverfit model\n\nIf we take a closer look at the overfit model, we can see that it actually fits the training data almost perfectly, but if we add more data from the same dataset, the performance looks much worse."
  },
  {
    "objectID": "lecture8-optimization/notes.html#tracking-validation-loss",
    "href": "lecture8-optimization/notes.html#tracking-validation-loss",
    "title": "Lecture 8: Regularization",
    "section": "Tracking validation loss",
    "text": "Tracking validation loss\nA common tool for quickly identifying poor performance is to track both the training and validation loss as we perform gradient descent. Doing this can let us see in real time if our model is underfitting or overfitting.\nIf we take a look a this type of plot from real neural network we see something interesting: the plot looks almost exactly like the model complexity plot we saw above.\n\n\n\n\n\n\n\nEarly in training both the training both the training and validation loss are improving, suggesting that at first the model is underfitting. After a while the training loss continues to improve, but the validation loss starts to get worse, suggesting that the model is beginning to overfit."
  },
  {
    "objectID": "lecture8-optimization/notes.html#early-stopping-1",
    "href": "lecture8-optimization/notes.html#early-stopping-1",
    "title": "Lecture 8: Regularization",
    "section": "Early stopping",
    "text": "Early stopping\nThe plot above suggests a simple strategy for preventing overfitting: simply stop gradient descent when the validation loss begins to increase! We call this approach early stopping.\n\nWe saw a simple way to implement this in the previous lecture: if the current validation loss is larger than the previous one, stop training.\n\nfor i in range(steps):\n    loss = compute_loss(model, training_data)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n    valid_loss = compute_loss(model, training_data)\n    if valid_loss &gt; valid_losses[-1]:\n        break\n        \n    valid_losses.append(valid_loss)\n\nIn the real world, the loss can be noisy:\n\nSo it may not make sense to stop the first time the validation loss increases. A common strategy to apply a more patient form of early stopping. In the case we stop if the validation loss hasn’t improved for some specified number of steps:\n\npatience = 5                    # Number of steps to wait before stopping\nsteps_since_improvement = 0     # Steps since validation loss improved\nmin_loss = 1e8                  # Minimum loss seen so far (start large)\n\nfor i in range(steps):\n    ...\n\n    valid_loss = compute_loss(model, training_data)\n\n    # If the validation loss improves reset the counter\n    if valid_loss &lt; min_loss:\n        steps_since_improvement = 0\n        min_loss = valid_loss\n\n    # Otherwise increment the counter\n    else:\n        steps_since_improvement += 1\n\n    # If its been patience steps since the last improvement, stop\n    if steps_since_improvement == patience:\n        break"
  },
  {
    "objectID": "lecture9-optimization/notes.html#symmetry-breaking",
    "href": "lecture9-optimization/notes.html#symmetry-breaking",
    "title": "Lecture 9: Optimization",
    "section": "Symmetry-breaking",
    "text": "Symmetry-breaking\nIn neural networks, we typically initialize parameters randomly. One important reason for random initialization is to make sure that different parameters have different starting values. To see why this is needed, let’s consider the prediction function for a simple neural network that takes in 1-dimensional inputs:\n\\[\nf(\\mathbf{x}) = \\sigma(\\mathbf{x}^T\\mathbf{W}_1)^T\\mathbf{w}_0=\\sigma(x_1 w_{11}) w_{01} +\\sigma (x_1 w_{12})w_{02}\n\\]\nIn this case we have 4 parameters: \\(w_{01}, w_{02}, w_{11}, w_{12}\\). If we initialize all to the same value, say \\(w_{**} = a\\), let’s see what happens to the derivatives we compute:\n\\[\n\\frac{d}{dw_{01}} f(\\mathbf{x}) = \\sigma(x_1 w_{11}) = \\sigma(x_1 a)\n\\]\n\\[\n\\frac{d}{dw_{02}} f(\\mathbf{x}) = \\sigma(x_1 w_{12}) = \\sigma(x_1 a)\n\\]\nWe see that \\(\\frac{d}{dw_{01}} = \\frac{d}{dw_{02}}\\)! Our gradient descent update will set:\n\\[\nw_{01}^{(1)} \\longleftarrow w_{01}^{(0)} - \\alpha \\frac{d}{dw_{01}} = a - \\alpha \\sigma(x_1 a)\n\\]\n\\[\nw_{02}^{(1)} \\longleftarrow w_{02}^{(0)} - \\alpha \\frac{d}{dw_{02}} = a - \\alpha \\sigma(x_1 a)\n\\]\nSo after each gradient descent update the two values will continue to be the same! The gradient decent algorithm has no way to distinguish between these two weights and so it is stuck finding solutions where \\(w_{01} = w_{02}\\) and \\(w_{11}=w_{12}\\). We call this the symmetry problem, and it means we no longer get any benefit from having multiple neurons.\nWe can see this in practice with a simple network:\n\n\n\nWhen the network is initialized with symmetry, the two neurons will always have the same output and our solution is poor.\n\n\n\n\nWhen initialized randomly, the two neurons can create different transforms and a much better solution is found.\n\n\nIf we plot the loss as a function of two \\(w_{01}\\) and \\(w_{02}\\) we can see what is happening graphically.\n\n\n\nInitializing the two parameters equal corresponds to sitting on a ridge of the loss surface, there are equally valid solutions on either side, but gradient descent gives us no way to chose between them.\n\n\n\n\nIf we plot the (negative) gradient of the loss we see that the gradient of any point on the ridge always points along the ridge. Gradient descent corresponds to following these arrows to find a minimum."
  },
  {
    "objectID": "lecture9-optimization/notes.html#visualizing-learning-rates",
    "href": "lecture9-optimization/notes.html#visualizing-learning-rates",
    "title": "Lecture 9: Optimization",
    "section": "Visualizing learning rates",
    "text": "Visualizing learning rates\nAs an aside, plotting the gradient as a vector field also gives us an convenient way to visualize the effects of different learning rates. Recall that the learning rate corresponds to how much we scale the gradient each time we take a step.\n\n\n\nA small learning rate means we will move slowly, so It may take a long time to find the minimum.\n\n\n\n\nA well-chosen learning rate lets us find a minimum quickly.\n\n\n\n\nA too-large learning rate means that steps may take us flying past the minimum!"
  },
  {
    "objectID": "lecture9-optimization/notes.html#scaled-initialization",
    "href": "lecture9-optimization/notes.html#scaled-initialization",
    "title": "Lecture 9: Optimization",
    "section": "Scaled initialization",
    "text": "Scaled initialization\nNow that we’ve seen the benefits of initializing randomly, we need to consider what distribution to initialize from. An obvious choice might be a standard normal distribution, with mean \\(0\\) and standard deviation \\(1\\).\n\\[w_{i} \\sim \\mathcal{N}(0, 1) \\quad \\forall\\ w_{i} \\in \\mathbf{w}\\]This has a subtle issue though. To see why let’s consider a linear function defined by randomly initialized weights:\n\\[\nf(\\mathbf{x}) = \\sum_{i=1}^d x_i w_i\n\\]\nLet’s consider the mean and variance of this output with respect to \\(\\mathbf{w}\\):\n\\[\n\\mathbb{E} \\big[f(\\mathbf{x})\\big] = \\mathbb{E} \\bigg[  \\sum_{i=1}^d x_i w_i \\bigg] =   \\sum_{i=1}^d x_i \\mathbb{E} \\big[w_i \\big] = 0, \\quad w_i \\sim \\mathcal{N}(0, 1)\n\\]\n\\[\n\\text{Var} \\big[f(\\mathbf{x})\\big] = \\text{Var}  \\bigg[  \\sum_{i=1}^d x_i w_i \\bigg] =   \\sum_{i=1}^d \\text{Var} \\big[ x_i w_i \\big] = \\sum_{i=1}^d x_i^2 \\text{Var} [w_i] = \\sum_{i=1}^d x_i^2\n\\]\nWe see a few things here, the mean is \\(0\\) and the variance depends on \\(x_i\\), which is reasonable. However we see that the variance also depends on \\(d\\), the dimensionality of the input. In particular it’s \\(\\mathcal{O}(d)\\). Why is this important? Because it means that if we increase the number of neurons at each layer in our network, the variance of the network’s predictions will also increase!\nIf our network has many neurons in each layer (large networks can have 1000’s!) the variance of outputs can be extreme, leading to poor initializations that correspond to extremely steep prediction functions. Here we can compare a few intializations from a network with just 8 neurons per layer to a network with 2.\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn practice this can make gradient descent difficult as these initialization are often very far from the minimum and the gradients are typically large, meaning small learning rates are needed to prevent divergence.\nA better approach would keep the variance consistent no matter how many inputs there are. We can reduce the variance by dividing our initial weights by some scale factor \\(s\\).\n\\[\nf(\\mathbf{x}) = \\sum_{i=1}^d x_i w_i\\bigg(\\frac{1}{s}\\bigg)\n\\]\nIf we want the variance to be independent of \\(d\\), then we want:\n\\[\ns = \\sqrt{d}\n\\]\nWe can verify this by computing the variance:\n\\[\n\\text{Var}  \\bigg[  \\sum_{i=1}^d x_i w_i \\bigg(\\frac{1}{\\sqrt{d}}\\bigg) \\bigg] =   \\sum_{i=1}^d \\text{Var} \\bigg[ x_i w_i \\bigg(\\frac{1}{\\sqrt{d}}\\bigg) \\bigg] = \\sum_{i=1}^d x_i^2 \\bigg(\\frac{1}{\\sqrt{d}}\\bigg)^2 \\text{Var} [w_i] = \\frac{1}{d}\\sum_{i=1}^d x_i^2\n\\]\nThis is equivalent to drawing our initial weights for each layer from a normal distribution with standard deviation equal to 1 over the square root of the number of inputs:\n\\[w_{i} \\sim \\mathcal{N}\\bigg(0, \\frac{1}{\\sqrt{d}}\\bigg) \\quad \\forall\\ w_{i} \\in \\mathbf{w},\\ \\mathbf{w}\\in \\mathbb{R}^{d}\\]\nThis is known as Kaiming normal initialization (sometimes also called He initialization, after the inventor Kaiming He).\nFor neural network layers where the weights are a matrix \\(\\mathbf{W} \\in \\mathbb{R}^{d \\times e}\\), this works the same way:\n\\[w_{ij} \\sim \\mathcal{N}\\bigg(0, \\frac{1}{\\sqrt{d}}\\bigg) \\quad \\forall\\ w_{ij} \\in \\mathbf{W},\\ \\mathbf{w}\\in \\mathbb{R}^{d \\times e}\\]\nA popular alternative scales the distribution according to both the number of inputs and outputs of the layer:\n\\[w_{ij} \\sim \\mathcal{N}\\bigg(0, \\sqrt{\\frac{2}{d + e}}\\bigg) \\quad \\forall\\ w_{ij} \\in \\mathbf{W},\\ \\mathbf{w}\\in \\mathbb{R}^{d \\times e}\\]\nThis is known as Xavier initialization (or Glorot initialization after the inventor Xavier Glorot).\nWe can compare initializations from a standard normal with initializations from a Kaiming normal.\n\n\nStandard normal \\(w_{i} \\sim \\mathcal{N}\\bigg(0, 1\\bigg)\\)\n\n\n\n\n\n\nKaiming normal \\(w_{i} \\sim \\mathcal{N}\\bigg(0, \\frac{1}{\\sqrt{d}}\\bigg)\\)"
  },
  {
    "objectID": "assignments/homework4-backpropagation/solutions.html",
    "href": "assignments/homework4-backpropagation/solutions.html",
    "title": "Homework 4: Automatic Differentiation and Neural Networks",
    "section": "",
    "text": "In this homework we will build a tiny automatic differentiation, matrix and neural network library from scratch!\n\n# Run me first!\nfrom hw4_support import *\n\n\n\nThis homework makes use of a few fancy features in Python that are worth knowing about if you are unfamiliar. - Variable length arguments (e.g. *args) - List comprehensions (e.g. [a**2 for a in range(5)]) - Magic methods (e.g. __add__)"
  },
  {
    "objectID": "assignments/homework4-backpropagation/solutions.html#overview",
    "href": "assignments/homework4-backpropagation/solutions.html#overview",
    "title": "Homework 4: Automatic Differentiation and Neural Networks",
    "section": "",
    "text": "In this homework we will build a tiny automatic differentiation, matrix and neural network library from scratch!\n\n# Run me first!\nfrom hw4_support import *\n\n\n\nThis homework makes use of a few fancy features in Python that are worth knowing about if you are unfamiliar. - Variable length arguments (e.g. *args) - List comprehensions (e.g. [a**2 for a in range(5)]) - Magic methods (e.g. __add__)"
  },
  {
    "objectID": "assignments/homework4-backpropagation/solutions.html#part-1-reverse-mode-automatic-differentiation",
    "href": "assignments/homework4-backpropagation/solutions.html#part-1-reverse-mode-automatic-differentiation",
    "title": "Homework 4: Automatic Differentiation and Neural Networks",
    "section": "Part 1: Reverse-mode automatic differentiation",
    "text": "Part 1: Reverse-mode automatic differentiation\nWe’ll start by developing an automatic differentiation class that uses reverse-mode automatic differentiation, as this is what will be most useful for neural networks.\nRecall that for reverse-mode AD to work, everytime we perform an operation on one or more numbers we need to store the result of that operation as well as the parent values (the inputs to the operation). We also need to be able to compute the derivative of that operation. Since for every operation we need to store several pieces of data and several functions, it makes sense to define a class to represent the result of an operation.\nFor example, if we want to make a class that represents the operation c=a+b our class needs several properties: - value: The value of the operation (c) - parents: The parent operations (a and b) - grad: The derivative of the final loss with respect to c (\\(\\frac{dL}{dc}\\)) - func: A function that computes the operation (a+b) - grads: A function that computes the derivatives of the operation (\\(\\frac{dc}{da}\\) and \\(\\frac{dc}{db}\\))\nFor this homework, we’ve provided the outline of such a class, called AutogradValue. This will be the base class for all of our possible operations and represents declaring a variable with a value (a = 5). This is useful because it lets us define values that we might want to find derivatives with respect to.\nLet’s see how this will work in practice. If we want to take derivatives we will first define the inputs using AutogradValue.\na = AutogradValue(5)\nb = AutogradValue(2)\nThen we can perform whatever operations we want on these inputs:\nc = a + b\nL = log(c)\nEach of these operations will produce a new AutogradValue object representing the result of that operation.\nFinally we can run the backward pass by running a method backward() (that we will write) on the outout L. This will compute the gradients of L with respect to each input that we defined (\\(\\frac{dL}{da}\\) and \\(\\frac{dL}{db}\\)). Rather than returning these derivatives, the backward() method will update the grad property of a and b, making it easy to access the correct derivative.\nL.backward()\ndL_da = a.grad\nWe’ll also be able to compute operations with non-AutogradValue numbers, but obviously won’t be able to compute derivaitives with respect to these values.\ns = 4\nL = s * a\ndL_da = a.grad # Will work because a is an AutogradValue\ndL_ds = s.grad # Will give an error because s is not an AutogradValue\nNow that we’ve seen what our final produce will look like, let’s define our AutogradValue class.\n\nclass AutogradValue:\n    '''\n    Base class for automatic differentiation operations. Represents variable delcaration.\n    Subclasses will overwrite func and grads to define new operations.\n\n    Properties:\n        parents (list): A list of the inputs to the operation, may be AutogradValue or float\n        args    (list): A list of raw values of each input (as floats)\n        grad    (float): The derivative of the final loss with respect to this value (dL/da)\n        value   (float): The value of the result of this operation\n    '''\n\n    def __init__(self, *args):\n        self.parents = list(args)\n        self.args = [arg.value if isinstance(arg, AutogradValue) else arg for arg in self.parents]\n        self.grad = 0.\n        self.value = self.forward_pass()\n\n    def func(self, input):\n        '''\n        Compute the value of the operation given the inputs.\n        For declaring a variable, this is just the identity function (return the input).\n\n        Args:\n            input (float): The input to the operation\n        Returns:\n            value (float): The result of the operation\n        '''\n        return input\n\n    def grads(self, *args):\n        '''\n        Compute the derivative of the operation with respect to each input.\n        In the base case the derivative of the identity function is just 1. (da/da = 1).\n\n        Args:\n            input (float): The input to the operation\n        Returns:\n            grads (tuple): The derivative of the operation with respect to each input\n                            Here there is only a single input, so we return a length-1 tuple.\n        '''\n        return (1,)\n    \n    def forward_pass(self):\n        # Calls func to compute the value of this operation \n        return self.func(*self.args)\n    \n    def __repr__(self):\n        # Python magic function for string representation.\n        return str(self.value)\n\n \n\nNow that we’ve defined the framework for an operation that can be used in automatic differentiation, we need to define some actual useful operations by subclassing AutogradValue\n\nQ1\nFill out the func and grads methods of each subclass below. Recall that func should always return the result of the operation and grads should always return a tuple of the derivative with respect to each input.\n\nclass _add(AutogradValue):\n    def func(self, a, b):\n        return a + b\n    \n    def grads(self, a, b):\n        return 1., 1.\n\nclass _sub(AutogradValue):\n    def func(self, a, b):\n        return a - b\n    \n    def grads(self, a, b):\n        return 1., -1.\n\nclass _neg(AutogradValue):\n    def func(self, a):\n        return -a\n    \n    def grads(self, a):\n        return (-1.,)\n    \nclass _mul(AutogradValue):\n    def func(self, a, b):\n        return a * b\n    \n    def grads(self, a, b):\n        return b, a\n\nclass _div(AutogradValue):\n    def func(self, a, b):\n        return a / b\n    \n    def grads(self, a, b):\n        return 1 / b, -a / (b * b)\n    \nclass _exp(AutogradValue):\n    def func(self, a):\n        return math.exp(a)\n    \n    def grads(self, a):\n        return (math.exp(a),)\n\nclass _log(AutogradValue):\n    def func(self, a):\n        return math.log(a)\n    \n    def grads(self, a):\n        return (1 / a,)\n\nIndentationError: expected an indented block (3924302752.py, line 22)\n\n\nBelow, we’ll define our basic functions and operators in terms of the operator classes we just wrote.\n\ndef exp(a):\n    return _exp(a) if isinstance(a, AutogradValue) else math.exp(a)\ndef log(a):\n    return _log(a) if isinstance(a, AutogradValue) else math.log(a)\n\n# Note: Remember that above we defined a class for each type of operation\n# so in this code we are overriding the basic operators for AutogradValue\n# such that they construct a new object of the class corresponding to the\n# given operation and return it. \n# (You don't need to everything that's happening here to do the HW)\nAutogradValue.exp = lambda a: _exp(a)\nAutogradValue.log = lambda a: _log(a)\nAutogradValue.__add__ = lambda a, b: _add(a, b)\nAutogradValue.__radd__ = lambda a, b: _add(b, a)\nAutogradValue.__sub__ = lambda a, b: _sub(a, b)\nAutogradValue.__rsub__ = lambda a, b: _sub(b, a)\nAutogradValue.__neg__ = lambda a: _neg(a)\nAutogradValue.__mul__ = lambda a, b: _mul(a, b)\nAutogradValue.__rmul__ = lambda a, b: _mul(b, a)\nAutogradValue.__truediv__ = lambda a, b: _div(a, b)\nAutogradValue.__rtruediv__ = lambda a, b: _div(b, a)\n\nWe should now be able to use our AutogradValue objects as if they are numbers!\n\na = AutogradValue(5)\nb = AutogradValue(2)\n\nprint((a + 5) * b)\nprint(log(b))\n\ntest_operators(AutogradValue)\n\nLet confirm that we do keep the entire compuational graph for operations defined in this way.\n\n\nQ2\nWrite a function graph_print that takes a single argument. If the argument is an AutogradValue (or one of its subclasses), print its value property and then call graph_print on each of its parents. If the argument is not an AutogradValue, just print it. The format of printing is not important.\nHint: You can use the built-in Python function isinstance to determine if something is an AutogradValue or one of its subclasses. e.g. isinstance(a, AutogradValue)\n\ndef graph_print(a):\n    # Check if we're an AutogradValue\n    if isinstance(a, AutogradValue):\n        # Recursively call on each parent\n        for p in a.parents:\n            graph_print(p)\n        print(a.value)\n    else:\n        print(a)\n\na = AutogradValue(5.)\nb = AutogradValue(2.)\nc = log((a + 5) * b)\ngraph_print(c)\n\nThe function should print (it’s ok if the numbers or order aren’t exact):\n2.995732273553991\n20.0\n10.0\n5.0\n5.0\n5\n2.0\n2.0\nNow in order to do automatic differentiation, we need to define how to do the backward pass. We’ll start with the backward_step for a single operation.\n\n\nQ3\nFill in the method backward_pass which computes a single step of the reverse pass through the computational graph (assume self is an AutogradValue instance). If backward_pass is called on a value c, the method should: - Assume that self.grad contains the derivaive of the final loss with respect to c (\\(\\frac{dL}{dc}\\)). - Check if each parent of c is an AutogradValue. If it is, update that parent’s grad property to account for c (e.g. for parent a, update the value of \\(\\frac{dL}{da}\\))\nFor example: if c represents the result of an addition so c = a + b, calling backward_pass on c will update the grad property of both a and b. (a.grad represents \\(\\frac{dL}{da}\\) and is initialized to 0).\nHint: grads will be one of the methods we wrote in Q1. Recall that if c has parents a and b then grads method will give \\(\\frac{dc}{da}\\) and \\(\\frac{dc}{db}\\).\n\ndef backward_pass(self):\n    local_grads = self.grads(*self.args)\n\n    # Loop through pairs of parents and their corresponding grads\n    for node, grad in zip(self.parents, local_grads):\n        # Update the gradient of each AutogradValue parent\n        if isinstance(node, AutogradValue):\n            node.grad += self.grad * grad\n\n\nAutogradValue.backward_pass = backward_pass\n\n# Test our implementation\ntest_backward_pass(AutogradValue)\n\nFinally we need to define the backward method itself. We will call this on the loss value to find the derivatives of the loss with respect to each input. This means working our way backward through the sequence of operations. Remember that if c=a+b, then if c.grad is \\(\\frac{dL}{dc}\\), calling backward_pass on c will update \\(\\frac{dL}{da}\\) (a.grad) and \\(\\frac{dL}{db}\\) (b.grad).\nThe complication is that c may be used in multiple operations, so we can’t call backward_pass on c until we’ve called backward_pass on each child operation of c otherwise c.grad won’t have the correct value of \\(\\frac{dL}{dc}\\), as in this example:\nc = a + b\ng = c * 2\nh = c + 4\nL = g * h\n\nL.backward_pass() # Updates dL/dg and dL/dh\nh.backward_pass() # Updates dL/dc\n\n##WRONG ORDER\nc.backward_pass() # Incorrect because dL/dc hasn't accounted for dL/dg\ng.backward_pass()\n\n## CORRECT ORDER\ng.backward_pass() # Updates dL/dc\nc.backward_pass() # Updates dL/da and dL/db\n\n\nQ4\nFill in the backward method for AutogradValue. Your backward method should call backward_pass on each operation used to compute the loss (self is the loss value). Some important things to keep in mind: - backward_pass should only be called once on each operation - backward_pass must be called on every child of an operation before it can be called on the operation. - You should not try to call backward_pass on values that aren’t instances of AutogradValue, even though this might be stored in operation.parents\nHint: The efficiency of this method will have a large impact on the running time of later problems! We won’t score efficiency in grading, but it still might be worth optimizing this function a bit.\nSimple, but slow implementation\n\ndef backward(self):\n    # We call backward on the loss, so dL/dL = 1\n    self.grad = 1.\n\n    # Setup a queue of nodes to visit, starting with self (the final loss)\n    queue = [self]\n    # Setup a list keep track of the order to call backward_pass()\n    order = []\n\n    # Visit each AutogradValue in the queue\n    while len(queue) &gt; 0:\n        node = queue.pop()\n        if isinstance(node, AutogradValue):\n\n            # We only want to keep the last instance of each node in the\n            # order, so if we visit a node already in the order, remove it\n            if node in order:\n                order.remove(node)\n\n            # Add the node to the end of the order and its paraent to the queue\n            order.append(node)\n            queue.extend(node.parents)\n    \n    # Once we have the order call backward pass on every node\n    for node in order:\n        node.backward_pass()\n\nFaster implementation by keeping track of visit counts\n\ndef backward(self):\n    # We call backward on the loss, so dL/dL = 1\n    self.grad = 1.\n    queue = [self]\n    order = []\n\n    # Additionally keep track of the visit counts for each node\n    counts = {}\n    while len(queue) &gt; 0:\n        node = queue.pop()\n        \n        # Rather than removing nodes from the order [slow, O(N)], \n        # just mark that it has been visited again [O(1)]\n        if isinstance(node, AutogradValue):\n            if node in counts:\n                counts[node] += 1\n            else:\n                counts[node] = 1\n\n            order.append(node)\n            queue.extend(node.parents)\n    \n    # Go through the order, but only call backward pass once we're at\n    # the last vist for a given node\n    for node in order:\n        counts[node] -= 1\n        if counts[node] == 0:\n            node.backward_pass()\n\nAutogradValue.backward = backward\n# Test our implementation\ntest_backward(AutogradValue)\n\nNow we can use our AutogradValue class to compute derivatives!\n\na = AutogradValue(5)\nb = AutogradValue(2)\n\nL = -log(5 *b + a)\nL.backward()\nprint(a.grad, b.grad)\n\nIf we want to train a neural network using our automatic differentiation implementation, we’re going to want to be able to use numpy to do matrix operations. Fortunately, the our AutogradValue class is (mostly) compatible with numpy!\nWe can create arrays of AutogradValue and take derivatives as shown below:\n\na = np.array([AutogradValue(5), AutogradValue(2)])\nL = np.dot(a, a)\nL.backward()\nprint('Gradient for a', a[0].grad, a[1].grad)\n\nIt would be a bit tedious to define every AutogradValue array in this way, so let’s write some convinience functions to make doing automatic differentiation with numpy easier.\n\n\nQ5\nComplete the following two functions wrap_array and unwrap_gradient.\nwrap_array should take a numpy array of floats and return a new array where every element has been made into an AutogradValue.\nunwrap_gradient should take a numpy array of AutogradValue and return a new array of floats, where every element is the extracted grad property of the corresponding element from the original array.\nBoth of these functions should work on 2-D arrays (matrices) at a minimum (but more general solutions that support 1 and/or &gt;2 dimensional arrays are also possible).\nHint: You can create an array from nested lists as shown above.\nWe’ll start by creating a function that applys a function to each element of an array\n\ndef element_map(f, a):\n    '''\n    Creates a new array the same shape as a, with a function f applied to each element.\n\n    Args:\n        a (function): The function to apply\n        a (array): The array to map\n    Returns:\n        g (array): An array g, such that g[i,j] = f(a[i,j])\n    '''\n\n    # Store the original shape\n    shape = a.shape\n    \n    # Create a 1-d array with the same elements using flatten()\n    # then iterate through applying f to each element\n    flat_wrapped = np.array([f(ai) for ai in a.flatten()])\n\n    # Reshape back to the original shape\n    return flat_wrapped.reshape(shape)\n\nWe can use our element_map function to implement both wrapping and unwrapping\n\n\n\ndef wrap_array(a):\n    '''\n    Wraps the elements of an array with AutogradValue\n\n    Args:\n        a (array of float): The array to wrap\n    Returns:\n        g (array of AutogradValue): An array g, such that g[i,j] = AutogradValue(a[i,j])\n    '''\n    return element_map(AutogradValue, a)\n    \n\ndef unwrap_gradient(a):\n    '''\n    Unwraps the gradient of an array with AutogradValues\n\n    Args:\n        a (array of AutogradValue): The array to unwrap\n    Returns:\n        g (array of float): An array g, such that g[i,j] = a[i,j].grad\n    '''\n    return element_map(lambda ai: ai.grad, a)\n\n\ntest_wrap_unwrap(wrap_array, unwrap_gradient, AutogradValue)"
  },
  {
    "objectID": "assignments/homework4-backpropagation/solutions.html#part-2-implementing-a-neural-network",
    "href": "assignments/homework4-backpropagation/solutions.html#part-2-implementing-a-neural-network",
    "title": "Homework 4: Automatic Differentiation and Neural Networks",
    "section": "Part 2: Implementing a neural network",
    "text": "Part 2: Implementing a neural network\nNow that we have everything we need to apply our automatic differentiation to train a neural network!\nBefore we do that though, let’s try out our automatic differentiation for logistic regression. Below is a slight modification of LogisticRegression implementation we saw in the last homework.\n\n\ndef sigmoid(x):\n    # Computes the sigmoid function\n    return 1. / (1. + np.exp(-x))\n\nclass LogisticRegression:\n    def __init__(self, dims):\n        '''\n        Args:\n            dims (int): d, the dimension of each input\n        '''\n        self.weights = np.zeros((dims + 1, 1))\n\n    def prediction_function(self, X, w):\n        '''\n        Get the result of our base function for prediction (i.e. x^t w)\n\n        Args:\n            X (array): An N x d matrix of observations.\n            w (array): A (d+1) x 1 vector of weights.\n        Returns:\n            pred (array): A length N vector of f(X).\n        '''\n        X = np.pad(X, ((0,0), (0,1)), constant_values=1.)\n        return np.dot(X, w)\n\n    def predict(self, X):\n        '''\n        Predict labels given a set of inputs.\n\n        Args:\n            X (array): An N x d matrix of observations.\n        Returns:\n            pred (array): An N x 1 column vector of predictions in {0, 1}\n        '''\n        return (self.prediction_function(X, self.weights) &gt; 0)\n    \n    def predict_probability(self, X):\n        '''\n        Predict the probability of each class given a set of inputs\n\n        Args:\n            X (array): An N x d matrix of observations.\n        Returns:\n            probs (array): An N x 1 column vector of predicted class probabilities\n        '''\n        return sigmoid(self.prediction_function(X, self.weights))\n\n    def accuracy(self, X, y):\n        '''\n        Compute the accuracy of the model's predictions on a dataset\n\n        Args:\n            X (array): An N x d matrix of observations.\n            y (array): A length N vector of labels.\n        Returns:\n            acc (float): The accuracy of the classifier\n        '''\n        y = y.reshape((-1, 1))\n        return (self.predict(X) == y).mean()\n\n    def nll(self, X, y, w=None):\n        '''\n        Compute the negative log-likelihood loss.\n\n        Args:\n            X (array): An N x d matrix of observations.\n            y (array): A length N vector of labels.\n            w (array, optional): A (d+1) x 1 matrix of weights.\n        Returns:\n            nll (float): The NLL loss\n        '''\n        if w is None:\n            w = self.weights\n\n        y = y.reshape((-1, 1))\n        xw = self.prediction_function(X, w)\n        py = sigmoid((2 * y - 1) * xw)\n        return -(np.log(py)).sum()\n    \n    def nll_gradient(self, X, y):\n        '''\n        Compute the gradient of the negative log-likelihood loss.\n\n        Args:\n            X (array): An N x d matrix of observations.\n            y (array): A length N vector of labels.\n        Returns:\n            grad (array): A length (d + 1) vector with the gradient\n        '''\n        y = y.reshape((-1, 1))\n        xw = self.prediction_function(X, self.weights)\n        py = sigmoid((2 * y - 1) * xw)\n        grad = ((1 - py) * (2 * y - 1)).reshape((-1, 1)) * np.pad(X, [(0,0), (0,1)], constant_values=1.)\n        return -np.sum(grad, axis=0)\n    \n    def nll_and_grad_no_autodiff(self, X, y):\n        # Compute nll_and_grad without automatic diferentiation\n        return self.nll(X, y), self.nll_gradient(X, y)\n\n\nQ6\nWrite the method nll_and_grad for the LogisticRegression class using the automatic differentiation tools we built above. Verify that it gives a similar answer to nll_and_grad_no_autodiff.\n\ndef nll_and_grad(self, X, y):\n    # Wrap the array we want to differentiate with respect to (weights)\n    w = wrap_array(self.weights)\n\n    # Run the NLL functoin and call backward to populate the gradients\n    nll = self.nll(X, y, w)\n    nll.backward()\n\n    # Get both the nll value and graident\n    return nll.value, unwrap_gradient(w)\nLogisticRegression.nll_and_grad = nll_and_grad\n\nOur automatic differentiation is very inefficient (we’ll fix this in the next homework!), so we’ll test our model on a very small dataset.\n\nX, y = make_moons(100, noise=0.1)\nmodel = LogisticRegression(2)\ngradient_descent(model, X, y, lr=3e-2, steps=250)\n\nprint('Model accuracy: %.3f' % model.accuracy(X, y))\nplot_boundary(model, X, y)\n\nNow let’s extend our model to be a neural network! We’ll create a neural network class that extends our logistic regression class. First we’ll setup the needed weight matrices.\n\n\nQ7\nFill in the Neural Network __init__ method below. The method should take in the input data dimension and a list of integers specifying the size of each hidden layer (the number of neurons in each layer). The function should create a list of numpy arrays of the appropriate shapes for the weight matrices.\nFor example if dims is 2 and hidden_sizes is [4, 4], then self.weights should have 3 entries of shapes [(4x2), (4x4), (1x4)]. This network is shown below.\n\n\nInput Layer ∈ ℝ²Hidden Layer ∈ ℝ⁴Hidden Layer ∈ ℝ⁴Output Layer ∈ ℝ¹\n\nIf you find it easier you could also define the weights in terms of \\(W^T\\) instead, in which case the shapes would be: [(2x4), (4x4), (4x1)]. You could also consider how to add a bias term at each layer as in logistic regression (but this isn’t nessecary for full credit).\nThe values in each array should be drawn from a normal distribution with standard deviation 1. You can create such a matrix in numpy using:\nnp.random.normal(scale=1., size=shape)\n\nclass NeuralNetwork(LogisticRegression):\n    def __init__(self, dims, hidden_sizes=[]):\n        # Create a list of all layer dimensions (including input and output)\n        sizes = [dims] + hidden_sizes + [1]\n        # Create each layer weight matrix (including bias dimension)\n        self.weights = [np.random.normal(scale=1., size=(i + 1, o)) \n                        for (i, o) in zip(sizes[:-1], sizes[1:])]\n \n\nRecall that for logistic regression the prediction function (before threholding or sigmoid) was \\(\\mathbf{X}\\mathbf{w}\\). We now want to implement the prediction function for our neural network class. This function should perform the appropriate feature transforms and multiply by the regression weights. For a neural network with a single hidden layer this will look like:\n\\(f(\\mathbf{X}) = \\sigma(\\mathbf{X}\\mathbf{W}_1^T)\\mathbf{w}_0\\)\nUse the sigmoid activation function for this problem.\nFor multiple layers we can also think of this a a chain of feature transforms: \\[\\Phi_1 = \\sigma(\\mathbf{X}\\mathbf{W}_1^T)\\] \\[\\Phi_2 = \\sigma(\\Phi_1 \\mathbf{W}_2^T)\\] \\[...\\] \\[\\Phi_l = \\sigma(\\Phi_{l-1} \\mathbf{W}_l^T)\\] \\[f(\\mathbf{X}) = \\Phi_l\\mathbf{w}_0\\] Where \\(\\Phi_i\\) is just the variable that represents the neurons at layer \\(i\\) (the result of the first \\(i\\) transforms applied to \\(\\mathbf{X}\\)).\n\n\nQ8\nImplement the prediction function as described above. Note that the prediction function should use the weights passed into the w argument rather than self.weights, this will make it easier to implement the next question.\n\ndef prediction_function(self, X, w):\n    '''\n    Get the result of our base function for prediction (i.e. x^t w)\n\n    Args:\n        X (array): An N x d matrix of observations.\n        w (list of arrays): A list of weight matrices\n    Returns:\n        pred (array): An N x 1 matrix of f(X).\n    '''\n    # Iterate through the weights of each layer and apply the linear function and activation\n    for wi in w[:-1]:\n        X = np.pad(X, ((0,0), (0,1)), constant_values=1.) # Only if we're using bias\n        X = sigmoid(np.dot(X, wi))\n\n    # For the output layer, we don't apply the activation\n    X = np.pad(X, ((0,0), (0,1)), constant_values=1.)\n    return np.dot(X, wi).reshape((-1, 1))\n\nNeuralNetwork.prediction_function = prediction_function\ntest_nn_prediction_function(NeuralNetwork)\n\n\n\nQ9\nImplement an nll_and_grad method for the NeuralNetwork class using your automatic differentiation implmentation to compute the gradient with respect to each weight matrix.\n\ndef nll_and_grad(self, X, y):\n    '''\n    Get the negative log-likelihood loss and its gradient\n\n    Args:\n        X (array): An N x d matrix of observations.\n        y (array): A length N vector of labels\n    Returns:\n        nll (float): The negative log-likelihood\n        grads (list of arrays): A list of the gradient of the nll with respect\n                                to each value in self.weights.\n    '''\n    # Same approach as Q6, but this time we need to wrap and unwrap lists of weights\n    w = [wrap_array(wi) for wi in self.weights]\n    nll = self.nll(X, y, w)\n    nll.backward()\n    return nll.value, [unwrap_gradient(wi) for wi in w]\n\nNeuralNetwork.nll_and_grad = nll_and_grad\n\nWe now have everything in place to train a neural network from scratch! Let’s try it on our tiny dataset. Feel free to change the inputs.\nHint: If this give very poor results and/or runs very slowly, make sure to carefully check the shape of each operation in your code to make sure it matches your expectation.\n\nX, y = make_moons(100, noise=0.1)\nmodel = NeuralNetwork(2, [5, 5])\ngradient_descent(model, X, y, lr=3e-2, steps=250)\n\nprint('Model accuracy: %.3f' % model.accuracy(X, y))\nplot_boundary(model, X, y)"
  },
  {
    "objectID": "assignments/homework4-backpropagation/solutions.html#part-4-forward-mode-automatic-differentiation",
    "href": "assignments/homework4-backpropagation/solutions.html#part-4-forward-mode-automatic-differentiation",
    "title": "Homework 4: Automatic Differentiation and Neural Networks",
    "section": "Part 4: Forward-mode automatic differentiation",
    "text": "Part 4: Forward-mode automatic differentiation\nLet’s now try out the other type of automatic differentiation that we learned about: forward-mode. To do this we’ll create a subclass of our AutogradValue class called ForwardValue.\nRecall that for this version of automatic differentiaion each operation needs to keep track of the derivative of it’s value with respect each original input. For example, in the code below the g object needs to store both \\(\\frac{dg}{da}\\) and \\(\\frac{dg}{db}\\).\na = ForwardValue(5)\nb = ForwardValue(2)\nc = a + b\ng = c * 2\nOur ForwardValue class will maintain a dict called forward_grads that maps each original input object to the derivative of the current value with respect to that input (so g will have a dict with keys that are the a and b objects).\nIn the base case, when we declare a variable the derivative with respect to itself is just 1 (\\(\\frac{da}{da}=1\\))\n\nclass ForwardValue(AutogradValue):\n    def __init__(self, *args):\n        super().__init__(*args)\n        self.forward_grads = {self: 1}\n\nWe see now that our forward_pass method needs to update forward_grads (e.g. to compute \\(\\frac{dg}{da}\\) and \\(\\frac{dg}{db}\\)) using the forward_grads values of its parents (e.g. \\(\\frac{dc}{da}\\) and \\(\\frac{dc}{db}\\))\n\nQ10\nImplement the forward_pass method below for forward-mode automatic differentiation. This method should update the foward_grads property of the operation such that: - foward_grads has an entry for every input that appears in foward_grads of any parent operation. - If an input appears in more than 1 parent, make sure to add the gradients appropritately (if g has parents b and c then \\(\\frac{dg}{da} = \\frac{dg}{db}\\frac{db}{da} + \\frac{dg}{dc}\\frac{dc}{da}\\) ) - Parents that are not AutogradValue objects are ignored\nIf our forward_pass method is working correctly, we should have the following behaivior:\n\n# Define our inputs as ForwardValue objects\na = ForwardValue(5)\nb = ForwardValue(2)\n\n# Perform operations\nc = a * b\ng = 3 * c + a\n\n\n# We should have the following in the forward_grads property of c and d (note that the keys are ForwardValue objects!)\nc.forward_grads = {a: 2, b: 5}  # dc/da and dc/db\ng.forward_grads = {a: 3 * 2 + 1, b: 3 * 5} # dg/da = dg/dc dc/da + dg/da, dg/db = dg/dc dc/db\n\nImplement the method below\n\ndef forward_pass(self):\n    self.forward_grads = {}\n    grads = self.grads(*self.args)\n                \n    # Again iterate through pairs of parent, local derivative\n    for node, grad  in zip(self.parents, grads):\n        # Check if the parent has a forward_grads property\n        if hasattr(node, 'forward_grads'):\n            # Iterate through all inputs in the parents forward_grad dict. \n            # Add it to our forward_grads if we haven't yet and update it\n            for key, value in node.forward_grads.items():\n                if key not in self.forward_grads:\n                    self.forward_grads[key] = 0\n                self.forward_grads[key] += value * grad\n                \n    # Make sure to still return the operation's value\n    return self.func(*self.args)\n\n# Overwrite the AutogradValue method so that operators still work\nAutogradValue.forward_pass = forward_pass\ntest_forward_mode(ForwardValue)\n\nWe can now take derivates of functions much like with our reverse-mode implementation!\n\na = ForwardValue(5)\nb = ForwardValue(2)\nL = -log(5 *b + a)\n\ndL_da = L.forward_grads[a]\ndL_db = L.forward_grads[b]\nprint('dL/da = %.3f, dL/db = %.3f' % (dL_da, dL_db))\n\n\n\nQ11\nComplete the ForwardModeNeuralNetwork class that inherits from NeuralNetwork by implementing nll_and_grad to use the forward-mode implementation you just wrote!\nHint: Just like with the NeuralNetwork class from before we need to first wrap the weight elements into objects, this time of class ForwardValue. After wrapping we should have: wrapped_w = [[ForwardValue(w[0,0]), ...], ...] Once we caluculate the loss, we should have a dictionary that maps ForwardValue objects to their respective derivatives: L.forward_grads = {ForwardValue(w[0,0]): dL/dw00, ...}. You will need to unwrap these derivatives as before to create a gradient array. This time the unwrap function will need to access derivative values from L.forward_grads.\n\nclass ForwardModeNeuralNetwork(NeuralNetwork):\n    def nll_and_grad(self, X, y):\n        # Here we'll reuse the element_map function we wrote for Q5 to wrap the weights\n        w = [element_map(ForwardValue, wi) for wi in self.weights]\n        nll = self.nll(X, y, w)\n\n        # The derivative for each element is contained in nll.forward_grads, so to unwrap\n        # we need to extract each element from that map\n        grads = [element_map(lambda x: nll.forward_grads[x], wi) for wi in w]\n        return nll.value, grads\n\nWe can again test it on our tiny dataset.\n\nX, y = make_moons(100, noise=0.1)\nmodel = ForwardModeNeuralNetwork(2, [5, 5])\ngradient_descent(model, X, y, lr=3e-2, steps=250)\n\nprint('Model accuracy: %.3f' % model.accuracy(X, y))\nplot_boundary(model, X, y)\n\n\n\nQ12\nBased on what we’ve learned and your experience here, why would we choose forward or reverse-mode automatic differentiation?\nAs we discussed in class, reverse-mode is typically more efficient when the number of inupts in large. In this case the problem was small and both were comparable. We might also find forward-mode to be simpler to implement."
  },
  {
    "objectID": "lecture9-optimization/viz.html",
    "href": "lecture9-optimization/viz.html",
    "title": "Stochastic Gradient descent visualization",
    "section": "",
    "text": "MathJax = {\n  const MathJax = await require('mathjax@2.7.5/MathJax.js?config=TeX-MML-AM_CHTML')\n    .catch(() =&gt; window.MathJax)\n  \n  // configure MathJax\n  MathJax.Hub.Config({\n    tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]},\n    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ],\n    processEscapes: true\n  })  \n  return MathJax\n}\n\nPlotly = require(\"https://cdn.plot.ly/plotly-latest.min.js\");\ntfbase = require('@tensorflow/tfjs@4.11.0')\npyodide = {\n  const p =\n    await require(\"https://cdn.jsdelivr.net/pyodide/v0.22.1/full/pyodide.js\");\n  console.log(p);\n  return p.loadPyodide();\n}\n\nPyScope = function() {\n  let scope = pyodide.toPy({});\n  \n  let py = async (strings, ...expressions) =&gt; {\n    let globals = {};\n    const code = strings.reduce((result, string, index) =&gt; {\n      if (expressions[index]) {\n        const name = `x${index}`;\n        globals[name] = expressions[index];\n        return result + string + name;\n      }\n      return result + string;\n    }, '');\n    await pyodide.loadPackagesFromImports(code);\n    scope.update(pyodide.globals);\n    const result = await pyodide.runPythonAsync(\n      code,\n      {\n        globals: scope\n      }\n    );\n    if (result?.t2Js) return result.t2Js();\n    if (result?.toJs) return result.toJs();\n    return result;\n  };\n  \n  return py;\n}\n\npy = {\n  let testscope = PyScope();\n  let py = async (strings, ...expressions) =&gt; {\n    let globals = {};\n    const code = strings.reduce((result, string, index) =&gt; {\n      if (expressions[index]) {\n        const name = `x${index}`;\n        globals[name] = expressions[index];\n        return result + string + name;\n      }\n      return result + string;\n    }, '');\n    await pyodide.loadPackagesFromImports(code);\n    pyodide.globals.update(pyodide.toPy(globals))\n    const result = await pyodide.runPythonAsync(\n      code,\n      {globals: pyodide.globals}\n    );\n    if (result?.t2Js) return result.t2Js();\n    if (result?.toJs) return result.toJs();\n    return result;\n  };\n\n  const sigmoidGradConfig  = {\n    kernelName: tfbase.Sigmoid,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) =&gt; {\n      const [x] = saved;\n      const y = tfbase.sigmoid(x);\n      return {x: () =&gt; tfbase.mul(dy, tfbase.mul(y, tfbase.sub(tfbase.scalar(1), y)))};\n    }\n  };\n  tfbase.registerGradient(sigmoidGradConfig);\n\n  const tanhGradConfig = {\n    kernelName: tfbase.Tanh,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) =&gt; {\n      const [x] = saved;\n      const y = tfbase.tanh(x);\n      return {x: () =&gt; tfbase.mul(tfbase.sub(tfbase.scalar(1), tfbase.square(y)), dy)};\n    }\n  };\n  tfbase.registerGradient(tanhGradConfig);\n   const expGradConfig = {\n    kernelName: tfbase.Exp,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) =&gt; {\n      const [x] = saved;\n      const y = tfbase.exp(x);\n      return {x: () =&gt; tfbase.mul(dy, y)};\n    }\n  }; \n  tfbase.registerGradient(expGradConfig);\n\n  function dispatchEvent(element){\n    element.dispatchEvent(new Event(\"input\", {bubbles: true}));\n  }\n  pyodide.globals.update(pyodide.toPy({Plotbase: Plot, tfbase: tfbase, Plotlybase: Plotly, dispatchEvent: dispatchEvent, d3base: d3}))\n  \nawait py`\nfrom pyodide.ffi import create_once_callable\nfrom types import SimpleNamespace\nfrom pyodide.ffi import to_js\nfrom js import Object, document\nimport pandas\nimport numpy as np\n\ntfbase = SimpleNamespace(**tfbase)\n\ndef convert_tensor(a, *args):\n  if isinstance(a, Parameter):\n    a = a.value.value\n  if isinstance(a, Tensor):\n    a = a.value\n  if isinstance(a, np.ndarray):\n    a = a.tolist()\n  return to_js(a)\n\ndef convert(a):\n  if isinstance(a, Parameter):\n    a = a.value.value\n  if isinstance(a, Tensor):\n    a = a.value\n  if isinstance(a, np.ndarray):\n    a = a.tolist()\n  return to_js(a, dict_converter=Object.fromEntries, default_converter=convert_tensor)\n\ndef convert_start(shape, start):\n  start = start or 0\n  if start &lt; 0:\n    start = shape + start\n  start = min(start, shape - 1)\n  return start\n\ndef convert_end(shape, start, end): \n  start = convert_start(shape, start)\n  if end is None:\n    end = shape\n  else:\n    end = convert_start(shape, end)\n  return end - start\n\nclass Tensor:\n  keepall = False\n\n  class Keep:\n    def __enter__(self):\n        self.value = Tensor.keepall\n        Tensor.keepall = True\n    \n    def __exit__(self, *args):\n        Tensor.keepall = self.value\n\n  def __init__(self, *args, value=None, keep=None, **kwargs):\n    if keep is None:\n      self.keep = Tensor.keepall\n    else:\n      self.keep = keep\n\n    if not (value is None):\n      self.value = value\n    elif len(args) and isinstance(args[0], Tensor):\n      self.value = tfbase.add(args[0].value, 0)\n    elif len(args) and args[0] is None:\n      self.value = tfbase.tensor(0.)\n    else:\n      args = [convert(a) for a in args]\n      kwargs = {k: convert(a) for (k, a) in kwargs.items()}\n      self.value = tfbase.tensor(*args, **kwargs)\n\n  def __getattr__(self, name):\n    if name == 'T':\n      return self.transpose()\n    attr = getattr(self.value, name)\n    if callable(attr):\n      def run(*args, **kwargs):\n        args = [convert(a) for a in args]\n        kwargs = {k: convert(a) for (k, a) in kwargs.items()}\n        output = attr(*args, **kwargs)\n        return Tensor(value=output)\n      # Prevent premature garbage collection\n      run._ref = self\n      return run\n    return attr\n\n  def __add__(a, b):\n    return Tensor(value=tfbase.add(convert(a), convert(b)))\n  def __radd__(a, b):\n    return Tensor(value=tfbase.add(convert(b), convert(a)))\n  def __sub__(a, b):\n    return Tensor(value=tfbase.sub(convert(a), convert(b)))\n  def __rsub__(a, b):\n    return Tensor(value=tfbase.sub(convert(b), convert(a)))\n  def __mul__(a, b):\n    return Tensor(value=tfbase.mul(convert(a), convert(b)))\n  def __rmul__(a, b):\n    return Tensor(value=tfbase.mul(convert(b), convert(a)))\n  def __truediv__(a, b):\n    return Tensor(value=tfbase.div(convert(a), convert(b)))\n  def __rtruediv__(a, b):\n    return Tensor(value=tfbase.div(convert(b), convert(a)))\n  def __floordiv__(a, b):\n    return Tensor(value=tfbase.floorDiv(convert(a), convert(b)))\n  def __rfloordiv__(a, b):\n    return Tensor(value=tfbase.floorDiv(convert(b), convert(a)))\n  def __pow__(a, b):\n    return Tensor(value=tfbase.pow(convert(a), convert(b)))\n  def __rpow__(a, b):\n    return Tensor(value=tfbase.pow(convert(b), convert(a)))\n  def __neg__(a):\n    return Tensor(value=tfbase.neg(convert(a)))\n  def __eq__(a, b):\n    return Tensor(value=tfbase.equal(convert(a), convert(b)))\n  def __neq__(a, b):\n    return Tensor(value=tfbase.notEqual(convert(a), convert(b)))\n  def __lt__(a, b):\n    return Tensor(value=tfbase.less(convert(a), convert(b)))\n  def __gt__(a, b):\n    return Tensor(value=tfbase.greater(convert(a), convert(b)))\n  def __leq__(a, b):\n    return Tensor(value=tfbase.lessEqual(convert(a), convert(b)))\n  def __geq__(a, b):\n    return Tensor(value=tfbase.greaterEqual(convert(a), convert(b)))\n\n  def __del__(self):\n    if hasattr(self.value, 'dispose') and not self.keep:\n      self.value.dispose()\n\n  def __iter__(self):\n    for x in self.value.arraySync():\n        yield Tensor(x)\n\n  def __getitem__(self, args):\n    tosqueeze = []\n    starts, ends, steps = [], [], []\n    value = self\n    \n    if not (type(args) is tuple):\n      args = (args,)\n    \n    for ind in range(len(args)):\n      if args[ind] is Ellipsis:\n        start = args[:ind]\n        rest = args[(ind + 1):]\n        args = start + tuple([slice(None)] * (len(self.value.shape) - (len(start) + len(rest)))) + rest\n        break\n    \n    for i, (shape, dim) in enumerate(zip(self.value.shape, args)):\n      if isinstance(dim, slice):\n        starts.append(dim.start or 0)\n        ends.append(dim.stop or shape)\n        steps.append(dim.step or 1)\n      elif Tensor(dim).shape:\n        t = Tensor(dim)\n        if t.value.dtype == 'bool':\n          inds = [ind for (ind, e) in enumerate(t.value.arraySync()) if e]\n          inds = tf.cast(tf.reshape(Tensor(inds), [-1]), 'int32')\n          value = tf.gather(value, inds, i)\n        else:\n          inds = tf.cast(tf.reshape(t, [-1]), 'int32')\n          value = tf.gather(value, inds, i)\n      else:\n        starts.append(dim)\n        ends.append(dim + 1)\n        steps.append(1)\n        tosqueeze.append(i)\n    value = tf.stridedSlice(value, convert(starts), convert(ends), convert(steps))\n    if len(tosqueeze) &gt; 0:\n      value = tf.squeeze(value, tosqueeze)\n    return value\n\n  def t2Js(self):\n    return to_js(self.value.arraySync())\n\nclass wrapper:\n  def __init__(self, f):\n    self.f = f\n\n  def __call__(self, x, *args, **kwargs):\n    with Tensor.Keep():\n      return convert(self.f(Tensor(value=x), *args, **kwargs))\n\nclass grad:\n  def __init__(self, f):\n    self.f = f\n    self.wrapper = wrapper(f)\n\n  def __call__(self, x, *args, **kwargs):\n    output = tfbase.grad(create_once_callable(self.wrapper))(x.value, *args, **kwargs)\n    return Tensor(value=output)\n\nclass wrappers:\n  def __init__(self, f):\n    self.f = f\n\n  def __call__(self, *args):\n    with Tensor.Keep():\n      wrapped_args = [Tensor(value=x) for x in args]\n      return convert(self.f(*wrapped_args))\n\nclass grads:\n  def __init__(self, f):\n    self.f = f\n    self.wrapper = wrappers(f)\n\n  def __call__(self, *args):\n    output = tfbase.grads(create_once_callable(self.wrapper))(to_js([arg.value for arg in args]))\n    return [Tensor(value=x) for x in output]\n\ntf = Tensor(value=tfbase)\nPlotbase = SimpleNamespace(**Plotbase)\nPlotlybase = SimpleNamespace(**Plotlybase)\nd3base = SimpleNamespace(**d3base)\n\ndef meshgrid(*args):\n  return tuple([Tensor(value=a) for a in tfbase.meshgrid(*[convert(arg) for arg in args])])\ntf.meshgrid = meshgrid\n\ndef default_convert(obj, default_f, other):\n  if isinstance(obj, Tensor):\n    obj = obj.t2Js()\n  if isinstance(obj, pandas.DataFrame):\n    obj = obj.to_dict('records') \n  return default_f(obj)\n\ndef plotconvert(a):\n  return to_js(a, dict_converter=Object.fromEntries, default_converter=default_convert)\n\nclass PlotWrapper:\n  def __init__(self, base=None):\n    self.base = base\n    \n  def __getattr__(self, name):\n    attr = getattr(self.base, name)\n    if callable(attr):\n      def run(*args, **kwargs):\n        args = [plotconvert(a) for a in args]\n        kwargs = {k: plotconvert(a) for (k, a) in kwargs.items()}\n        return attr(*args, **kwargs)\n      return run\n    return attr\n\nPlot = PlotWrapper(Plotbase)\nPlotly = PlotWrapper(Plotlybase)\nd3 = PlotWrapper(d3base)\n\ndef PlotlyFigure(width=800, height=None, hide_toolbar=True, overlay=True):\n  if height is None:\n    height = 0.75 * width\n\n  width, height = int(width), int(height)\n  container = document.createElement('div')\n  container.style.width = str(width) + 'px'\n  container.style.height = str(height) + 'px'\n  \n  lineplot = document.createElement('div')\n  lineplot.classList.add(\"plotlydiv\")\n\n  if hide_toolbar:\n    container.classList.add(\"hidetoolbar\")\n\n  container.append(lineplot)\n  if overlay:\n    overlay = document.createElement('div')\n    overlay.classList.add(\"plotlyoverlay\")\n    \n    container.append(overlay)\n    \n    container.style.position = 'relative'\n    overlay.style.top = '0'\n    overlay.style.bottom = '0'\n    overlay.style.width = '100%'\n    overlay.style.position = 'absolute'\n  return container\n  \ndef PlotlyInput(width=800, height=None, hide_toolbar=True, sync=None):\n  container = PlotlyFigure(width, height, hide_toolbar)\n  lineplot, overlay = container.childNodes[0], container.childNodes[1]\n  if sync is None:\n    sync = container\n\n  class mover:\n    def __init__(self):\n      self.mousedown = False\n    \n    def __call__(self, event):\n      if event.type == 'mousedown':\n        self.mousedown = True\n      if event.type == 'mouseleave':\n        self.mousedown = False\n      if event.type == 'mouseup':\n        self.mousedown = False\n  \n      if self.mousedown:\n        x = float(lineplot._fullLayout.xaxis.p2c(event.layerX - lineplot._fullLayout.margin.l))\n        y = float(lineplot._fullLayout.yaxis.p2c(event.layerY - lineplot._fullLayout.margin.t))\n        sync.value = to_js([x, y])\n        dispatchEvent(sync)\n        \n  \n  e = mover()\n  overlay.addEventListener('mousemove', to_js(e))\n  overlay.addEventListener('mousedown', to_js(e))\n  overlay.addEventListener('mouseup', to_js(e))\n  overlay.addEventListener('mouseleave', to_js(e))\n  container.value = to_js([0., 0.])\n  return container\n\ndef PlotlyReactive(container, traces=[], layout={}, options={}):\n  full_layout = dict(width=int(container.style.width.replace('px', '')), height=int(container.style.height.replace('px', '')))\n  full_layout.update(layout)\n  full_options = {'displayModeBar' : not container.classList.contains('hidetoolbar')}\n  full_options.update(options)\n  plot = container.childNodes[0]\n  Plotly.react(plot, traces, full_layout, full_options)\n\ndef colorMap(t, cmap='inferno', cmin=None, cmax=None, scale='linear', res=100):\n  import matplotlib.cm as cm\n  if cmin is None:\n    cmin = tf.min(t)\n  if cmax is None:\n    cmax = tf.max(t)\n  \n  t = (t - cmin) / (cmax - cmin)\n  if scale == 'log':\n    e = tf.exp(1)\n    t = t * (e - 1) + 1\n    t = tf.log(t)\n  cmap = Tensor(cm.get_cmap(cmap, res + 1)(range(res + 1)))\n  t = t * res\n  shape = t.shape\n  tflat = tf.reshape(t, [-1])\n  tfloor = tf.gather(cmap, tf.floor(tflat).cast('int32'))\n  tceil = tf.gather(cmap, tf.ceil(tflat).cast('int32'))\n  tfrac = tf.reshape(tflat - tf.floor(tflat), [-1, 1])\n  tflat = tfrac * tceil + (1. - tfrac) * tfloor\n  t = tf.reshape(tflat, list(shape) + [4])\n  return t\n\ndef plotTensor(t, canvas, size=None, cmap=None, interpolation='nearest', **kwargs):\n  if not (cmap is None):\n    t = colorMap(t, cmap, **kwargs)\n  if size is None:\n    size = (canvas.height, canvas.width)\n  if interpolation == 'bilinear':\n    t = tfbase.image['resizeBilinear'](t.value, list(size))\n  else:\n    t = tfbase.image['resizeNearestNeighbor'](t.value, list(size))\n  tfbase.browser['toPixels'](t, canvas)\n\nfrom itertools import chain\nimport math\n\nclass Module:\n    def __init__(self):\n        self._submodules = dict()\n        self.eval = False\n        self._store = False\n\n    def parameters(self):\n        return chain.from_iterable(map(lambda x: x.parameters(), self._submodules.values()))\n    \n    def __setattr__(self, name, value):\n        if isinstance(value, Module):\n            self._submodules[name] = value\n        super().__setattr__(name, value)\n\n    def __call__(self, *args, **kwargs):\n        value = self.forward(*args, **kwargs)\n        self._store = False\n        return value\n    \n    def forward(self):\n        raise NotImplementedError()\n    \n    def train(self):\n        self.eval = False\n        for sm in self._submodules.values():\n            sm.train()\n    \n    def eval(self):\n        self.eval = True\n        for sm in self._submodules.values():\n            sm.eval()\n\n    def store(self):\n        self.store = True\n        for sm in self._submodules.values():\n            sm.eval()\n\nclass Parameter(Module):\n    def __init__(self, value):\n        super().__init__()\n        self.value = value\n        self.temp = None\n        self.grad = None\n\n    def parameters(self):\n        return [self]\n    \nclass Sequential(Module):\n    def __init__(self, *args):\n        super().__init__()\n        self.sequence = []\n        for arg in args:\n            if isinstance(arg, Module):\n                self.sequence.append(arg)\n            else:\n                self.sequence.extend(arg)\n        \n        self._submodules = {k: v for (k,v) in enumerate(self.sequence)}\n\n    def __getitem__(self, index):\n        return self.sequence[index]\n\n    def forward(self, X):\n        for m in self.sequence:\n            X = m(X)\n        return X\n    \nModuleList = Sequential\n\nclass Sigmoid(Module):\n    def forward(self, X):\n        return tf.sigmoid(X)\n    \nclass ReLU(Module):\n    def forward(self, X):\n        return tf.relu(X)\n    \nclass Tanh(Module):\n    def forward(self, X):\n        return tf.tanh(X)\n    \nclass Linear(Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        # Kaiming He initialization\n        self.W = Parameter(tf.randomNormal([in_features, out_features]) * math.sqrt((2 / out_features) / 3))\n        self.b = Parameter(tf.randomNormal([out_features]) * math.sqrt((2 / out_features) / 3))\n        self.input = None\n\n    def forward(self, x):\n        # Returns a new Matrix\n        self.input = None\n        return tf.dot(x, self.W) + self.b\n\n\n\n        \nclass Optimizer:\n    def __init__(self, model, loss=None, store=False):\n        self.parameters = list(model.parameters())\n        self.model = model\n        self.loss = loss\n        self.store = store\n\n    def _grads(self, loss, *args, **kwargs):\n        def loss_internal(*params):\n            for val, param in zip(params, self.parameters):\n                param.temp = param.value\n                param.value = val\n            try:\n                l = loss(self.model, *args, **kwargs)\n            finally:\n                for param in self.parameters:\n                    param.value = param.temp\n                    param.temp = None\n            return l\n        \n        return grads(loss_internal)(*map(lambda p: p.value, self.parameters))\n    \n    def _step(self, grads):\n        raise NotImplementedError()\n    \n    def step(self, *args, **kwargs):\n        grads = self._grads(self.loss, *args, **kwargs)\n        if self.store:\n          for grad, param in zip(grads, self.parameters):\n            param.grad = grad\n        return self._step(grads)\n    \n    def stepWithLoss(self, loss, *args, **kwargs):\n        grads = self._grads(loss, *args, **kwargs)\n        return self._step(grads)\n    \nclass SGD(Optimizer):\n    def __init__(self, model, loss, lr=0.001, store=False):\n        super().__init__(model, loss, store)\n        self.lr = lr\n\n    def _step(self, grads):\n        for grad, param in zip(grads, self.parameters):\n            param.value = param.value - self.lr * grad\n`\n  \n  return py;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg = FileAttachment(\"auto-mpg.csv\").csv()\n\n\n\n\n\n\n\n\n\nscatter = py`\n# Scatterplot figure\nscatterfig = PlotlyFigure()\nscatterfig\n`\n\n\n\n\n\n\n\nviewof batchsize = Inputs.range([1, 250], {value: 5, step: 1, label: \" Batch size\"})\n\n\n\n\n\n\n\nviewof learningrate = Inputs.range([0, 3], {value: 1, step: 0.01, label: \" Learning rate\"})\n\n\n\n\n\n\n\nviewof steps = Inputs.range([1, 10], {value: 1, step: 1, label: \"  Steps\"})\n\n\n\n\n\n\n\nviewof threed = Inputs.toggle({value: false, label: \"Show 3D\"})\n\n\n\n\n\n\n\n\nplots = py`\nlossplot = PlotlyInput()\nbatchlossplot = PlotlyInput(sync=lossplot)\n\nthreedlossplot = PlotlyFigure(hide_toolbar=False, overlay=False)\nthreedbatchlossplot = PlotlyFigure(hide_toolbar=False, overlay=False)\n`\n\nviewof weights = py`\n# ${plots}\nlossplot\n`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof l2 = Inputs.range([0, 10], {value: 0, step: 0.01, label: \"  L2 weight\"})\n\n\n\n\n\n\n\nviewof l1 = Inputs.range([0, 10], {value: 0, step: 0.01, label: \"  L1 weight\"})\n\n\n\n\n\n\n\nviewof momentum = Inputs.range([0, 1], {value: 0, step: 0.01, label: \" Momentum\"})\n\n\n\n\n\n\n\nviewof rmsprop = Inputs.range([0, 1], {value: 0, step: 0.01, label: \" RMSProp\"})\n\n\n\n\n\n\n\npy`\n# ${plots}\nthreedlossplot  if bool(${threed}) else ''\n`\n\n\n\n\n\n\n\n\nbatchloss = py`\n# ${plots}\nbatchlossplot\n`\n\n\n\n\n\n\n\nviewof newbatch = Inputs.button('New Batch')\n\n\n\n\n\n\n\nviewof transform = Inputs.select(['none', 'sin', 'cos', 'square'], {label: 'Weight transform'})\n\n\n\n\n\n\n\nviewof bscale = Inputs.range([0, 10], {value: 1, step: 0.01, label: \" Bias scale\"})\n\n\n\n\n\n\n\npy`\n# ${plots}\nthreedbatchlossplot if bool(${threed}) else ''\n`\n\n\n\n\n\n\n\n\n\ndata = py`\n# ${plots}\n# Setup the data and prediction functions\nimport pandas as pd\ndf = pd.DataFrame(${mpg})[['weight', 'mpg']]\ndf = df.astype(float).dropna().values\n\nx, y = df[:, :1], df[:, 1:]\nx = Tensor((x - x.mean()) / x.std())\ny = Tensor((y - y.mean()) / y.std())\n\ndef get_batch(batchsize, x, y):\n  batchinds = tf.randomUniformInt((batchsize,), 0, x.shape[0])\n  xbatch = tf.gather(x, batchinds)\n  ybatch = tf.gather(y, batchinds)\n  return xbatch, ybatch\n\ntransforms = dict(none=lambda a: a, sin=tf.sin, cos=tf.cos, square=tf.square)\ntransform = transforms[str(${transform})]\nscale = Tensor([[float(${bscale}), 1.]])\n\ndef predict(w, x):\n  w = transform(w.reshape((-1, 2)) * scale)\n  x = x.reshape((-1, 1))\n  x = tf.concat([x, tf.onesLike(x)], 1)\n  return tf.dot(x, w.T)\n\nwrange = tf.linspace(-3, 3, 25)\nbrange = tf.linspace(-3, 3, 25)\nww, bb = tf.meshgrid(wrange, brange)\nparamgrid = tf.stack([ww.flatten(), bb.flatten()]).T\neyetheta = 0\n\n(x, y)\n`\n\nsurfaces = py`\n# ${batch} ${plots}\n# Plot the loss surface\n\n\nl1weight = float(${l1})\nl2weight = float(${l2})\n\n\ndef loss(w, x, y):\n  w = w.reshape((-1, 2))\n  return (tf.mean((predict(w, x) - y) ** 2, 0)) + l1weight * tf.abs(w).sum(1) + l2weight * (w ** 2).sum(1) \n\nlossgrid = loss(paramgrid, x, y).reshape(ww.shape)\nlosscontour = plotconvert(dict(x=wrange, y=brange, z=lossgrid, type='contour', ncontours=25, ))\nlosssurface = plotconvert(dict(x=wrange, y=brange, z=lossgrid, type='surface', showlegend=False, showscale=False, opacity=0.8, contours=dict(x=dict(show=True), y=dict(show=True))))\n\nbatchlossgrid = loss(paramgrid, xbatch, ybatch).reshape(ww.shape)\nbatchlosscontour = plotconvert(dict(x=wrange, y=brange, z=batchlossgrid, type='contour', ncontours=25,))\nbatchlosssurface = plotconvert(dict(x=wrange, y=brange, z=batchlossgrid, showlegend=False, showscale=False, type='surface', opacity=0.8, contours=dict(x=dict(show=True), y=dict(show=True))))\n`\n\npy`\n# ${surfaces}\n\ncweights = ${weights}\nstartpoint = dict(x=[cweights[0]], y=[cweights[1]], mode='markers', showlegend=False, marker=dict(color='firebrick', size=10, line= {'color': 'black', 'width': 3}))\n\nlr = float(${learningrate})\nfullweightlist = [Tensor(cweights)]\nbatchweightlist = [Tensor(cweights)]\nsteps = int(${steps})\n\nmomentum = float(${momentum})\nnxbatch, nybatch = batches[0]\nbatchgrad = grad(lambda t: loss(t, nxbatch, nybatch))(batchweightlist[-1])\nbeta = float(${rmsprop})\nvelocity = batchgrad\nmagnitude = batchgrad ** 2\nif beta &gt; 0:\n  batchgrad = batchgrad / tf.sqrt(magnitude + 1e-8)\n\nfor i, (nxbatch, nybatch) in zip(range(max(1, steps)), batches):\n  fullgrad = lr * grad(lambda t: loss(t, x, y))(fullweightlist[-1])\n\n  bgrad = grad(lambda t: loss(t, nxbatch, nybatch))(batchweightlist[-1])\n  velocity = momentum * velocity + (1 - momentum) * bgrad\n  magnitude = beta * magnitude + (1. - beta) * (bgrad ** 2)\n  batchgrad = velocity\n  if beta &gt; 0:\n    batchgrad = velocity / tf.sqrt(magnitude + 1e-8)\n  \n  fullweightlist.append((fullweightlist[-1] - fullgrad).flatten())\n  batchweightlist.append((batchweightlist[-1] - lr * batchgrad).flatten())\n  \n\nfullweights = tf.stack(fullweightlist)\nbatchweights = tf.stack(batchweightlist)\n\ngradplot = dict(x=fullweights[:, 0], y=fullweights[:, 1], showlegend=False, line=dict(color='black'))\nbatchgradplot = dict(x=batchweights[:, 0], y=batchweights[:, 1], showlegend=False, line=dict(color='orange'))\n\n\nzloss = loss(fullweights, x, y)\nbatchzloss = loss(batchweights, x, y)\nthreedgradplot = dict(x=fullweights[:, 0], y=fullweights[:, 1], z=zloss, showlegend=False, marker=dict(size=4), line=dict(color='black', width=4), type='scatter3d')\nthreedbatchgradplot = dict(x=batchweights[:, 0], y=batchweights[:, 1], z=batchzloss, showlegend=False, marker=dict(size=4), line=dict(color='orange', width=4), type='scatter3d')\n\nPlotlyReactive(lossplot, [losscontour, startpoint, gradplot, batchgradplot], {'xaxis': {'range': [-3, 3]}, 'yaxis': {'range': [-3, 3]}})\nPlotlyReactive(batchlossplot, [batchlosscontour, startpoint, gradplot, batchgradplot], {'xaxis': {'range': [-3, 3]}, 'yaxis': {'range': [-3, 3]}})\n\nthreed = bool(${threed})\nif threed:\n  PlotlyReactive(threedlossplot, [losssurface, threedgradplot, threedbatchgradplot], {'scene': {'hovermode' : False, 'xaxis': {'range': [-3, 3], 'visible': False}, 'yaxis': {'range': [-3, 3], 'visible': False}, 'zaxis': {'visible': False}}})\n  PlotlyReactive(threedbatchlossplot, [batchlosssurface], {'scene': {'hovermode' : False, 'xaxis': {'range': [-3, 3], 'visible': False}, 'yaxis': {'range': [-3, 3], 'visible': False}, 'zaxis': {'visible': False}}})\nelse:\n  PlotlyReactive(threedlossplot, [])\n  PlotlyReactive(threedbatchlossplot, [])\n`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npy`\n# ${batch}\n# Plot the data scatterplot and prediction function\nscatterdata = dict(x=x.reshape((-1,)), y=y.reshape((-1,)), mode='markers', label='All data', marker=dict(color='rgba(17, 157, 255,0.5)'))\nbatchdata = dict(x=xbatch.reshape((-1,)), y=ybatch.reshape((-1,)), mode='markers', marker=dict(color='firebrick'))\n\nxrange = tf.linspace(-2, 3, 50)\ncweights = Tensor(${weights})\npfunction = dict(x=xrange.flatten(), y=predict(cweights, xrange).flatten(), line=dict(color='black'))\nPlotlyReactive(scatterfig, [scatterdata, batchdata, pfunction], {'xaxis': {'range': [-2, 3]}, 'yaxis': {'range': [-2, 3]}})\n`\n\n\n\n\n\n\n\nbatch = py`\n# ${data}, ${newbatch}\nbatchsize = int(${batchsize})\nbatches = [get_batch(batchsize, x, y) for i in range(int(${steps}))]\nxbatch, ybatch = batches[0]\n`\n\n\n\n\n\n\n\n\n\npy`\n#eyetheta += 0.01\n\nthreed = bool(${threed})\nif False:\n  def rotate_z(x, y, z, theta):\n    w = x+1j*y\n    return np.real(np.exp(1j*theta)*w), np.imag(np.exp(1j*theta)*w), z\n\n  xe, ye, ze = rotate_z(1.25, 1.25, 1.25, eyetheta)\n\n  Plotly.relayout(threedlossplot.childNodes[0], to_js({'scene': {'hovermode' : False, 'xaxis': {'range': [-3, 3], 'visible': False}, 'yaxis': {'range': [-3, 3], 'visible': False}, 'zaxis': {'visible': False}, 'dragmode': False, 'camera': {'eye': {'x': float(xe), 'y': float(ye), 'z': float(ze)}}}}, dict_converter=Object.fromEntries))\n  Plotly.relayout(threedbatchlossplot.childNodes[0], to_js({'scene': {'hovermode' : False, 'xaxis': {'range': [-3, 3], 'visible': False}, 'yaxis': {'range': [-3, 3], 'visible': False}, 'zaxis': {'visible': False}, 'dragmode': False, 'camera': {'eye': {'x': float(xe), 'y': float(ye), 'z': float(ze)}}}}, dict_converter=Object.fromEntries))\n`"
  },
  {
    "objectID": "lecture9-optimization/notes.html#estimating-loss",
    "href": "lecture9-optimization/notes.html#estimating-loss",
    "title": "Lecture 9: Optimization",
    "section": "Estimating loss",
    "text": "Estimating loss\nNeural network MSE loss:\n\\[\\underset{\\text{MSE}}{\\textbf{Loss}} (\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = \\frac{1}{N}\\sum_{i=1}^N (f(\\mathbf{x}_i, \\mathbf{w}) - y_i)^2\\]\nEstimate by sampling:\n\\[\\underset{\\text{MSE}}{\\textbf{Loss}} (\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) \\approx (f(\\mathbf{x}_i, \\mathbf{w}) - y_i)^2, \\quad i \\sim \\text{Uniform}(1, N)\\]\nExpectation of sampled loss is the true loss!\n\\[\\mathbb{E}_i[(f(\\mathbf{x}_i, \\mathbf{w}) - y_i)^2] = \\sum_{i=1}^N p(i)(f(\\mathbf{x}_i, \\mathbf{w}) - y_i)^2 =\\frac{1}{N} \\sum_{i=1}^N (f(\\mathbf{x}_i, \\mathbf{w}) - y_i)^2\\]\nIn general any loss that can be written as a mean of individual losses can be estimated in this way:\n\\[\\textbf{Loss} (\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = \\frac{1}{N} \\sum_{i=1}^N \\textbf{Loss} (\\mathbf{w}, \\mathbf{x}_i, y_i)\\]\n\\[\\textbf{Loss} (\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = \\mathbb{E}[\\textbf{Loss} (\\mathbf{w}, \\mathbf{x}_i, y_i)], \\quad i\\sim \\text{Uniform}(1,N)\\]"
  },
  {
    "objectID": "lecture9-optimization/notes.html#estimating-gradients",
    "href": "lecture9-optimization/notes.html#estimating-gradients",
    "title": "Lecture 9: Optimization",
    "section": "Estimating gradients",
    "text": "Estimating gradients\nGradient descent update:\n\\[ \\mathbf{w}^{(k+1)} \\longleftarrow \\mathbf{w}^{(k)} - \\alpha \\nabla_{\\mathbf{w}} \\textbf{Loss}(\\mathbf{w}^{(k)}, \\mathbf{X}, \\mathbf{y})\\]\nGradient can be composed into a sum of gradients and estimated the same way!\n\\[\\nabla_{\\mathbf{w}} \\textbf{Loss} (\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = \\nabla_{\\mathbf{w}} \\bigg( \\frac{1}{N} \\sum_{i=1}^N \\textbf{Loss} (\\mathbf{w}, \\mathbf{x}_i, y_i)\\bigg)\\]\n\\[=\\frac{1}{N} \\sum_{i=1}^N  \\nabla_{\\mathbf{w}} \\textbf{Loss} (\\mathbf{w}, \\mathbf{x}_i, y_i) = \\mathbb{E}[\\nabla_{\\mathbf{w}} \\textbf{Loss} (\\mathbf{w}, \\mathbf{x}_i, y_i)], \\quad i\\sim \\text{Uniform}(1, N)\\]\nStochastic gradient descent update:\n\\[ \\mathbf{w}^{(k+1)} \\longleftarrow \\mathbf{w}^{(k)} - \\alpha \\nabla_{\\mathbf{w}} \\textbf{Loss}(\\mathbf{w}^{(k)}, \\mathbf{x}_i, y_i), \\quad i\\sim \\text{Uniform}(1, N)\\]"
  },
  {
    "objectID": "lecture9-optimization/notes.html#minibatch-sgd",
    "href": "lecture9-optimization/notes.html#minibatch-sgd",
    "title": "Lecture 9: Optimization",
    "section": "Minibatch SGD",
    "text": "Minibatch SGD\nCan estimate gradients with a minibatch of \\(B\\) observations:\n\\[\\text{Batch:}\\ \\{(\\mathbf{x}_{b_1}, y_{b_1}), (\\mathbf{x}_{b_2}, y_{b_2}), ...,  (\\mathbf{x}_{b_B}, y_{b_B})\\}, \\quad \\{b_1, b_2, ...,b_B\\} \\sim \\text{Uniform}(1, N)\\]\n\\[\\nabla_{\\mathbf{w}} \\textbf{Loss} (\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) \\approx \\frac{1}{B} \\sum_{i=1}^B \\nabla_{\\mathbf{w}} \\textbf{Loss} (\\mathbf{w}, \\mathbf{x}_{b_i}, y_{b_i}), \\quad \\{b_1, b_2, ...,b_B\\} \\sim \\text{Uniform}(1, N)\\]\nThis still gives the correct expectation\n\\[\\mathbb{E}\\bigg[\\frac{1}{B} \\sum_{i=1}^B \\nabla_{\\mathbf{w}} \\textbf{Loss} (\\mathbf{w}, \\mathbf{x}_{b_i}, y_{b_i})\\bigg] = \\bigg(\\frac{1}{B}\\bigg) \\sum_{i=1}^B\\mathbb{E}\\bigg[ \\nabla_{\\mathbf{w}} \\textbf{Loss} (\\mathbf{w}, \\mathbf{x}_{b_i}, y_{b_i})\\bigg]\\] \\[ = \\nabla_{\\mathbf{w}} \\textbf{Loss} (\\mathbf{w}, \\mathbf{X}, \\mathbf{y})\\]\nThe variance decreases with the size of the batch!\n\\[\\text{Var}\\bigg[\\frac{1}{B} \\sum_{i=1}^B \\nabla_{\\mathbf{w}} \\textbf{Loss} (\\mathbf{w}, \\mathbf{x}_{b_i}, y_{b_i})\\bigg] =  \\bigg(\\frac{1}{B^2}\\bigg) \\sum_{i=1}^B\\text{Var}\\bigg[ \\nabla_{\\mathbf{w}} \\textbf{Loss} (\\mathbf{w}, \\mathbf{x}_{b_i}, y_{b_i})\\bigg]\\] \\[= \\bigg(\\frac{1}{B}\\bigg)\\text{Var}\\bigg[ \\nabla_{\\mathbf{w}} \\textbf{Loss} (\\mathbf{w}, \\mathbf{x}_{b_i}, y_{b_i})\\bigg]\\]"
  },
  {
    "objectID": "lecture9-optimization/notes.html#momentum",
    "href": "lecture9-optimization/notes.html#momentum",
    "title": "Lecture 9: Optimization",
    "section": "Momentum",
    "text": "Momentum\nGradient descent with momentum updates the average gradient then uses the running average to take descent steps.\n\\[ \\mathbf{v}^{(k+1)} \\longleftarrow \\beta \\mathbf{v}^{(k)} + (1-\\beta) \\nabla_{\\mathbf{w}} \\textbf{Loss}(\\mathbf{w}^{(k)}, \\mathbf{X}, \\mathbf{y})\\]\n\\[ \\mathbf{w}^{(k+1)} \\longleftarrow \\mathbf{w}^{(k)} - \\alpha v^{(k+1)}\\]"
  },
  {
    "objectID": "lecture9-optimization/notes.html#sgd-momentum",
    "href": "lecture9-optimization/notes.html#sgd-momentum",
    "title": "Lecture 9: Optimization",
    "section": "SGD + Momentum",
    "text": "SGD + Momentum\nWe can apply momentum for stochastic gradient descent as well\n\\[ \\mathbf{v}^{(k+1)} \\longleftarrow \\beta \\mathbf{v}^{(k)} + (1-\\beta) \\nabla_{\\mathbf{w}} \\textbf{Loss}(\\mathbf{w}^{(k)}, \\mathbf{x}_i, y_i), \\quad i\\sim \\text{Uniform}(1,N)\\]\n\\[ \\mathbf{w}^{(k+1)} \\longleftarrow \\mathbf{w}^{(k)} - \\alpha v^{(k+1)}\\]\n\\[\\nabla_{\\mathbf{w}} \\textbf{Loss}(\\mathbf{w}^{(k)}, \\mathbf{X}, \\mathbf{y}) \\approx \\sum_{j=1}^k \\beta^{k-j}(1-\\beta) \\nabla_{\\mathbf{w}} \\textbf{Loss}(\\mathbf{w}^{(j)}, \\mathbf{x}_{i^{(j)}}, y_{i^{(j)}})\\]"
  },
  {
    "objectID": "lecture9-optimization/notes.html#adaptive-gradients",
    "href": "lecture9-optimization/notes.html#adaptive-gradients",
    "title": "Lecture 9: Optimization",
    "section": "Adaptive gradients",
    "text": "Adaptive gradients\n\\[\\nabla_{\\mathbf{w}} \\textbf{Loss}(\\mathbf{w}^{(k)}, \\mathbf{X}, \\mathbf{y}) = \\begin{bmatrix} \\frac{dL}{dw^{(k)}_1} \\\\ \\frac{dL}{dw^{(k)}_2} \\\\ \\vdots \\end{bmatrix}\\]\n\\[\\begin{bmatrix} 3.1\\\\ 2.2 \\\\ \\vdots \\end{bmatrix} \\leftarrow\n\\begin{bmatrix} 5.0 \\\\ 1.8 \\\\ \\vdots \\end{bmatrix}\\leftarrow\n\\begin{bmatrix} 1.5 \\\\ 4.4 \\\\ \\vdots \\end{bmatrix}...\\]\n\\[\\begin{bmatrix} 10.1\\\\ 0.04 \\\\ \\vdots \\end{bmatrix} \\leftarrow\n\\begin{bmatrix} 8.6 \\\\ 0.02 \\\\ \\vdots \\end{bmatrix}\\leftarrow\n\\begin{bmatrix} 9.4 \\\\ 0.009 \\\\ \\vdots \\end{bmatrix}...\\]\n\\[ \\mathbf{s}^{(k+1)} \\longleftarrow \\beta \\mathbf{s}^{(k)} + (1-\\beta) (\\nabla_{\\mathbf{w}} \\textbf{Loss}(\\mathbf{w}^{(k)}, \\mathbf{X}, \\mathbf{y}))^2\\]\n\\[ \\mathbf{w}^{(k+1)} \\longleftarrow \\mathbf{w}^{(k)} - \\alpha \\frac{\\nabla_{\\mathbf{w}} \\textbf{Loss}(\\mathbf{w}^{(k)}, \\mathbf{X}, \\mathbf{y})}{\\sqrt{\\mathbf{s}^{(k+1)} + \\epsilon}}\\] \\[\\epsilon &lt;&lt; 1, \\quad \\text{e.g. } \\epsilon = 1e^{-7}\\]\n\\[\\frac{\\nabla_{\\mathbf{w}} \\textbf{Loss}(\\mathbf{w}^{(k)}, \\mathbf{X}, \\mathbf{y})}{\\sqrt{\\mathbf{s}^{(k+1)}}} =\n\\begin{bmatrix} \\frac{\\frac{dL}{dw_1}}{\\sqrt{\\big(\\frac{dL}{dw_1}}\\big)^2} \\\\ \\frac{\\frac{dL}{dw_2}}{\\sqrt{\\big(\\frac{dL}{dw_2}}\\big)^2} \\\\ \\vdots \\end{bmatrix}  =\n\\begin{bmatrix} \\text{sign}\\big(\\frac{dL}{dw_1} \\big) \\\\ \\text{sign}\\big(\\frac{dL}{dw_2} \\big) \\\\ \\vdots \\end{bmatrix} = \\begin{bmatrix} +1 \\\\ -1 \\\\ \\vdots \\end{bmatrix} \\]"
  },
  {
    "objectID": "lecture9-optimization/notes.html#adam",
    "href": "lecture9-optimization/notes.html#adam",
    "title": "Lecture 9: Optimization",
    "section": "Adam",
    "text": "Adam\n\\[ \\mathbf{v}^{(k+1)} \\longleftarrow \\beta_1 \\mathbf{v}^{(k)} + (1-\\beta_1) \\nabla_{\\mathbf{w}} \\textbf{Loss}(\\mathbf{w}^{(k)}, \\mathbf{X}, \\mathbf{y})\\] \\[ \\mathbf{s}^{(k+1)} \\longleftarrow \\beta_2 \\mathbf{s}^{(k)} + (1-\\beta_2) (\\nabla_{\\mathbf{w}} \\textbf{Loss}(\\mathbf{w}^{(k)}, \\mathbf{X}, \\mathbf{y}))^2\\] \\[ \\mathbf{w}^{(k+1)} \\longleftarrow \\mathbf{w}^{(k)} - \\alpha \\frac{\\mathbf{v}^{(k+1)}\n}{\\sqrt{\\mathbf{s}^{(k+1)} + \\epsilon}}\\] \\[ \\mathbf{w}^{(k+1)} \\longleftarrow \\mathbf{w}^{(k)} - \\alpha \\frac{\\frac{\\mathbf{v}^{(k+1)}}{(1-\\beta_1^k)}\n}{\\sqrt{\\frac{\\mathbf{s}^{(k+1)}}{(1-\\beta_2^k)} + \\epsilon}}\\] \\[\\mathbf{v}^{(0)} = \\mathbf{0}, \\quad \\mathbf{s}^{(0)} = \\mathbf{0}\\] \\[\\frac{\\mathbf{v}^{(k+1)}}{(1-\\beta_1^k)} = \\frac{\\beta_1 \\mathbf{0} + (1-\\beta_1)\\nabla_{\\mathbf{w}} \\textbf{Loss}(\\mathbf{w}^{(k)}, \\mathbf{X}, \\mathbf{y})}{(1-\\beta_1^1)} = \\nabla_{\\mathbf{w}} \\textbf{Loss}(\\mathbf{w}^{(k)}, \\mathbf{X}, \\mathbf{y})\\]"
  },
  {
    "objectID": "lecture9-optimization/notes.html#adaptive-gradients-rmsprop",
    "href": "lecture9-optimization/notes.html#adaptive-gradients-rmsprop",
    "title": "Lecture 9: Optimization",
    "section": "Adaptive gradients (RMSProp)",
    "text": "Adaptive gradients (RMSProp)\n\\[\\nabla_{\\mathbf{w}} \\textbf{Loss}(\\mathbf{w}^{(k)}, \\mathbf{X}, \\mathbf{y}) = \\begin{bmatrix} \\frac{dL}{dw^{(k)}_1} \\\\ \\frac{dL}{dw^{(k)}_2} \\\\ \\vdots \\end{bmatrix}\\]\n\\[\\begin{bmatrix} 3.1\\\\ 2.2 \\\\ \\vdots \\end{bmatrix} \\leftarrow\n\\begin{bmatrix} 5.0 \\\\ 1.8 \\\\ \\vdots \\end{bmatrix}\\leftarrow\n\\begin{bmatrix} 1.5 \\\\ 4.4 \\\\ \\vdots \\end{bmatrix}...\\]\n\\[\\begin{bmatrix} 10.1\\\\ 0.04 \\\\ \\vdots \\end{bmatrix} \\leftarrow\n\\begin{bmatrix} 8.6 \\\\ 0.02 \\\\ \\vdots \\end{bmatrix}\\leftarrow\n\\begin{bmatrix} 9.4 \\\\ 0.009 \\\\ \\vdots \\end{bmatrix}...\\]\n\\[ \\mathbf{s}^{(k+1)} \\longleftarrow \\beta \\mathbf{s}^{(k)} + (1-\\beta) (\\nabla_{\\mathbf{w}} \\textbf{Loss}(\\mathbf{w}^{(k)}, \\mathbf{X}, \\mathbf{y}))^2\\]\n\\[ \\mathbf{w}^{(k+1)} \\longleftarrow \\mathbf{w}^{(k)} - \\alpha \\frac{\\nabla_{\\mathbf{w}} \\textbf{Loss}(\\mathbf{w}^{(k)}, \\mathbf{X}, \\mathbf{y})}{\\sqrt{\\mathbf{s}^{(k+1)} + \\epsilon}}\\] \\[\\epsilon &lt;&lt; 1, \\quad \\text{e.g. } \\epsilon = 1e^{-7}\\]\n\\[\\frac{\\nabla_{\\mathbf{w}} \\textbf{Loss}(\\mathbf{w}^{(k)}, \\mathbf{X}, \\mathbf{y})}{\\sqrt{\\mathbf{s}^{(k+1)}}} =\n\\begin{bmatrix} \\frac{\\frac{dL}{dw_1}}{\\sqrt{\\big(\\frac{dL}{dw_1}}\\big)^2} \\\\ \\frac{\\frac{dL}{dw_2}}{\\sqrt{\\big(\\frac{dL}{dw_2}}\\big)^2} \\\\ \\vdots \\end{bmatrix}  =\n\\begin{bmatrix} \\text{sign}\\big(\\frac{dL}{dw_1} \\big) \\\\ \\text{sign}\\big(\\frac{dL}{dw_2} \\big) \\\\ \\vdots \\end{bmatrix} = \\begin{bmatrix} +1 \\\\ -1 \\\\ \\vdots \\end{bmatrix} \\]"
  },
  {
    "objectID": "lecture9-optimization/notes.html#vanishing-and-exploding-gradients",
    "href": "lecture9-optimization/notes.html#vanishing-and-exploding-gradients",
    "title": "Lecture 9: Optimization",
    "section": "Vanishing and exploding gradients",
    "text": "Vanishing and exploding gradients\n\n\nNeural networks are a composition of functions:\n\\[f(\\mathbf{x}) = f_0(f_1(f_2(...f_L(\\mathbf{x})...)))\\]\n\\[f(\\mathbf{x}) = \\text{relu}(\\text{relu}(\\text{relu}(...\\mathbf{x}^T\\mathbf{W}_L...)^T\\mathbf{W}_2)^T\\mathbf{W}_1)^T\\mathbf{w}_0\\] \\[\\text{relu}(x) = \\max(x, 0)\\]\n\\[\\nabla_{\\mathbf{W}_L}f(\\mathbf{x})  = \\frac{d\\mathbf{f}_0}{d\\mathbf{f}_1}\\frac{d\\mathbf{f}_1}{d\\mathbf{f}_2}...\\frac{d\\mathbf{f}_{L-1}}{d\\mathbf{f}_L}\\frac{d\\mathbf{f}_L}{d\\mathbf{W}_L}  = \\mathbf{x}^T\\prod_{l=1}^{L-1}\\frac{d\\mathbf{f}_{l}}{d\\mathbf{f}_{l-1}} \\]\nSimplified case: \\[\\frac{df}{dx}  = \\frac{d{f}_0}{d{f}_1}\\frac{d{f}_1}{d{f}_2}...\\frac{d{f}_{L-1}}{d{f}_L}\\frac{d{f}_L}{d{w}_L}  = {x}^T\\prod_{l=1}^{L-1}\\frac{d{f}_{l}}{d{f}_{l-1}} \\]\n\\[w_L, x, \\frac{df_0}{df_1},... \\in \\mathbb{R}, \\quad \\bigg|\\frac{df_{l-1}}{df_l}\\bigg| \\approx M\\]\n\\[\\bigg|\\frac{df}{dw_L}\\bigg| = |x| \\prod_{l=1}^{L-1}\\bigg| \\frac{df_{l}}{df_{l-1}}\\bigg| \\approx |x|\\big(\\textcolor{red}{M^L}\\big)\\] Exploding gradients: \\[\\textbf{If: } M &gt; 1 \\longrightarrow \\frac{df}{dw_L} &gt;&gt; 1\\] Vanishing gradients: \\[\\textbf{If: } M &lt; 1 \\longrightarrow \\frac{df}{dw_L} \\approx 0\\]"
  },
  {
    "objectID": "lecture9-optimization/notes.html#vanishing-and-exploding-gradients-1",
    "href": "lecture9-optimization/notes.html#vanishing-and-exploding-gradients-1",
    "title": "Lecture 9: Optimization",
    "section": "Vanishing and exploding gradients",
    "text": "Vanishing and exploding gradients"
  },
  {
    "objectID": "lecture9-optimization/notes.html#gradient-clipping",
    "href": "lecture9-optimization/notes.html#gradient-clipping",
    "title": "Lecture 9: Optimization",
    "section": "Gradient clipping",
    "text": "Gradient clipping\nExplicitly clip the gradient to prevent it form becoming too large.\n\\[\\textbf{clip}_{\\text{value}}\\big(\\mathbf{x}, \\epsilon\\big) = \\begin{bmatrix} \\min(\\max(x_1, -\\epsilon), \\epsilon) \\\\ \\min(\\max(x_2, - \\epsilon), \\epsilon) \\\\ \\vdots \\end{bmatrix}\\]\n\\[\\textbf{clip}_{\\text{norm}}\\big(\\mathbf{x}, \\epsilon\\big) = \\begin{cases}\n\\frac{\\epsilon\\mathbf{x} }{\\| \\mathbf{x} \\|_2} \\quad & \\textbf{if: } \\|\\mathbf{x}\\|_2 &gt; \\epsilon \\\\\n\\mathbf{x} \\  \\quad\\quad & \\textbf{if: } \\|\\mathbf{x}\\|_2 \\leq \\epsilon\n\\end{cases}\\]\n\\[ \\mathbf{w}^{(k+1)} \\longleftarrow \\mathbf{w}^{(k)} - \\alpha\\ \\textbf{clip}\\big(\\nabla_{\\mathbf{w}} \\textbf{Loss}(\\mathbf{w}^{(k)}, \\mathbf{X}, \\mathbf{y})\\big)\\]"
  },
  {
    "objectID": "lecture9-optimization/notes.html#layer-normalization",
    "href": "lecture9-optimization/notes.html#layer-normalization",
    "title": "Lecture 9: Optimization",
    "section": "Layer normalization",
    "text": "Layer normalization\nNormalize over the layer:\n\\[\\text{LayerNorm}(\\mathbf{x}) = \\frac{\\mathbf{x} - \\bar{x}}{\\sqrt{s^2 + \\epsilon}}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_d\\end{bmatrix}\\]\nTraining & test time: \\[\\bar{x} = \\frac{1}{d}\\sum_{i=1}^{d} x_i\\quad \\text{(output mean)}\\] Biased estimator: \\[s^2 = \\frac{1}{d}\\sum_{i=1}^{d} \\bigg(x_i - \\bigg(\\frac{1}{d}\\sum_{i=1}^{d} x_i\\bigg)\\bigg)^2\\quad \\text{(output var.)}\\] Unbiased estimator: \\[s^2 = \\frac{1}{d-1}\\sum_{i=1}^{d} \\bigg(x_i - \\bigg(\\frac{1}{d}\\sum_{i=1}^{d} x_i\\bigg)\\bigg)^2\\quad \\text{(output var.)}\\]"
  },
  {
    "objectID": "lecture9-optimization/notes.html#batch-normalization",
    "href": "lecture9-optimization/notes.html#batch-normalization",
    "title": "Lecture 9: Optimization",
    "section": "Batch normalization",
    "text": "Batch normalization\nNormalize over the batch:\n\\[\\text{BatchNorm}(x) = \\frac{ x - \\mathbb{E}[x]}{\\sqrt{\\text{Var}[x] + \\epsilon}}\\]\nTraining time: \\[\\text{Batch: } \\{x_1, x_2,...,x_B\\}\\] \\[\\mathbb{E}[x] \\approx \\bar{x} = \\frac{1}{B}\\sum_{i=1}^{B} x_i\\quad \\text{(sample mean)}\\] Biased estimator: \\[\\text{Var}[x] \\approx s^2 = \\frac{1}{B}\\sum_{i=1}^{B} \\bigg(x_i - \\bigg(\\frac{1}{B}\\sum_{i=1}^{B} x_i\\bigg)\\bigg)^2\\quad \\text{(sample var.)}\\] Unbiased estimator: \\[\\text{Var}[x] \\approx  s^2 = \\frac{1}{B-1}\\sum_{i=1}^{B} \\bigg(x_i - \\bigg(\\frac{1}{B}\\sum_{i=1}^{B} x_i\\bigg)\\bigg)^2\\quad \\text{(sample var.)}\\]\n\\[\\underset{\\text{train}}{\\text{BatchNorm}}(x) = \\frac{ x - \\bar{x}}{\\sqrt{s^2 + \\epsilon}}\\]\nRunning estimate: \\[\\bar{\\mu}^{(k+1)} \\longleftarrow \\beta \\bar{\\mu}^{(k)} + (1-\\beta) \\bar{x}^{(k)}\\] \\[\\bar{\\sigma}^{2(k+1)} \\longleftarrow \\beta \\bar{\\sigma}^{2(k)} + (1-\\beta) s^{2(k)}\\]\n\\[\\underset{\\text{test}}{\\text{BatchNorm}}(x) = \\frac{ x - \\bar{\\mu}}{\\sqrt{\\bar{\\sigma}^2 + \\epsilon}}\\]"
  },
  {
    "objectID": "lecture9-optimization/notes.html#group-normalization",
    "href": "lecture9-optimization/notes.html#group-normalization",
    "title": "Lecture 9: Optimization",
    "section": "Group normalization",
    "text": "Group normalization"
  },
  {
    "objectID": "lecture9-optimization/notes.html#scaled-normalization",
    "href": "lecture9-optimization/notes.html#scaled-normalization",
    "title": "Lecture 9: Optimization",
    "section": "Scaled normalization",
    "text": "Scaled normalization\n\\[\\text{BatchNorm}(x) = \\frac{ x - \\mathbb{E}[x]}{\\sqrt{\\text{Var}[x] + \\epsilon}} \\gamma + \\kappa\\] \\[\\text{LayerNorm}(\\mathbf{x}) = \\frac{\\mathbf{x} - \\bar{x}}{\\sqrt{s^2 + \\epsilon}} \\gamma + \\kappa\\]"
  },
  {
    "objectID": "lecture10-normalization/notes.html",
    "href": "lecture10-normalization/notes.html",
    "title": "Lecture 10: Normalization",
    "section": "",
    "text": "Neural networks are a composition of functions:\n\\[f(\\mathbf{x}) = f_0(f_1(f_2(...f_L(\\mathbf{x})...)))\\]\n\\[f(\\mathbf{x}) = \\text{relu}(\\text{relu}(\\text{relu}(...\\mathbf{x}^T\\mathbf{W}_L...)^T\\mathbf{W}_2)^T\\mathbf{W}_1)^T\\mathbf{w}_0\\] \\[\\text{relu}(x) = \\max(x, 0)\\]\n\\[\\nabla_{\\mathbf{W}_L}f(\\mathbf{x})  = \\frac{d\\mathbf{f}_0}{d\\mathbf{f}_1}\\frac{d\\mathbf{f}_1}{d\\mathbf{f}_2}...\\frac{d\\mathbf{f}_{L-1}}{d\\mathbf{f}_L}\\frac{d\\mathbf{f}_L}{d\\mathbf{W}_L}  = \\mathbf{x}^T\\prod_{l=1}^{L-1}\\frac{d\\mathbf{f}_{l}}{d\\mathbf{f}_{l-1}} \\]\nSimplified case: \\[\\frac{df}{dx}  = \\frac{d{f}_0}{d{f}_1}\\frac{d{f}_1}{d{f}_2}...\\frac{d{f}_{L-1}}{d{f}_L}\\frac{d{f}_L}{d{w}_L}  = {x}^T\\prod_{l=1}^{L-1}\\frac{d{f}_{l}}{d{f}_{l-1}} \\]\n\\[w_L, x, \\frac{df_0}{df_1},... \\in \\mathbb{R}, \\quad \\bigg|\\frac{df_{l-1}}{df_l}\\bigg| \\approx M\\]\n\\[\\bigg|\\frac{df}{dw_L}\\bigg| = |x| \\prod_{l=1}^{L-1}\\bigg| \\frac{df_{l}}{df_{l-1}}\\bigg| \\approx |x|\\big(\\textcolor{red}{M^L}\\big)\\] Exploding gradients: \\[\\textbf{If: } M &gt; 1 \\longrightarrow \\frac{df}{dw_L} &gt;&gt; 1\\] Vanishing gradients: \\[\\textbf{If: } M &lt; 1 \\longrightarrow \\frac{df}{dw_L} \\approx 0\\]\n\n\n\nExplicitly clip the gradient to prevent it form becoming too large.\n\\[\\textbf{clip}_{\\text{value}}\\big(\\mathbf{x}, \\epsilon\\big) = \\begin{bmatrix} \\min(\\max(x_1, -\\epsilon), \\epsilon) \\\\ \\min(\\max(x_2, - \\epsilon), \\epsilon) \\\\ \\vdots \\end{bmatrix}\\]\n\\[\\textbf{clip}_{\\text{norm}}\\big(\\mathbf{x}, \\epsilon\\big) = \\begin{cases}\n\\frac{\\epsilon\\mathbf{x} }{\\| \\mathbf{x} \\|_2} \\quad & \\textbf{if: } \\|\\mathbf{x}\\|_2 &gt; \\epsilon \\\\\n\\mathbf{x} \\  \\quad\\quad & \\textbf{if: } \\|\\mathbf{x}\\|_2 \\leq \\epsilon\n\\end{cases}\\]\n\\[ \\mathbf{w}^{(k+1)} \\longleftarrow \\mathbf{w}^{(k)} - \\alpha\\ \\textbf{clip}\\big(\\nabla_{\\mathbf{w}} \\textbf{Loss}(\\mathbf{w}^{(k)}, \\mathbf{X}, \\mathbf{y})\\big)\\]"
  },
  {
    "objectID": "lecture10-normalization/notes.html#vanishing-and-exploding-gradients",
    "href": "lecture10-normalization/notes.html#vanishing-and-exploding-gradients",
    "title": "Lecture 10: Normalization",
    "section": "",
    "text": "Neural networks are a composition of functions:\n\\[f(\\mathbf{x}) = f_0(f_1(f_2(...f_L(\\mathbf{x})...)))\\]\n\\[f(\\mathbf{x}) = \\text{relu}(\\text{relu}(\\text{relu}(...\\mathbf{x}^T\\mathbf{W}_L...)^T\\mathbf{W}_2)^T\\mathbf{W}_1)^T\\mathbf{w}_0\\] \\[\\text{relu}(x) = \\max(x, 0)\\]\n\\[\\nabla_{\\mathbf{W}_L}f(\\mathbf{x})  = \\frac{d\\mathbf{f}_0}{d\\mathbf{f}_1}\\frac{d\\mathbf{f}_1}{d\\mathbf{f}_2}...\\frac{d\\mathbf{f}_{L-1}}{d\\mathbf{f}_L}\\frac{d\\mathbf{f}_L}{d\\mathbf{W}_L}  = \\mathbf{x}^T\\prod_{l=1}^{L-1}\\frac{d\\mathbf{f}_{l}}{d\\mathbf{f}_{l-1}} \\]\nSimplified case: \\[\\frac{df}{dx}  = \\frac{d{f}_0}{d{f}_1}\\frac{d{f}_1}{d{f}_2}...\\frac{d{f}_{L-1}}{d{f}_L}\\frac{d{f}_L}{d{w}_L}  = {x}^T\\prod_{l=1}^{L-1}\\frac{d{f}_{l}}{d{f}_{l-1}} \\]\n\\[w_L, x, \\frac{df_0}{df_1},... \\in \\mathbb{R}, \\quad \\bigg|\\frac{df_{l-1}}{df_l}\\bigg| \\approx M\\]\n\\[\\bigg|\\frac{df}{dw_L}\\bigg| = |x| \\prod_{l=1}^{L-1}\\bigg| \\frac{df_{l}}{df_{l-1}}\\bigg| \\approx |x|\\big(\\textcolor{red}{M^L}\\big)\\] Exploding gradients: \\[\\textbf{If: } M &gt; 1 \\longrightarrow \\frac{df}{dw_L} &gt;&gt; 1\\] Vanishing gradients: \\[\\textbf{If: } M &lt; 1 \\longrightarrow \\frac{df}{dw_L} \\approx 0\\]"
  },
  {
    "objectID": "lecture10-normalization/notes.html#gradient-clipping",
    "href": "lecture10-normalization/notes.html#gradient-clipping",
    "title": "Lecture 10: Normalization",
    "section": "",
    "text": "Explicitly clip the gradient to prevent it form becoming too large.\n\\[\\textbf{clip}_{\\text{value}}\\big(\\mathbf{x}, \\epsilon\\big) = \\begin{bmatrix} \\min(\\max(x_1, -\\epsilon), \\epsilon) \\\\ \\min(\\max(x_2, - \\epsilon), \\epsilon) \\\\ \\vdots \\end{bmatrix}\\]\n\\[\\textbf{clip}_{\\text{norm}}\\big(\\mathbf{x}, \\epsilon\\big) = \\begin{cases}\n\\frac{\\epsilon\\mathbf{x} }{\\| \\mathbf{x} \\|_2} \\quad & \\textbf{if: } \\|\\mathbf{x}\\|_2 &gt; \\epsilon \\\\\n\\mathbf{x} \\  \\quad\\quad & \\textbf{if: } \\|\\mathbf{x}\\|_2 \\leq \\epsilon\n\\end{cases}\\]\n\\[ \\mathbf{w}^{(k+1)} \\longleftarrow \\mathbf{w}^{(k)} - \\alpha\\ \\textbf{clip}\\big(\\nabla_{\\mathbf{w}} \\textbf{Loss}(\\mathbf{w}^{(k)}, \\mathbf{X}, \\mathbf{y})\\big)\\]"
  },
  {
    "objectID": "lecture10-normalization/notes.html#batch-normalization",
    "href": "lecture10-normalization/notes.html#batch-normalization",
    "title": "Lecture 10: Normalization",
    "section": "Batch normalization",
    "text": "Batch normalization\nNormalize over the batch:\n\\[\\text{BatchNorm}(x) = \\frac{ x - \\mathbb{E}[x]}{\\sqrt{\\text{Var}[x] + \\epsilon}}\\]\nTraining time: \\[\\text{Batch: } \\{x_1, x_2,...,x_B\\}\\] \\[\\mathbb{E}[x] \\approx \\bar{x} = \\frac{1}{B}\\sum_{i=1}^{B} x_i\\quad \\text{(sample mean)}\\] Biased estimator: \\[\\text{Var}[x] \\approx s^2 = \\frac{1}{B}\\sum_{i=1}^{B} \\bigg(x_i - \\bigg(\\frac{1}{B}\\sum_{i=1}^{B} x_i\\bigg)\\bigg)^2\\quad \\text{(sample var.)}\\] Unbiased estimator: \\[\\text{Var}[x] \\approx  s^2 = \\frac{1}{B-1}\\sum_{i=1}^{B} \\bigg(x_i - \\bigg(\\frac{1}{B}\\sum_{i=1}^{B} x_i\\bigg)\\bigg)^2\\quad \\text{(sample var.)}\\]\n\\[\\underset{\\text{train}}{\\text{BatchNorm}}(x) = \\frac{ x - \\bar{x}}{\\sqrt{s^2 + \\epsilon}}\\]\nRunning estimate: \\[\\bar{\\mu}^{(k+1)} \\longleftarrow \\beta \\bar{\\mu}^{(k)} + (1-\\beta) \\bar{x}^{(k)}\\] \\[\\bar{\\sigma}^{2(k+1)} \\longleftarrow \\beta \\bar{\\sigma}^{2(k)} + (1-\\beta) s^{2(k)}\\]\n\\[\\underset{\\text{test}}{\\text{BatchNorm}}(x) = \\frac{ x - \\bar{\\mu}}{\\sqrt{\\bar{\\sigma}^2 + \\epsilon}}\\]"
  },
  {
    "objectID": "lecture10-normalization/notes.html#layer-normalization",
    "href": "lecture10-normalization/notes.html#layer-normalization",
    "title": "Lecture 10: Normalization",
    "section": "Layer normalization",
    "text": "Layer normalization\nNormalize over the layer:\n\\[\\text{LayerNorm}(\\mathbf{x}) = \\frac{\\mathbf{x} - \\bar{x}}{\\sqrt{s^2 + \\epsilon}}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_d\\end{bmatrix}\\]\nTraining & test time: \\[\\bar{x} = \\frac{1}{d}\\sum_{i=1}^{d} x_i\\quad \\text{(output mean)}\\] Biased estimator: \\[s^2 = \\frac{1}{d}\\sum_{i=1}^{d} \\bigg(x_i - \\bigg(\\frac{1}{d}\\sum_{i=1}^{d} x_i\\bigg)\\bigg)^2\\quad \\text{(output var.)}\\] Unbiased estimator: \\[s^2 = \\frac{1}{d-1}\\sum_{i=1}^{d} \\bigg(x_i - \\bigg(\\frac{1}{d}\\sum_{i=1}^{d} x_i\\bigg)\\bigg)^2\\quad \\text{(output var.)}\\]"
  },
  {
    "objectID": "lecture10-normalization/notes.html#scaled-normalization",
    "href": "lecture10-normalization/notes.html#scaled-normalization",
    "title": "Lecture 10: Normalization",
    "section": "Scaled normalization",
    "text": "Scaled normalization\n\\[\\text{BatchNorm}(x) = \\frac{ x - \\mathbb{E}[x]}{\\sqrt{\\text{Var}[x] + \\epsilon}} \\gamma + \\kappa\\] \\[\\text{LayerNorm}(\\mathbf{x}) = \\frac{\\mathbf{x} - \\bar{x}}{\\sqrt{s^2 + \\epsilon}} \\gamma + \\kappa\\]"
  },
  {
    "objectID": "assignments/final-project/outline.html",
    "href": "assignments/final-project/outline.html",
    "title": "Final project outline",
    "section": "",
    "text": "The first step for the project will be to form a group and choose a project topic. For your proposal you should:\n\nForm a group of 2-4 students (ideally 3) and choose a cool team name. If you would like help finding a group, please email me!\nChoose a project topic. You may choose a topic from the list below or you may propose your own topic.\nWith your team, write a short (up to 1 page) proposal for your project and submit it on gradescope. Your proposal should include:\n\nThe names of the team members\nA one paragraph high-level description of the goal(s) of the project (e.g. object detection in images, classifying text data, etc.), how this goal could be useful in real-world applications and why this topic interests you.\nA one paragraph description of how you intend to approach this problem and any challenges you forsee. This should include identifying datasets that you might use, identifying at least one referance (academic paper or website) for a technique you intend to try.\nA one paragraph description of how you will evaluate success for your application."
  },
  {
    "objectID": "assignments/final-project/outline.html#proposal-due-wednesday-118-before-class",
    "href": "assignments/final-project/outline.html#proposal-due-wednesday-118-before-class",
    "title": "Final project outline",
    "section": "",
    "text": "The first step for the project will be to form a group and choose a project topic. For your proposal you should:\n\nForm a group of 2-4 students (ideally 3) and choose a cool team name. If you would like help finding a group, please email me!\nChoose a project topic. You may choose a topic from the list below or you may propose your own topic.\nWith your team, write a short (up to 1 page) proposal for your project and submit it on gradescope. Your proposal should include:\n\nThe names of the team members\nA one paragraph high-level description of the goal(s) of the project (e.g. object detection in images, classifying text data, etc.), how this goal could be useful in real-world applications and why this topic interests you.\nA one paragraph description of how you intend to approach this problem and any challenges you forsee. This should include identifying datasets that you might use, identifying at least one referance (academic paper or website) for a technique you intend to try.\nA one paragraph description of how you will evaluate success for your application."
  },
  {
    "objectID": "assignments/final-project/outline.html#check-in",
    "href": "assignments/final-project/outline.html#check-in",
    "title": "Final project outline",
    "section": "Check-in",
    "text": "Check-in"
  },
  {
    "objectID": "assignments/final-project/outline.html#final-deliverable",
    "href": "assignments/final-project/outline.html#final-deliverable",
    "title": "Final project outline",
    "section": "Final deliverable",
    "text": "Final deliverable\nThe final deliverable for this project will consist of four parts:\n\nTechnical report: Your team will make a single-page website using Quaro"
  },
  {
    "objectID": "assignments/final-project/outline.html#possible-projects",
    "href": "assignments/final-project/outline.html#possible-projects",
    "title": "Final project outline",
    "section": "Possible projects",
    "text": "Possible projects\n\nNeural style transfer\nLink: https://arxiv.org/pdf/1508.06576.pdf\nSummary: Style transfer is the process of taking an existing image and applying an artistic style to it, such as making a photograph look like a painting or a drawing (check out the examples in the linked paper!). This can be accomplished with neural networks. For this project you could: implement the neural style transfer algorithm, evaluate it with different kinds of images and compare it to other methods for restyling images.\nSuggested datasets: Any artistic images you’d like!\n\n\nSemi-supervised learning with MixMatch\nLink: https://arxiv.org/pdf/1905.02249.pdf\nSummary: Semi-supervised learning is the problem of learning when we don’t have all the labels for our training data. MixMatch is a state-of-the-art approach for semi-supervised image classification. In this project you could: implement the Mix-Match algorithm, compare the different versions of it discussed in the paper and evaluate it on several different datasets. You could also test it on your own proposed semi-supervised learning task.\nSuggested datasets: Street view house numbers, CIFAR-10, STL-10\n\n\nAudio generation and classification with WaveNet\nLink: https://arxiv.org/pdf/1609.03499.pdf\nSummary: WaveNet is at network that forms the basis for many text-to-speech systems (think Alexa or Siri) it also allows for classifying audio. For this project you could: implement WaveNet, train it to generate speech (or other audio like music!) and evaluate it compared to existing tools for generation. You could also try to use it to classify speech or music.\nSuggested datasets: Spoken digits, Speech commands, Crema-D, GTZAN\n\n\nU-Nets for segmentation, depth-prediction, colorization or super-resolution\nLink: https://arxiv.org/pdf/1505.04597.pdf\nSummary: U-Nets are a very flexible type of neural network used for many computer vision tasks. They were originally introduced for segmenting different parts of medical images, but can used for everything from colorizing images to upscaling images to predicting depth in am image. For this project you could: implement the U-Net architecture, train a U-Net on one or more of the above tasks and evaluate its performance.\nSuggested datasets: Oxford flowers ImageNet, NYU Depth\n\n\nObject detection with YOLO\nLink: https://arxiv.org/pdf/1506.02640v5.pdf\nSummary: Object detection is the task of locating objects within an image. This is a step more difficult than just classifying images, but is very useful in practice. For this project you could: implement the YOLO object detection model, test different variations of the model and evaluate it on new data.\nSuggested datasets: VOC, Wider face, Kitti\n\n\nImage generation with Generative Adversarial Networks\nLink: https://arxiv.org/pdf/1406.2661.pdf\nSummary: Generative adversarial networks (GANs for short) are an effective way to generate realistic looking images using neural networks. They have caused a considerable amount of excitement and concern for their performance in generating realistic images of humans. For this project you could: implement a generative adversarial network, explore reasonable ways to evaluate the performance of GANs and dive into the ethical implications.\nSuggested datasets: MNIST, Omniglot, CIFAR-10, FFHQ\n\n\nImage classification with visual transformers\nLink: https://arxiv.org/pdf/2010.11929.pdf\nSummary: Transformer-based neural networks have transformed the field of natural language processing in recent years, as evidenced by the performance of models such as ChatGPT. There is growing evidence that they are also extremely useful for classifying images. For this project you might: implement the visual transformer architecture, compare it to convolutional neural network based architecture for image classification and visualize features to understand the differences in the approaches. You might also consider applying it to your own dataset.\nSuggested datasets: Oxford flowers, CIFAR-10, ImageNet\n\n\nText generation or classification with GPT\nLink: https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf\nSummary: Large language models, such at GPT-3 and GPT-4 have gained a lot of attention recently, as their performance in generating plausible text is (debatably) approaching human levels. The GPT model is used by Chat-GPT and many other applications to model language. For this project you could implement and train your own version of the original (GPT-1) model, compare it against available tools such as Chat-GPT and explore how to distinguish generated text from real human writing.\nSuggested datasets: Amazon reviews IMDB reviews\n\n\nOther possible projects:\n\nMachine learning with differential privacy\nhttps://arxiv.org/pdf/1607.00133.pdf\n\n\nGraph neural networks\nhttps://arxiv.org/pdf/1810.00826.pdf"
  },
  {
    "objectID": "assignments/final-project/outline.html#check-in-due-monday-1120-before-class",
    "href": "assignments/final-project/outline.html#check-in-due-monday-1120-before-class",
    "title": "Final project outline",
    "section": "Check-in (Due Monday 11/20 before class)",
    "text": "Check-in (Due Monday 11/20 before class)\nAs a progress report you and your team will submit a short (1 page) summery of progress on Gradescope. This summary will include: - A high-level description of any updates you have made to the goals of your project. - A description of what methods you have tried and any preliminary results. - A timeline for finishing the remaining goals of your project. - A brief description of the contributions made by each team member."
  },
  {
    "objectID": "assignments/final-project/outline.html#final-deliverable-due-126",
    "href": "assignments/final-project/outline.html#final-deliverable-due-126",
    "title": "Final project outline",
    "section": "Final deliverable (Due 12/6)",
    "text": "Final deliverable (Due 12/6)\nThe final deliverable for this project will consist of four parts:\n\nTechnical report: Your team will make a single-page website describing your approach and results.\nImplementation: Your team will create a github repository with your annotated implementation.\nAnalysis: Your team will write a short report analyzing the potential applications and ethical considerations of your technique.\nPresentation: Your team will put together a short 5-10 minute presentation explaining the method that you learned about and summarizing your results."
  },
  {
    "objectID": "assignments/homework5-pytorch/solutions.html",
    "href": "assignments/homework5-pytorch/solutions.html",
    "title": "Homework 5: PyTorch",
    "section": "",
    "text": "In the last homework we implemented automatic differentiation in terms of scalar values. For reverse mode, we saw that the update for a node \\(w\\) from a its child \\(a\\) could be written as: \\[\\frac{dL}{dw} = \\frac{dL}{da}\\frac{da}{dw}\\]\nIn this case all three values in this equation were scalars and therefore the computation was quite simple. However we saw that this approach was quite inefficient when dealing with large arrays of numbers. A better approach that we saw in class was to have each node correspond to a vector (or possible a matrix). In this case if \\(\\mathbf{w}\\) is a length \\(n\\) vector and \\(\\mathbf{a}\\) is a length \\(m\\) vector we would say that: \\[\\mathbf{w} \\in \\mathbb{R}^n, \\quad \\mathbf{a} \\in \\mathbb{R}^m\\] corresponding update equation would be written as: \\[ \\frac{dL}{d\\mathbf{w}} = \\frac{dL}{d\\mathbf{a}}^T\\frac{da}{d\\mathbf{w}}\\]\nRecall that we called this update a vector-jacobian product. #### Q1 Assuming that \\(L\\) is still a scalar (\\(L\\in \\mathbb{R}\\)), what is the shape of each term in the equation above?\nYour answer should be as matrix/vector dimensions e.g. \\((a \\times b)\\)\n\\[\\frac{dL}{d\\mathbf{w}} = n\\] \\[\\frac{dL}{d\\mathbf{a}} = m\\] \\[\\frac{d\\mathbf{a}}{d\\mathbf{w}} = m \\times n\\]\n\n\nAssume that we have the following formula for \\(\\mathbf{a}\\): \\[\\mathbf{a}= \\exp \\big( \\mathbf{A} \\mathbf{w} \\big) + \\mathbf{w}^2\\] Where \\(\\mathbf{A}\\) is a constant \\(n \\times n\\) matrix. Given the gradient of \\(L\\) with respect to \\(\\mathbf{a}\\): \\(\\frac{dL}{d\\mathbf{a}}\\), what is the gradient of \\(L\\) with respect to \\(\\mathbf{w}\\) (\\(\\frac{dL}{d\\mathbf{w}}\\)) in terms of: \\(\\frac{dL}{d\\mathbf{a}}\\), \\(\\mathbf{A}\\) and \\(\\mathbf{w}\\)?\nHint: You do not need to write a formula for the Jacobian (\\(\\frac{d\\mathbf{a}}{d\\mathbf{w}}\\)). As we saw in class many VJPs can be written without definining the Jacobian explicitly (e.g. for element-wise functions). You can use \\(\\odot\\) (\\odot) to denote an element-wise product between vectors: \\[ \\mathbf{a} \\odot \\mathbf{b} = \\begin{bmatrix}\na_1 b_1 \\\\\na_2 b_3 \\\\\n\\vdots \\\\\na_d b_d \\\\\n\\end{bmatrix}\n\\]\nHere we’ll start by writing out the vector-Jacobian product we’re looking for: \\[\\frac{dL}{d\\mathbf{w}} = \\frac{dL}{d\\mathbf{a}}^T\\frac{d\\mathbf{a}}{d\\mathbf{w}} = \\frac{dL}{d\\mathbf{a}}^T\\frac{d}{d\\mathbf{w}}\\bigg( \\exp \\big( \\mathbf{A} \\mathbf{w} \\big) + \\mathbf{w}^2\\bigg)\\] Using the addition rule, we can distribute the derivative: \\[=\\frac{dL}{d\\mathbf{a}}^T\\frac{d}{d\\mathbf{w}}\\bigg( \\exp \\big( \\mathbf{A} \\mathbf{w} \\big)\\bigg) + \\frac{dL}{d\\mathbf{a}}^T\\frac{d}{d\\mathbf{w}}\\bigg(\\mathbf{w}^2\\bigg)\\]\nFor simplicity, let’s substitute \\(\\mathbf{b} = \\mathbf{A}\\mathbf{w}\\) for now, so we can apply the chain rule \\[=\\frac{dL}{d\\mathbf{a}}^T\\frac{d}{d\\mathbf{b}}\\bigg( \\exp \\big( \\mathbf{b} \\big)\\bigg)\\frac{d\\mathbf{b}}{d\\mathbf{w}} + \\frac{dL}{d\\mathbf{a}}^T\\frac{d}{d\\mathbf{w}}\\bigg(\\mathbf{w}^2\\bigg)\\]\nWe see that \\(\\exp(w)\\) and \\(w^2\\) are both element-wise operations, so we can use the rule we learned in class:\n\\[  = \\bigg(\\frac{dL}{d\\mathbf{a}} \\odot \\exp(d\\mathbf{b})\\bigg)^T \\frac{d\\mathbf{b}}{d\\mathbf{w}} + 2 \\bigg( \\frac{dL}{d\\mathbf{a}}  \\odot \\mathbf{w} \\bigg)\\]\nFinally we know that \\(\\mathbf{b} = \\mathbf{A}\\mathbf{w}\\), therefore \\(\\frac{d\\mathbf{b}}{d\\mathbf{w}} = \\mathbf{A}\\)\n\\[ \\frac{dL}{d\\mathbf{w}}  = \\bigg(\\frac{dL}{d\\mathbf{a}} \\odot \\exp(\\mathbf{A}\\mathbf{w})\\bigg)^T \\mathbf{A} + 2 \\bigg( \\frac{dL}{d\\mathbf{a}}  \\odot \\mathbf{w} \\bigg)\\]\nWe can verify our answer using PyTorch!\n\nimport torch\ntorch.manual_seed(0)\n\n# Setup the variables\nA = torch.randn(5, 5)\nw = torch.randn(5)\nw.requires_grad = True\n\n# Compute the gradient with autograd\na = torch.exp(torch.matmul(A, w)) + w ** 2\nL = torch.sum(a)\nL.backward()\nwgrad = w.grad\nwgrad\n\ntensor([ 2.5438, -4.3855,  1.7762, -0.3767,  4.9248])\n\n\n\n# Compute your VJP\ndL_da = torch.ones_like(a)\ndL_dw = torch.matmul(dL_da * torch.exp(torch.matmul(A, w)), A) + 2 * w * dL_da # YOUR CODE HERE\nassert torch.all(torch.isclose(wgrad, dL_dw))\ndL_dw\n\ntensor([ 2.5438, -4.3855,  1.7762, -0.3767,  4.9248], grad_fn=&lt;AddBackward0&gt;)"
  },
  {
    "objectID": "assignments/homework5-pytorch/solutions.html#part-1-vector-jacobian-products",
    "href": "assignments/homework5-pytorch/solutions.html#part-1-vector-jacobian-products",
    "title": "Homework 5: PyTorch",
    "section": "",
    "text": "In the last homework we implemented automatic differentiation in terms of scalar values. For reverse mode, we saw that the update for a node \\(w\\) from a its child \\(a\\) could be written as: \\[\\frac{dL}{dw} = \\frac{dL}{da}\\frac{da}{dw}\\]\nIn this case all three values in this equation were scalars and therefore the computation was quite simple. However we saw that this approach was quite inefficient when dealing with large arrays of numbers. A better approach that we saw in class was to have each node correspond to a vector (or possible a matrix). In this case if \\(\\mathbf{w}\\) is a length \\(n\\) vector and \\(\\mathbf{a}\\) is a length \\(m\\) vector we would say that: \\[\\mathbf{w} \\in \\mathbb{R}^n, \\quad \\mathbf{a} \\in \\mathbb{R}^m\\] corresponding update equation would be written as: \\[ \\frac{dL}{d\\mathbf{w}} = \\frac{dL}{d\\mathbf{a}}^T\\frac{da}{d\\mathbf{w}}\\]\nRecall that we called this update a vector-jacobian product. #### Q1 Assuming that \\(L\\) is still a scalar (\\(L\\in \\mathbb{R}\\)), what is the shape of each term in the equation above?\nYour answer should be as matrix/vector dimensions e.g. \\((a \\times b)\\)\n\\[\\frac{dL}{d\\mathbf{w}} = n\\] \\[\\frac{dL}{d\\mathbf{a}} = m\\] \\[\\frac{d\\mathbf{a}}{d\\mathbf{w}} = m \\times n\\]\n\n\nAssume that we have the following formula for \\(\\mathbf{a}\\): \\[\\mathbf{a}= \\exp \\big( \\mathbf{A} \\mathbf{w} \\big) + \\mathbf{w}^2\\] Where \\(\\mathbf{A}\\) is a constant \\(n \\times n\\) matrix. Given the gradient of \\(L\\) with respect to \\(\\mathbf{a}\\): \\(\\frac{dL}{d\\mathbf{a}}\\), what is the gradient of \\(L\\) with respect to \\(\\mathbf{w}\\) (\\(\\frac{dL}{d\\mathbf{w}}\\)) in terms of: \\(\\frac{dL}{d\\mathbf{a}}\\), \\(\\mathbf{A}\\) and \\(\\mathbf{w}\\)?\nHint: You do not need to write a formula for the Jacobian (\\(\\frac{d\\mathbf{a}}{d\\mathbf{w}}\\)). As we saw in class many VJPs can be written without definining the Jacobian explicitly (e.g. for element-wise functions). You can use \\(\\odot\\) (\\odot) to denote an element-wise product between vectors: \\[ \\mathbf{a} \\odot \\mathbf{b} = \\begin{bmatrix}\na_1 b_1 \\\\\na_2 b_3 \\\\\n\\vdots \\\\\na_d b_d \\\\\n\\end{bmatrix}\n\\]\nHere we’ll start by writing out the vector-Jacobian product we’re looking for: \\[\\frac{dL}{d\\mathbf{w}} = \\frac{dL}{d\\mathbf{a}}^T\\frac{d\\mathbf{a}}{d\\mathbf{w}} = \\frac{dL}{d\\mathbf{a}}^T\\frac{d}{d\\mathbf{w}}\\bigg( \\exp \\big( \\mathbf{A} \\mathbf{w} \\big) + \\mathbf{w}^2\\bigg)\\] Using the addition rule, we can distribute the derivative: \\[=\\frac{dL}{d\\mathbf{a}}^T\\frac{d}{d\\mathbf{w}}\\bigg( \\exp \\big( \\mathbf{A} \\mathbf{w} \\big)\\bigg) + \\frac{dL}{d\\mathbf{a}}^T\\frac{d}{d\\mathbf{w}}\\bigg(\\mathbf{w}^2\\bigg)\\]\nFor simplicity, let’s substitute \\(\\mathbf{b} = \\mathbf{A}\\mathbf{w}\\) for now, so we can apply the chain rule \\[=\\frac{dL}{d\\mathbf{a}}^T\\frac{d}{d\\mathbf{b}}\\bigg( \\exp \\big( \\mathbf{b} \\big)\\bigg)\\frac{d\\mathbf{b}}{d\\mathbf{w}} + \\frac{dL}{d\\mathbf{a}}^T\\frac{d}{d\\mathbf{w}}\\bigg(\\mathbf{w}^2\\bigg)\\]\nWe see that \\(\\exp(w)\\) and \\(w^2\\) are both element-wise operations, so we can use the rule we learned in class:\n\\[  = \\bigg(\\frac{dL}{d\\mathbf{a}} \\odot \\exp(d\\mathbf{b})\\bigg)^T \\frac{d\\mathbf{b}}{d\\mathbf{w}} + 2 \\bigg( \\frac{dL}{d\\mathbf{a}}  \\odot \\mathbf{w} \\bigg)\\]\nFinally we know that \\(\\mathbf{b} = \\mathbf{A}\\mathbf{w}\\), therefore \\(\\frac{d\\mathbf{b}}{d\\mathbf{w}} = \\mathbf{A}\\)\n\\[ \\frac{dL}{d\\mathbf{w}}  = \\bigg(\\frac{dL}{d\\mathbf{a}} \\odot \\exp(\\mathbf{A}\\mathbf{w})\\bigg)^T \\mathbf{A} + 2 \\bigg( \\frac{dL}{d\\mathbf{a}}  \\odot \\mathbf{w} \\bigg)\\]\nWe can verify our answer using PyTorch!\n\nimport torch\ntorch.manual_seed(0)\n\n# Setup the variables\nA = torch.randn(5, 5)\nw = torch.randn(5)\nw.requires_grad = True\n\n# Compute the gradient with autograd\na = torch.exp(torch.matmul(A, w)) + w ** 2\nL = torch.sum(a)\nL.backward()\nwgrad = w.grad\nwgrad\n\ntensor([ 2.5438, -4.3855,  1.7762, -0.3767,  4.9248])\n\n\n\n# Compute your VJP\ndL_da = torch.ones_like(a)\ndL_dw = torch.matmul(dL_da * torch.exp(torch.matmul(A, w)), A) + 2 * w * dL_da # YOUR CODE HERE\nassert torch.all(torch.isclose(wgrad, dL_dw))\ndL_dw\n\ntensor([ 2.5438, -4.3855,  1.7762, -0.3767,  4.9248], grad_fn=&lt;AddBackward0&gt;)"
  },
  {
    "objectID": "assignments/homework5-pytorch/solutions.html#part-2-introduction-to-pytorch",
    "href": "assignments/homework5-pytorch/solutions.html#part-2-introduction-to-pytorch",
    "title": "Homework 5: PyTorch",
    "section": "Part 2: Introduction to PyTorch",
    "text": "Part 2: Introduction to PyTorch\nNow that we’ve successfully built our own tool for automatic differentiation and neural networks, let’s look at an industry-standard tool for accomplishing the same tasks: PyTorch.\nThroughout this homework to may find it helpful to refer to the PyTorch documentation, as well as the lecture notebook on Pytorch.\nWe saw in class that we can create a function with parameters in PyTorch using the torch.nn module that we’ll import as just nn. We can do this by creating a subclass of nn.Module and defining the parameters with nn.Parameter.\n\nimport torch\nfrom torch import nn\nfrom hw5_support import *\n\nclass LinearZeros(nn.Module):\n    '''\n    A PyTorch module representing a linear/affine function with weights W and bias b: \n        f(X) = XW + b\n    W is an (in_dimensions x out_dimensions) matrix and b is an (out_dimensions) vector.\n\n    This version of the Linear module initializes the parameters to 0.\n    '''\n    def __init__(self, in_dimensions, out_dimensions):\n        # Call the nn.Module __init__ function\n        super().__init__()\n\n        # Create parameters that we can fit with gradient descent.\n        self.weights = nn.Parameter(torch.zeros(in_dimensions, out_dimensions))\n        self.bias = nn.Parameter(torch.zeros(out_dimensions))\n\n    def forward(self, x):\n        # Compute the function. Note that we use torch.matmul rather than torch.dot!\n        # This assumes X is 2-dimensional (a matrix)!\n        return torch.matmul(x, self.weights) + self.bias\n\nWe can create a 1-dimensional linear function by creating a LinearZeros object, specifying that both the input and output dimensions should be 1. The method model.parameters() will give us access to all the weights we can fit with gradient descent.\n\nmodel = LinearZeros(1, 1)\nlist(model.parameters())\n\n[Parameter containing:\n tensor([[0.]], requires_grad=True),\n Parameter containing:\n tensor([0.], requires_grad=True)]\n\n\nWe can also call this model just like any other function.\n\n# Create 4 1-dimensional inputs\nx = torch.ones((4, 1))\n\nmodel(x)\n\ntensor([[0.],\n        [0.],\n        [0.],\n        [0.]], grad_fn=&lt;AddBackward0&gt;)\n\n\nLet’s start by creating a simple dataset to use for the next few problems. We’ll use a regression dataset similar to the one we saw in class. In this case, I’ve provied data already split into training and validation sets.\n\n# Create the training inputs and labels\ntorch.manual_seed(0)\nx = torch.rand(200, 1) * 10 - 5.\ny = x ** 2 / 2 + torch.sin(x * 5) - 5\n\n# Create the validation inputs and labels\nxvalid = torch.rand(200, 1) * 10 - 5.\nyvalid = xvalid ** 2 / 2 + torch.sin(xvalid * 5 + torch.pi) - 5\n\nplotRegression(x, y, xvalid, yvalid) \n\n\n\n\nWe can make predictions for our data using the model we just definied:\n\npredictions = model(x)\npredictions[:5]\n\ntensor([[0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.]], grad_fn=&lt;SliceBackward0&gt;)\n\n\nHowever if we plot the prediction function, we’ll see that it isn’t very good as we haven’t optimized the parameters yet:\n\nplotRegression(x, y, xvalid, yvalid, model=model) \n\n\n\n\nThe first thing we’ll need to optimize our model is a loss function. As we saw in class, the convention in PyTorch is to separate the loss from the model, so we’ll write a simple function that takes in predictions and labels, returning the mean squared error loss.\n\nQ3\nComplete the mse_loss function below. The function should compute the same MSE loss we’ve seen in previous homeworks, but using PyTorch operations.\n\\[\\textbf{Loss}(\\mathbf{a}, \\mathbf{y}) = \\frac{1}{N}\\sum_{i=1}^N(a_i - y_i)^2\\]\n\ndef mse_loss(prediction, labels):\n    return torch.mean((prediction.reshape((-1,)) - labels.reshape((-1,))) ** 2)\n\n# Test to check \ntorch.manual_seed(0)\nassert torch.isclose(mse_loss(torch.randn(10, 1), torch.randn(10, 1)), torch.tensor(1.1550), 1e-3)\n\nWith our loss in hand, we can run gradient descent to optimize our model’s parameters. This time, we’ll use the torch.optim module, which includes many useful variations of gradient descent.\n\n\nQ4\nComplete the gradient descent function below. The function should: - Create an optim.SGD optimizer for the model’s parameters with the specified learning rate - At each step: - Compute the model output and loss (loss_func) on the training data - Compute the gradients of the loss with respect to the model parameters - Take a gradient descent step - Reset the parameter gradients to 0 - Compute the validation loss\n\nfrom torch import optim\n\ndef gradient_descent(model, loss_func, x, y, xvalid, yvalid, lr=0.1, steps=5000):\n    optimizer = optim.SGD(model.parameters(), lr=lr)\n\n    losses = []\n    valid_losses = []\n    for _ in tqdm.trange(steps):\n        output = model(x)\n        loss = loss_func(output, y)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        valid_loss = loss_func(model(xvalid), yvalid)\n        losses.append(loss.detach().numpy())\n        valid_losses.append(valid_loss.detach().numpy())\n\n    return losses, valid_losses\n\n\ntest_gradient_descent(gradient_descent, mse_loss, x, y, xvalid, yvalid)\nmodel = LinearZeros(1, 1)\nlosses, valid_losses = gradient_descent(model, mse_loss, x, y, xvalid, yvalid)\nplotRegression(x, y, xvalid, yvalid, loss_history=losses, valid_loss_history=valid_losses, model=model)\n\n100%|██████████| 1/1 [00:00&lt;00:00, 297.11it/s]\n100%|██████████| 5000/5000 [00:00&lt;00:00, 9927.69it/s] \n\n\nPassed!\n\n\n\n\n\nNow that we have a function to train a PyTorch model, we can try something bigger and more exciting! Let’s train a neural network.\n\n\nQ5\nCreate a PyTorch model for a neural network with the following specification: - The network should have 4 hidden layers, each with 20 neurons - The network should take 1-dimensional inputs as above - Each layer should use the LinearZeros module we just wrote - Each linear layer should be followed by a ReLU activation (except the output), use the nn.ReLU() module.\nHint: Remember that you can use the nn.Sequential class to easily compose a sequence of functions in PyTorch.\n\nmodel = nn.Sequential(\n            LinearZeros(1, 20),\n            nn.ReLU(),\n            LinearZeros(20, 20),\n            nn.ReLU(),\n            LinearZeros(20, 20),\n            nn.ReLU(),\n            LinearZeros(20, 20),\n            nn.ReLU(),\n            LinearZeros(20, 1),\n            )\n\n# Test the model build\ntest_build(model, LinearZeros, dropout_type=None, type='zeros')\n\n# Run the model\nlosses, valid_losses = gradient_descent(model, mse_loss, x, y, xvalid, yvalid)\nplotRegression(x, y, xvalid, yvalid, loss_history=losses, valid_loss_history=valid_losses, model=model)\n\nPassed!\n\n\n100%|██████████| 5000/5000 [00:01&lt;00:00, 3225.93it/s]\n\n\n\n\n\n\n\nQ6\nWhat happened when you attempted to train the model above? Why did this happen? Give a short 1-2 sentence answer.\nNothing happened because we initialized all weights to 0.\nLet’s try modifying our Linear module with a different strategy for initialization.\n\n\nQ7\nModify the LinearZeros implementation from above to initialize the weights and bias parameters from a standard normal distribution \\(w,b \\sim \\mathcal{N}(0, 1)\\). Then modify your model from Q6 to use this new module.\nHint: You may find the torch.randn function useful here. You might also find that the model doesn’t train! We’ll address this in the next question.\n\nclass LinearNormal(nn.Module):\n    def __init__(self, in_dimensions, out_dimensions):\n        super().__init__()\n        self.weights = nn.Parameter(torch.randn(in_dimensions, out_dimensions))\n        self.bias = nn.Parameter(torch.randn(out_dimensions))\n\n    def forward(self, x):\n        return torch.matmul(x, self.weights) + self.bias\n    \nmodel = nn.Sequential(\n            LinearNormal(1, 20),\n            nn.ReLU(),\n            LinearNormal(20, 20),\n            nn.ReLU(),\n            LinearNormal(20, 20),\n            nn.ReLU(),\n            LinearNormal(20, 20),\n            nn.ReLU(),\n            LinearNormal(20, 1),\n            )\n\n# Test the model build and LinearKaiming\ntest_normal(LinearNormal)\ntest_build(model, LinearNormal, dropout_type=None, type='normal')\n\nlosses, valid_losses = gradient_descent(model, mse_loss, x, y, xvalid, yvalid, lr=0.1)\nplotRegression(x, y, xvalid, yvalid, loss_history=losses, valid_loss_history=valid_losses, model=model)\n\nPassed!\nPassed!\n\n\n100%|██████████| 5000/5000 [00:01&lt;00:00, 3264.70it/s]\n\n\n\n\n\n\n\nQ8\nIn the previous question you might have found that gradient descent didn’t work. This could suggest that our learning rate is set wrong. Think about a strategy that you might use to find an appropriate learning rate for fitting this model and try it out below. Then explain the strategy that you used. Is there any way you could improve this strategy to make finding a learning rate quicker?\n\n# Modify this code to choose a good learning rate\nmodel = nn.Sequential(\n            LinearNormal(1, 20),\n            nn.ReLU(),\n            LinearNormal(20, 20),\n            nn.ReLU(),\n            LinearNormal(20, 20),\n            nn.ReLU(),\n            LinearNormal(20, 20),\n            nn.ReLU(),\n            LinearNormal(20, 1),\n            )\n\nlr = 0.00001\nlosses, valid_losses = gradient_descent(model, mse_loss, x, y, xvalid, yvalid, lr=lr)\nplotRegression(x, y, xvalid, yvalid, loss_history=losses, valid_loss_history=valid_losses, model=model)\n\n100%|██████████| 5000/5000 [00:01&lt;00:00, 3348.33it/s]\n\n\n\n\n\nEXPLAIN YOUR APPROACH HERE\n\n\nQ9\nWe saw in class that a common, useful approach for initializing neural networks is to use a Kaiming normal initialization. In this approach we draw each initial weight from a normal distribution where the standard deviation is scaled by the square root of the number of input dimensions to the layer. If \\(\\mathbf{W} \\in \\mathbb{R}^{d\\times e}\\) then: \\[w_{ij} \\sim \\mathcal{N}\\bigg(0, \\frac{1}{\\sqrt{d}}\\bigg) \\quad \\mathbf{W}: d \\times e\\] \\[b_j = 0 \\quad \\mathbf{b}: e\\] We’ll initialize the biases to \\(0\\). Below, implement a linear module using the Kaiming normal initialization, then repeat Q5 using the LinearKaiming class and the learning rate you chose in Q8. Then if needed, adjust the learning rate until your model almost perfectly fits the training data.\n\nclass LinearKaiming(nn.Module):\n    def __init__(self, in_dimensions, out_dimensions):\n        super().__init__()\n        self.weights = nn.Parameter(torch.randn(in_dimensions, out_dimensions) / np.sqrt(in_dimensions))\n        self.bias =nn.Parameter(torch.zeros(out_dimensions))\n\n    def forward(self, x):\n        return torch.matmul(x, self.weights) + self.bias\n    \nmodel = nn.Sequential(\n            LinearKaiming(1, 20),\n            nn.ReLU(),\n            LinearKaiming(20, 20),\n            nn.ReLU(),\n            LinearKaiming(20, 20),\n            nn.ReLU(),\n            LinearKaiming(20, 20),\n            nn.ReLU(),\n            LinearKaiming(20, 1),\n            )\n\n# Test the model build and LinearKaiming\ntest_kaiming(LinearKaiming)\ntest_build(model, LinearKaiming, dropout_type=None, type='normal')\n\nlr = 0.01\nlosses, valid_losses = gradient_descent(model, mse_loss, x, y, xvalid, yvalid, lr=lr)\nplotRegression(x, y, xvalid, yvalid, loss_history=losses, valid_loss_history=valid_losses, model=model)\n\nPassed!\nPassed!\n\n\n100%|██████████| 5000/5000 [00:01&lt;00:00, 3270.61it/s]\n\n\n\n\n\nIf all went well so far, we should find that our model fits our data well, but perhaps a little bit too well. Let’s try out some of the strategies we’ve seen to reduce overfitting, starting with early stopping.\n\n\nQ10\nModify your gradient descent algorithm to implment a basic form of early stopping: stop gradient descent as soon as the validation loss increases from the previous iteration. Test this approach with the same model from Q9.\n\ndef gradient_descent_early_stopping(model, loss_func, x, y, xvalid, yvalid, lr=0.1, steps=5000):\n    optimizer = optim.SGD(model.parameters(), lr=lr)\n\n    losses = []\n    valid_losses = []\n    for _ in tqdm.trange(steps):\n        output = model(x)\n        loss = loss_func(output, y)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        valid_loss = loss_func(model(xvalid), yvalid)\n        losses.append(loss.detach().numpy())\n        valid_losses.append(valid_loss.detach().numpy())\n        if len(valid_losses) &gt; 1 and valid_losses[-2] &lt; valid_losses[-1]:\n            break\n\n    return losses, valid_losses\n\nmodel = nn.Sequential(\n            LinearKaiming(1, 20),\n            nn.ReLU(),\n            LinearKaiming(20, 20),\n            nn.ReLU(),\n            LinearKaiming(20, 20),\n            nn.ReLU(),\n            LinearKaiming(20, 20),\n            nn.ReLU(),\n            LinearKaiming(20, 1),\n            )\nlr = 0.01\nlosses, valid_losses = gradient_descent_early_stopping(model, mse_loss, x, y, xvalid, yvalid, lr=lr)\nplotRegression(x, y, xvalid, yvalid, loss_history=losses, valid_loss_history=valid_losses, model=model)\n\n  1%|          | 32/5000 [00:00&lt;00:02, 2312.30it/s]\n\n\n\n\n\n\n\nQ11\nDid this approach work as intended? Why or why not? Think about how you might improve this approach and explain any ideas you have in 1-2 sentences.\nNo, it stopped too early.\n\n\nQ12\nModify your early stopping gradient descent so that it always runs for at least 50 iterations. Then after 50 iterations stop if at any point the validation loss is larger than the average validation loss for the previous 50 iterations.\n\ndef gradient_descent_patient_early_stopping(model, loss_func, x, y, xvalid, yvalid, lr=0.1, steps=5000):\n    optimizer = optim.SGD(model.parameters(), lr=lr)\n\n    losses = []\n    valid_losses = []\n    for _ in tqdm.trange(steps):\n        output = model(x)\n        loss = loss_func(output, y)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        valid_loss = loss_func(model(xvalid), yvalid)\n        losses.append(loss.detach().numpy())\n        valid_losses.append(valid_loss.detach().numpy())\n        if len(valid_losses) &gt; 50 and np.mean(valid_losses[-50:-1]) &lt; valid_losses[-1]:\n            break\n\n    return losses, valid_losses\n\nmodel = nn.Sequential(\n            LinearKaiming(1, 20),\n            nn.ReLU(),\n            LinearKaiming(20, 20),\n            nn.ReLU(),\n            LinearKaiming(20, 20),\n            nn.ReLU(),\n            LinearKaiming(20, 20),\n            nn.ReLU(),\n            LinearKaiming(20, 1),\n            )\nlr = 0.003\nlosses, valid_losses = gradient_descent_patient_early_stopping(model, mse_loss, x, y, xvalid, yvalid, lr=lr)\nplotRegression(x, y, xvalid, yvalid, loss_history=losses, valid_loss_history=valid_losses, model=model)\n\n  2%|▏         | 104/5000 [00:00&lt;00:01, 2870.24it/s]\n\n\n\n\n\nNow let’s try out L2 regualrization! We will consider a scaled version of L2 regularization, where for a \\(d \\times e\\) weight matrix \\(\\mathbf{W}\\) we will define the L2 loss as: \\[\\textbf{Loss}_{L2}(\\mathbf{W})= \\frac{\\lambda}{d e}\\sum_{i=1}^d\\sum_{j=1}^e w_{ij}^2 \\quad \\mathbf{W}: d \\times e\\] Here \\(\\lambda\\) is a value that we can choose to control how much weight we put on our L2 loss (we’ll call it l2_weight below).\n\n\nQ13\nModify your original gradient descent algorithm from Q4 (no early stopping) to add the L2 loss for each parameter in the model to the loss.\nHint: Recall that we can access every parameter in the model using the model.parameters() method. In this question you do not need to worry about distinguishing between weights and biases, you can apply L2 regularization to biases as well if it simplifies your approach. Your validation loss should not include the regularization terms.\n\nfrom torch import optim\n\ndef gradient_descent_l2(model, loss_func, x, y, xvalid, yvalid, lr=0.1, l2_weight=1., steps=5000):\n    optimizer = optim.SGD(model.parameters(), lr=lr)\n\n    losses = []\n    valid_losses = []\n    for _ in tqdm.trange(steps):\n        output = model(x)\n        loss = loss_func(output, y)\n        losses.append(loss.detach().numpy()) \n\n        l2_loss = 0\n        for p in model.parameters():\n            l2_loss = l2_loss + torch.mean(p ** 2)\n        loss = loss + l2_weight * l2_loss        \n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        valid_loss = loss_func(model(xvalid), yvalid)\n        valid_losses.append(valid_loss.detach().numpy())\n\n    return losses, valid_losses\n\n# Test our function\ntest_gradient_descent(gradient_descent_l2, mse_loss, x, y, xvalid, yvalid, l2=True)\n\nTypeError: test_gradient_descent() got an unexpected keyword argument 'l2'\n\n\n\n\nQ14\nApply gradient_descent_l2 as in previous problems. Find an appropriate setting of l2_weight that minimizes the validation loss.\nHint: How you go about choosing l2_weight is up to you! Your validation loss should be lower than the validation loss without regularization.\n\nmodel = nn.Sequential(\n            LinearKaiming(1, 20),\n            nn.ReLU(),\n            LinearKaiming(20, 20),\n            nn.ReLU(),\n            LinearKaiming(20, 20),\n            nn.ReLU(),\n            LinearKaiming(20, 20),\n            nn.ReLU(),\n            LinearKaiming(20, 1),\n            )\nlr = 0.003\nl2_weight = 1.\nlosses, valid_losses = gradient_descent_l2(model, mse_loss, x, y, xvalid, yvalid, lr=lr, l2_weight=l2_weight)\nplotRegression(x, y, xvalid, yvalid, loss_history=losses, valid_loss_history=valid_losses, model=model)\n\n100%|██████████| 5000/5000 [00:02&lt;00:00, 1883.76it/s]\n\n\n\n\n\nFinally let’s try out dropout regularization. We will implement dropout as its own module, so we can think of it as a function that transforms a vector or matix into a vector or matrix of the same shape with elements randomly set to \\(0\\). In this case we can write the dropout function as: \\[\\text{Dropout}(\\mathbf{X}, r) = \\mathbf{D} \\odot \\mathbf{X}, \\quad \\mathbf{D} =\n\\begin{bmatrix}\nd_{11} & d_{12} & \\dots & d_{1n} \\\\\nd_{21} & d_{22} & \\dots & d_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nd_{m1} & d_{m2} & \\dots &  d_{mn}\n\\end{bmatrix},\\ d_{ij} \\sim \\text{Bernoulli}(1-r)\\]\nHere \\(\\odot\\) denotes element-wise multiplication (so \\(\\mathbf{D}\\) and \\(\\mathbf{X}\\) are the same shape), \\(r\\) is the dropout rate so \\(p(d_{ij}=0)=r\\).\nAt evaluation time, we do not want to randomly drop elements. Instead we will scale \\(\\mathbf{X}\\) by \\((1-r)\\): \\[\\text{Dropout}_{\\text{eval}}(\\mathbf{X}, r) = (1-r)\\mathbf{X}\\]\n\n\nQ15\nComplete the implementation of the Dropout module below.\nHint: The built-in training property of an nn.Module instance specifies if our model is in training mode or evaluation mode. By default models are in training mode (training == True), but we can set a model to evaluation mode by calling model.eval(). Then we can use model.train() to set it back to training mode.\nYou may find the function torch.rand_like() helpful for this problem. You might also find it helpful to know that you can convert and boolean tensor X into a float tensor by calling X.float() (True becomes 1., False becomes 0.)\n\nclass Dropout(nn.Module):\n    def __init__(self, rate=0.01):\n        # Rate specifies the dropout rate (r)\n        super().__init__()\n        self.rate = rate\n\n    def forward(self, x):\n        # YOUR CODE HERE\n        if self.training:\n            return x * (torch.rand_like(x) &gt; self.rate).float()\n        else:\n            return x * (1 - self.rate)\n        \ntest_dropout(Dropout)\n\nPassed!\n\n\n\n\nQ16\nModify your gradient_descent function to put the model into train mode before calculating the training loss and into eval mode before calculating the validation loss. Then create a model based on your network from Q9, but this time add a Dropout layer before each LinearKaiming layer. You can use the default dropout rate of 0.01 or try something different! Verify that dropout gives different results to previous approaches.\n\ndef gradient_descent(model, loss_func, x, y, xvalid, yvalid, lr=0.1, steps=5000):\n    optimizer = optim.SGD(model.parameters(), lr=lr)\n\n    losses = []\n    valid_losses = []\n    for _ in tqdm.trange(steps):\n        model.train()\n        output = model(x)\n        loss = loss_func(output, y)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        model.eval()\n        valid_loss = loss_func(model(xvalid), yvalid)\n        losses.append(loss.detach().numpy())\n        valid_losses.append(valid_loss.detach().numpy())\n\n    return losses, valid_losses\n\nmodel = nn.Sequential(\n            Dropout(),\n            LinearKaiming(1, 20),\n            nn.ReLU(),\n            Dropout(),\n            LinearKaiming(20, 20),\n            nn.ReLU(),\n            Dropout(),\n            LinearKaiming(20, 20),\n            nn.ReLU(),\n            Dropout(),\n            LinearKaiming(20, 20),\n            nn.ReLU(),\n            Dropout(),\n            LinearKaiming(20, 1),\n            )\n\n# Test our model build\ntest_build(model, LinearKaiming, dropout_type=Dropout, type='normal')\n\nlr = 0.003\nlosses, valid_losses = gradient_descent(model, mse_loss, x, y, xvalid, yvalid, lr=lr)\nplotRegression(x, y, xvalid, yvalid, loss_history=losses, valid_loss_history=valid_losses, model=model)\n\nPassed!\n\n\n100%|██████████| 5000/5000 [00:02&lt;00:00, 1964.57it/s]\n\n\n\n\n\n\ntest_build(model, LinearKaiming, dropout_type=Dropout, type='normal')\n\nPassed!"
  },
  {
    "objectID": "lecture12-convolutions/figures.html",
    "href": "lecture12-convolutions/figures.html",
    "title": "CS 152: Neural Networks",
    "section": "",
    "text": "import warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nimport contextlib\nwith open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f):\n    from manim import *\nimport autograd.numpy as np\n\n\nclass LectureScene(Scene):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.camera.background_color = \"#ffffff\"\n        self.template = TexTemplate()\n        self.template.add_to_preamble(r\"\\usepackage{amsmath}\")\n\nclass ThreeDLectureScene(ThreeDScene):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.camera.background_color = \"#ffffff\"\n        self.template = TexTemplate()\n        self.template.add_to_preamble(r\"\\usepackage{amsmath}\")\n    \n\nclass VectorScene(LectureScene):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.ax = Axes(\n            x_range=[-7.5, 7.5, 1],\n            y_range=[-5, 5, 1],\n            x_length=12,\n            y_length=8,\n            axis_config={\"color\": GREY},\n        )\n        \n        #axes_labels.set_color(GREY)\n        self.add(self.ax)\n\nclass PositiveVectorScene(LectureScene):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.ax = Axes(\n            x_range=[-2.5, 12.5, 1],\n            y_range=[-1, 9, 1],\n            x_length=12,\n            y_length=8,\n            axis_config={\"color\": GREY},\n        )\n                #axes_labels.set_color(GREY)\n        self.add(self.ax)\n\nclass ComparisonVectorScene(LectureScene):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.ax1 = Axes(\n            x_range=[-5, 5, 1],\n            y_range=[-5, 5, 1],\n            x_length=6,\n            y_length=6,\n            axis_config={\"color\": GREY},\n        )\n        self.ax2 = Axes(\n            x_range=[-5, 5, 1],\n            y_range=[-5, 5, 1],\n            x_length=6,\n            y_length=6,\n            axis_config={\"color\": GREY},\n        )\n        axgroup = Group(self.ax1, self.ax2)\n        axgroup.arrange_in_grid(buf=2)\n        \n        #axes_labels.set_color(GREY)\n        self.add(axgroup)\nfrom manim_ml.neural_network import NeuralNetwork, Convolutional2DLayer, MaxPooling2DLayer\n\n\ntype(BLACK)\n\nstr\n\n\n\n%%manim -pql -v WARNING Conv\ndef make_sequence(numbers, sl=0.5, colors=BLACK, fills=WHITE, text_colors=None, opacity=1.,  annot=True, stride=1, padding=0):\n    n = len(numbers)\n    squares = []\n\n    if type(colors) is str:\n        colors = [colors] * n\n    if text_colors is None:\n        text_colors = colors\n    elif type(text_colors) is str:\n        text_colors = [text_colors] * n\n    if type(fills) is str:\n        fills = [fills] * n\n\n    for i in range(n):\n        pos = i * stride * sl * RIGHT\n        square = Square(color=colors[i], fill_color=fills[i], side_length=sl,  fill_opacity=opacity).move_to(pos)\n        if annot:\n            text = Text(str(numbers[i]), color=text_colors[i]).scale(sl).move_to(pos)\n            squares.append(VGroup(square, text))\n        else:\n            squares.append(VGroup(square))\n    \n    if padding &gt; 0:\n        pads = []\n        for i in range(padding):\n            pos = (n + i) * sl * RIGHT\n            pads.append(Text('0', color=GRAY).scale(sl).move_to(pos))\n            pos = (-(i + 1)) * sl * RIGHT\n            pads.append(Text('0', color=GRAY).scale(sl).move_to(pos))\n        return VGroup(*squares), VGroup(*pads)\n\n    return VGroup(*squares)\n\ndef make_stack(numbers, sl=0.5, colors=BLACK, fills=WHITE, text_colors=None, opacity=1., annot=True, stride=1):\n    n = len(numbers)\n\n    if type(colors) is str:\n        colors = [colors] * n\n    if text_colors is None:\n        text_colors = colors\n    elif type(text_colors) is str:\n        text_colors = [text_colors] * n\n    if type(fills) is str:\n        fills = [fills] * n\n\n    seqs = []\n    for i in range(n):\n        seqs.append(make_sequence(numbers[i], sl, colors[i], fills[i], text_colors[i], opacity, annot, stride).shift(i * sl * DOWN))\n    return VGroup(*seqs)\n\nclass Conv(LectureScene):\n    def construct(self):\n        self.add(Text('Input', color=GRAY, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT + -2 * UP))\n        self.add(Text('Kernel', color=RED, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT + -1 * UP))\n        self.add(Text('Output', color=GRAY, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT))\n        self.add(Text('Convolution (kernel size: 3)', color=BLACK,  font='Noto Sans SemiBold').scale(0.8).move_to(3.5 * UP))\n\n\n        kernelarr = [-1, 3, -1]\n        kw = len(kernelarr) // 2\n        kernel = make_sequence(kernelarr, annot=True, opacity=0.2, fills=RED, colors=RED).move_to(-1 * UP)\n        seqarr = np.random.randint(-9, 9, (15,))\n\n        sequence = make_sequence(seqarr, annot=True).move_to(-2 * UP)\n        outputarr = [sum([seqarr[s - kw + i] * k for i, k in enumerate(kernelarr)]) for s in range(kw, len(seqarr) - kw)]\n\n        output = make_sequence(outputarr, annot=True, text_colors=WHITE).move_to(0 * UP)\n\n        output_window = make_sequence([1], annot=False, opacity=.2, fills=RED, colors=RED).move_to(0 * UP)\n        kernel_window = make_sequence([1, 1, 1], annot=False, fills=RED, opacity=0.2, colors=RED).move_to(-2 * UP)\n\n        lines = []\n        for s in [-0.5, 0., 0.5]:\n            l = Line(-1.75 * UP, -1.25 * UP, color=RED).shift(s * RIGHT)\n            lab = MathTex(r'\\times', color=RED).scale(0.5).next_to(l, 0.5 * LEFT) \n            l2 = CubicBezier(-.75 * UP + s * RIGHT, -.5 * UP + s * RIGHT, -.5 * UP, -.25 * UP, color=RED)\n            lines += [l, lab, l2]\n        lines.append(MathTex(r'+', color=RED).scale(0.5).next_to(l2, 1.5 * LEFT).shift(0.1 * UP))\n        lines.append(kernel_window)\n        kernel_window = VGroup(*lines)\n\n        highlight = VGroup(output_window, kernel_window, kernel)\n        highlight.shift(0.5 * 6 * LEFT)\n        eq = Text(f'Output[{0 :2}]=({seqarr[0] :2})({kernelarr[0] :2})+({seqarr[1] :2})({kernelarr[1] :2})+({seqarr[2] :2})({kernelarr[2] :2})={outputarr[0] :3}', color=BLACK, font_size=36, font='Roboto Mono').shift(2 * UP)\n        self.add(sequence, output, highlight, eq)\n\n                \n        output[0][1].set_color(BLACK)\n        for i in range(12):\n            neweq = Text(f'Output[{i+1 :2}]=({seqarr[0 + i] :2})({kernelarr[0] :2})+({seqarr[1 + i] :2})({kernelarr[1] :2})+({seqarr[2 + i] :2})({kernelarr[2] :2})={outputarr[1 + i] :3}', color=BLACK, font_size=36, font='Roboto Mono').shift(2 * UP)\n            self.play(highlight.animate.shift(RIGHT * 0.5), output[i + 1][1].animate.set_color(BLACK), FadeIn(neweq), FadeOut(eq))\n            eq = neweq\n        self.wait(2)\n\n                                                                                                              \n\n\n\n      Your browser does not support the video element.\n    \n\n\n\n%%manim -pql -v WARNING Conv\n\nclass Conv(LectureScene):\n    def construct(self):\n        self.add(Text('Input', color=GRAY, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT + -2 * UP))\n        self.add(Text('Kernel', color=RED, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT + -1 * UP))\n        output_label = Text('Output', color=GRAY, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT)\n        self.add(output_label)\n        #self.add(Text('Convolution (kernel size: 3)', color=BLACK,  font='Noto Sans SemiBold').scale(0.8).move_to(3.5 * UP))\n\n\n        kernelarr = [-1, 3, -1]\n        kw = len(kernelarr) // 2\n        kernel = make_sequence(kernelarr, annot=True, opacity=0.2, fills=RED, colors=RED).move_to(-1 * UP)\n        seqarr = np.random.randint(-9, 9, (15,))\n\n        sequence = make_sequence(seqarr, annot=True).move_to(-2 * UP)\n        outputarr = [sum([seqarr[s - kw + i] * k for i, k in enumerate(kernelarr)]) for s in range(kw, len(seqarr) - kw)]\n\n        output = make_sequence(outputarr, annot=True, text_colors=WHITE).move_to(0 * UP)\n\n        output_window = make_sequence([1], annot=False, opacity=.2, fills=RED, colors=RED).move_to(0 * UP)\n        kernel_window = make_sequence([1, 1, 1], annot=False, fills=RED, opacity=0.2, colors=RED).move_to(-2 * UP)\n\n        lines = []\n        for s in [-0.5, 0., 0.5]:\n            l = Line(-1.75 * UP, -1.25 * UP, color=RED).shift(s * RIGHT)\n            lab = MathTex(r'\\times', color=RED).scale(0.5).next_to(l, 0.5 * LEFT) \n            l2 = CubicBezier(-.75 * UP + s * RIGHT, -.5 * UP + s * RIGHT, -.5 * UP, -.25 * UP, color=RED)\n            lines += [l, lab, l2]\n        lines.append(MathTex(r'+', color=RED).scale(0.5).next_to(l2, 1.5 * LEFT).shift(0.1 * UP))\n        lines.append(kernel_window)\n        kernel_window = VGroup(*lines)\n\n        highlight = VGroup(output_window, kernel_window, kernel)\n        highlight.shift(0.5 * 6 * LEFT)\n        eq = Text(f'Output[{0 :2}]=({seqarr[0] :2})({kernelarr[0] :2})+({seqarr[1] :2})({kernelarr[1] :2})+({seqarr[2] :2})({kernelarr[2] :2})={outputarr[0] :3}', color=BLACK, font_size=36, font='Roboto Mono').shift(2 * UP)\n        self.add(sequence, output, highlight, eq)\n\n                \n        output[0][1].set_color(BLACK)\n        for i in range(12):\n            neweq = Text(f'Output[{i+1 :2}]=({seqarr[0 + i] :2})({kernelarr[0] :2})+({seqarr[1 + i] :2})({kernelarr[1] :2})+({seqarr[2 + i] :2})({kernelarr[2] :2})={outputarr[1 + i] :3}', color=BLACK, font_size=36, font='Roboto Mono').shift(2 * UP)\n            self.play(highlight.animate.shift(RIGHT * 0.5), output[i + 1][1].animate.set_color(BLACK), FadeIn(neweq), FadeOut(eq), )\n            eq = neweq\n\n        kernel_window = VGroup(*lines)\n        self.wait(1)\n        self.play(FadeOut(kernel_window), FadeOut(output_window), run_time=0.25)\n        self.play(kernel.animate.shift(RIGHT * 2), output.animate.shift(1.5 * UP), output_label.animate.shift(1.5 * UP), run_time=0.5, )\n\n\n\n        kernelarr = [0, 0, 0]\n        kw = len(kernelarr) // 2\n        kernel = make_sequence(kernelarr, annot=True, opacity=0.2, fills=GREEN, colors=GREEN).move_to(-1 * UP)\n\n        kernel_window = make_sequence([1, 1, 1], annot=False, fills=GREEN, opacity=0.2, colors=GREEN).move_to(-2 * UP)\n\n        lines = []\n        for s in [-0.5, 0., 0.5]:\n            l = Line(-1.75 * UP, -1.25 * UP, color=GREEN).shift(s * RIGHT)\n            lab = MathTex(r'+', color=GREEN).scale(0.5).next_to(l, 0.5 * LEFT) \n            lines += [l, lab]\n        lines.append(kernel_window)\n        kernel_window = VGroup(*lines)\n        highlight = VGroup(kernel_window, kernel)\n        highlight.shift(0.5 * 6 * LEFT)\n        self.add(sequence, output, highlight)\n\n        for i in range(12):\n            kernelarr[0] += seqarr[0 + i]\n            kernelarr[1] += seqarr[1 + i]\n            kernelarr[2] += seqarr[2 + i]\n            \n            new0 = Text(str(kernelarr[0]), color=GREEN).move_to(kernel[0][1].get_center()).scale(0.5)\n            new1 = Text(str(kernelarr[1]), color=GREEN).move_to(kernel[1][1].get_center()).scale(0.5)\n            new2 = Text(str(kernelarr[2]), color=GREEN).move_to(kernel[2][1].get_center()).scale(0.5)\n            self.play(kernel[0][1].animate.become(new0), kernel[1][1].animate.become(new1), kernel[2][1].animate.become(new2), run_time=0.1)\n            self.play(highlight.animate.shift(RIGHT * 0.5), output[i + 1][1].animate.set_color(BLACK), run_time=0.5)\n        self.wait(2)\n\n\n                                                                                                              \n\n\n\n      Your browser does not support the video element.\n    \n\n\n\n%%manim -pql -v WARNING Conv\n\nfrom copy import deepcopy\ncolors = [GOLD, RED, PURPLE]\nclass Conv(LectureScene):\n    def construct(self):\n        self.add(Text('Input', color=GRAY, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT + -2 * UP))\n        self.add(Text('Kernel', color=RED, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT + -1 * UP))\n        output_label = Text('Output', color=GRAY, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT)\n        self.add(output_label)\n        self.add(Text('Convolution (kernel size: 3)', color=BLACK,  font='Noto Sans SemiBold').scale(0.8).move_to(3.5 * UP))\n\n\n        kernelarr = [-1, 3, -1]\n        kw = len(kernelarr) // 2\n        kernel = make_sequence(kernelarr, annot=True, opacity=1., colors=GRAY_E, fills=colors).move_to(-1 * UP)\n        kernel.z_index=10\n        seqarr = np.random.randint(-9, 9, (15,))\n\n        sequence = make_sequence(seqarr, annot=True).move_to(-2 * UP)\n        outputarr = [sum([seqarr[s - kw + i] * k for i, k in enumerate(kernelarr)]) for s in range(kw, len(seqarr) - kw)]\n\n        output = make_sequence(outputarr, annot=True, text_colors=WHITE).move_to(0 * UP)\n\n        output_window = make_sequence([1], annot=False, opacity=.2, fills=GRAY_E, colors=GRAY_E).move_to(0 * UP)\n        kernel_window = make_sequence([1, 1, 1], annot=False, fills=colors, opacity=0.2, colors=GRAY_E, ).move_to(-2 * UP)\n\n        lines = []\n        static_lines = []\n        for s, c in zip([-0.5, 0., 0.5], colors):\n            l = Line(-1.75 * UP, -1.25 * UP, color=c).shift(s * RIGHT)\n            lab = MathTex(r'\\times', color=c).scale(0.5).next_to(l, 0.5 * LEFT) \n            l2 = CubicBezier(-.75 * UP + s * RIGHT, -.5 * UP + s * RIGHT, -.5 * UP, -.25 * UP, color=c)\n            lines += [l, lab, l2]\n\n            l = Line(-1.75 * UP + s * RIGHT, -.25 * UP, color=c)#.shift()\n            l2 = CubicBezier(-.75 * UP + s * RIGHT, -.5 * UP + s * RIGHT, -.5 * UP, -.25 * UP, color=c)\n            static_lines += [l2]\n\n        lines.append(MathTex(r'+', color=RED).scale(0.5).next_to(l2, 1.5 * LEFT).shift(0.1 * UP))\n        lines.append(kernel_window)\n        static_lines = VGroup(*static_lines)\n        \n\n        kernel_window = VGroup(*lines)\n\n        highlight = VGroup(output_window, kernel_window, kernel)\n        highlight.shift(0.5 * 6 * LEFT)\n        eq = Text(f'Output[{0 :2}]=({seqarr[0] :2})({kernelarr[0] :2})+({seqarr[1] :2})({kernelarr[1] :2})+({seqarr[2] :2})({kernelarr[2] :2})={outputarr[0] :3}', color=BLACK, font_size=36, font='Roboto Mono').shift(2 * UP)\n        self.add(sequence, output, highlight)\n\n                \n        output[0][1].set_color(BLACK)\n        for i in range(12):\n            neweq = Text(f'Output[{i+1 :2}]=({seqarr[0 + i] :2})({kernelarr[0] :2})+({seqarr[1 + i] :2})({kernelarr[1] :2})+({seqarr[2 + i] :2})({kernelarr[2] :2})={outputarr[1 + i] :3}', color=BLACK, font_size=36, font='Roboto Mono').shift(2 * UP)\n            sl = deepcopy(static_lines).move_to(highlight)\n            sl.z_index = -100\n            self.play(highlight.animate.shift(RIGHT * 0.5), output[i + 1][1].animate.set_color(BLACK), FadeIn(sl) )\n            eq = neweq\n\n        return\n\n        kernel_window = VGroup(*lines)\n        self.wait(1)\n        self.play(FadeOut(kernel_window), FadeOut(output_window), run_time=0.25)\n        self.play(kernel.animate.shift(0.5 * RIGHT), output.animate.shift(.5 * UP), output_label.animate.shift(.5 * UP), sequence.animate.shift(.5 * DOWN), run_time=0.5, )\n\n\n        seqarr = np.random.randint(-9, 9, (15,))\n\n        sequence = make_sequence(seqarr, annot=True, text_colors=WHITE).move_to(-2 * UP)\n        outputarr = np.random.randint(-9, 9, (13,))\n\n        output = make_sequence(outputarr, annot=True).move_to(0 * UP)\n\n        self.play(FadeIn(output), FadeIn(sequence), kernel.animate.set_color(GREEN))\n        self.play(Rotate(kernel, 180 * DEGREES))\n\n        output_window = make_sequence([1], annot=False, opacity=.2, fills=RED, colors=RED).move_to(0 * UP)\n        kernel_window = make_sequence([1, 1, 1], annot=False, fills=RED, opacity=0.2, colors=RED).move_to(-2 * UP)\n\n        output_window = make_sequence([1], annot=False, opacity=.2, fills=GREEN, colors=GREEN).move_to(-2 * UP)\n        kernel_window = make_sequence([1, 1, 1], annot=False, fills=GREEN, opacity=0.2, colors=GREEN).move_to(0 * UP)\n\n        lines = []\n        for s in [-0.5, 0., 0.5]:\n            l = Line(-.75 * UP, -.25 * UP, color=GREEN).shift(s * RIGHT)\n            lab = MathTex(r'\\times', color=GREEN).scale(0.5).next_to(l, 0.5 * LEFT) \n            l2 = CubicBezier(-1.75 * UP, -1.5 * UP, -1.5 * UP + s * RIGHT, -1.25 * UP + s * RIGHT, color=GREEN)\n            lines += [l, lab, l2]\n        lines.append(MathTex(r'+', color=GREEN).scale(0.5).next_to(l2, 1.5 * LEFT).shift(0.1 * UP))\n        lines.append(kernel_window)\n        lines.append(output_window)\n        kernel_window = VGroup(*lines)\n        kernel_window.move_to(kernel)\n        self.play(FadeIn(kernel_window), FadeIn(output_window))\n\n\n        highlight = VGroup(output_window, kernel_window, kernel)\n        sequence[-1][1].set_color(BLACK)\n        for i in range(13):\n            self.play(highlight.animate.shift(LEFT * 0.5), sequence[(14 - i -1)][1].animate.set_color(BLACK) )\n            eq = neweq\n        self.play(highlight.animate.shift(LEFT * 0.5), sequence[(0)][1].animate.set_color(BLACK) )\n        self.wait(2)\n\n                                                                                                             \n\n\n\n      Your browser does not support the video element.\n    \n\n\n\n%%manim -pql -v WARNING Conv\n\nclass Conv(LectureScene):\n    def construct(self):\n        self.add(Text('Input', color=GRAY, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT + -2 * UP))\n        self.add(Text('Kernel', color=RED, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT + -1 * UP))\n        output_label = Text('Output', color=GRAY, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT)\n        self.add(output_label)\n        self.add(Text('Convolution (kernel size: 3)', color=BLACK,  font='Noto Sans SemiBold').scale(0.8).move_to(3.5 * UP))\n\n\n        kernelarr = [-1, 3, -1]\n        kw = len(kernelarr) // 2\n        kernel = make_sequence(kernelarr, annot=True, opacity=0.2, fills=RED, colors=RED).move_to(-1 * UP)\n        seqarr = np.random.randint(-9, 9, (15,))\n\n        sequence = make_sequence(seqarr, annot=True).move_to(-2 * UP)\n        outputarr = [sum([seqarr[s - kw + i] * k for i, k in enumerate(kernelarr)]) for s in range(kw, len(seqarr) - kw)]\n\n        output = make_sequence(outputarr, annot=True, text_colors=WHITE).move_to(0 * UP)\n\n        output_window = make_sequence([1], annot=False, opacity=.2, fills=RED, colors=RED).move_to(0 * UP)\n        kernel_window = make_sequence([1, 1, 1], annot=False, fills=RED, opacity=0.2, colors=RED).move_to(-2 * UP)\n\n        lines = []\n        for s in [-0.5, 0., 0.5]:\n            l = Line(-1.75 * UP, -1.25 * UP, color=RED).shift(s * RIGHT)\n            lab = MathTex(r'\\times', color=RED).scale(0.5).next_to(l, 0.5 * LEFT) \n            l2 = CubicBezier(-.75 * UP + s * RIGHT, -.5 * UP + s * RIGHT, -.5 * UP, -.25 * UP, color=RED)\n            lines += [l, lab, l2]\n        lines.append(MathTex(r'+', color=RED).scale(0.5).next_to(l2, 1.5 * LEFT).shift(0.1 * UP))\n        lines.append(kernel_window)\n        kernel_window = VGroup(*lines)\n\n        highlight = VGroup(output_window, kernel_window, kernel)\n        highlight.shift(0.5 * 6 * LEFT)\n        eq = Text(f'Output[{0 :2}]=({seqarr[0] :2})({kernelarr[0] :2})+({seqarr[1] :2})({kernelarr[1] :2})+({seqarr[2] :2})({kernelarr[2] :2})={outputarr[0] :3}', color=BLACK, font_size=36, font='Roboto Mono').shift(2 * UP)\n        self.add(sequence, output, highlight)\n\n                \n        output[0][1].set_color(BLACK)\n        for i in range(12):\n            neweq = Text(f'Output[{i+1 :2}]=({seqarr[0 + i] :2})({kernelarr[0] :2})+({seqarr[1 + i] :2})({kernelarr[1] :2})+({seqarr[2 + i] :2})({kernelarr[2] :2})={outputarr[1 + i] :3}', color=BLACK, font_size=36, font='Roboto Mono').shift(2 * UP)\n            self.play(highlight.animate.shift(RIGHT * 0.5), output[i + 1][1].animate.set_color(BLACK) )\n            eq = neweq\n\n        kernel_window = VGroup(*lines)\n        self.wait(1)\n        self.play(FadeOut(kernel_window), FadeOut(output_window), run_time=0.25)\n        self.play(kernel.animate.shift(0.5 * RIGHT), output.animate.shift(.5 * UP), output_label.animate.shift(.5 * UP), sequence.animate.shift(.5 * DOWN), run_time=0.5, )\n\n\n        seqarr = np.random.randint(-9, 9, (15,))\n\n        sequence = make_sequence(seqarr, annot=True, text_colors=WHITE).move_to(-2 * UP)\n        outputarr = np.random.randint(-9, 9, (13,))\n\n        output = make_sequence(outputarr, annot=True).move_to(0 * UP)\n\n        self.play(FadeIn(output), FadeIn(sequence), kernel.animate.set_color(GREEN))\n        self.play(Rotate(kernel, 180 * DEGREES))\n\n        output_window = make_sequence([1], annot=False, opacity=.2, fills=RED, colors=RED).move_to(0 * UP)\n        kernel_window = make_sequence([1, 1, 1], annot=False, fills=RED, opacity=0.2, colors=RED).move_to(-2 * UP)\n\n        output_window = make_sequence([1], annot=False, opacity=.2, fills=GREEN, colors=GREEN).move_to(-2 * UP)\n        kernel_window = make_sequence([1, 1, 1], annot=False, fills=GREEN, opacity=0.2, colors=GREEN).move_to(0 * UP)\n\n        lines = []\n        for s in [-0.5, 0., 0.5]:\n            l = Line(-.75 * UP, -.25 * UP, color=GREEN).shift(s * RIGHT)\n            lab = MathTex(r'\\times', color=GREEN).scale(0.5).next_to(l, 0.5 * LEFT) \n            l2 = CubicBezier(-1.75 * UP, -1.5 * UP, -1.5 * UP + s * RIGHT, -1.25 * UP + s * RIGHT, color=GREEN)\n            lines += [l, lab, l2]\n        lines.append(MathTex(r'+', color=GREEN).scale(0.5).next_to(l2, 1.5 * LEFT).shift(0.1 * UP))\n        lines.append(kernel_window)\n        lines.append(output_window)\n        kernel_window = VGroup(*lines)\n        kernel_window.move_to(kernel)\n        self.play(FadeIn(kernel_window), FadeIn(output_window))\n\n\n        highlight = VGroup(output_window, kernel_window, kernel)\n        sequence[-1][1].set_color(BLACK)\n        for i in range(13):\n            self.play(highlight.animate.shift(LEFT * 0.5), sequence[(14 - i -1)][1].animate.set_color(BLACK) )\n            eq = neweq\n        self.play(highlight.animate.shift(LEFT * 0.5), sequence[(0)][1].animate.set_color(BLACK) )\n        self.wait(2)\n\n                                                                                                              \n\n\n\n      Your browser does not support the video element.\n    \n\n\n\n%%manim -pql -v WARNING Conv\n\nclass Conv(LectureScene):\n    def construct(self):\n        self.add(Text('Input', color=GRAY, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT + -2 * UP))\n        self.add(Text('Kernel', color=RED, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT + -1 * UP))\n        self.add(Text('Output', color=GRAY, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT))\n        self.add(Text('Convolution (kernel size: 5)', color=BLACK,  font='Noto Sans SemiBold').scale(0.8).move_to(3.5 * UP))\n\n\n        kernelarr = [1, -1, 3, -1, 1]\n        kw = len(kernelarr) // 2\n        kernel = make_sequence(kernelarr, annot=True, opacity=0.2, fills=RED, colors=RED).move_to(-1 * UP)\n        seqarr = np.random.randint(-9, 9, (15,))\n\n        sequence = make_sequence(seqarr, annot=True).move_to(-2 * UP)\n        outputarr = [sum([seqarr[s - kw + i] * k for i, k in enumerate(kernelarr)]) for s in range(kw, len(seqarr) - kw)]\n\n        output = make_sequence(outputarr, annot=True, text_colors=WHITE).move_to(0 * UP)\n\n        output_window = make_sequence([1], annot=False, opacity=.2, fills=RED, colors=RED).move_to(0 * UP)\n        kernel_window = make_sequence([1, 1, 1, 1, 1], annot=False, fills=RED, opacity=0.2, colors=RED).move_to(-2 * UP)\n\n        lines = []\n        for s in [-1., -0.5, 0., 0.5, 1.]:\n            l = Line(-1.75 * UP, -1.25 * UP, color=RED).shift(s * RIGHT)\n            lab = MathTex(r'\\times', color=RED).scale(0.5).next_to(l, 0.5 * LEFT) \n            l2 = CubicBezier(-.75 * UP + s * RIGHT, -.5 * UP + s * RIGHT, -.5 * UP, -.25 * UP, color=RED)\n            lines += [l, lab, l2]\n        lines.append(MathTex(r'+', color=RED).scale(0.5).next_to(l2, 1.5 * LEFT).shift(0.1 * UP))\n        lines.append(kernel_window)\n        kernel_window = VGroup(*lines)\n\n        highlight = VGroup(output_window, kernel_window, kernel)\n        highlight.shift(0.5 * 5 * LEFT)\n        self.add(sequence, output, highlight)\n\n                \n        output[0][1].set_color(BLACK)\n        for i in range(10):\n            self.play(highlight.animate.shift(RIGHT * 0.5), output[i + 1][1].animate.set_color(BLACK))\n        self.wait(2)\n\n                                                                                                              \n\n\n\n      Your browser does not support the video element.\n    \n\n\n\n%%manim -pql -v WARNING Shuffle\n\nclass Shuffle(LectureScene):\n    def construct(self):\n        self.add(Text('Input', color=GRAY, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT + -2 * UP))\n        self.add(Text('Kernel', color=RED, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT + -1 * UP))\n        self.add(Text('Output', color=GRAY, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT))\n        self.add(Text('Convolution (size: 3) + Shuffle', color=BLACK,  font='Noto Sans SemiBold').scale(0.8).move_to(3.5 * UP))\n\n        kernelarr = [-1, 3, -1]\n        kw = len(kernelarr) // 2\n        kernel = make_sequence(kernelarr, annot=True, opacity=0.2, fills=RED, colors=RED).move_to(-1 * UP)\n        seqarr = np.random.randint(-9, 9, (16,))\n\n        sequence = make_sequence(seqarr, annot=True).move_to(-2 * UP)\n        outputarr = [sum([seqarr[s - kw + i] * k for i, k in enumerate(kernelarr)]) for s in range(kw, len(seqarr) - kw)]\n\n        output = make_sequence(outputarr, annot=True, text_colors=WHITE).move_to(0 * UP)\n\n        output_window = make_sequence([1], annot=False, opacity=0.2, fills=RED, colors=RED).move_to(0 * UP)\n        kernel_window = make_sequence([1, 1, 1], annot=False,  opacity=0.2, fills=RED, colors=RED).move_to(-2 * UP)\n\n        lines = []\n        for s in [-0.5, 0., 0.5]:\n            l = Line(-1.75 * UP, -1.25 * UP, color=RED).shift(s * RIGHT)\n            lab = MathTex(r'\\times', color=RED).scale(0.5).next_to(l, 0.5 * LEFT) \n            l2 = CubicBezier(-.75 * UP + s * RIGHT, -.5 * UP + s * RIGHT, -.5 * UP, -.25 * UP, color=RED)\n            lines += [l, lab, l2]\n        lines.append(MathTex(r'+', color=RED).scale(0.5).next_to(l2, 1.5 * LEFT).shift(0.1 * UP))\n        lines.append(kernel_window)\n        kernel_window = VGroup(*lines)\n        \n        highlight = VGroup(output_window, kernel_window, kernel)\n        highlight.shift(0.5 * 6.5 * LEFT)\n        self.add(sequence, output, highlight)\n\n                \n        output[0][1].set_color(BLACK)\n        for i in range(13):\n            self.play(highlight.animate.shift(RIGHT * 0.5), output[i + 1][1].animate.set_color(BLACK), run_time=0.5)\n        n=16\n        self.play(output_window.animate.set_opacity(0.), kernel_window.animate.set_opacity(0.), sequence.animate.set_opacity(0.5), run_time=0.5)\n\n        stack_anims = []\n        for i, o in enumerate(output):\n            if i % 2 == 0:\n                stack_anims.append(o.animate.shift(RIGHT * 0.25 + 0.25 * UP))\n            else:\n                stack_anims.append(o.animate.shift(LEFT * 0.25 + 0.25 * DOWN))\n        self.play(*stack_anims)\n\n        \n\n        shift_anims = []\n        for i, o in enumerate(output):\n            j = i // 2\n            s = j - (n / 4)\n            shift_anims.append(o.animate.shift(LEFT * s * 0.5 + 0.5 * LEFT))\n        self.play(*shift_anims)\n\n        self.wait(3)\n\n                                                                                                           \n\n\n\n      Your browser does not support the video element.\n    \n\n\n\n%%manim -pql -v WARNING MovingAround\n\nclass MovingAround(LectureScene):\n    def construct(self):\n        self.add(Text('Input', color=GRAY, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT + -2 * UP))\n        self.add(Text('Kernel', color=RED, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT + -1 * UP))\n        self.add(Text('Pooling', color=BLUE, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT + 1 * UP))\n        self.add(Text('Output', color=GRAY, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT + 2 * UP))\n        self.add(Text('Conv. output', color=GRAY, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT))\n        self.add(Text('Convolution (size: 3) + Max Pooling (size: 3, stride: 2)', color=BLACK,  font='Noto Sans SemiBold').scale(0.8).move_to(3.5 * UP))\n\n        kernelarr = [-1, 3, -1]\n        kw = len(kernelarr) // 2\n        kernel = make_sequence(kernelarr, annot=True, opacity=0.2, fills=RED, colors=RED).move_to(-1 * UP)\n        seqarr = np.random.randint(-9, 9, (16,))\n\n        sequence = make_sequence(seqarr, annot=True).move_to(-2 * UP)\n        outputarr = [sum([seqarr[s - kw + i] * k for i, k in enumerate(kernelarr)]) for s in range(kw, len(seqarr) - kw)]\n\n        outputarr2 = maxpool1d(outputarr, 3, stride=2)\n\n        output = make_sequence(outputarr, annot=True, text_colors=WHITE).move_to(0 * UP)\n        output2 = make_sequence(outputarr2.astype(int), annot=True, stride=2, text_colors=WHITE).move_to(2 * UP + 0.25 * LEFT)\n\n        output_window = make_sequence([1], annot=False, opacity=0.2, fills=RED, colors=RED).move_to(0 * UP)\n        kernel_window = make_sequence([1, 1, 1], annot=False, opacity=0.2, fills=RED,  colors=RED).move_to(-2 * UP)\n\n        lines = []\n        for s in [-0.5, 0., 0.5]:\n            l = Line(-1.75 * UP, -1.25 * UP, color=RED).shift(s * RIGHT)\n            lab = MathTex(r'\\times', color=RED).scale(0.5).next_to(l, 0.5 * LEFT) \n            l2 = CubicBezier(-.75 * UP + s * RIGHT, -.5 * UP + s * RIGHT, -.5 * UP, -.25 * UP, color=RED)\n            lines += [l, lab, l2]\n        lines.append(MathTex(r'+', color=RED).scale(0.5).next_to(l2, 1.5 * LEFT).shift(0.1 * UP))\n        lines.append(kernel_window)\n        kernel_window = VGroup(*lines)\n\n        maxpool_window = make_sequence([1, 1, 1], annot=False, opacity=0.2, colors=BLUE, fills=BLUE).move_to(0 * UP)\n\n        lines = []\n        for s in [-0.5, 0., 0.5]:\n            l2 = CubicBezier(0.25 * UP + s * RIGHT, 0.25 * UP + UP + s * RIGHT, UP, 1.75 * UP, color=BLUE)\n            lines += [l2]\n        lines.append(MathTex(r'\\max', color=BLUE).scale(0.5).next_to(l2, 1.5 * LEFT).shift(0.1 * UP))\n        lines.append(maxpool_window)\n        maxpool_window = VGroup(*lines)\n\n\n        maxpool_output_window = make_sequence([1], annot=False, opacity=0.2, colors=BLUE, fills=BLUE).move_to(2 * UP)\n        maxpool_highlight = VGroup(maxpool_output_window, maxpool_window).shift(0.5 * 5.5 * LEFT)\n        \n        highlight = VGroup(output_window, kernel_window, kernel)\n        highlight.shift(0.5 * 6.5 * LEFT)\n        self.add(sequence, output, highlight, output2, )\n\n                \n        output[0][1].set_color(BLACK)\n        for i in range(13):\n            self.play(highlight.animate.shift(RIGHT * 0.5), output[i + 1][1].animate.set_color(BLACK), run_time=0.1)\n        n=16\n\n        self.play(FadeIn(maxpool_highlight), highlight.animate.set_opacity(0.), output2[0][1].animate.set_color(BLACK), sequence.animate.set_opacity(0.5))\n\n        for i in range(5):\n            self.play(maxpool_highlight.animate.shift(RIGHT * 1.), output2[i + 1][1].animate.set_color(BLACK))\n\n        self.play(maxpool_highlight.animate.set_opacity(0.), output.animate.set_opacity(0.5))\n        shift_anims = []\n        for i, o in enumerate(output2):\n            j = i\n            s = j - (4)\n            shift_anims.append(o.animate.shift(LEFT * s * 0.5 + 0.5 * LEFT))\n        self.play(*shift_anims)\n        self.wait(3)\n        \n\n                                                                                                               \n\n\n\n      Your browser does not support the video element.\n    \n\n\n\n%%manim -pql -v WARNING MovingAround\n\nclass MovingAround(LectureScene):\n    def construct(self):\n        self.add(Text('Input', color=GRAY, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT + -2 * UP))\n        self.add(Text('Kernel', color=RED, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT + -1 * UP))\n        self.add(Text('Output', color=GRAY, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT))\n        self.add(Text('Convolution (size: 3, stride: 2)', color=BLACK,  font='Noto Sans SemiBold').scale(0.8).move_to(3.5 * UP))\n        kernelarr = [-1, 3, -1]\n        kw = len(kernelarr) // 2\n        kernel = make_sequence(kernelarr, annot=True, opacity=0.2, fills=RED, colors=RED).move_to(-1 * UP)\n        seqarr = np.random.randint(-9, 9, (15,))\n\n        sequence = make_sequence(seqarr, annot=True).move_to(-2 * UP)\n        outputarr = [sum([seqarr[s - kw + i] * k for i, k in enumerate(kernelarr)]) for s in range(kw, len(seqarr) - kw)]\n\n        output = make_sequence(outputarr[::2], annot=True, stride=2, text_colors=WHITE).move_to(0 * UP)\n\n        output_window = make_sequence([1], annot=False, opacity=0.2, fills=RED, colors=RED).move_to(0 * UP)\n        kernel_window = make_sequence([1, 1, 1], annot=False, opacity=0.2, fills=RED, colors=RED).move_to(-2 * UP)\n\n        lines = []\n        for s in [-0.5, 0., 0.5]:\n            l = Line(-1.75 * UP, -1.25 * UP, color=RED).shift(s * RIGHT)\n            lab = MathTex(r'\\times', color=RED).scale(0.5).next_to(l, 0.5 * LEFT) \n            l2 = CubicBezier(-.75 * UP + s * RIGHT, -.5 * UP + s * RIGHT, -.5 * UP, -.25 * UP, color=RED)\n            lines += [l, lab, l2]\n        lines.append(MathTex(r'+', color=RED).scale(0.5).next_to(l2, 1.5 * LEFT).shift(0.1 * UP))\n        lines.append(kernel_window)\n        kernel_window = VGroup(*lines)\n        \n        highlight = VGroup(output_window, kernel_window, kernel)\n        highlight.shift(0.5 * 6 * LEFT)\n        self.add(sequence, output, highlight)\n                \n        output[0][1].set_color(BLACK)\n        for i in range(6):\n            self.play(highlight.animate.shift(RIGHT * 1), output[i + 1][1].animate.set_color(BLACK))\n\n        self.play(output_window.animate.set_opacity(0.), sequence.animate.set_opacity(0.5), kernel_window.animate.set_opacity(0.))\n        shift_anims = []\n        for i, o in enumerate(output):\n            j = i\n            s = j - (4)\n            shift_anims.append(o.animate.shift(LEFT * s * 0.5 + 0.5 * LEFT))\n        self.play(*shift_anims)\n        self.wait(2)\n\n                                                                                                    \n\n\n\n      Your browser does not support the video element.\n    \n\n\n\n%%manim -pql -v WARNING MovingAround\n\nfrom scipy.ndimage import correlate\n\n\nclass MovingAround(LectureScene):\n    def construct(self):\n        self.add(Text('Input', color=GRAY, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT + -2 * UP))\n        self.add(Text('Kernel', color=RED, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT + -1 * UP))\n        self.add(Text('Output', color=GRAY, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT))\n        self.add(Text('Convolution (size: 3, padding: 1)', color=BLACK,  font='Noto Sans SemiBold').scale(0.8).move_to(3.5 * UP))\n        kernelarr = [-1, 3, -1]\n        kw = len(kernelarr) // 2\n        kernel = make_sequence(kernelarr, annot=True, opacity=0.2, fills=RED, colors=RED).move_to(-1 * UP)\n        seqarr = np.random.randint(-9, 9, (15,))\n\n        sequence, pads = make_sequence(seqarr, annot=True, padding=1)\n        sequence.move_to(-2 * UP), pads.move_to(-2 * UP)\n        outputarr = convolution1d(seqarr, kernelarr, padding=1).astype(int)\n\n        output = make_sequence(outputarr, annot=True, text_colors=WHITE).move_to(0 * UP)\n\n        output_window = make_sequence([1], annot=False, opacity=0.2, fills=RED, colors=RED).move_to(0 * UP)\n        kernel_window = make_sequence([1, 1, 1], annot=False, opacity=0.2, fills=RED, colors=RED).move_to(-2 * UP)\n\n        lines = []\n        for s in [-0.5, 0., 0.5]:\n            l = Line(-1.75 * UP, -1.25 * UP, color=RED).shift(s * RIGHT)\n            lab = MathTex(r'\\times', color=RED).scale(0.5).next_to(l, 0.5 * LEFT) \n            l2 = CubicBezier(-.75 * UP + s * RIGHT, -.5 * UP + s * RIGHT, -.5 * UP, -.25 * UP, color=RED)\n            lines += [l, lab, l2]\n        lines.append(MathTex(r'+', color=RED).scale(0.5).next_to(l2, 1.5 * LEFT).shift(0.1 * UP))\n        lines.append(kernel_window)\n        kernel_window = VGroup(*lines)\n        \n        highlight = VGroup(output_window, kernel_window, kernel)\n        highlight.shift(0.5 * 7 * LEFT)\n        \n        self.add(sequence, output, pads)\n        self.play(FadeIn(highlight), output[0][1].animate.set_color(BLACK))\n                \n        \n        for i in range(14):\n            self.play(highlight.animate.shift(RIGHT * 0.5), output[i + 1][1].animate.set_color(BLACK), run_time=0.5)\n        \n        self.wait(2)\n        \n\n                                                                                                    \n\n\n\n      Your browser does not support the video element.\n    \n\n\n\n%%manim -pql -v WARNING MovingAround\n\nclass MovingAround(LectureScene):\n\n    def construct(self):\n        self.add(Text('Input', color=GRAY, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT + -2 * UP))\n        self.add(Text('Kernel', color=RED, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT))\n        self.add(Text('Output', color=GRAY, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT + 2 * UP))\n        self.add(Text('Convolution (size: 3, channels: 3, output channels: 1)', color=BLACK,  font='Noto Sans SemiBold').scale(0.8).move_to(3.5 * UP))\n        n = 16\n        \n        seqarr = np.random.randint(-9, 9, (3, n,))\n        sequence = make_stack(seqarr, annot=True).move_to(-2 * UP)\n\n\n        kernelarr = np.random.randint(-3, 3, (3, 3,))\n        kw = len(kernelarr[0]) // 2\n        kernel = make_stack(kernelarr, annot=True, opacity=0.2, colors=[RED, GREEN, BLUE], fills=[RED, GREEN, BLUE]).move_to(0 * UP)\n        outputarr = convolution1d(seqarr.T, kernelarr).astype(int) #np.zeros(14).astype(int)#[sum([seqarr[s - kw + i] * k for i, k in enumerate(kernelarr)]) for s in range(kw, len(seqarr) - kw)]\n        output = make_sequence(outputarr, annot=True, text_colors=WHITE).move_to(1.5 * UP)\n\n        output_window = make_sequence([1], annot=False, opacity=0.2, colors=GRAY, fills=GRAY).move_to(1.5 * UP)\n        kernel_window = make_stack(kernelarr, annot=False, opacity=0.2, colors=[RED, GREEN, BLUE], fills=[RED, GREEN, BLUE]).move_to(-2 * UP)\n\n        lines = []\n        for s in [-0.5, 0., 0.5]:\n            l = Line(-1.25 * UP, -.75 * UP, color=GRAY).shift(s * RIGHT)\n            lab = MathTex(r'\\text{dot}', color=GRAY).scale(0.35).next_to(l, 0.5 * LEFT) \n            l2 = CubicBezier(-.25 * UP + UP + s * RIGHT,  UP + s * RIGHT,  UP, .25 * UP + UP, color=GRAY)\n            lines += [l, lab, l2]\n        lines.append(MathTex(r'+', color=GRAY).scale(0.5).next_to(l2, 1.5 * LEFT).shift(0.1 * UP))\n        lines.append(kernel_window)\n        kernel_window = VGroup(*lines)\n        \n        highlight = VGroup(output_window, kernel_window, kernel)\n        highlight.shift(0.5 * (n-3) / 2 * LEFT)\n        self.add(sequence, output, highlight)\n                \n        output[0][1].set_color(BLACK)\n        for i in range(n - 3):\n            self.play(highlight.animate.shift(RIGHT * 0.5), output[i + 1][1].animate.set_color(BLACK), run_time=0.5)\n\n        self.wait(2)\n\n        #stack_anims = []\n        #for i, o in enumerate(output):\n        #    if i % 2 == 0:\n        #        stack_anims.append(o.animate.shift(RIGHT * 0.25 + 0.25 * UP))\n        #    else:\n        #        stack_anims.append(o.animate.shift(LEFT * 0.25 + 0.25 * DOWN))\n        #self.play(*stack_anims)\n\n        #shift_anims = []\n        #for i, o in enumerate(output):\n        #    j = i // 2\n        #    s = j - (n / 4)\n        #    shift_anims.append(o.animate.shift(LEFT * s * 0.5))\n        #self.play(*shift_anims)\n\n                                                                                                    \n\n\n\n      Your browser does not support the video element.\n    \n\n\n\n%%manim -pql -v WARNING MovingAround\n\nclass MovingAround(LectureScene):\n    def construct(self):\n        self.add(Text('Input', color=GRAY, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT + -2 * UP))\n        self.add(Text('Kernel', color=RED, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT + 0 * UP))\n        self.add(Text('Output', color=GRAY, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT + 2 * UP))\n        self.add(Text('Convolution (size: 3, channels: 3, output channels: 3)', color=BLACK,  font='Noto Sans SemiBold').scale(0.8).move_to(3.5 * UP))\n        n = 16\n        \n        seqarr = np.random.randint(-9, 9, (3, n,))\n        sequence = make_stack(seqarr, annot=True).move_to(-2 * UP)\n\n\n        kernelarr = np.random.randint(-3, 3, (3, 3,))\n        kw = len(kernelarr[0]) // 2\n        kernel = make_stack(kernelarr, annot=True, opacity=0.2, colors=[RED, GREEN, BLUE], fills=[RED, GREEN, BLUE]).move_to(0 * UP)\n        outputarr = convolution1d(seqarr.T, kernelarr).astype(int) #np.zeros(14).astype(int)#[sum([seqarr[s - kw + i] * k for i, k in enumerate(kernelarr)]) for s in range(kw, len(seqarr) - kw)]\n        output = make_sequence(outputarr, annot=True, text_colors=WHITE).move_to(1.5 * UP)\n        output_window = make_sequence([1], annot=False, opacity=0.2, colors=GRAY, fills=GRAY).move_to(1.5 * UP)\n        kernel_window = make_stack(kernelarr, annot=False, opacity=0.2, colors=[RED, GREEN, BLUE], fills=[RED, GREEN, BLUE]).move_to(-2 * UP)\n\n        lines = []\n        for s in [-0.5, 0., 0.5]:\n            l = Line(-1.25 * UP, -.75 * UP, color=GRAY).shift(s * RIGHT)\n            lab = MathTex(r'\\text{dot}', color=GRAY).scale(0.35).next_to(l, 0.5 * LEFT) \n            l2 = CubicBezier(-.25 * UP + UP + s * RIGHT,  UP + s * RIGHT,  UP, .25 * UP + UP, color=GRAY)\n            lines += [l, lab, l2]\n        lines.append(MathTex(r'+', color=GRAY).scale(0.5).next_to(l2, 1.5 * LEFT).shift(0.1 * UP))\n        lines.append(kernel_window)\n        kernel_window = VGroup(*lines)\n\n        highlight = VGroup(output_window, kernel_window, kernel)\n        highlight.shift(0.5 * (n-3) / 2 * LEFT)\n        self.add(sequence, output, highlight)\n  \n        output[0][1].set_color(BLACK)\n        for i in range(n - 3):\n            self.play(highlight.animate.shift(RIGHT * 0.5), output[i + 1][1].animate.set_color(BLACK), run_time=0.3)\n\n        self.play(FadeOut(kernel_window),  FadeOut(output_window) , run_time=0.25)\n        self.play(kernel.animate.move_to(5.5 * RIGHT), run_time=0.5)\n        self.play(kernel.animate.shift(2 * UP), run_time=0.5)\n\n\n        kernelarr = np.random.randint(-3, 3, (3, 3,))\n        kw = len(kernelarr[0]) // 2\n        kernel = make_stack(kernelarr, annot=True, opacity=0.2, colors=[RED, GREEN, BLUE], fills=[RED, GREEN, BLUE]).move_to(0 * UP)\n        outputarr = convolution1d(seqarr.T, kernelarr).astype(int) #np.zeros(14).astype(int)#[sum([seqarr[s - kw + i] * k for i, k in enumerate(kernelarr)]) for s in range(kw, len(seqarr) - kw)]\n        output = make_sequence(outputarr, annot=True, text_colors=WHITE).move_to(2 * UP)\n        output_window = make_sequence([1], annot=False, opacity=0.2, colors=GRAY, fills=GRAY).move_to(2 * UP)\n        kernel_window = make_stack(kernelarr, annot=False, opacity=0.2, colors=[RED, GREEN, BLUE], fills=[RED, GREEN, BLUE]).move_to(-2 * UP)\n\n        lines = []\n        for s in [-0.5, 0., 0.5]:\n            l = Line(-1.25 * UP, -.75 * UP, color=GRAY).shift(s * RIGHT)\n            lab = MathTex(r'\\text{dot}', color=GRAY).scale(0.35).next_to(l, 0.5 * LEFT) \n            l2 = CubicBezier(-.25 * UP + UP + s * RIGHT,  UP + s * RIGHT,  UP, .25 * UP + UP, color=GRAY)\n            lines += [l, lab, l2]\n        lines.append(MathTex(r'+', color=GRAY).scale(0.5).next_to(l2, 1.5 * LEFT).shift(0.1 * UP))\n        lines.append(kernel_window)\n        kernel_window = VGroup(*lines)\n        \n        highlight = VGroup(output_window, kernel_window, kernel)\n        highlight.shift(0.5 * (n-3) / 2 * LEFT)\n        self.play(FadeIn(output), FadeIn(highlight), run_time=0.5)\n\n        output[0][1].set_color(BLACK)\n        for i in range(n - 3):\n            self.play(highlight.animate.shift(RIGHT * 0.5), output[i + 1][1].animate.set_color(BLACK), run_time=0.3)\n\n        self.play(FadeOut(kernel_window),  FadeOut(output_window), run_time=0.25)\n        self.play(kernel.animate.move_to(5.5 * RIGHT), run_time=0.5)\n        self.play(kernel.animate.shift(2 * DOWN),  run_time=0.5)\n\n        kernelarr = np.random.randint(-3, 3, (3, 3,))\n        kw = len(kernelarr[0]) // 2\n        kernel = make_stack(kernelarr, annot=True, opacity=0.2, colors=[RED, GREEN, BLUE], fills=[RED, GREEN, BLUE]).move_to(0 * UP)\n        outputarr = convolution1d(seqarr.T, kernelarr).astype(int) #np.zeros(14).astype(int)#[sum([seqarr[s - kw + i] * k for i, k in enumerate(kernelarr)]) for s in range(kw, len(seqarr) - kw)]\n        output = make_sequence(outputarr, annot=True, text_colors=WHITE).move_to(2.5 * UP)\n        output_window = make_sequence([1], annot=False, opacity=0.2, colors=GRAY, fills=GRAY).move_to(2.5 * UP)\n        kernel_window = make_stack(kernelarr, annot=False, opacity=0.2, colors=[RED, GREEN, BLUE], fills=[RED, GREEN, BLUE]).move_to(-2 * UP)\n\n        lines = []\n        for s in [-0.5, 0., 0.5]:\n            l = Line(-1.25 * UP, -.75 * UP, color=GRAY).shift(s * RIGHT)\n            lab = MathTex(r'\\text{dot}', color=GRAY).scale(0.35).next_to(l, 0.5 * LEFT) \n            l2 = CubicBezier(-.25 * UP + UP + s * RIGHT,  UP + s * RIGHT,  UP, .25 * UP + UP, color=GRAY)\n            lines += [l, lab, l2]\n        lines.append(MathTex(r'+', color=GRAY).scale(0.5).next_to(l2, 1.5 * LEFT).shift(0.1 * UP))\n        lines.append(kernel_window)\n        kernel_window = VGroup(*lines)\n\n        highlight = VGroup(output_window, kernel_window, kernel)\n        highlight.shift(0.5 * (n-3) / 2 * LEFT)\n        self.play(FadeIn(output), FadeIn(highlight), run_time=0.5)\n\n        output[0][1].set_color(BLACK)\n        for i in range(n - 3):\n            self.play(highlight.animate.shift(RIGHT * 0.5), output[i + 1][1].animate.set_color(BLACK), run_time=0.3)\n\n        self.play(FadeOut(kernel_window),  FadeOut(output_window), run_time=0.25)\n        self.play(kernel.animate.move_to(5.5 * RIGHT), run_time=0.5)\n        self.play( sequence.animate.set_opacity(0.5), run_time=0.5)\n\n        self.wait(2)\n\n                                                                                                      \n\n\n\n      Your browser does not support the video element.\n    \n\n\n\ndef convolution1d(x, kernel):\n    kernel_width = len(kernel)\n    input_size = len(x)\n\n    output_size = input_size - (kernel_width - 1)\n    output = np.zeros((output_size,))\n    for i in range(output_size):\n        for j in range(kernel_width):\n            output[i] += x[i + j] * kernel[j]\n    return output\n\n\ndef convolution1d(x, kernel):\n    kernel_width = len(kernel)\n    input_size = len(x)\n\n    output_size = input_size - (kernel_width - 1)\n    output = np.zeros((output_size,))\n    for i in range(output_size):\n        for j in range(kernel_width):\n            output[i] += np.dot(x[i + j], kernel[j])\n    return output\n\n\ndef convolution1d(x, kernel, padding=0):\n\n    kernel_width = len(kernel)\n    input_size = len(x)\n\n    output_size = input_size - (kernel_width - 1) + 2 * padding\n    output = np.zeros((output_size,))\n    for i in range(output_size):\n        for j in range(kernel_width):\n            ind = i + j - padding\n            if ind &gt;= 0 and ind &lt; input_size:\n                output[i] += np.dot(x[ind], kernel[j])\n    return output\n\n\ndef convolution1d(x, kernel, padding=0, stride=1):\n\n    kernel_width = len(kernel)\n    input_size = len(x)\n\n    output_size = (input_size - (kernel_width - 1) + 2 * padding - 1) // stride + 1\n    output = np.zeros((output_size,))\n    for i in range(len(output)):\n        for j in range(kernel_width):\n            ind = i * stride + j - padding\n            if ind &gt;= 0 and ind &lt; input_size:\n                output[i] += np.dot(x[ind], kernel[j])\n    return output\n\n\ndef convolution1d(x, kernel, padding=0, stride=1, dialation=1):\n\n    kernel_width = len(kernel)\n    input_size = len(x)\n\n    output_size = (input_size - dialation * (kernel_width - 1) + 2 * padding - 1) // stride + 1\n    output = np.zeros((output_size,))\n    for i in range(len(output)):\n        for j in range(kernel_width):\n            ind = i * stride + j * dialation - padding\n            if ind &gt;= 0 and ind &lt; input_size:\n                output[i] += np.dot(x[ind], kernel[j])\n    return output\n\n\ndef convolution1d(x, kernel, bias=0, padding=0, stride=1, dialation=1):\n\n    kernel_width = len(kernel)\n    input_size = len(x)\n\n    output_size = (input_size - dialation * (kernel_width - 1) + 2 * padding - 1) // stride + 1\n    output = np.zeros((output_size,))\n    for i in range(len(output)):\n        output[i] = bias\n        for j in range(kernel_width):\n            ind = i * stride + j * dialation - padding\n            if ind &gt;= 0 and ind &lt; input_size:\n                output[i] += np.dot(x[ind], kernel[j])\n    return output\n\n\ndef maxpool1d(x, window, padding=0, stride=1):\n    input_size = len(x)\n\n    output_size = (input_size - (window - 1) + 2 * padding - 1) // stride + 1\n    output = np.zeros((output_size,))\n    for i in range(len(output)):\n        output[i] = -np.infty\n        for j in range(window):\n            ind = i * stride + j - padding\n            if ind &gt;= 0 and ind &lt; input_size:\n                output[i] = np.maximum(x[ind], output[i])\n    return output\n\n\n\nfrom manim import *\n\nManim Community v0.17.3\n\n\n\n\n\n%%manim -pql -v WARNING MovingAround\n\nfrom manim_ml.neural_network import NeuralNetwork, FeedForwardLayer\nclass MovingAround(Scene):\n    def construct(self):\n        nn = NeuralNetwork([\n            FeedForwardLayer(num_nodes=3),\n            FeedForwardLayer(num_nodes=5),\n            FeedForwardLayer(num_nodes=3)\n        ])\n        self.add(nn)\n        # Make the animation\n        forward_pass_animation = nn.make_forward_pass_animation()\n        # Play the animation\n        self.play(forward_pass_animation)\n\nConstructing layers\nCurrent layer: FeedForwardLayer\nCurrent layer: FeedForwardLayer\nCurrent layer: FeedForwardLayer\nNeuralNetwork([\n    FeedForwardLayer(z_index=3, title_text= , ),\n    FeedForwardToFeedForward(input_layer=FeedForwardLayer,output_layer=FeedForwardLayer,)(z_index=2, title_text= , ),\n    FeedForwardLayer(z_index=3, title_text= , ),\n    FeedForwardToFeedForward(input_layer=FeedForwardLayer,output_layer=FeedForwardLayer,)(z_index=2, title_text= , ),\n    FeedForwardLayer(z_index=3, title_text= , ),\n])\n\n\n                                                                               \n\n\n\n      Your browser does not support the video element.\n    \n\n\n\nimport manim_ml\n\n\nmanim_ml.ThreeDScene\n\n&lt;module 'manim_ml' from '/Users/gabe/Documents/Courses/CS152-Neural-Networks-Fall-2023.github.io/.venv/lib/python3.9/site-packages/manim_ml/__init__.py'&gt;"
  },
  {
    "objectID": "assignments/final-project/outline.html#final-deliverables-due-126-1159pm",
    "href": "assignments/final-project/outline.html#final-deliverables-due-126-1159pm",
    "title": "Final project outline",
    "section": "Final deliverables (Due 12/6, 11:59pm)",
    "text": "Final deliverables (Due 12/6, 11:59pm)\nPlease see the final project template here, which includes instructions and guidelines for what to submit. Submissions will be through gradescope."
  },
  {
    "objectID": "lecture13-RNNs/figures.html",
    "href": "lecture13-RNNs/figures.html",
    "title": "CS 152: Neural Networks",
    "section": "",
    "text": "import warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nimport contextlib\nwith open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f):\n    from manim import *\nimport autograd.numpy as np\n\n\nclass LectureScene(Scene):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.camera.background_color = \"#ffffff\"\n        self.template = TexTemplate()\n        self.template.add_to_preamble(r\"\\usepackage{amsmath}\")\n\nclass ThreeDLectureScene(ThreeDScene):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.camera.background_color = \"#ffffff\"\n        self.template = TexTemplate()\n        self.template.add_to_preamble(r\"\\usepackage{amsmath}\")\n    \n\nclass VectorScene(LectureScene):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.ax = Axes(\n            x_range=[-7.5, 7.5, 1],\n            y_range=[-5, 5, 1],\n            x_length=12,\n            y_length=8,\n            axis_config={\"color\": GREY},\n        )\n        \n        #axes_labels.set_color(GREY)\n        self.add(self.ax)\n\nclass PositiveVectorScene(LectureScene):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.ax = Axes(\n            x_range=[-2.5, 12.5, 1],\n            y_range=[-1, 9, 1],\n            x_length=12,\n            y_length=8,\n            axis_config={\"color\": GREY},\n        )\n                #axes_labels.set_color(GREY)\n        self.add(self.ax)\n\nclass ComparisonVectorScene(LectureScene):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.ax1 = Axes(\n            x_range=[-5, 5, 1],\n            y_range=[-5, 5, 1],\n            x_length=6,\n            y_length=6,\n            axis_config={\"color\": GREY},\n        )\n        self.ax2 = Axes(\n            x_range=[-5, 5, 1],\n            y_range=[-5, 5, 1],\n            x_length=6,\n            y_length=6,\n            axis_config={\"color\": GREY},\n        )\n        axgroup = Group(self.ax1, self.ax2)\n        axgroup.arrange_in_grid(buf=2)\n        \n        #axes_labels.set_color(GREY)\n        self.add(axgroup)\nfrom manim_ml.neural_network import NeuralNetwork, Convolutional2DLayer, MaxPooling2DLayer\n\n\ntype(BLACK)\n\n\n%%manim -pql -v WARNING Conv\ndef make_sequence(numbers, sl=0.5, colors=BLACK, fills=WHITE, text_colors=None, opacity=1.,  annot=True, stride=1, padding=0):\n    n = len(numbers)\n    squares = []\n\n    if type(colors) is str:\n        colors = [colors] * n\n    if text_colors is None:\n        text_colors = colors\n    elif type(text_colors) is str:\n        text_colors = [text_colors] * n\n    if type(fills) is str:\n        fills = [fills] * n\n\n    for i in range(n):\n        pos = i * stride * sl * RIGHT\n        square = Square(color=colors[i], fill_color=fills[i], side_length=sl,  fill_opacity=opacity).move_to(pos)\n        if annot:\n            text = Text(str(numbers[i]), color=text_colors[i]).scale(sl).move_to(pos)\n            squares.append(VGroup(square, text))\n        else:\n            squares.append(VGroup(square))\n    \n    if padding &gt; 0:\n        pads = []\n        for i in range(padding):\n            pos = (n + i) * sl * RIGHT\n            pads.append(Text('0', color=GRAY).scale(sl).move_to(pos))\n            pos = (-(i + 1)) * sl * RIGHT\n            pads.append(Text('0', color=GRAY).scale(sl).move_to(pos))\n        return VGroup(*squares), VGroup(*pads)\n\n    return VGroup(*squares)\n\ndef make_stack(numbers, sl=0.5, colors=BLACK, fills=WHITE, text_colors=None, opacity=1., annot=True, stride=1):\n    n = len(numbers)\n\n    if type(colors) is str:\n        colors = [colors] * n\n    if text_colors is None:\n        text_colors = colors\n    elif type(text_colors) is str:\n        text_colors = [text_colors] * n\n    if type(fills) is str:\n        fills = [fills] * n\n\n    seqs = []\n    for i in range(n):\n        seqs.append(make_sequence(numbers[i], sl, colors[i], fills[i], text_colors[i], opacity, annot, stride).shift(i * sl * DOWN))\n    return VGroup(*seqs)\n\nclass Conv(LectureScene):\n    def construct(self):\n        self.add(Text('Input', color=GRAY, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT + -2 * UP))\n        self.add(Text('Kernel', color=RED, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT + -1 * UP))\n        self.add(Text('Output', color=GRAY, opacity=0.5, font='Noto Sans').scale(0.6).move_to(5 * LEFT))\n        self.add(Text('Convolution (kernel size: 3)', color=BLACK,  font='Noto Sans SemiBold').scale(0.8).move_to(3.5 * UP))\n\n\n        kernelarr = [-1, 3, -1]\n        kw = len(kernelarr) // 2\n        kernel = make_sequence(kernelarr, annot=True, opacity=0.2, fills=RED, colors=RED).move_to(-1 * UP)\n        seqarr = np.random.randint(-9, 9, (15,))\n\n        sequence = make_sequence(seqarr, annot=True).move_to(-2 * UP)\n        outputarr = [sum([seqarr[s - kw + i] * k for i, k in enumerate(kernelarr)]) for s in range(kw, len(seqarr) - kw)]\n\n        output = make_sequence(outputarr, annot=True, text_colors=WHITE).move_to(0 * UP)\n\n        output_window = make_sequence([1], annot=False, opacity=.2, fills=RED, colors=RED).move_to(0 * UP)\n        kernel_window = make_sequence([1, 1, 1], annot=False, fills=RED, opacity=0.2, colors=RED).move_to(-2 * UP)\n\n        lines = []\n        for s in [-0.5, 0., 0.5]:\n            l = Line(-1.75 * UP, -1.25 * UP, color=RED).shift(s * RIGHT)\n            lab = MathTex(r'\\times', color=RED).scale(0.5).next_to(l, 0.5 * LEFT) \n            l2 = CubicBezier(-.75 * UP + s * RIGHT, -.5 * UP + s * RIGHT, -.5 * UP, -.25 * UP, color=RED)\n            lines += [l, lab, l2]\n        lines.append(MathTex(r'+', color=RED).scale(0.5).next_to(l2, 1.5 * LEFT).shift(0.1 * UP))\n        lines.append(kernel_window)\n        kernel_window = VGroup(*lines)\n\n        highlight = VGroup(output_window, kernel_window, kernel)\n        highlight.shift(0.5 * 6 * LEFT)\n        eq = Text(f'Output[{0 :2}]=({seqarr[0] :2})({kernelarr[0] :2})+({seqarr[1] :2})({kernelarr[1] :2})+({seqarr[2] :2})({kernelarr[2] :2})={outputarr[0] :3}', color=BLACK, font_size=36, font='Roboto Mono').shift(2 * UP)\n        self.add(sequence, output, highlight, eq)\n\n                \n        output[0][1].set_color(BLACK)\n        for i in range(12):\n            neweq = Text(f'Output[{i+1 :2}]=({seqarr[0 + i] :2})({kernelarr[0] :2})+({seqarr[1 + i] :2})({kernelarr[1] :2})+({seqarr[2 + i] :2})({kernelarr[2] :2})={outputarr[1 + i] :3}', color=BLACK, font_size=36, font='Roboto Mono').shift(2 * UP)\n            self.play(highlight.animate.shift(RIGHT * 0.5), output[i + 1][1].animate.set_color(BLACK), FadeIn(neweq), FadeOut(eq))\n            eq = neweq\n        self.wait(2)\n\n\nclass Mat\n\n\n%%manim -pql -v WARNING MovingAround\n\nclass MovingAround(LectureScene):\n    def construct(self):\n        sentence = \"[START] I love purple cats [.] Purple cats are great [END]\".split()\n        length = len(sentence)\n        heights = np.linspace(1, -1, length)\n        yrange = 2\n        rowheight = (heights[1] - heights[0]) * yrange\n\n        dim = 3\n\n        embeddings = np.random.randn(length, dim)\n        rows = []\n        for r, (word, y) in enumerate(zip(sentence, heights)):\n            label = Text(word, color='BLACK').scale( 2 * yrange * (1 / length)).shift(-5 * RIGHT + yrange * y * UP)\n\n            row = []\n            for c in range(dim):\n                box = Square(side_length=rowheight, color=BLACK, fill_color=LIGHTER_GRAY, fill_opacity=1.).shift(-3 * RIGHT + yrange * y * UP).shift(rowheight * RIGHT * c)\n                num = Text('%.2f' % embeddings[r, c], color=BLACK).scale( 1.25 * yrange * (1 / length) ).shift(-3 * RIGHT + yrange * y * UP).shift(rowheight * RIGHT * c)\n                row.append(VGroup(box, num))\n            rows.append(VGroup(label, VGroup(*row)))\n        rows = VGroup(*rows)\n\n        wq = np.random.randn(dim, dim)\n        wk = np.random.randn(dim, dim)\n        wv = np.random.randn(dim, dim)\n\n        ms = []\n        for (mat, y, color) in [(wq, 2, GREEN), (wk, 0, BLUE), (wv, -2, ORANGE)]:\n            m = []\n            for r in range(dim):\n                for c in range(dim):\n                    box = Square(side_length=rowheight, color=BLACK, fill_color=LIGHTER_GRAY, fill_opacity=1.).shift(-1 * RIGHT +  y * UP).shift(rowheight * RIGHT * c + rowheight * UP * r)\n                    num = Text('%.2f' % mat[r, c], color=BLACK).scale( 1.25 * yrange * (1 / length) ).shift(-1 * RIGHT + y * UP).shift(rowheight * RIGHT * c + rowheight * UP * r)\n                    m.append(VGroup(box, num))\n            ms.append(VGroup(*m))\n        ms = VGroup(*ms)\n\n        self.add(rows, ms)\n\n\ndef convolution1d(x, kernel):\n    kernel_width = len(kernel)\n    input_size = len(x)\n\n    output_size = input_size - (kernel_width - 1)\n    output = np.zeros((output_size,))\n    for i in range(output_size):\n        for j in range(kernel_width):\n            output[i] += x[i + j] * kernel[j]\n    return output\n\n\ndef convolution1d(x, kernel):\n    kernel_width = len(kernel)\n    input_size = len(x)\n\n    output_size = input_size - (kernel_width - 1)\n    output = np.zeros((output_size,))\n    for i in range(output_size):\n        for j in range(kernel_width):\n            output[i] += np.dot(x[i + j], kernel[j])\n    return output\n\n\ndef convolution1d(x, kernel, padding=0):\n\n    kernel_width = len(kernel)\n    input_size = len(x)\n\n    output_size = input_size - (kernel_width - 1) + 2 * padding\n    output = np.zeros((output_size,))\n    for i in range(output_size):\n        for j in range(kernel_width):\n            ind = i + j - padding\n            if ind &gt;= 0 and ind &lt; input_size:\n                output[i] += np.dot(x[ind], kernel[j])\n    return output\n\n\ndef convolution1d(x, kernel, padding=0, stride=1):\n\n    kernel_width = len(kernel)\n    input_size = len(x)\n\n    output_size = (input_size - (kernel_width - 1) + 2 * padding - 1) // stride + 1\n    output = np.zeros((output_size,))\n    for i in range(len(output)):\n        for j in range(kernel_width):\n            ind = i * stride + j - padding\n            if ind &gt;= 0 and ind &lt; input_size:\n                output[i] += np.dot(x[ind], kernel[j])\n    return output\n\n\ndef convolution1d(x, kernel, padding=0, stride=1, dialation=1):\n\n    kernel_width = len(kernel)\n    input_size = len(x)\n\n    output_size = (input_size - dialation * (kernel_width - 1) + 2 * padding - 1) // stride + 1\n    output = np.zeros((output_size,))\n    for i in range(len(output)):\n        for j in range(kernel_width):\n            ind = i * stride + j * dialation - padding\n            if ind &gt;= 0 and ind &lt; input_size:\n                output[i] += np.dot(x[ind], kernel[j])\n    return output\n\n\ndef convolution1d(x, kernel, bias=0, padding=0, stride=1, dialation=1):\n\n    kernel_width = len(kernel)\n    input_size = len(x)\n\n    output_size = (input_size - dialation * (kernel_width - 1) + 2 * padding - 1) // stride + 1\n    output = np.zeros((output_size,))\n    for i in range(len(output)):\n        output[i] = bias\n        for j in range(kernel_width):\n            ind = i * stride + j * dialation - padding\n            if ind &gt;= 0 and ind &lt; input_size:\n                output[i] += np.dot(x[ind], kernel[j])\n    return output\n\n\ndef maxpool1d(x, window, padding=0, stride=1):\n    input_size = len(x)\n\n    output_size = (input_size - (window - 1) + 2 * padding - 1) // stride + 1\n    output = np.zeros((output_size,))\n    for i in range(len(output)):\n        output[i] = -np.infty\n        for j in range(window):\n            ind = i * stride + j - padding\n            if ind &gt;= 0 and ind &lt; input_size:\n                output[i] = np.maximum(x[ind], output[i])\n    return output\n\n\ndef avgpool1d(x, window, padding=0, stride=1):\n    input_size = len(x)\n\n    output_size = (input_size - (window - 1) + 2 * padding - 1) // stride + 1\n    output = np.zeros((output_size,))\n    for i in range(len(output)):\n        output[i] = 0.\n        for j in range(window):\n            ind = i * stride + j - padding\n            if ind &gt;= 0 and ind &lt; input_size:\n                output[i] += x[ind] / window\n    return output\n\n\n\nfrom manim import *\n\n\n%%manim -pql -v WARNING MovingAround\n\nfrom manim_ml.neural_network import NeuralNetwork, FeedForwardLayer\nclass MovingAround(Scene):\n    def construct(self):\n        nn = NeuralNetwork([\n            FeedForwardLayer(num_nodes=3),\n            FeedForwardLayer(num_nodes=5),\n            FeedForwardLayer(num_nodes=3)\n        ])\n        self.add(nn)\n        # Make the animation\n        forward_pass_animation = nn.make_forward_pass_animation()\n        # Play the animation\n        self.play(forward_pass_animation)\n\n\nimport manim_ml\n\n\nmanim_ml.ThreeDScene"
  },
  {
    "objectID": "assignments/final-project/outline.html#final-deliverables-due-1212-1159pm",
    "href": "assignments/final-project/outline.html#final-deliverables-due-1212-1159pm",
    "title": "Final project outline",
    "section": "Final deliverables (Due 12/12, 11:59pm)",
    "text": "Final deliverables (Due 12/12, 11:59pm)\nPlease see the final project template here, which includes instructions and guidelines for what to submit. Submissions will be through gradescope."
  },
  {
    "objectID": "assignments/homework6-resnets/solutions.html",
    "href": "assignments/homework6-resnets/solutions.html",
    "title": "Homework 6: Optimization and Normalization",
    "section": "",
    "text": "For this homework and for the final project, you may find it useful to run your code with access to a sufficient GPU, which may allow your code to run faster (we will talk about why later in the course). If you do not have access to a powerful GPU on your personal computer (e.g. if you primarily use a laptop), then there are 2 options you may consider for using a remotely hosted GPU.\nNote that some laptops may actually run this code faster than the course server, so you may want to try it first on your laptop regardless\n\n# This is the path that the dataset for this homework will be downloaded to. \n# If you are running on the course server or Colab you can keep this line, if you are\n# running on a personal computer, you may want to change this location.\ndata_path = './data' #'/cs/cs152/data'\n\n\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nfrom torchsummary import summary\n\nfrom torchvision.datasets import FashionMNIST\nfrom torchvision.transforms import Compose, Normalize, ToTensor\n\nfrom fastprogress.fastprogress import master_bar, progress_bar\n\nimport matplotlib.pyplot as plt\n\n\n# Use the GPUs if they are available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using '{device}' device.\")\n\n# Model hyperparameters\nneurons_per_hidden_layer = [20] * 2\n\n# Mini-Batch SGD hyperparameters\nbatch_size = 256\nnum_epochs = 10\nlearning_rate = 0.001\n\ncriterion = nn.CrossEntropyLoss()\n\nUsing 'cpu' device.\n\n\n\n\n\n\ndef get_fmnist_data_loaders(path, batch_size, valid_batch_size=0):\n    # Computing normalization constants for Fashion-MNIST (commented out since we only need to do this once)\n    # train_loader, valid_loader = get_fmnist_data_loaders(data_path, 0)\n    # X, _ = next(iter(train_loader))\n    # s, m = torch.std_mean(X)\n\n\n    # Data specific transforms\n    data_mean = (0.2860,)\n    data_std = (0.3530,)\n    xforms = Compose([ToTensor(), Normalize(data_mean, data_std)])\n\n    # Training data loader\n    train_dataset = FashionMNIST(root=path, train=True, download=True, transform=xforms)\n\n    # Set the batch size to N if batch_size is 0\n    tbs = len(train_dataset) if batch_size == 0 else batch_size\n    train_loader = DataLoader(train_dataset, batch_size=tbs, shuffle=True)\n\n    # Validation data loader\n    valid_dataset = FashionMNIST(root=path, train=False, download=True, transform=xforms)\n\n    # Set the batch size to N if batch_size is 0\n    vbs = len(valid_dataset) if valid_batch_size == 0 else valid_batch_size\n    valid_loader = DataLoader(valid_dataset, batch_size=vbs, shuffle=True)\n\n    return train_loader, valid_loader\n\n\n# Load the example dataset (Fashion MNIST)\ntrain_loader, valid_loader = get_fmnist_data_loaders(data_path, batch_size)\n\nprint(\"Training dataset shape   :\", train_loader.dataset.data.shape)\nprint(\"Validation dataset shape :\", valid_loader.dataset.data.shape)\n\nTraining dataset shape   : torch.Size([60000, 28, 28])\nValidation dataset shape : torch.Size([10000, 28, 28])\n\n\n\n# Let's plot a few images as an example\nnum_to_show = 8\nimages = train_loader.dataset.data[:num_to_show]\ntargets = train_loader.dataset.targets[:num_to_show]\nlabels = [train_loader.dataset.classes[t] for t in targets]\n\nfig, axes = plt.subplots(1, num_to_show)\n\nfor axis, image, label in zip(axes, images, labels):\n    axis.imshow(image.squeeze(), cmap=\"Greys\")\n    axis.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n    axis.set_xticks([])\n    axis.set_yticks([])\n    axis.set_title(f\"{label}\")\n\n\n\n\n\n\n\n\nclass Layer(nn.Module):\n    # This class will represent a basic neural network layer with a linear function\n    # and an activation (in this case ReLU)\n    def __init__(self, in_dimensions, out_dimensions):\n        super().__init__()\n        self.linear = nn.Linear(in_dimensions, out_dimensions)\n        self.activation = nn.ReLU()\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = self.activation(x)\n        return x\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self, layer_sizes, layer_class=Layer):\n        super(NeuralNetwork, self).__init__()\n\n        # The first \"layer\" just rearranges the Nx28x28 input into Nx784\n        first_layer = nn.Flatten()\n\n        # The hidden layers include:\n        # 1. a linear component (computing Z) and\n        # 2. a non-linear comonent (computing A)\n        hidden_layers = [\n            (layer_class(nlminus1, nl) if nlminus1 == nl else Layer(nlminus1, nl))\n            for nl, nlminus1 in zip(layer_sizes[1:-1], layer_sizes)\n        ]\n\n        # The output layer must be Linear without an activation. See:\n        #   https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n        output_layer = nn.Linear(layer_sizes[-2], layer_sizes[-1])\n\n        # Group all layers into the sequential container\n        all_layers = [first_layer] + hidden_layers + [output_layer]\n        self.layers = nn.Sequential(*all_layers)\n\n    def forward(self, X):\n        return self.layers(X)\n\n\n\n\n\nclass SGDOptimizer:\n    def __init__(self, parameters, lr=0.01):\n        # Set the learning rate\n        self.lr = lr\n        # Store the set of parameters that we'll be optimizing\n        self.parameters = list(parameters)\n\n    def step(self):\n        # Take a step of gradient descent\n        for ind, parameter in enumerate(self.parameters):\n            # Compute the update to the parameter\n            update = self.lr * parameter.grad\n\n            # Update the parameter: w &lt;- w - lr * grad\n            parameter -= update \n\n\n\n\n\n# Here we'll define a function to train and evaluate a neural network with a specified architecture\n# using a specified optimizer.\ndef run_model(optimizer=SGDOptimizer, \n              layer_type=Layer, \n              number_of_hidden_layers=2,\n              neurons_per_hidden_layer=20, \n              learning_rate=0.001):\n    \n    # Get the dataset\n    train_loader, valid_loader = get_fmnist_data_loaders(data_path, batch_size)\n\n    # The input layer size depends on the dataset\n    nx = train_loader.dataset.data.shape[1:].numel()\n\n    # The output layer size depends on the dataset\n    ny = len(train_loader.dataset.classes)\n\n    # Preprend the input and append the output layer sizes\n    layer_sizes = [nx] + [neurons_per_hidden_layer] * number_of_hidden_layers + [ny]\n\n    # Do model creation here so that the model is recreated each time the cell is run\n    model = NeuralNetwork(layer_sizes, layer_type).to(device)\n\n    t = 0\n    # Create the optimizer, just like we have with the built-in optimizer\n    opt = optimizer(model.parameters(), learning_rate)\n\n    # A master bar for fancy output progress\n    mb = master_bar(range(num_epochs))\n\n    # Information for plots\n    mb.names = [\"Train Loss\", \"Valid Loss\"]\n    train_losses = []\n    valid_losses = []\n\n    for epoch in mb:\n\n        #\n        # Training\n        #\n        model.train()\n\n        train_N = len(train_loader.dataset)\n        num_train_batches = len(train_loader)\n        train_dataiterator = iter(train_loader)\n\n        train_loss_mean = 0\n\n        for batch in progress_bar(range(num_train_batches), parent=mb):\n\n            # Grab the batch of data and send it to the correct device\n            train_X, train_Y = next(train_dataiterator)\n            train_X, train_Y = train_X.to(device), train_Y.to(device)\n\n            # Compute the output\n            train_output = model(train_X)\n\n            # Compute loss\n            train_loss = criterion(train_output, train_Y)\n\n            num_in_batch = len(train_X)\n            tloss = train_loss.item() * num_in_batch / train_N\n            train_loss_mean += tloss\n            train_losses.append(train_loss.item())\n\n            # Compute gradient\n            model.zero_grad()\n            train_loss.backward()\n            \n            # Take a step of gradient descent\n            t += 1\n            with torch.no_grad():\n                opt.step()\n\n        #\n        # Validation\n        #\n        model.eval()\n\n        valid_N = len(valid_loader.dataset)\n        num_valid_batches = len(valid_loader)\n\n        valid_loss_mean = 0\n        valid_correct = 0\n\n        with torch.no_grad():\n\n            # valid_loader is probably just one large batch, so not using progress bar\n            for valid_X, valid_Y in valid_loader:\n\n                valid_X, valid_Y = valid_X.to(device), valid_Y.to(device)\n\n                valid_output = model(valid_X)\n\n                valid_loss = criterion(valid_output, valid_Y)\n\n                num_in_batch = len(valid_X)\n                vloss = valid_loss.item() * num_in_batch / valid_N\n                valid_loss_mean += vloss\n                valid_losses.append(valid_loss.item())\n\n                # Convert network output into predictions (one-hot -&gt; number)\n                predictions = valid_output.argmax(1)\n\n                # Sum up total number that were correct\n                valid_correct += (predictions == valid_Y).type(torch.float).sum().item()\n\n        valid_accuracy = 100 * (valid_correct / valid_N)\n\n        # Report information\n        tloss = f\"Train Loss = {train_loss_mean:.4f}\"\n        vloss = f\"Valid Loss = {valid_loss_mean:.4f}\"\n        vaccu = f\"Valid Accuracy = {(valid_accuracy):&gt;0.1f}%\"\n        mb.write(f\"[{epoch+1:&gt;2}/{num_epochs}] {tloss}; {vloss}; {vaccu}\")\n\n        # Update plot data\n        max_loss = max(max(train_losses), max(valid_losses))\n        min_loss = min(min(train_losses), min(valid_losses))\n\n        x_margin = 0.2\n        x_bounds = [0 - x_margin, num_epochs + x_margin]\n\n        y_margin = 0.1\n        y_bounds = [min_loss - y_margin, max_loss + y_margin]\n\n        valid_Xaxis = torch.linspace(0, epoch + 1, len(train_losses))\n        valid_xaxis = torch.linspace(1, epoch + 1, len(valid_losses))\n        graph_data = [[valid_Xaxis, train_losses], [valid_xaxis, valid_losses]]\n\n        mb.update_graph(graph_data, x_bounds, y_bounds)\n\n    print(f\"[{epoch+1:&gt;2}/{num_epochs}] {tloss}; {vloss}; {vaccu}\")\n\nrun_model()\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n[10/10] Train Loss = 1.1842; Valid Loss = 1.1478; Valid Accuracy = 65.2%"
  },
  {
    "objectID": "assignments/homework6-resnets/solutions.html#set-hyperparameters",
    "href": "assignments/homework6-resnets/solutions.html#set-hyperparameters",
    "title": "Homework 6: Optimization and Normalization",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nfrom torchsummary import summary\n\nfrom torchvision.datasets import FashionMNIST\nfrom torchvision.transforms import Compose, Normalize, ToTensor\n\nfrom fastprogress.fastprogress import master_bar, progress_bar\n\nimport matplotlib.pyplot as plt\n\n\n# Use the GPUs if they are available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using '{device}' device.\")\n\n# Model hyperparameters\nneurons_per_hidden_layer = [20] * 2\n\n# Mini-Batch SGD hyperparameters\nbatch_size = 256\nnum_epochs = 10\nlearning_rate = 0.001\n\ncriterion = nn.CrossEntropyLoss()\n\nUsing 'cpu' device."
  },
  {
    "objectID": "assignments/homework6-resnets/solutions.html#prepare-the-dataset",
    "href": "assignments/homework6-resnets/solutions.html#prepare-the-dataset",
    "title": "Homework 6: Optimization and Normalization",
    "section": "",
    "text": "def get_fmnist_data_loaders(path, batch_size, valid_batch_size=0):\n    # Computing normalization constants for Fashion-MNIST (commented out since we only need to do this once)\n    # train_loader, valid_loader = get_fmnist_data_loaders(data_path, 0)\n    # X, _ = next(iter(train_loader))\n    # s, m = torch.std_mean(X)\n\n\n    # Data specific transforms\n    data_mean = (0.2860,)\n    data_std = (0.3530,)\n    xforms = Compose([ToTensor(), Normalize(data_mean, data_std)])\n\n    # Training data loader\n    train_dataset = FashionMNIST(root=path, train=True, download=True, transform=xforms)\n\n    # Set the batch size to N if batch_size is 0\n    tbs = len(train_dataset) if batch_size == 0 else batch_size\n    train_loader = DataLoader(train_dataset, batch_size=tbs, shuffle=True)\n\n    # Validation data loader\n    valid_dataset = FashionMNIST(root=path, train=False, download=True, transform=xforms)\n\n    # Set the batch size to N if batch_size is 0\n    vbs = len(valid_dataset) if valid_batch_size == 0 else valid_batch_size\n    valid_loader = DataLoader(valid_dataset, batch_size=vbs, shuffle=True)\n\n    return train_loader, valid_loader\n\n\n# Load the example dataset (Fashion MNIST)\ntrain_loader, valid_loader = get_fmnist_data_loaders(data_path, batch_size)\n\nprint(\"Training dataset shape   :\", train_loader.dataset.data.shape)\nprint(\"Validation dataset shape :\", valid_loader.dataset.data.shape)\n\nTraining dataset shape   : torch.Size([60000, 28, 28])\nValidation dataset shape : torch.Size([10000, 28, 28])\n\n\n\n# Let's plot a few images as an example\nnum_to_show = 8\nimages = train_loader.dataset.data[:num_to_show]\ntargets = train_loader.dataset.targets[:num_to_show]\nlabels = [train_loader.dataset.classes[t] for t in targets]\n\nfig, axes = plt.subplots(1, num_to_show)\n\nfor axis, image, label in zip(axes, images, labels):\n    axis.imshow(image.squeeze(), cmap=\"Greys\")\n    axis.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n    axis.set_xticks([])\n    axis.set_yticks([])\n    axis.set_title(f\"{label}\")"
  },
  {
    "objectID": "assignments/homework6-resnets/solutions.html#create-a-neural-network",
    "href": "assignments/homework6-resnets/solutions.html#create-a-neural-network",
    "title": "Homework 6: Optimization and Normalization",
    "section": "",
    "text": "class Layer(nn.Module):\n    # This class will represent a basic neural network layer with a linear function\n    # and an activation (in this case ReLU)\n    def __init__(self, in_dimensions, out_dimensions):\n        super().__init__()\n        self.linear = nn.Linear(in_dimensions, out_dimensions)\n        self.activation = nn.ReLU()\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = self.activation(x)\n        return x\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self, layer_sizes, layer_class=Layer):\n        super(NeuralNetwork, self).__init__()\n\n        # The first \"layer\" just rearranges the Nx28x28 input into Nx784\n        first_layer = nn.Flatten()\n\n        # The hidden layers include:\n        # 1. a linear component (computing Z) and\n        # 2. a non-linear comonent (computing A)\n        hidden_layers = [\n            (layer_class(nlminus1, nl) if nlminus1 == nl else Layer(nlminus1, nl))\n            for nl, nlminus1 in zip(layer_sizes[1:-1], layer_sizes)\n        ]\n\n        # The output layer must be Linear without an activation. See:\n        #   https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n        output_layer = nn.Linear(layer_sizes[-2], layer_sizes[-1])\n\n        # Group all layers into the sequential container\n        all_layers = [first_layer] + hidden_layers + [output_layer]\n        self.layers = nn.Sequential(*all_layers)\n\n    def forward(self, X):\n        return self.layers(X)"
  },
  {
    "objectID": "assignments/homework6-resnets/solutions.html#implement-an-optimizer",
    "href": "assignments/homework6-resnets/solutions.html#implement-an-optimizer",
    "title": "Homework 6: Optimization and Normalization",
    "section": "",
    "text": "class SGDOptimizer:\n    def __init__(self, parameters, lr=0.01):\n        # Set the learning rate\n        self.lr = lr\n        # Store the set of parameters that we'll be optimizing\n        self.parameters = list(parameters)\n\n    def step(self):\n        # Take a step of gradient descent\n        for ind, parameter in enumerate(self.parameters):\n            # Compute the update to the parameter\n            update = self.lr * parameter.grad\n\n            # Update the parameter: w &lt;- w - lr * grad\n            parameter -= update"
  },
  {
    "objectID": "assignments/homework6-resnets/solutions.html#train-classifier",
    "href": "assignments/homework6-resnets/solutions.html#train-classifier",
    "title": "Homework 6: Optimization and Normalization",
    "section": "",
    "text": "# Here we'll define a function to train and evaluate a neural network with a specified architecture\n# using a specified optimizer.\ndef run_model(optimizer=SGDOptimizer, \n              layer_type=Layer, \n              number_of_hidden_layers=2,\n              neurons_per_hidden_layer=20, \n              learning_rate=0.001):\n    \n    # Get the dataset\n    train_loader, valid_loader = get_fmnist_data_loaders(data_path, batch_size)\n\n    # The input layer size depends on the dataset\n    nx = train_loader.dataset.data.shape[1:].numel()\n\n    # The output layer size depends on the dataset\n    ny = len(train_loader.dataset.classes)\n\n    # Preprend the input and append the output layer sizes\n    layer_sizes = [nx] + [neurons_per_hidden_layer] * number_of_hidden_layers + [ny]\n\n    # Do model creation here so that the model is recreated each time the cell is run\n    model = NeuralNetwork(layer_sizes, layer_type).to(device)\n\n    t = 0\n    # Create the optimizer, just like we have with the built-in optimizer\n    opt = optimizer(model.parameters(), learning_rate)\n\n    # A master bar for fancy output progress\n    mb = master_bar(range(num_epochs))\n\n    # Information for plots\n    mb.names = [\"Train Loss\", \"Valid Loss\"]\n    train_losses = []\n    valid_losses = []\n\n    for epoch in mb:\n\n        #\n        # Training\n        #\n        model.train()\n\n        train_N = len(train_loader.dataset)\n        num_train_batches = len(train_loader)\n        train_dataiterator = iter(train_loader)\n\n        train_loss_mean = 0\n\n        for batch in progress_bar(range(num_train_batches), parent=mb):\n\n            # Grab the batch of data and send it to the correct device\n            train_X, train_Y = next(train_dataiterator)\n            train_X, train_Y = train_X.to(device), train_Y.to(device)\n\n            # Compute the output\n            train_output = model(train_X)\n\n            # Compute loss\n            train_loss = criterion(train_output, train_Y)\n\n            num_in_batch = len(train_X)\n            tloss = train_loss.item() * num_in_batch / train_N\n            train_loss_mean += tloss\n            train_losses.append(train_loss.item())\n\n            # Compute gradient\n            model.zero_grad()\n            train_loss.backward()\n            \n            # Take a step of gradient descent\n            t += 1\n            with torch.no_grad():\n                opt.step()\n\n        #\n        # Validation\n        #\n        model.eval()\n\n        valid_N = len(valid_loader.dataset)\n        num_valid_batches = len(valid_loader)\n\n        valid_loss_mean = 0\n        valid_correct = 0\n\n        with torch.no_grad():\n\n            # valid_loader is probably just one large batch, so not using progress bar\n            for valid_X, valid_Y in valid_loader:\n\n                valid_X, valid_Y = valid_X.to(device), valid_Y.to(device)\n\n                valid_output = model(valid_X)\n\n                valid_loss = criterion(valid_output, valid_Y)\n\n                num_in_batch = len(valid_X)\n                vloss = valid_loss.item() * num_in_batch / valid_N\n                valid_loss_mean += vloss\n                valid_losses.append(valid_loss.item())\n\n                # Convert network output into predictions (one-hot -&gt; number)\n                predictions = valid_output.argmax(1)\n\n                # Sum up total number that were correct\n                valid_correct += (predictions == valid_Y).type(torch.float).sum().item()\n\n        valid_accuracy = 100 * (valid_correct / valid_N)\n\n        # Report information\n        tloss = f\"Train Loss = {train_loss_mean:.4f}\"\n        vloss = f\"Valid Loss = {valid_loss_mean:.4f}\"\n        vaccu = f\"Valid Accuracy = {(valid_accuracy):&gt;0.1f}%\"\n        mb.write(f\"[{epoch+1:&gt;2}/{num_epochs}] {tloss}; {vloss}; {vaccu}\")\n\n        # Update plot data\n        max_loss = max(max(train_losses), max(valid_losses))\n        min_loss = min(min(train_losses), min(valid_losses))\n\n        x_margin = 0.2\n        x_bounds = [0 - x_margin, num_epochs + x_margin]\n\n        y_margin = 0.1\n        y_bounds = [min_loss - y_margin, max_loss + y_margin]\n\n        valid_Xaxis = torch.linspace(0, epoch + 1, len(train_losses))\n        valid_xaxis = torch.linspace(1, epoch + 1, len(valid_losses))\n        graph_data = [[valid_Xaxis, train_losses], [valid_xaxis, valid_losses]]\n\n        mb.update_graph(graph_data, x_bounds, y_bounds)\n\n    print(f\"[{epoch+1:&gt;2}/{num_epochs}] {tloss}; {vloss}; {vaccu}\")\n\nrun_model()\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n[10/10] Train Loss = 1.1842; Valid Loss = 1.1478; Valid Accuracy = 65.2%"
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#parameterized-functions",
    "href": "lecture2-linear-regression/notes.html#parameterized-functions",
    "title": "Lecture 2: Linear regression",
    "section": "Parameterized Functions",
    "text": "Parameterized Functions\nLinear and affine functions are examples of classes of functions, they define a general form for many different functions. Using the affine example, we see that we can define a particular function by choosing values for \\(w_i\\) and \\(b\\).\n\\[\nf(\\mathbf{x}) = \\sum_{i=1}^n x_iw_i +b\n\\]\nWe will refer to the values that define a function within our class (e.g. \\(w_i\\) and \\(b\\)) as the parameters of the function, by changing these values, we can change the function.\nWe typically refer to \\(\\mathbf{w}\\) specifically as the weight vector (or weights) and \\(b\\) as the bias. To summarize:\n\\[\n\\textbf{Affine function:  }f(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{w}+b,\\quad \\textbf{Parameters:}\\quad \\big(\\text{Weights:  } \\mathbf{w},\\ \\text{Bias:  } b \\big)\n\\]"
  },
  {
    "objectID": "lecture1-background/notes.html#legends",
    "href": "lecture1-background/notes.html#legends",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "Legends",
    "text": "Legends\nWhen plotting multiple point sets or curves we’ll likely want to be able to distinguish what each plot represents. We can do this by specifying a label for each plot, then adding a legend that tells us what each represents.\n\nx_values = np.linspace(0,8,100)\ny1 = np.sin(x_values)         # sinusoidal function\ny2 = (x_values - 3)**2 / 12   # a simple quadratic curve\ny3 = 0.5*x_values - 1.0       # a simple linear function\n\nplt.plot(x_values, y1, 'b-', label='sin(x)');  # plot two curves\nplt.plot(x_values, y2, 'g--', label='$(x-3)^2 / 12$')\nplt.plot(x_values, y3, 'r:', label='$.5x-1$'); # add a curve to the plot\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x177762520&gt;\n\n\n\n\n\nHere surrounding text in a label with $ $ allows us to write mathamatical equations as in LaTex!"
  }
]