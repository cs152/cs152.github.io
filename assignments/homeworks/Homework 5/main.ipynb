{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckmBOk_nmdDl"
   },
   "source": [
    "# **Homework 5:** Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1YamPyumdDo"
   },
   "source": [
    "### Collaborators\n",
    "\n",
    "Please list anyone you discussed or collaborated on this assignment with below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84hFrkAvmdDo"
   },
   "source": [
    "LIST COLLABORATORS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Dmq3BcumdDo"
   },
   "source": [
    "### Course feedback\n",
    "\n",
    "Please submit this week's course survey here: https://forms.gle/PJjGy8wrhhQnAhbb6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "854xH9gZmdDp"
   },
   "source": [
    "## Overview\n",
    "\n",
    "In this homework we will build a tiny reverse-mode automatic differentiation library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cE8XDumNmdDp"
   },
   "outputs": [],
   "source": [
    "# Uncomment and run if using Colab!\n",
    "\n",
    "#import urllib.request\n",
    "#remote_url = 'https://gist.githubusercontent.com/gabehope/d3e6b10338a1ba78f53204fc7502eda5/raw/52631870b1475b5ef8d9701f1c676fa97bf7b300/hw5_support.py'\n",
    "#with urllib.request.urlopen(remote_url) as remote, open('hw5_support.py', 'w') as local:\n",
    "#  [local.write(str(line, encoding='utf-8')) for line in remote]\n",
    "\n",
    "# Run me first!\n",
    "from hw5_support import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGqAAWGAmdDp"
   },
   "source": [
    "#### Python features\n",
    "This homework makes use of a few fancy features in Python that are worth knowing about if you are unfamiliar.\n",
    "- [Variable length arguments](https://book.pythontips.com/en/latest/args_and_kwargs.html) (e.g. `*args`)\n",
    "- [List comprehensions](https://book.pythontips.com/en/latest/comprehensions.html#list-comprehensions) (e.g. `[a**2 for a in range(5)]`)\n",
    "- [Magic methods](https://rszalski.github.io/magicmethods/) (e.g. `__add__`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Forward-mode automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by developing an automatic differentiation class that uses *forward-mode automatic differentiation*.\n",
    "\n",
    "Recall that for this version of automatic differentiaion each operation needs to keep track of the derivative of it's value with respect *each* original input. Since for every operation we need to store these extra pieces of data and functions for computing both the operation and its derivative, it makes sense to define a *class* to represent the result of an operation.\n",
    "\n",
    "For example, if we want to make a class that represents the operation `c=a+b` our class needs several properties:\n",
    "- `value`: The value of the operation (`c`)\n",
    "- `forward_grads`: A dictionary that contains the derivatives with respect to each original input (e.g. ($\\frac{dc}{da}$ and $\\frac{dc}{db}$)).\n",
    "- `func`: A function that computes the operation (`a+b`)\n",
    "- `grads`: A function that *computes* the derivatives of the operation ($\\frac{dc}{da}$ and $\\frac{dc}{db}$)\n",
    "\n",
    "For this homework, we've provided the outline of such a class, called `ForwardValue`. This will be the base class for all of our possible operations and represents declaring a variable with a value (`a = 5`). This is useful because it lets us define values that we might want to find derivatives with respect to.\n",
    "\n",
    "Let's see how this will work in practice. If we want to take derivatives we will first define the inputs using `ForwardValue`.\n",
    "\n",
    "```{python}\n",
    "a = ForwardValue(5)\n",
    "b = ForwardValue(2)\n",
    "```\n",
    "Then we can perform whatever operations we want on these inputs:\n",
    "```{python}\n",
    "c = a + b\n",
    "L = log(c)\n",
    "```\n",
    "Each of these operations will produce a new `ForwardValue` object representing the result of that operation.\n",
    "\n",
    "As each result should maintain the derivatives with respect to each *original* inputs, we can access the final derivatives we're interested ($\\frac{dL}{da}$ and $\\frac{dL}{db}$) in from `L`.\n",
    "\n",
    "```{python}\n",
    "dL_da = L.forward_grads[a]\n",
    "dL_db = L.forward_grads[b]\n",
    "```\n",
    "\n",
    "\n",
    "We'll also be able to compute operations with non-AutogradValue numbers, but obviously won't be able to compute derivaitives with respect to these values.\n",
    "```{python}\n",
    "s = 4\n",
    "L = s * a\n",
    "dL_da = L.forward_grads[a] # Will work because a is an ForwardValue\n",
    "dL_ds = L.forward_grads[s] # Will give an error because s is not an ForwardValue\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen what our final product will look like, let's define our `ForwardValue` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutogradValue:\n",
    "    '''\n",
    "    Base class for automatic differentiation operations. Represents variable delcaration.\n",
    "    Subclasses will overwrite func and grads to define new operations.\n",
    "\n",
    "    Properties:\n",
    "        parents (list): A list of the inputs to the operation, may be AutogradValue or float\n",
    "        parent_values    (list): A list of raw values of each input (as floats)\n",
    "        forward_grads (dict): A dictionary mapping inputs to gradients\n",
    "        grad    (float): The derivative of the final loss with respect to this value (dL/da)\n",
    "        value   (float): The value of the result of this operation\n",
    "    '''\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        self.parents = list(args)\n",
    "        self.parent_values = [arg.value if isinstance(arg, AutogradValue) else arg for arg in args]\n",
    "        self.forward_grads = {}\n",
    "        self.value = self.forward_pass()\n",
    "        self.grad = 0. # Used later for reverse mode\n",
    "\n",
    "    def func(self, input):\n",
    "        '''\n",
    "        Compute the value of the operation given the inputs.\n",
    "        For declaring a variable, this is just the identity function (return the input).\n",
    "\n",
    "        Args:\n",
    "            input (float): The input to the operation\n",
    "        Returns:\n",
    "            value (float): The result of the operation\n",
    "        '''\n",
    "        return input\n",
    "\n",
    "    def grads(self, *args):\n",
    "        '''\n",
    "        Compute the derivative of the operation with respect to each input.\n",
    "        In the base case the derivative of the identity function is just 1. (da/da = 1).\n",
    "\n",
    "        Args:\n",
    "            input (float): The input to the operation\n",
    "        Returns:\n",
    "            grads (tuple): The derivative of the operation with respect to each input\n",
    "                            Here there is only a single input, so we return a length-1 tuple.\n",
    "        '''\n",
    "        return (1,)\n",
    "\n",
    "    def forward_pass(self):\n",
    "        # Calls func to compute the value of this operation\n",
    "        return self.func(*self.parent_values)\n",
    "\n",
    "    def __repr__(self):\n",
    "        # Python magic function for string representation.\n",
    "        return str(self.value)\n",
    "\n",
    "\n",
    "class ForwardValue(AutogradValue):\n",
    "    '''\n",
    "    Subclass for forward-mode automatic differentiation. Initialized the forward_grads\n",
    "    dict to include this value.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        super().__init__(*args)\n",
    "        if len(self.forward_grads.keys()) == 0:\n",
    "            self.forward_grads = {self: 1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the base case, when we declare a variable the derivative with respect to itself is just 1 ($\\frac{da}{da}=1$)\n",
    "```{python}\n",
    "da_da = a.forward_grads[a] # Will be 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined the framework for an operation that can be used in automatic differentiation, we need to define some actual useful operations by subclassing `ForwardValue`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q1:** Defining operations\n",
    "Fill out the `func` and `grads` methods of each subclass below. Recall that `func` should always return the result of the operation and `grads` should always return a `tuple` of the derivative with respect to each input.\n",
    "\n",
    "***Hint:** Look at the `_add` and `_neg` examples as a template!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _add(AutogradValue):\n",
    "    # Addition operator (a + b)\n",
    "    def func(self, a, b):\n",
    "        return a + b\n",
    "\n",
    "    def grads(self, a, b):\n",
    "        return 1., 1.\n",
    "\n",
    "class _neg(AutogradValue):\n",
    "    # Negation operator (-a)\n",
    "    def func(self, a):\n",
    "        return -a\n",
    "\n",
    "    def grads(self, a):\n",
    "        return (-1.,)\n",
    "\n",
    "class _sub(AutogradValue):\n",
    "    # Subtraction operator (a - b)\n",
    "    def func(self, a, b):\n",
    "        # Your code here\n",
    "\n",
    "    def grads(self, a, b):\n",
    "        # Your code here\n",
    "\n",
    "class _mul(AutogradValue):\n",
    "    # Multiplication operator (a * b)\n",
    "    def func(self, a, b):\n",
    "        # Your code here\n",
    "\n",
    "    def grads(self, a, b):\n",
    "        # Your code here\n",
    "\n",
    "class _div(AutogradValue):\n",
    "    # Division operator (a / b)\n",
    "    def func(self, a, b):\n",
    "        # Your code here\n",
    "\n",
    "    def grads(self, a, b):\n",
    "        # Your code here\n",
    "\n",
    "class _exp(AutogradValue):\n",
    "    # Exponent operator (e^a, or exp(a))\n",
    "    def func(self, a):\n",
    "        # Your code here\n",
    "\n",
    "    def grads(self, a):\n",
    "        # Your code here\n",
    "\n",
    "class _log(AutogradValue):\n",
    "    # (Natural) log operator (log(a))\n",
    "    def func(self, a):\n",
    "        # Your code here\n",
    "\n",
    "    def grads(self, a):\n",
    "        # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we'll define our basic functions and operators in terms of the operator classes we just wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp(a):\n",
    "    return _exp(a) if isinstance(a, AutogradValue) else math.exp(a)\n",
    "def log(a):\n",
    "    return _log(a) if isinstance(a, AutogradValue) else math.log(a)\n",
    "\n",
    "# Note: Remember that above we defined a class for each type of operation\n",
    "# so in this code we are overriding the basic operators for AutogradValue\n",
    "# such that they construct a new object of the class corresponding to the\n",
    "# given operation and return it.\n",
    "# (You don't need to everything that's happening here to do the HW)\n",
    "AutogradValue.exp = lambda a: _exp(a)\n",
    "AutogradValue.log = lambda a: _log(a)\n",
    "AutogradValue.__add__ = lambda a, b: _add(a, b)\n",
    "AutogradValue.__radd__ = lambda a, b: _add(b, a)\n",
    "AutogradValue.__sub__ = lambda a, b: _sub(a, b)\n",
    "AutogradValue.__rsub__ = lambda a, b: _sub(b, a)\n",
    "AutogradValue.__neg__ = lambda a: _neg(a)\n",
    "AutogradValue.__mul__ = lambda a, b: _mul(a, b)\n",
    "AutogradValue.__rmul__ = lambda a, b: _mul(b, a)\n",
    "AutogradValue.__truediv__ = lambda a, b: _div(a, b)\n",
    "AutogradValue.__rtruediv__ = lambda a, b: _div(b, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now be able to use our `ForwardValue` objects as if they are numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ForwardValue(5)\n",
    "b = ForwardValue(2)\n",
    "\n",
    "print((a + 5) * b)\n",
    "print(log(b))\n",
    "\n",
    "test_operators(ForwardValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see now that our `forward_pass` method needs to update `forward_grads` (e.g. to compute $\\frac{dg}{da}$ and $\\frac{dg}{db}$) using the `forward_grads` values of its parents (e.g. $\\frac{dc}{da}$ and $\\frac{dc}{db}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q2:** Defining forward-mode autodiff\n",
    "Update the `forward_pass` method below for forward-mode automatic differentiation. This method should update the `forward_grads` property of the operation such that:\n",
    "- `forward_grads` has an entry for every input that appears in `forward_grads` of *any* parent operation.\n",
    "- If an input appears in more than 1 parent, make sure to *add* the gradients appropritately (if `g` has parents `b` and `c` then $\\frac{dg}{da} = \\frac{dg}{db}\\frac{db}{da} + \\frac{dg}{dc}\\frac{dc}{da}$  )\n",
    "- Parents that are not `AutogradValue` objects are ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our `forward_pass` method is working correctly, we should have the following behaivior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our inputs as ForwardValue objects\n",
    "a = ForwardValue(5)\n",
    "b = ForwardValue(2)\n",
    "\n",
    "# Perform operations\n",
    "c = a * b\n",
    "g = 3 * c + a\n",
    "\n",
    "\n",
    "# We should have the following in the forward_grads property of c and d (note that the keys are ForwardValue objects!)\n",
    "c.forward_grads = {a: 2, b: 5}  # dc/da and dc/db\n",
    "g.forward_grads = {a: 3 * 2 + 1, b: 3 * 5} # dg/da = dg/dc dc/da + dg/da, dg/db = dg/dc dc/db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the method below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(self, args):\n",
    "    # Clear forward_grads if it exists\n",
    "    self.forward_grads = {}\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "\n",
    "    # Make sure to still return the operation's value\n",
    "    return self.func(*self.parent_values)\n",
    "\n",
    "# Overwrite the AutogradValue method so that operators still work\n",
    "AutogradValue.forward_pass = forward_pass\n",
    "test_forward_mode(ForwardValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take derivates of functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ForwardValue(5)\n",
    "b = ForwardValue(2)\n",
    "L = -log(5 *b + a)\n",
    "\n",
    "dL_da = L.forward_grads[a]\n",
    "dL_db = L.forward_grads[b]\n",
    "print('dL/da = %.3f, dL/db = %.3f' % (dL_da, dL_db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also implement our own very simple version of Autograd's `grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(f):\n",
    "    def ad_function(x, *args):\n",
    "        x = ForwardValue(x)\n",
    "        output = f(x, *args)\n",
    "        return output.forward_grads[x]\n",
    "    return ad_function\n",
    "\n",
    "# Define a function\n",
    "def f(x):\n",
    "    return x ** 2\n",
    "\n",
    "# Use 'grad' to compute the derivative function\n",
    "f_prime = grad(f)\n",
    "\n",
    "# Verify that we get the correct answer\n",
    "x = 5.\n",
    "print('x:\\t', x)\n",
    "print('f(x):\\t', f(x))\n",
    "print(\"f'(x):\\t\", f_prime(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-Ibv3elmdDp"
   },
   "source": [
    "## Part 2: Reverse-mode automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKJsXsUnmdDp"
   },
   "source": [
    "We'll start by developing an automatic differentiation class that uses *reverse-mode automatic differentiation*, as this is what will be most useful for neural networks.\n",
    "\n",
    "Recall that for reverse-mode AD to work, everytime we perform an operation on one or more numbers we need to store the result of that operation as well as the *parent values* (the inputs to the operation). We also need to be able to compute the derivative of that operation. Since for every operation we need to store several pieces of data and several functions, just like forward-mode automactic differentiation, it makes sense to define a class to represent the result of an operation.\n",
    "\n",
    "In this case, we'll reuse the `AutogradValue` class we defined above as the the base class. The set of properties will be the same, except that instead of keeping track of a `forward_grads` dictionary, we'll keep track of a new `grad` property.\n",
    "- `grad`: The derivative of the final loss with respect to `c` ($\\frac{dL}{dc}$)\n",
    "\n",
    "Remember that this will be the base class for all of our possible operations and represents declaring a variable with a value (`a = 5`).\n",
    "\n",
    "Let's see how this will work in practice. If we want to take derivatives using reverse-mode, we will first define the inputs using `AutogradValue`.\n",
    "\n",
    "```{python}\n",
    "a = AutogradValue(5)\n",
    "b = AutogradValue(2)\n",
    "```\n",
    "As before, we can perform whatever operations we want on these inputs:\n",
    "```{python}\n",
    "c = a + b\n",
    "L = log(c)\n",
    "```\n",
    "Each of these operations will produce a new `AutogradValue` object representing the result of that operation.\n",
    "\n",
    "Finally we can run the backward pass by running a method `backward()` (that we will write) on the outout `L`. This will compute the gradients of  `L` with respect to each input that we defined ($\\frac{dL}{da}$ and $\\frac{dL}{db}$). Rather than returning these derivatives, the `backward()` method will *update* the `grad` property of `a` and `b`, making it easy to access the correct derivative.\n",
    "\n",
    "```{python}\n",
    "L.backward()\n",
    "dL_da = a.grad\n",
    "```\n",
    "\n",
    "Again, we'll be able to compute operations with non-AutogradValue numbers, but won't be able to compute derivaitives with respect to these values.\n",
    "```{python}\n",
    "s = 4\n",
    "L = s * a\n",
    "dL_da = a.grad # Will work because a is an AutogradValue\n",
    "dL_ds = s.grad # Will give an error because s is not an AutogradValue\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEN_myG4mdDq"
   },
   "source": [
    "Now that we've seen what our final produce will look like, let's define our `AutogradValue` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdGlWf_ImdDs"
   },
   "source": [
    "Let's confirm that we do keep the entire compuational graph for operations defined in this way.\n",
    "\n",
    "#### **Q3:** Computational graph\n",
    "Write a function `graph_print` that takes a single argument. If the argument is an `AutogradValue` (or one of its subclasses), print its `value` property and then call `graph_print` on each of its parents. If the argument is not an `AutogradValue`, just print it. The format of printing is not important.\n",
    "\n",
    "***Hint:** You can use the built-in Python function `isinstance` to determine if something is an `AutogradValue` or one of its subclasses. e.g. `isinstance(a, AutogradValue)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRvB3PmYmdDs"
   },
   "outputs": [],
   "source": [
    "def graph_print(a):\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "a = AutogradValue(5.)\n",
    "b = AutogradValue(2.)\n",
    "c = log((a + 5) * b)\n",
    "graph_print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJ5wqkSumdDs"
   },
   "source": [
    "The function should print (it's ok if the numbers or order aren't exact):\n",
    "```\n",
    "2.995732273553991\n",
    "20.0\n",
    "10.0\n",
    "5.0\n",
    "5.0\n",
    "5\n",
    "2.0\n",
    "2.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4viVgxkPmdDs"
   },
   "source": [
    "Now in order to do automatic differentiation, we need to define how to do the backward pass. We'll start with the backward_step for a single operation.\n",
    "\n",
    "#### **Q4:** Backward pass\n",
    "Fill in the method `backward_pass` which computes a single step of the reverse pass through the computational graph (assume `self` is an `AutogradValue` instance). If `backward_pass` is called on a value `c`, the method should:\n",
    "- Assume that `self.grad` contains the derivaive of the final loss with respect to `c` ($\\frac{dL}{dc}$).\n",
    "- Check if each parent of `c`  is an `AutogradValue`. If it is, update that parent's `grad` property to account for `c` (e.g. for parent `a`, update the value of $\\frac{dL}{da}$)\n",
    "\n",
    "**For example:** if `c` represents the result of an addition so `c = a + b`,\n",
    "calling `backward_pass` on `c` will update the `grad` property of both `a` and `b`. (`a.grad` represents $\\frac{dL}{da}$ and is initialized to `0`).\n",
    "\n",
    "***Hint:** `grads` will be one of the methods we wrote in the last homework (and shown above). Recall that if `c` has parents `a` and `b` then `grads` method will give $\\frac{dc}{da}$ and $\\frac{dc}{db}$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oub6-gpsmdDs"
   },
   "outputs": [],
   "source": [
    "def backward_pass(self):\n",
    "    ## YOUR CODE HERE\n",
    "    local_grads = self.grads(*self.args)\n",
    "\n",
    "\n",
    "AutogradValue.backward_pass = backward_pass\n",
    "\n",
    "# Test our implementation\n",
    "test_backward_pass(AutogradValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Uaa4AuHmdDw"
   },
   "source": [
    "Finally we need to define the backward method itself. We will call this on the loss value to find the derivatives of the loss with respect to each input. This means working our way backward through the sequence of operations. Remember that if `c=a+b`, then if `c.grad` is $\\frac{dL}{dc}$, calling `backward_pass` on `c` will update $\\frac{dL}{da}$ (`a.grad`) and $\\frac{dL}{db}$ (`b.grad`).\n",
    "\n",
    "The complication is that `c` may be used in multiple operations, so we can't call `backward_pass` on `c` until we've called `backward_pass` on each child operation of `c` otherwise `c.grad` won't have the correct value of $\\frac{dL}{dc}$, as in this example:\n",
    "\n",
    "```{python}\n",
    "c = a + b\n",
    "g = c * 2\n",
    "h = c + 4\n",
    "L = g * h\n",
    "\n",
    "L.backward_pass() # Updates dL/dg and dL/dh\n",
    "h.backward_pass() # Updates dL/dc\n",
    "\n",
    "##WRONG ORDER\n",
    "c.backward_pass() # Incorrect because dL/dc hasn't accounted for dL/dg\n",
    "g.backward_pass()\n",
    "\n",
    "## CORRECT ORDER\n",
    "g.backward_pass() # Updates dL/dc\n",
    "c.backward_pass() # Updates dL/da and dL/db\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2W8is1bUmdDx"
   },
   "source": [
    "#### **Q5:** Backward method\n",
    "Fill in the `backward` method for `AutogradValue`. Your backward method should call `backward_pass` on each operation used to compute the loss (`self` is the loss value). Some important things to keep in mind:\n",
    "- `backward_pass` should only be called **once** on each operation\n",
    "- `backward_pass` must be called on **every child** of an operation before it can be called on the operation.\n",
    "- You should not try to call `backward_pass` on values that aren't instances of `AutogradValue`, even though they might be stored in `operation.parents`\n",
    "\n",
    "***Hint:** We discussed a simple approach to this problem in class! We won't score efficiency in grading, but it still might be worth optimizing this function a bit.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7ylfIvNmdDx"
   },
   "outputs": [],
   "source": [
    "def backward(self):\n",
    "    # We call backward on the loss, so dL/dL = 1\n",
    "    self.grad = 1.\n",
    "\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "AutogradValue.backward = backward\n",
    "# Test our implementation\n",
    "test_backward(AutogradValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yPfKa3vmdDx"
   },
   "source": [
    "Now we can use our `AutogradValue` class to compute derivatives!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WvEONxmrmdDx"
   },
   "outputs": [],
   "source": [
    "a = AutogradValue(5)\n",
    "b = AutogradValue(2)\n",
    "\n",
    "L = -log(5 *b + a)\n",
    "L.backward()\n",
    "print(a.grad, b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PqgOxjJmdDx"
   },
   "source": [
    "If we want to train a neural network using our automatic differentiation implementation, we're going to want to be able to use numpy to do matrix operations. Fortunately, the our `AutogradValue` class is (mostly) compatible with numpy!\n",
    "\n",
    "We can create arrays of `AutogradValue` and take derivatives as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UmE9DvVnmdDx"
   },
   "outputs": [],
   "source": [
    "a = np.array([AutogradValue(5), AutogradValue(2)])\n",
    "L = np.dot(a, a)\n",
    "L.backward()\n",
    "print('Gradient for a', a[0].grad, a[1].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Bp0nWfymdDx"
   },
   "source": [
    "It would be a bit tedious to define every AutogradValue array in this way, so let's write some convinience functions to make doing automatic differentiation with numpy easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_hhoyzVmdDx"
   },
   "source": [
    "#### **Q6:** Array support\n",
    "Complete the following two functions `wrap_array` and `unwrap_gradient`.\n",
    "\n",
    "`wrap_array` should take a numpy array of floats and return a new array where every element has been made into an `AutogradValue`.\n",
    "\n",
    "`unwrap_gradient` should take a numpy array of `AutogradValue` and return a new array of floats, where every element is the extracted `grad` property of the corresponding element from the original array.\n",
    "\n",
    "Both of these functions should work on 2-D arrays (matrices) at a minimum (but more general solutions that support 1 and/or >2 dimensional arrays are also possible).\n",
    "\n",
    "***Hint:** You can create an array from nested lists as `np.array([[1, 2], [3, 4]])`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0KG9iTTTmdDx"
   },
   "outputs": [],
   "source": [
    "def wrap_array(a):\n",
    "    '''\n",
    "    Wraps the elements of an array with AutogradValue\n",
    "\n",
    "    Args:\n",
    "        a (array of float): The array to wrap\n",
    "    Returns:\n",
    "        g (array of AutogradValue): An array g, such that g[i,j] = AutogradValue(a[i,j])\n",
    "    '''\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "def unwrap_gradient(a):\n",
    "    '''\n",
    "    Unwraps the gradient of an array with AutogradValues\n",
    "\n",
    "    Args:\n",
    "        a (array of AutogradValue): The array to unwrap\n",
    "    Returns:\n",
    "        g (array of float): An array g, such that g[i,j] = a[i,j].grad\n",
    "    '''\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "\n",
    "test_wrap_unwrap(wrap_array, unwrap_gradient, AutogradValue)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYx3pLl0mdDx"
   },
   "source": [
    "## Part 2: Training a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cgidu0mmdDx"
   },
   "source": [
    "Now we're ready to test out our `AutogradValue` implementation in the context it's designed for: neural networks! Below is a (slightly modified) version of the neural network class we wrote for the last homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgYBPXPXmdDx"
   },
   "outputs": [],
   "source": [
    "\n",
    "def pad(a):\n",
    "    # Pads an array with a column of 1s (for bias term)\n",
    "    return a.pad() if isinstance(a, AutogradValue) else np.pad(a, ((0, 0), (0, 1)), constant_values=1., mode='constant')\n",
    "\n",
    "def matmul(a, b):\n",
    "    # Multiplys two matrices\n",
    "    return _matmul(a, b) if isinstance(a, AutogradValue) else np.matmul(a, b)\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Computes the sigmoid function\n",
    "    return 1. / (1. + (-x).exp()) if isinstance(a, AutogradValue) else 1. / (1. + exp(-x))\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, dims, hidden_sizes=[]):\n",
    "        # Create a list of all layer dimensions (including input and output)\n",
    "        sizes = [dims] + hidden_sizes + [1]\n",
    "        # Create each layer weight matrix (including bias dimension)\n",
    "        self.weights = [np.random.normal(scale=1., size=(i + 1, o))\n",
    "                        for (i, o) in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def prediction_function(self, X, w):\n",
    "        '''\n",
    "        Get the result of our base function for prediction (i.e. x^t w)\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "            w (list of arrays): A list of weight matrices\n",
    "        Returns:\n",
    "            pred (array): An N x 1 matrix of f(X).\n",
    "        '''\n",
    "        # Iterate through the weights of each layer and apply the linear function and activation\n",
    "        for wi in w[:-1]:\n",
    "            X = pad(X) # Only if we're using bias\n",
    "            X = sigmoid(matmul(X, wi))\n",
    "\n",
    "        # For the output layer, we don't apply the activation\n",
    "        X = pad(X)\n",
    "        return matmul(X, w[-1])\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict labels given a set of inputs.\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "        Returns:\n",
    "            pred (array): An N x 1 column vector of predictions in {0, 1}\n",
    "        '''\n",
    "        return (self.prediction_function(X, self.weights) > 0)\n",
    "\n",
    "    def predict_probability(self, X):\n",
    "        '''\n",
    "        Predict the probability of each class given a set of inputs\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "        Returns:\n",
    "            probs (array): An N x 1 column vector of predicted class probabilities\n",
    "        '''\n",
    "        return sigmoid(self.prediction_function(X, self.weights))\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        '''\n",
    "        Compute the accuracy of the model's predictions on a dataset\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "            y (array): A length N vector of labels.\n",
    "        Returns:\n",
    "            acc (float): The accuracy of the classifier\n",
    "        '''\n",
    "        y = y.reshape((-1, 1))\n",
    "        return (self.predict(X) == y).mean()\n",
    "\n",
    "    def nll(self, X, y, w=None):\n",
    "        '''\n",
    "        Compute the negative log-likelihood loss.\n",
    "\n",
    "        Args:\n",
    "            X (array): An N x d matrix of observations.\n",
    "            y (array): A length N vector of labels.\n",
    "            w (array, optional): A (d+1) x 1 matrix of weights.\n",
    "        Returns:\n",
    "            nll (float): The NLL loss\n",
    "        '''\n",
    "        if w is None:\n",
    "            w = self.weights\n",
    "\n",
    "        y = y.reshape((-1, 1))\n",
    "        xw = self.prediction_function(X, w)\n",
    "        py = sigmoid((2 * y - 1) * xw)\n",
    "        return -(log(py)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qS-UQZImdDx"
   },
   "source": [
    "#### **Q7:** Autograd for a neural network\n",
    "Implement an `nll_and_grad` method for the `NeuralNetwork` class using your reverse-mode automatic differentiation implmentation to compute the gradient with respect to each weight matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tjPd3UnXmdDx"
   },
   "outputs": [],
   "source": [
    "def nll_and_grad(self, X, y):\n",
    "    '''\n",
    "    Get the negative log-likelihood loss and its gradient\n",
    "\n",
    "    Args:\n",
    "        X (array): An N x d matrix of observations.\n",
    "        y (array): A length N vector of labels\n",
    "    Returns:\n",
    "        nll (float): The negative log-likelihood\n",
    "        grads (list of arrays): A list of the gradient of the nll with respect\n",
    "                                to each value in self.weights.\n",
    "    '''\n",
    "    ## YOUR CODE HERE\n",
    "    loss =\n",
    "    grads =\n",
    "    return loss.value, grads\n",
    "\n",
    "NeuralNetwork.nll_and_grad = nll_and_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjQ3HlJmmdDx"
   },
   "source": [
    "We now have everything in place to train a neural network from scratch! Let's try it on our tiny dataset. Feel free to change the inputs.\n",
    "\n",
    "***Hint**: If this give very poor results and/or runs very slowly, make sure to carefully check the shape of each operation in your code to make sure it matches your expectation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2yR_dWYmdDx"
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(100, noise=0.1)\n",
    "model = NeuralNetwork(2, [5, 5])\n",
    "gradient_descent(model, X, y, lr=3e-2, steps=250)\n",
    "\n",
    "print('Model accuracy: %.3f' % model.accuracy(X, y))\n",
    "plot_boundary(model, X, y)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
