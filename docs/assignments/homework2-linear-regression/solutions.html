<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.319">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>CS 152: Neural Networks - Homework 2: Linear regression and maximum likelihood</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">CS 152: Neural Networks</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../calendar/calendar.html" rel="" target="">
 <span class="menu-text">Calendar</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/orgs/CS152-Neural-Networks-Fall-2023/repositories" rel="" target="">
 <span class="menu-text">Homeworks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://harveymuddcollege.instructure.com/courses/615/" rel="" target="">
 <span class="menu-text">Canvas</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/orgs/CS152-Neural-Networks-Fall-2023/" rel="" target="">
 <span class="menu-text">Github</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#part-1-linear-regression" id="toc-part-1-linear-regression" class="nav-link active" data-scroll-target="#part-1-linear-regression">Part 1: Linear regression</a></li>
  <li><a href="#part-2-applications-to-real-data" id="toc-part-2-applications-to-real-data" class="nav-link" data-scroll-target="#part-2-applications-to-real-data">Part 2: Applications to real data</a></li>
  <li><a href="#part-3-maximum-likelihood-training" id="toc-part-3-maximum-likelihood-training" class="nav-link" data-scroll-target="#part-3-maximum-likelihood-training">Part 3: Maximum Likelihood Training</a>
  <ul class="collapse">
  <li><a href="#background" id="toc-background" class="nav-link" data-scroll-target="#background">Background:</a></li>
  <li><a href="#laplace-maximum-likelihood-estimation" id="toc-laplace-maximum-likelihood-estimation" class="nav-link" data-scroll-target="#laplace-maximum-likelihood-estimation">Laplace Maximum Likelihood Estimation</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><strong>Homework 2:</strong> Linear regression and maximum likelihood</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Run me first!</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">'darkgrid'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In this homework, we will use the following convention for dimentionality:</p>
<p><span class="math inline">\(N:\quad\text{Number of observations in a dataset, so } \mathcal{D} = \{ (\mathbf{x}_1, y_1),\ (\mathbf{x}_2, y_2),\ ... \,(\mathbf{x}_N, y_N) \}\)</span></p>
<p><span class="math inline">\(d:\quad\ \text{Dimension of input (number of features), so } \mathbf{x}_i \in \mathbb{R}^d\)</span></p>
<section id="part-1-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="part-1-linear-regression">Part 1: Linear regression</h2>
<p>Let’s begin by reviewing the context of linear regression.</p>
<p>Recall that the linear regression model makes predictions of the following form:</p>
<p><span class="math display">\[f(\mathbf{x})=\mathbf{x}^T\mathbf{w} + b\]</span></p>
<p>Or if we consdier the augmented representation:</p>
<p><span class="math display">\[\mathbf{x} \rightarrow \begin{bmatrix}\mathbf{x} \\ 1 \end{bmatrix} = \begin{bmatrix}x_1 \\ x_2 \\ \vdots \\ x_d \\ 1 \end{bmatrix},\quad \mathbf{w} \rightarrow \begin{bmatrix}\mathbf{w} \\ b \end{bmatrix} = \begin{bmatrix}w_1 \\ w_2 \\ \vdots \\ w_d \\ b \end{bmatrix}\]</span> The we can use the more convinient linear form: <span class="math display">\[f(\mathbf{x}) = \mathbf{x}^T\mathbf{w}\]</span></p>
<p><strong>Q1:</strong> Write a function that takes in an input <span class="math inline">\((\mathbf{x})\)</span> and a set of weights <span class="math inline">\((\mathbf{w})\)</span> and makes a predicion using the function above. <strong>Your implementation should assume that the bias <span class="math inline">\((b)\)</span> is included in the weights <span class="math inline">\((\mathbf{w})\)</span> and thus should <em>augment</em> the input to account for this</strong>.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Test inputs</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="op">-</span><span class="dv">3</span>, <span class="dv">5</span>])</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.array([<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>, <span class="fl">0.5</span>, <span class="dv">2</span>])</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="op">-</span><span class="fl">2.5</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> linear_regression(x, w):</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.pad(x, ((<span class="dv">0</span>, <span class="dv">1</span>)), constant_values<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.dot(x, w)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">## Validate the function</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> y <span class="op">==</span> linear_regression(x, w)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As discussed in class, we can compactly refer to an entire <em>dataset</em> of inputs and outputs using the notation <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> respectively. Our convention will be that <em>row</em> <span class="math inline">\(i\)</span> of <span class="math inline">\(\mathbf{X}\)</span> will correspond to the <span class="math inline">\(i^{th}\)</span> observed input <span class="math inline">\(\mathbf{x}_i\)</span>, while the corresponding entry <span class="math inline">\(y_i\)</span> of <span class="math inline">\(\mathbf{y}\)</span> is the observed output. In other words <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are defined as: <span class="math display">\[
\mathbf{X} =
\begin{bmatrix} X_{11} &amp; X_{12} &amp; \dots  &amp; X_{1d} &amp; 1 \\
                X_{21} &amp; X_{22} &amp; \dots  &amp; X_{2d} &amp; 1 \\
                \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
                X_{N1} &amp; X_{N2} &amp; \dots  &amp; X_{Nd} &amp; 1 \\  
                \end{bmatrix} = \begin{bmatrix} \mathbf{x}_1^T \\ \mathbf{x}_2^T \\ \vdots \\ \mathbf{x}_N^T \end{bmatrix}, \quad
                \mathbf{y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_N \end{bmatrix}
\]</span></p>
<p>With this notation, we can make predictions for an entire set of inputs as: <span class="math display">\[f(\mathbf{X}) = \mathbf{X}\mathbf{w}\]</span> Where the output is now a <em>vector</em> of predictions. We see that each entry corresponds to the prediction for input <span class="math inline">\(\mathbf{x}_i\)</span>: <span class="math display">\[f(\mathbf{X})_i = \mathbf{x}_i^T\mathbf{w} = f(\mathbf{x}_i)\]</span></p>
<p><strong>Q2:</strong> Modify (if needed) your linear regression prediction function to accept a set of inputs as matrix as described above and return a vector of predictions. Once again you should <em>not</em> assume the data includes the extra bias dimension. Then plot the prediction function given by the provided weights on the range <span class="math inline">\([0, 5]\)</span>. <em>Recall that <code>np.linspace(a, b, n)</code> produces a vector of <code>n</code> equally spaced numbers between <code>a</code> and <code>b</code>.</em></p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Test inputs</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.array([<span class="fl">0.5</span>, <span class="dv">2</span>])</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> linear_regression(x, w):</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Account for the extra dimension in pad</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.pad(x, ((<span class="dv">0</span>, <span class="dv">0</span>), (<span class="dv">0</span>, <span class="dv">1</span>)), constant_values<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.dot(x, w)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">200</span>).reshape((<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> linear_regression(X, w)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>plt.plot(X.flatten(), y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-4-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Recall that <em>ordinary least squares</em> finds the parameter <span class="math inline">\(\mathbf{w}\)</span> that minimizes the <em>sum of squared errors</em> between the true outputs <span class="math inline">\(\mathbf{y}\)</span> and predictions. <span class="math display">\[\min_{\mathbf{w}} \sum_{i=1}^N \left(\mathbf{x}_i^T\mathbf{w} - y_i\right)^2\]</span></p>
<p>Observe that this is the same formula as the error we used for evaluating Richardson iteration in the last homework, we’ve simply renamed the variables from <span class="math inline">\((\mathbf{A}, \mathbf{x}, \mathbf{b})\)</span> to <span class="math inline">\((\mathbf{X}, \mathbf{w}, \mathbf{y})\)</span>!</p>
<p>We’ve now seen 3 equivalent ways to write the same formula. From most compact to least compact these are: <span class="math display">\[\textbf{1: }  \|\mathbf{X}\mathbf{w} - \mathbf{y}\|_2^2 \quad \textbf{2: } \sum_{i=1}^N \left(\mathbf{x}_i^T\mathbf{w} - y_i\right)^2 \quad \textbf{3: } \sum_{i=1}^n \left(\sum_{j=1}^n X_{ij}w_j - y_i\right)^2\]</span></p>
<p><em>Note: Notation can be confusing. If you are having trouble following, don’t hesitate to ask for help!</em></p>
<p><strong>Q3:</strong> Using the gradient formula you derived in the last homework, write a function that returns both the mean squared error <em>and</em> the gradient of the error with respect to <span class="math inline">\(\mathbf{w}\)</span>, given <span class="math inline">\(\mathbf{w}\)</span>, <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>. <em>Hint: don’t forget about to augment X when computing the gradient!</em></p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_and_grad(w, X, y):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    error <span class="op">=</span> linear_regression(X, w) <span class="op">-</span> y</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    mse <span class="op">=</span> np.mean(error <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    grad_w <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> np.dot(error, np.pad(X, [(<span class="dv">0</span>,<span class="dv">0</span>), (<span class="dv">0</span>,<span class="dv">1</span>)], constant_values<span class="op">=</span><span class="dv">1</span>)) <span class="op">/</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mse, grad_w</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Recall that we can use the <em>gradient descent</em> algorithm to find the input that minimizes a function, using the corresponding gradient function.</p>
<p>Given an initial guess of the optimal input, <span class="math inline">\(\mathbf{w}^{(0)}\)</span>, gradient descent uses the following update to improve the guess: <span class="math display">\[\mathbf{w}^{(i+1)} \longleftarrow \mathbf{w}^{(i)} - \alpha \nabla f(\mathbf{w}),\]</span> where <span class="math inline">\(\alpha\)</span> is the <em>learning rate</em> or <em>step size</em>.</p>
<p><strong>Q4:</strong> Write a function to perform gradient descent. The function should take as input: - <code>value_and_grad</code>: A function that produces the output and gradient of the function to optimize (e.g.&nbsp;the <code>mse_and_grad</code>) - <code>w0</code>: An inital guess <span class="math inline">\(\mathbf{w}^{(0)}\)</span> - <code>lr</code>: The learning rate <span class="math inline">\((\alpha)\)</span> - <code>niter</code>: The number of updates to perform - <code>*args</code>: Any additional argumets to pass to <code>value_and_grad</code> (e.g.&nbsp;<code>X</code> and <code>y</code>)</p>
<p>The function should return: - <code>w</code>: The final guess - <code>losses</code>: A list (or array) that tracks the value of the function at each update</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent(value_and_grad, w0, lr, niter, <span class="op">*</span>args):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the inital loss</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    initial_loss, grad <span class="op">=</span> value_and_grad(w0, <span class="op">*</span>args)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> [initial_loss]</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> w0</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(niter):</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> w <span class="op">-</span> lr <span class="op">*</span> grad</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        loss, grad <span class="op">=</span> value_and_grad(w, <span class="op">*</span>args)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        losses.append(loss)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> w, losses</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="part-2-applications-to-real-data" class="level2">
<h2 class="anchored" data-anchor-id="part-2-applications-to-real-data">Part 2: Applications to real data</h2>
<p>Now that we’ve setup a full implementation of linear regression, let’s test it on a real dataset. We’ll use the MPG dataset that we in class. The following code will load the data.</p>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.genfromtxt(<span class="st">'auto-mpg.csv'</span>, delimiter<span class="op">=</span><span class="st">','</span>, missing_values<span class="op">=</span>[<span class="st">'?'</span>], filling_values<span class="op">=</span>[<span class="dv">0</span>])</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># MPG is the output value</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> <span class="st">'MPG'</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[:, <span class="dv">0</span>]</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># The other variables are in inputs in the order listed</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">'displacement'</span>, <span class="st">'weight'</span>, <span class="st">'acceleration'</span>, <span class="st">'year'</span>]</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[:, [<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>]]</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> (X <span class="op">-</span> X.mean(axis<span class="op">=</span><span class="dv">0</span>)) <span class="op">/</span> X.std(axis<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s start by fitting a model that just uses the feature <code>weight</code>.</p>
<p><strong>Q5:</strong> Use the <code>gradient_descent</code> and <code>mse_and_grad</code> functions you wrote above to fit a linear regression model that takes in a car’s weight and predicts its MPG rating. Start with the weights equal to <code>0</code> and perform <code>50</code> updates of gradient descent with a learning rate of <code>0.1</code>. Plot the loss (MSE) as a function of the number of updates.</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>Xweight <span class="op">=</span> X[:, <span class="dv">1</span>:<span class="dv">2</span>]</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>w0 <span class="op">=</span> np.zeros((<span class="dv">2</span>,))</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>(w, losses) <span class="op">=</span> gradient_descent(mse_and_grad, w0, <span class="fl">0.1</span>, <span class="dv">50</span>, Xweight, y)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co">### PLOTTING CODE HERE</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">4</span>))</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>plt.plot(losses)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(losses[<span class="op">-</span><span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>18.780939855850434</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-8-output-2.png" class="img-fluid"></p>
</div>
</div>
<p><strong>Q6:</strong> Plot a scatterplot of weight vs.&nbsp;MPG. Then on the same plot, plot the prediction function for the linear regression model you just fit on the input range <span class="math inline">\([-3, 3]]\)</span>.</p>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>plt.scatter(Xweight.flatten(), y)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>).reshape((<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>plt.plot(x.flatten(), linear_regression(x, w), c<span class="op">=</span><span class="st">'r'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-9-output-1.png" class="img-fluid"></p>
</div>
</div>
<p><strong>Q7:</strong> Repeat <strong>Q5</strong> using all 5 features and compare the final loss to the final loss using only <code>weight</code> as an input. Describe any differences you observe.</p>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>w0 <span class="op">=</span> np.zeros((<span class="dv">5</span>,))</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>w, losses <span class="op">=</span> gradient_descent(mse_and_grad, w0, <span class="fl">0.1</span>, <span class="dv">50</span>, X, y)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">4</span>))</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>plt.plot(losses)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(losses[<span class="op">-</span><span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>12.013140556897708</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-10-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>We see that in both cases the loss converges very quickly, but with 5 features, the final loss is significantly smaller (12 vs 18)</p>
<p>In machine learning we are typically less interested in how our model predicts the data we’ve already seen than we are in how well it makes predictions for <em>new</em> data. One way to estimate how well our model our model will generalize to new data is to <em>hold out</em> data while fitting our model. To do this we will split our dataset into two smaller datasets: a <em>training dataset</em> that we will use to fit our model, and a <em>test</em> or <em>held-out</em> dataset that we will only use to evaluate our model. By computing the loss on this test dataset, we can get a sense of how well our model will make prediction for new data.</p>
<p><span class="math display">\[\mathcal{D} = \{ (\mathbf{x}_1, y_1),\ (\mathbf{x}_2, y_2),\ ... \,(\mathbf{x}_N, y_N) \}\quad \longrightarrow \quad
\mathcal{D}_{train} = \{ (\mathbf{x}_1, y_1),\ (\mathbf{x}_2, y_2),\ ... \,(\mathbf{x}_{Ntrain}, y_{Ntrain}) \},\  
\mathcal{D}_{test} = \{ (\mathbf{x}_1, y_1),\ (\mathbf{x}_2, y_2),\ ... \,(\mathbf{x}_{Ntest}, y_{Ntest}) \}
\]</span></p>
<p><strong>Q8:</strong> Split the MPG dataset into <em>training</em> and <em>test</em> datasets. Use 70% of the observations for training and 30% for test. Then repeat <strong>Q5</strong> to fit a linear regression model on <em>just the training dataset</em> using only the <code>weight</code> feature (you don’t need to plot the loss for this question). Report the final loss (MSE) on the training dataset and on the test dataset.</p>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>Xweight_train <span class="op">=</span> Xweight[:<span class="bu">int</span>(X.shape[<span class="dv">0</span>] <span class="op">*</span> <span class="fl">.7</span>)]</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> y[:<span class="bu">int</span>(X.shape[<span class="dv">0</span>] <span class="op">*</span> <span class="fl">.7</span>)]</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>Xweight_test <span class="op">=</span> Xweight[<span class="bu">int</span>(X.shape[<span class="dv">0</span>] <span class="op">*</span> <span class="fl">.7</span>):]</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> y[<span class="bu">int</span>(X.shape[<span class="dv">0</span>] <span class="op">*</span> <span class="fl">.7</span>):]</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>w0 <span class="op">=</span> np.zeros((<span class="dv">2</span>,))</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>w, losses <span class="op">=</span> gradient_descent(mse_and_grad, w0, <span class="fl">0.1</span>, <span class="dv">50</span>, Xweight_train, y_train)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>mse_train, _ <span class="op">=</span> mse_and_grad(w, Xweight_train, y_train)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>mse_test, _ <span class="op">=</span> mse_and_grad(w, Xweight_test, y_test)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Loss on training data: </span><span class="sc">%.4f</span><span class="st">, loss on test data: </span><span class="sc">%.4f</span><span class="st">'</span> <span class="op">%</span> (mse_train, mse_test))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on training data: 8.4172, loss on test data: 60.1368</code></pre>
</div>
</div>
<p><strong>Q9:</strong> Repeat <strong>Q8</strong> using the all 5 features. Compare the results to the model using only <code>weight</code>.</p>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> X[:<span class="bu">int</span>(X.shape[<span class="dv">0</span>] <span class="op">*</span> <span class="fl">.7</span>)]</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> y[:<span class="bu">int</span>(X.shape[<span class="dv">0</span>] <span class="op">*</span> <span class="fl">.7</span>)]</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> X[<span class="bu">int</span>(X.shape[<span class="dv">0</span>] <span class="op">*</span> <span class="fl">.7</span>):]</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> y[<span class="bu">int</span>(X.shape[<span class="dv">0</span>] <span class="op">*</span> <span class="fl">.7</span>):]</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>w0 <span class="op">=</span> np.zeros((<span class="dv">5</span>,))</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>w, losses <span class="op">=</span> gradient_descent(mse_and_grad, w0, <span class="fl">0.1</span>, <span class="dv">50</span>, X_train, y_train)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>mse_train, _ <span class="op">=</span> mse_and_grad(w, X_train, y_train)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>mse_test, _ <span class="op">=</span> mse_and_grad(w, X_test, y_test)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Loss on training data: </span><span class="sc">%.4f</span><span class="st">, loss on test data: </span><span class="sc">%.4f</span><span class="st">'</span> <span class="op">%</span> (mse_train, mse_test))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on training data: 7.5947, loss on test data: 43.7603</code></pre>
</div>
</div>
<p>We see that with all 5 features, both the training and the test loss are lower than with only weight!</p>
</section>
<section id="part-3-maximum-likelihood-training" class="level2">
<h2 class="anchored" data-anchor-id="part-3-maximum-likelihood-training">Part 3: Maximum Likelihood Training</h2>
<section id="background" class="level3">
<h3 class="anchored" data-anchor-id="background">Background:</h3>
<p>We saw in lecture that we can view linear regression as the following probibalistic model: <span class="math display">\[
y_i \sim \mathcal{N}\big(\mathbf{x}_i^T \mathbf{w},\ \sigma^2\big)
\]</span> Where <span class="math inline">\(\mathcal{N}(\mu, \sigma)\)</span> is the <em>Normal distribution</em>, which has the following probability density function: <span class="math display">\[
p(y\mid \mu, \sigma) = \frac{1}{\sigma \sqrt{2 \pi}} \text{exp}\bigg(-\frac{1}{2\sigma^2} (y -\mu)^2\bigg)
\]</span> We saw that a reasonable way to choose the optimal <span class="math inline">\(\mathbf{w}\)</span> is to <em>maximize the likelihood</em> of our observed dataset, which is equivalent to <em>minimizing the negative log likelihood</em>: <span class="math display">\[\underset{\mathbf{w}}{\text{argmax}} \prod_{i=1}^N p(y_i\mid \mathbf{x}_i, \mathbf{w}, \sigma) =
\underset{\mathbf{w}}{\text{argmin}} -\sum_{i=1}^N \log p(y_i\mid \mathbf{x}_i, \mathbf{w}, \sigma) = \underset{\mathbf{w}}{\text{argmin}}\ \frac{1}{2\sigma^2}\sum_{i=1}^N  (y_i -\mathbf{x}_i^T\mathbf{w})^2 + C
\]</span> Where <span class="math inline">\(C\)</span> is a constant: <span class="math inline">\(C = N \log \sigma \sqrt{2\pi}\)</span>. We further saw that this is equivalent to minimizing the <em>mean squared error</em> for any choice of <span class="math inline">\(\sigma\)</span>.</p>
</section>
<section id="laplace-maximum-likelihood-estimation" class="level3">
<h3 class="anchored" data-anchor-id="laplace-maximum-likelihood-estimation">Laplace Maximum Likelihood Estimation</h3>
<p>A natural question about the above model is: why choose the Normal distribution? In principal, we could define a linear model using <em>any</em> distribution over the real numbers <span class="math inline">\(\mathbb{R}\)</span>. Let’s explore an alternative choice: the <em>Laplace distribution</em> (<a href="https://en.wikipedia.org/wiki/Laplace_distribution">Wiki</a>). The Laplace distribution <span class="math inline">\(L(\mu, a)\)</span> has the following PDF:</p>
<p><span class="math display">\[p(y\mid \mu, a) = \frac{1}{2a} \exp\bigg(- \frac{|y-\mu|}{a} \bigg) \]</span></p>
<p>As for the normal distribution <span class="math inline">\(\mu\)</span> is the mean, while <span class="math inline">\(a\)</span> defines the “width” of the distribution (analogous to variance for the Normal). We can compare the two distributions visually:</p>
<div class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">200</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>mu, sigma2_or_a <span class="op">=</span> <span class="dv">0</span>, <span class="dv">1</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>p_y_normal <span class="op">=</span> (<span class="dv">1</span> <span class="op">/</span> np.sqrt(sigma2_or_a <span class="op">*</span> <span class="dv">2</span> <span class="op">*</span> np.pi)) <span class="op">*</span> np.exp(<span class="op">-</span> (<span class="fl">0.5</span> <span class="op">/</span> sigma2_or_a) <span class="op">*</span> (y <span class="op">-</span> mu) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>p_y_laplace <span class="op">=</span> (<span class="dv">1</span> <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> sigma2_or_a)) <span class="op">*</span> np.exp(<span class="op">-</span> (<span class="dv">1</span> <span class="op">/</span> sigma2_or_a) <span class="op">*</span> np.<span class="bu">abs</span>(y <span class="op">-</span> mu))</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">4</span>,<span class="dv">3</span>))</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>plt.plot(y, p_y_normal, label<span class="op">=</span><span class="vs">r"Normal PDF"</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>plt.plot(y, p_y_laplace, label<span class="op">=</span><span class="vs">r"Laplace PDF"</span>)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'$y$'</span>)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'$p(y)$'</span>)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="cf">pass</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-13-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Let’s now consider the Laplace version of our probabilistic linear model: <span class="math display">\[
y_i \sim L\big(\mathbf{x}_i^T \mathbf{w},\ a\big)
\]</span> In this case that the negative log-likelihood is defined as: <span class="math display">\[\mathbf{NLL}(\mathbf{w}, b, \mathbf{X}, \mathbf{y}) = -\sum_{i=1}^N \log p(y_i\mid \mathbf{x}_i, \mathbf{w}, a)\]</span></p>
<p><strong>Q10:</strong> Write out the negative log-likelihood for this model in terms of <span class="math inline">\(\mathbf{w}, a, \mathbf{X}, y\)</span> using the Laplace PDF shown above.</p>
<p>We can simply replace <span class="math inline">\(p(y_i|\mathbf{x}_i, a)\)</span> with the formula given above so we get: <span class="math display">\[\mathbf{NLL}(\mathbf{w}, a, \mathbf{X}, \mathbf{y}) = -\sum_{i=1}^N \log \frac{1}{2a} \exp \bigg(-\frac{|y_i-\mathbf{x}_i^T\mathbf{w}|}{a}\bigg)\]</span></p>
<p><span class="math display">\[ = \frac{1}{a}\sum_{i=1}^N |y_i-\mathbf{x}_i^T\mathbf{w}| + N\log 2a\]</span></p>
<p>Note that if we drop the constants, we would call this loss the <em>sum absolute error</em>.</p>
<p><strong>Q11:</strong> Find the gradient with respect to <span class="math inline">\(\mathbf{w}\)</span> of the negative log-likelihood for this model. <em>Hint: remember that the gradient is the vector of partial derivatives, so you may use the approach we used to find this vector for the squared error function in the previous homework!</em></p>
<p>You should use the following definition of the derivative of the absolute value <span class="math inline">\(|\cdot |\)</span> operator: <span class="math display">\[\frac{d}{dx}|x| = sign(x), \quad sign(x) = \begin{cases} +1 \quad \text{ if  } x &gt; 0 \\ -1 \quad \text{ if  } x &lt; 0 \\ \ \ \ 0 \quad \text{ if  } x = 0 \end{cases}\]</span> (Technically <span class="math inline">\(\frac{d}{dx}|x|\)</span> is undefined at <span class="math inline">\(x=0\)</span>, but it is convinient to assume <span class="math inline">\(\frac{d}{dx}|0|=0\)</span> in practice.)</p>
<p><span class="math display">\[\nabla_{\mathbf{w}}\mathbf{NLL}(\mathbf{w}, b, \mathbf{X}, \mathbf{y}) =  \frac{d}{d\mathbf{w}} \frac{1}{a}\sum_{i=1}^N |y_i-\mathbf{x}_i^T\mathbf{w}| + N\log 2a\]</span></p>
<p><span class="math display">\[=   \frac{1}{a}\sum_{i=1}^N \frac{d}{d\mathbf{w}}|y_i-\mathbf{x}_i^T\mathbf{w}|\]</span></p>
<p><span class="math display">\[=   \frac{1}{a}\sum_{i=1}^N sign(y_i-\mathbf{x}_i^T\mathbf{w})\frac{d}{d\mathbf{w}}(y_i-\mathbf{x}_i^T\mathbf{w})\]</span></p>
<p><span class="math display">\[=   \frac{1}{a}\sum_{i=1}^N sign(y_i-\mathbf{x}_i^T\mathbf{w})\mathbf{x}_i\]</span></p>
<p><strong>Q12:</strong> Using the formula you just derived, write a function that returns both the negative log-likelihood <em>and</em> the gradient of the negative log-likelihood with respect to <span class="math inline">\(\mathbf{w}\)</span>, given <span class="math inline">\(\mathbf{w}\)</span>, <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> (assume that <span class="math inline">\(a=1\)</span>). To make our loss more comparable to MSE, we’ll divide both outputs by <span class="math inline">\(N\)</span>.</p>
<div class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> nll_and_grad(w, X, y):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    N <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    error <span class="op">=</span> linear_regression(X, w) <span class="op">-</span> y</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    mae <span class="op">=</span> np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(error))</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    nll <span class="op">=</span> mae <span class="op">+</span> np.log(<span class="dv">2</span>) </span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    Xaug <span class="op">=</span> np.pad(X, [(<span class="dv">0</span>, <span class="dv">0</span>), (<span class="dv">0</span>, <span class="dv">1</span>)], constant_values<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    grad_w <span class="op">=</span> np.dot(np.sign(error), Xaug)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nll <span class="op">/</span> N, grad_w <span class="op">/</span> N</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Q13:</strong> Use the <code>gradient_descent</code> and <code>nll_and_grad</code> functions you wrote above to fit a linear regression model that takes in a car’s weight and predicts its MPG rating. Start with the weights equal to <code>0</code> and perform <code>50</code> updates of gradient descent with a learning rate of <code>1.</code>. Plot the loss (NLL) as a function of the number of updates. <em>Use the full dataset as in <strong>Q5</strong>.</em></p>
<div class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>Xweight <span class="op">=</span> X[:, <span class="dv">1</span>:<span class="dv">2</span>]</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>w0 <span class="op">=</span> np.zeros(<span class="dv">2</span>,)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co">### YOUR CODE HERE</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>w, losses <span class="op">=</span> gradient_descent(nll_and_grad, w0, <span class="fl">0.1</span>, <span class="dv">500</span>, Xweight, y)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>wmse, mselosses <span class="op">=</span> gradient_descent(mse_and_grad, w0, <span class="fl">0.1</span>, <span class="dv">50</span>, Xweight, y)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="co">### PLOTTING CODE HERE</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">4</span>))</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>plt.plot(losses, label<span class="op">=</span><span class="st">'Laplace NLL'</span>)</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>plt.plot(mselosses, label<span class="op">=</span><span class="st">'Mean squared error'</span>)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(losses[<span class="op">-</span><span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>3.262693397826824</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-15-output-2.png" class="img-fluid"></p>
</div>
</div>
<p><strong>Q14:</strong> Plot a scatterplot of weight vs.&nbsp;MPG. Then on the same plot, plot the prediction function for the linear regression model you just fit on the input range <span class="math inline">\([-3, 3]\)</span>. Describe any differences you see vs.&nbsp;the model you fit with MSE.</p>
<div class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">4</span>))</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>plt.scatter(Xweight[:,<span class="dv">0</span>], y)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>plt.plot(np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">200</span>), linear_regression(np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">200</span>).reshape((<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)), w), label<span class="op">=</span><span class="st">'Laplace NLL weights'</span>)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>plt.plot(np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">200</span>), linear_regression(np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">200</span>).reshape((<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)), wmse), label<span class="op">=</span><span class="st">'MSE weights'</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="52">
<pre><code>&lt;matplotlib.legend.Legend at 0x17f7c2b20&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-16-output-2.png" class="img-fluid"></p>
</div>
</div>
<p><strong>Q15:</strong> Using the parameters that you just found, plot the Laplace NLL loss as a function of the first entry in <code>w</code> (<code>w1</code> below), holding the other (the bias) constant. Then, using the same plot and parameters, plot the MSE loss as a function of the first entry in <code>w</code>.</p>
<div class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example of deconstructing and reconstructing the parameter vector.</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>w1, b <span class="op">=</span> w</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>wi <span class="op">=</span> np.array([w1, b])</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> [nll_and_grad(np.array([wi, b]), Xweight, y)[<span class="dv">0</span>] <span class="cf">for</span> wi <span class="kw">in</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">0</span>)]</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>mse_losses <span class="op">=</span> [mse_and_grad(np.array([wi, b]), Xweight, y)[<span class="dv">0</span>] <span class="cf">for</span> wi <span class="kw">in</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">0</span>)]</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">4</span>))</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>plt.plot(mse_losses, label<span class="op">=</span><span class="st">'Mean squared error'</span>)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>plt.plot(losses, label<span class="op">=</span><span class="st">'Laplace NLL'</span>)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'$w_1$'</span>)</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Loss'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="50">
<pre><code>Text(0.5, 0, 'Loss')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-17-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>In the cells below, copy your code for <strong>Q5</strong> and <strong>Q13</strong>. Try out different learning rates and and numbers of iterations. Observe what happens to the loss plot in each case.</p>
<p><strong>Q16:</strong> Based on what you’ve obsevered above and in the previous questions, why might you choose the Normal distribution or the Laplace distribution for linear regression?</p>
<p>We see that the MSE weights are more sensitive to outliers than the Laplace NLL weights.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>