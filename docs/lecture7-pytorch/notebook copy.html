<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.319">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>CS 152: Neural Networks - Lecture 7: PyTorch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">CS 152: Neural Networks</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../calendar/calendar.html" rel="" target="">
 <span class="menu-text">Calendar</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/orgs/CS152-Neural-Networks-Fall-2023/repositories" rel="" target="">
 <span class="menu-text">Homeworks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://harveymuddcollege.instructure.com/courses/615/" rel="" target="">
 <span class="menu-text">Canvas</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/orgs/CS152-Neural-Networks-Fall-2023/" rel="" target="">
 <span class="menu-text">Github</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-to-pytorch" id="toc-introduction-to-pytorch" class="nav-link active" data-scroll-target="#introduction-to-pytorch">Introduction to PyTorch</a></li>
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link" data-scroll-target="#gradient-descent">Gradient descent</a></li>
  <li><a href="#torch.nn" id="toc-torch.nn" class="nav-link" data-scroll-target="#torch.nn">torch.nn</a></li>
  <li><a href="#evaluating-models" id="toc-evaluating-models" class="nav-link" data-scroll-target="#evaluating-models">Evaluating models</a></li>
  <li><a href="#underfittting" id="toc-underfittting" class="nav-link" data-scroll-target="#underfittting">Underfittting</a></li>
  <li><a href="#overfitting" id="toc-overfitting" class="nav-link" data-scroll-target="#overfitting">Overfitting</a></li>
  <li><a href="#early-stopping" id="toc-early-stopping" class="nav-link" data-scroll-target="#early-stopping">Early stopping</a></li>
  <li><a href="#train-validation-and-test" id="toc-train-validation-and-test" class="nav-link" data-scroll-target="#train-validation-and-test">Train, validation and test</a></li>
  <li><a href="#cross-validation" id="toc-cross-validation" class="nav-link" data-scroll-target="#cross-validation">Cross validation</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Lecture 7: PyTorch</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="cell" data-execution_count="245">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tqdm</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.inspection <span class="im">import</span> DecisionBoundaryDisplay</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_boundary(model, X, y, alpha<span class="op">=</span><span class="dv">1</span>, title<span class="op">=</span><span class="st">''</span>):</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">xrange</span> <span class="op">=</span> (<span class="op">-</span>X[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">+</span> X[:, <span class="dv">0</span>].<span class="bu">max</span>()) <span class="op">/</span> <span class="dv">10</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    yrange <span class="op">=</span> (<span class="op">-</span>X[:, y].<span class="bu">min</span>() <span class="op">+</span> X[:, y].<span class="bu">max</span>()) <span class="op">/</span> <span class="dv">10</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    feature_1, feature_2 <span class="op">=</span> np.meshgrid(</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        np.linspace(X[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="bu">xrange</span>, X[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="bu">xrange</span>, <span class="dv">250</span>),</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        np.linspace(X[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> yrange, X[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> yrange, <span class="dv">250</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    grid <span class="op">=</span> np.vstack([feature_1.ravel(), feature_2.ravel()]).T</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> np.reshape(model.predict(torch.tensor(grid).<span class="bu">float</span>()).detach().numpy(), feature_1.shape)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    display <span class="op">=</span> DecisionBoundaryDisplay(</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        xx0<span class="op">=</span>feature_1, xx1<span class="op">=</span>feature_2, response<span class="op">=</span>y_pred</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    display.plot()</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    display.ax_.scatter(</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, alpha<span class="op">=</span>alpha, edgecolor<span class="op">=</span><span class="st">"black"</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    plt.title(title)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="introduction-to-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-pytorch">Introduction to PyTorch</h2>
<p>The most basic object in PyTorch is a <code>tensor</code>. Tensor objects behave much like the <code>AutogradValue</code> objects we are creating in the homework! We can create a <code>tensor</code> object with a given value as follows</p>
<div class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor(<span class="fl">4.</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="45">
<pre><code>tensor(4.)</code></pre>
</div>
</div>
<p>Performing basic operations on <code>tensor</code> objects gives tensor objects.</p>
<div class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> x <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">5</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>a</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="46">
<pre><code>tensor(21.)</code></pre>
</div>
</div>
<p><code>tensor</code> objects also support reverse-mode automatic differentiation! To use this, we must specify that we will want to compute the derivative with respect to a given <code>tensor</code>. We can do this with the <code>requires_grad</code> argument.</p>
<div class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor(<span class="fl">4.</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Once we have a <code>tensor</code> that <code>requires_grad</code>, we can perform operations on it to compute a loss.</p>
<div class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> x <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">5</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> torch.log(a) <span class="co"># Functions like log must be called through torch</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>L</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="48">
<pre><code>tensor(3.0445, grad_fn=&lt;LogBackward0&gt;)</code></pre>
</div>
</div>
<p>Once we have a loss running the backward pass is done exactly as in the homework. First we call <code>backward()</code> on the loss <code>tensor</code> object, then we can access the derivative through the <code>grad</code> property of <code>x</code>.</p>
<div class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>L.backward()</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>x.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="49">
<pre><code>tensor(0.3810)</code></pre>
</div>
</div>
<p>We can also create <code>tensor</code> objects that wrap arrays.</p>
<div class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor(np.array([<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>]))</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="50">
<pre><code>tensor([3, 4, 5])</code></pre>
</div>
</div>
<p>We can also just directly create tensors as we would numpy arrays</p>
<div class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor([<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>])</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="51">
<pre><code>tensor([3, 4, 5])</code></pre>
</div>
</div>
<p>Including convienience constructors.</p>
<div class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torch.ones((<span class="dv">5</span>,)))</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torch.zeros((<span class="dv">2</span>, <span class="dv">3</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([1., 1., 1., 1., 1.])
tensor([[0., 0., 0.],
        [0., 0., 0.]])</code></pre>
</div>
</div>
<p>Automatic differentiation still works for arrays. In this case it gives use the gradient of the loss (hence the <code>grad</code> property).</p>
<div class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor([<span class="fl">3.</span>, <span class="fl">4.</span>, <span class="fl">5.</span>], requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> torch.<span class="bu">sum</span>(x <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>L</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="53">
<pre><code>tensor(50., grad_fn=&lt;SumBackward0&gt;)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>L.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/var/folders/t9/fjsr8h1x7c7cg4vqw_xhjq2w0000gn/T/ipykernel_79454/323164392.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:491.)
  L.grad</code></pre>
</div>
</div>
<div class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>L.backward()</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>x.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="55">
<pre><code>tensor([ 6.,  8., 10.])</code></pre>
</div>
</div>
<p>We can convert <code>tensor</code> objects back to numpy by calling <code>x.detach().numpy()</code>. (<code>detach</code> removes the variable from any automatic differentiation computations)</p>
<div class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>x.detach().numpy()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="56">
<pre><code>array([3., 4., 5.], dtype=float32)</code></pre>
</div>
</div>
<p>At this point it’s probably worth remarking on where the name <code>tensor</code> comes from.</p>
<p>So far we’ve discussed 3 kinds of array objects - <strong>Scalars:</strong> which are just single values (0-dimensional) - <strong>Vectors:</strong> 1-dimensional arrays of numbers - <strong>Matrices:</strong> 2-dimensional arrays of numbers</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="image.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Alt text</figcaption><p></p>
</figure>
</div>
<p>A <strong>tensor</strong> is the generalization of a vector or matrix to <em>any</em> number of dimensions. For example, a 3-dimensional tensor can be seen in multiple ways.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="image-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Alt text</figcaption><p></p>
</figure>
</div>
<p>A <code>tensor</code> object can be created with any number of dimensions. For example, we could create a 2x2x2 tensor as:</p>
<div class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> torch.tensor([[[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>]], [[<span class="dv">5</span>, <span class="dv">6</span>], [<span class="dv">7</span>, <span class="dv">8</span>]]])</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>t</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="57">
<pre><code>tensor([[[1, 2],
         [3, 4]],

        [[5, 6],
         [7, 8]]])</code></pre>
</div>
</div>
<p>Or we could create the tensor in the image using <code>arange</code> and <code>reshape</code>.</p>
<div class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> torch.arange(<span class="dv">30</span>).reshape((<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">5</span>))</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>t</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="58">
<pre><code>tensor([[[ 0,  1,  2,  3,  4],
         [ 5,  6,  7,  8,  9]],

        [[10, 11, 12, 13, 14],
         [15, 16, 17, 18, 19]],

        [[20, 21, 22, 23, 24],
         [25, 26, 27, 28, 29]]])</code></pre>
</div>
</div>
<p>4-dimensional tensors can also be visualized</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="image-2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Alt text</figcaption><p></p>
</figure>
</div>
<div class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> torch.ones((<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">5</span>))</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>t.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="59">
<pre><code>torch.Size([3, 2, 4, 5])</code></pre>
</div>
</div>
<p>There are some notable differences between torch and numpy when it comes to operations. The important one to watch out for at this point is matrix multiplation. In numpy we accomplished with with <code>np.dot</code>:</p>
<div class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.ones((<span class="dv">4</span>, <span class="dv">5</span>))</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.ones((<span class="dv">5</span>, <span class="dv">2</span>))</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>np.dot(x, w)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="60">
<pre><code>array([[5., 5.],
       [5., 5.],
       [5., 5.],
       [5., 5.]])</code></pre>
</div>
</div>
<p>In PyTorch <code>torch.dot</code> only does vector dot products and thus only applies to 1-dimensional <code>tensor</code> objects:</p>
<div class="cell" data-execution_count="61">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.ones((<span class="dv">4</span>, <span class="dv">5</span>))</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> torch.ones((<span class="dv">5</span>, <span class="dv">2</span>))</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>torch.dot(x, w)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>RuntimeError: 1D tensors expected, but got 2D and 2D tensors</code></pre>
</div>
</div>
<p>Instead we use the <code>torch.matmul</code> function for this purpose</p>
<div class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>torch.matmul(x, w)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="62">
<pre><code>tensor([[5., 5.],
        [5., 5.],
        [5., 5.],
        [5., 5.]])</code></pre>
</div>
</div>
<p>PyTorch also has many handy built-in functions that numpy doesn’t have, such as <code>sigmoid</code>.</p>
<div class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">50</span>)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> torch.sigmoid(x)</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>plt.plot(x, s)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-21-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>This makes it very easy to implement something like logistic regression.</p>
<div class="cell" data-execution_count="64">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LogisticRegression:</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dims):</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> torch.ones((dims,), requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> torch.zeros((), requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_probability(<span class="va">self</span>, X):</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>        f_X <span class="op">=</span> torch.matmul(X, <span class="va">self</span>.weights) <span class="op">+</span> <span class="va">self</span>.bias</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sigmoid(f_X)</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.predict_probability(X) <span class="op">&gt;</span> <span class="fl">0.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s try loading a dataset, converting it to <code>tensor</code> and making predictions</p>
<div class="cell" data-execution_count="65">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(noise<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> torch.tensor(X).<span class="bu">float</span>(), torch.tensor(y)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(<span class="dv">2</span>)</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-23-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>When working with PyTorch, it is convention to separate the loss function from the model, where the loss function will just take predictions and labels.</p>
<div class="cell" data-execution_count="66">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> NLL(pred, y):</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    LL <span class="op">=</span> y <span class="op">*</span> torch.log(pred) <span class="op">+</span> (<span class="fl">1.</span> <span class="op">-</span> y) <span class="op">*</span> torch.log(<span class="fl">1.</span> <span class="op">-</span> pred)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>LL.<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="gradient-descent" class="level2">
<h2 class="anchored" data-anchor-id="gradient-descent">Gradient descent</h2>
<p>Gradient descent is also implemented in PyTorch in the <code>optim</code> module.</p>
<div class="cell" data-execution_count="67">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> optim</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Gradient descent works a bit differently in PyTorch than what we’ve seen. We first need to construct a gradient descent <em>object</em> which specifies which values we’re optimizing and what the learning rate will be. We specify the values to optimize by simply passing a list of weights/parameters to the constructor.</p>
<p>In PyTorch, basic gradient descent is encapsulated in the <code>optim.SGD</code> class (<code>SGD</code> stands for <em>stochastic gradient descent</em>, we’ll talk about what <em>stochastic</em> means in this context next week.)</p>
<div class="cell" data-execution_count="68">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD([model.weights, model.bias], lr<span class="op">=</span><span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Notice that this object doesn’t even take in the function we’re trying to optimize, only the inputs. We need to call the function ourselves <em>and</em> run <code>backward()</code> to compute the gradients.</p>
<div class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> model.predict_probability(X)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> NLL(predictions, y)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>loss.backward()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s look at our model <code>weights</code></p>
<div class="cell" data-execution_count="70">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>model.weights</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>model.weights.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="70">
<pre><code>tensor([-5.6526, 24.1355])</code></pre>
</div>
</div>
<p>We can take a single step of gradient descent using the <code>step</code> method of the optimizer.</p>
<div class="cell" data-execution_count="71">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>optimizer.step()</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>model.weights</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="71">
<pre><code>tensor([ 1.5653, -1.4135], requires_grad=True)</code></pre>
</div>
</div>
<p>We see that this actually updates the weights themselves!</p>
<p>It’s important to note that in PyTorch, calling <code>backward</code> does <strong>not</strong> clear the value stored in grad. So computing the gradient multiple times will result in updates to the gradient.</p>
<div class="cell" data-execution_count="72">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.weights.grad)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>NLL(model.predict_probability(X), y).backward()</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.weights.grad)</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>NLL(model.predict_probability(X), y).backward()</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.weights.grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([-5.6526, 24.1355])
tensor([-14.5242,  28.7704])
tensor([-23.3958,  33.4053])</code></pre>
</div>
</div>
<p>We can clear the stored gradients using the optimizer.</p>
<div class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>optimizer.zero_grad()</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.weights.grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>None</code></pre>
</div>
</div>
<p>So far we’ve only taking a single step of gradient descent. In order to run many steps, we need to write a loop to do everything we just saw.</p>
<div class="cell" data-execution_count="74">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model.predict_probability(X)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> NLL(predictions, y)</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>45.934814453125
40.60100555419922
32.82542037963867
30.3773136138916
28.918834686279297
28.118915557861328
27.602989196777344
27.248106002807617
26.989551544189453
26.794662475585938</code></pre>
</div>
</div>
<p>We should now see that our model has been optimized!</p>
<div class="cell" data-execution_count="75">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-33-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="torch.nn" class="level2">
<h2 class="anchored" data-anchor-id="torch.nn">torch.nn</h2>
<p>While PyTorch as a tool for automatic differentiation and optimization would be useful by itself. It actually gives us a lot more than that!</p>
<p>On of the most important features of PyTorch is its model-building tools in the <code>torch.nn</code> module. This gives us a lot of powerful features that we can use to build complex neural networks!</p>
<div class="cell" data-execution_count="76">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s start by building out logistic regression model in the <code>torch.nn</code> framwork. In order for a model to benefit from <code>torch.nn</code> our model class needs to inheret from <code>nn.Module</code></p>
<div class="cell" data-execution_count="77">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LogisticRegression(nn.Module):</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dims):</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> nn.Parameter(torch.ones((dims,)))</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> nn.Parameter(torch.zeros(()))</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sigmoid(torch.matmul(X, <span class="va">self</span>.weights) <span class="op">+</span> <span class="va">self</span>.bias)</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_probability(<span class="va">self</span>, X):</span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X)</span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X) <span class="op">&gt;</span> <span class="fl">0.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>There are 2 changes to note here. The first is that we wrapped our <code>weights</code> and <code>bias</code> terms in <code>nn.Parameter</code>. This tells PyTorch that these are the parameters we will want to optimize. We don’t need to specify <code>requires_grad</code> for parameters, PyTorch will take care of that for us.</p>
<p>The second is that we moved the implmentation of <code>predict_probability</code> to <code>forward</code>. In PyTorch models the <code>forward</code> method is special, it defines the model as a function. If we call the model as a function <code>forward</code> will be called internally.</p>
<div class="cell" data-execution_count="78">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(<span class="dv">2</span>)</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>model(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="78">
<pre><code>tensor([0.5652, 0.7431, 0.8238, 0.5468, 0.8991, 0.5719, 0.8651, 0.7784, 0.6153,
        0.7053, 0.7419, 0.3129, 0.4502, 0.5031, 0.4508, 0.7075, 0.7626, 0.4756,
        0.4158, 0.4693, 0.7904, 0.7241, 0.2430, 0.5297, 0.7944, 0.8496, 0.6825,
        0.6149, 0.6730, 0.5231, 0.6151, 0.6108, 0.5509, 0.7476, 0.6634, 0.8512,
        0.8117, 0.7527, 0.5092, 0.7742, 0.8012, 0.7604, 0.6411, 0.3242, 0.2805,
        0.4016, 0.6296, 0.3000, 0.7045, 0.7307, 0.7442, 0.8091, 0.5104, 0.8006,
        0.6166, 0.5173, 0.6404, 0.5779, 0.8199, 0.9030, 0.7872, 0.6059, 0.8091,
        0.9367, 0.5806, 0.7576, 0.8079, 0.3847, 0.4926, 0.4909, 0.8605, 0.6233,
        0.5605, 0.5980, 0.3629, 0.6874, 0.8761, 0.7593, 0.8267, 0.7912, 0.7147,
        0.4980, 0.4396, 0.9295, 0.8956, 0.5096, 0.7148, 0.7788, 0.7961, 0.2600,
        0.4964, 0.6008, 0.5710, 0.7882, 0.9379, 0.8573, 0.7131, 0.8105, 0.6835,
        0.5392], grad_fn=&lt;SigmoidBackward0&gt;)</code></pre>
</div>
</div>
<p>This means that we can use instances of <code>nn.Module</code> as parameterized functions. For example, we might create a general linear (technically affine) function in the same way.</p>
<p><span class="math display">\[f(\mathbf{x}) = \mathbf{x}^T\mathbf{W}^T + \mathbf{b},  \quad f: \mathbb{R}^i \rightarrow \mathbb{R}^o\]</span></p>
<p>Note that here we are <strong>not</strong> assuming an augmented representation of <span class="math inline">\(\mathbf{x}\)</span>.</p>
<div class="cell" data-execution_count="79">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Linear(nn.Module):</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, inputs, outputs):</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weightsT <span class="op">=</span> nn.Parameter(torch.ones((inputs, outputs)))</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> nn.Parameter(torch.zeros((outputs,)))</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.matmul(X, <span class="va">self</span>.weightsT) <span class="op">+</span> <span class="va">self</span>.bias</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can use this module to implement out logistic regression model above.</p>
<div class="cell" data-execution_count="80">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LogisticRegression(nn.Module):</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dims):</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear <span class="op">=</span> Linear(dims, <span class="dv">1</span>)                       <span class="co"># Dims input 1 output</span></span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sigmoid(<span class="va">self</span>.linear(X)).reshape((<span class="op">-</span><span class="dv">1</span>,)) <span class="co"># Turn output into a vector</span></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_probability(<span class="va">self</span>, X):</span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X)</span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X) <span class="op">&gt;</span> <span class="fl">0.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="81">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(<span class="dv">2</span>)</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>model(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="81">
<pre><code>tensor([0.5652, 0.7431, 0.8238, 0.5468, 0.8991, 0.5719, 0.8651, 0.7784, 0.6153,
        0.7053, 0.7419, 0.3129, 0.4502, 0.5031, 0.4508, 0.7075, 0.7626, 0.4756,
        0.4158, 0.4693, 0.7904, 0.7241, 0.2430, 0.5297, 0.7944, 0.8496, 0.6825,
        0.6149, 0.6730, 0.5231, 0.6151, 0.6108, 0.5509, 0.7476, 0.6634, 0.8512,
        0.8117, 0.7527, 0.5092, 0.7742, 0.8012, 0.7604, 0.6411, 0.3242, 0.2805,
        0.4016, 0.6296, 0.3000, 0.7045, 0.7307, 0.7442, 0.8091, 0.5104, 0.8006,
        0.6166, 0.5173, 0.6404, 0.5779, 0.8199, 0.9030, 0.7872, 0.6059, 0.8091,
        0.9367, 0.5806, 0.7576, 0.8079, 0.3847, 0.4926, 0.4909, 0.8605, 0.6233,
        0.5605, 0.5980, 0.3629, 0.6874, 0.8761, 0.7593, 0.8267, 0.7912, 0.7147,
        0.4980, 0.4396, 0.9295, 0.8956, 0.5096, 0.7148, 0.7788, 0.7961, 0.2600,
        0.4964, 0.6008, 0.5710, 0.7882, 0.9379, 0.8573, 0.7131, 0.8105, 0.6835,
        0.5392], grad_fn=&lt;ReshapeAliasBackward0&gt;)</code></pre>
</div>
</div>
<p>The power here is that because <code>Linear</code> is also an instance of <code>nn.Module</code>, PyTorch knows that it’s weights should also be considered part of our models weights. We can access the weights of a model using the <code>parameters()</code> method.</p>
<div class="cell" data-execution_count="82">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(model.parameters())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="82">
<pre><code>[Parameter containing:
 tensor([[1.],
         [1.]], requires_grad=True),
 Parameter containing:
 tensor([0.], requires_grad=True)]</code></pre>
</div>
</div>
<p>This lets us easily apply gradient descent:</p>
<div class="cell" data-execution_count="83">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(X)        <span class="co"># Now we can just call model!</span></span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> NLL(predictions, y)</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>76.53839111328125
45.934814453125
40.60100555419922
32.82542037963867
30.3773136138916
28.918834686279297
28.118915557861328
27.602989196777344
27.248106002807617
26.989551544189453</code></pre>
</div>
</div>
<div class="cell" data-execution_count="84">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-42-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>PyTorch unsurprisingly also provides a built-in <code>Linear</code> module. As <code>nn.Linear</code>.</p>
<div class="cell" data-execution_count="85">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>nn.Linear(<span class="dv">2</span>, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="85">
<pre><code>Linear(in_features=2, out_features=1, bias=True)</code></pre>
</div>
</div>
<p>Knowing how to make a parameterized function in PyTorch, let’s consider making a neural network layer with a sigmoid activation function.</p>
<p><span class="math display">\[f(\mathbf{x}) = \sigma(\mathbf{x}^T\mathbf{W}^T + \mathbf{b})\]</span></p>
<div class="cell" data-execution_count="86">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SigmoidLayer(nn.Module):</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, inputs, outputs):</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear <span class="op">=</span> nn.Linear(inputs, outputs)</span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sigmoid(<span class="va">self</span>.linear(X))</span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>        </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s create a layer with 10 neurons. (So <span class="math inline">\(\mathbf{W}:\ (10 \times 2)\)</span>)</p>
<div class="cell" data-execution_count="87">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>layer <span class="op">=</span> SigmoidLayer(<span class="dv">2</span>, <span class="dv">10</span>)</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X.shape)</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>layer(X).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([100, 2])</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="87">
<pre><code>torch.Size([100, 10])</code></pre>
</div>
</div>
<p>Let’s use this to create a neural network class for binary classification!</p>
<div class="cell" data-execution_count="88">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NeuralNetwork(nn.Module):</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dims, hidden_size):</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer <span class="op">=</span> SigmoidLayer(dims, hidden_size)</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear <span class="op">=</span> Linear(hidden_size, <span class="dv">1</span>)                       </span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a>        hidden_neurons <span class="op">=</span> <span class="va">self</span>.layer(X)</span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.linear(hidden_neurons)</span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sigmoid(output).reshape((<span class="op">-</span><span class="dv">1</span>,))</span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_probability(<span class="va">self</span>, X):</span>
<span id="cb74-13"><a href="#cb74-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X)</span>
<span id="cb74-14"><a href="#cb74-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-15"><a href="#cb74-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb74-16"><a href="#cb74-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X) <span class="op">&gt;</span> <span class="fl">0.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We see that PyTorch recognizes both the parameters of the logistic regression and the parameters of our neural network feature transform:</p>
<div class="cell" data-execution_count="89">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> NeuralNetwork(<span class="dv">2</span>, <span class="dv">10</span>)</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(model.parameters())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="89">
<pre><code>[Parameter containing:
 tensor([[-0.0632, -0.0324],
         [-0.4227, -0.0076],
         [ 0.5793, -0.4920],
         [ 0.1640, -0.5442],
         [ 0.2326,  0.3651],
         [-0.0184,  0.6859],
         [ 0.0893,  0.1236],
         [-0.4774, -0.3702],
         [-0.0048,  0.4830],
         [-0.4720,  0.5458]], requires_grad=True),
 Parameter containing:
 tensor([-0.2695, -0.1038, -0.7001,  0.3398, -0.0591,  0.6680,  0.3601, -0.3093,
          0.0831, -0.4315], requires_grad=True),
 Parameter containing:
 tensor([[1.],
         [1.],
         [1.],
         [1.],
         [1.],
         [1.],
         [1.],
         [1.],
         [1.],
         [1.]], requires_grad=True),
 Parameter containing:
 tensor([0.], requires_grad=True)]</code></pre>
</div>
</div>
<p>This means that we can easily run our optimization as before.</p>
<div class="cell" data-execution_count="90">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(X)        <span class="co"># Now we can just call model!</span></span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> NLL(predictions, y)</span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>251.74571228027344
461.0118103027344
62.689453125
58.230892181396484
51.833839416503906
45.31672668457031
42.269630432128906
41.090309143066406
42.59092712402344
44.23348617553711
51.76810073852539
45.51247024536133
48.5499267578125
37.86709976196289
36.245426177978516
32.07354736328125
30.741405487060547
29.285472869873047
28.51435089111328
27.839073181152344
27.37657928466797
26.992008209228516
26.69622802734375
26.455289840698242
26.262691497802734
26.106895446777344
25.978981018066406
25.872283935546875
25.78037452697754
25.699132919311523
25.625377655029297
25.55675506591797
25.492244720458984
25.430410385131836
25.371490478515625
25.31437873840332
25.260303497314453
25.208097457885742
25.160297393798828
25.115331649780273
25.07819175720215
25.046260833740234
25.029979705810547
25.02396011352539
25.0517578125
25.099485397338867
25.22385025024414
25.381305694580078
25.71344757080078
26.064964294433594
26.785503387451172
27.31593894958496
28.478464126586914
28.659645080566406
29.6827335357666
28.770679473876953
28.902992248535156
27.449777603149414
27.00601577758789
25.92099380493164
25.47977066040039
24.875839233398438
24.58094596862793
24.25676918029785
24.065155029296875
23.871578216552734
23.734432220458984
23.599082946777344
23.48827362060547
23.378538131713867
23.279327392578125
23.17983627319336
23.08414077758789
22.986806869506836
22.889585494995117
22.789363861083984
22.686796188354492
22.579761505126953
22.468294143676758
22.35063362121582
22.226394653320312
22.093841552734375
21.952234268188477
21.799684524536133
21.6351318359375
21.456491470336914
21.26246452331543
21.05084991455078
20.82036781311035
20.568992614746094
20.29586410522461
19.999610900878906
19.680295944213867
19.337650299072266
18.972978591918945
18.587278366088867
18.183042526245117
17.762325286865234
17.328487396240234
16.884227752685547</code></pre>
</div>
</div>
<div class="cell" data-execution_count="91">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-49-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>PyTorch also gives us an easier (but less flexible) way to define a composition of modules like this. In PyTorch we can define this simple network using <code>nn.Sequential</code></p>
<div class="cell" data-execution_count="92">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">10</span>),</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">10</span>, <span class="dv">1</span>),</span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>model(X).reshape((<span class="op">-</span><span class="dv">1</span>,))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="92">
<pre><code>tensor([0.3653, 0.3638, 0.3685, 0.3575, 0.3788, 0.3567, 0.3749, 0.3824, 0.3716,
        0.3777, 0.3612, 0.3494, 0.3608, 0.3496, 0.3610, 0.3802, 0.3737, 0.3504,
        0.3581, 0.3621, 0.3859, 0.3814, 0.3410, 0.3605, 0.3768, 0.3890, 0.3599,
        0.3586, 0.3567, 0.3529, 0.3723, 0.3711, 0.3682, 0.3644, 0.3774, 0.3742,
        0.3804, 0.3808, 0.3526, 0.3757, 0.3669, 0.3735, 0.3731, 0.3516, 0.3461,
        0.3566, 0.3558, 0.3496, 0.3594, 0.3775, 0.3755, 0.3808, 0.3586, 0.3792,
        0.3543, 0.3657, 0.3559, 0.3642, 0.3848, 0.3816, 0.3824, 0.3546, 0.3808,
        0.3906, 0.3661, 0.3723, 0.3823, 0.3552, 0.3561, 0.3543, 0.3730, 0.3723,
        0.3523, 0.3669, 0.3531, 0.3590, 0.3764, 0.3702, 0.3680, 0.3841, 0.3798,
        0.3555, 0.3480, 0.3861, 0.3814, 0.3495, 0.3793, 0.3814, 0.3839, 0.3427,
        0.3540, 0.3623, 0.3700, 0.3661, 0.3915, 0.3718, 0.3660, 0.3685, 0.3601,
        0.3597], grad_fn=&lt;ReshapeAliasBackward0&gt;)</code></pre>
</div>
</div>
<p>Here <code>nn.Sigmoid</code> is a built-in module that just applies the sigmoid function. Its implementation would look like:</p>
<div class="cell" data-execution_count="93">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SigmoidLayer(nn.Module):</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sigmoid(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We could use this to create a network with several hidden layers:</p>
<div class="cell" data-execution_count="94">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">10</span>),</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">10</span>, <span class="dv">10</span>),</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">10</span>, <span class="dv">10</span>),</span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">10</span>, <span class="dv">1</span>),</span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb83-11"><a href="#cb83-11" aria-hidden="true" tabindex="-1"></a>model(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="94">
<pre><code>tensor([[0.4041],
        [0.4038],
        [0.4038],
        [0.4040],
        [0.4038],
        [0.4039],
        [0.4038],
        [0.4041],
        [0.4042],
        [0.4041],
        [0.4038],
        [0.4042],
        [0.4042],
        [0.4039],
        [0.4042],
        [0.4042],
        [0.4040],
        [0.4039],
        [0.4042],
        [0.4042],
        [0.4042],
        [0.4042],
        [0.4041],
        [0.4041],
        [0.4040],
        [0.4041],
        [0.4039],
        [0.4039],
        [0.4038],
        [0.4039],
        [0.4042],
        [0.4042],
        [0.4042],
        [0.4039],
        [0.4042],
        [0.4038],
        [0.4040],
        [0.4041],
        [0.4039],
        [0.4040],
        [0.4038],
        [0.4040],
        [0.4042],
        [0.4042],
        [0.4041],
        [0.4042],
        [0.4039],
        [0.4042],
        [0.4038],
        [0.4041],
        [0.4040],
        [0.4040],
        [0.4041],
        [0.4040],
        [0.4038],
        [0.4042],
        [0.4038],
        [0.4041],
        [0.4041],
        [0.4038],
        [0.4041],
        [0.4039],
        [0.4040],
        [0.4039],
        [0.4041],
        [0.4040],
        [0.4041],
        [0.4042],
        [0.4040],
        [0.4040],
        [0.4038],
        [0.4042],
        [0.4039],
        [0.4041],
        [0.4042],
        [0.4038],
        [0.4038],
        [0.4039],
        [0.4038],
        [0.4041],
        [0.4042],
        [0.4040],
        [0.4039],
        [0.4038],
        [0.4039],
        [0.4039],
        [0.4042],
        [0.4041],
        [0.4041],
        [0.4041],
        [0.4040],
        [0.4040],
        [0.4042],
        [0.4038],
        [0.4039],
        [0.4038],
        [0.4039],
        [0.4038],
        [0.4039],
        [0.4040]], grad_fn=&lt;SigmoidBackward0&gt;)</code></pre>
</div>
</div>
<p>PyTorch also provides built-in loss functions. The PyTorch function for the negative log-likelihood for logistic regression is called <code>nn.functional.binary_cross_entropy</code>. It has some sharp edges though.</p>
<p>For one, it expects <code>y</code> to be a float type. We can convert a PyTorch <code>int</code> tensor into a <code>float</code> one by calling the <code>float</code> method.</p>
<p>We also see that our sequential model returns a column vector, so <code>y</code> should match that as well.</p>
<div class="cell" data-execution_count="95">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>yfloat <span class="op">=</span> y.<span class="bu">float</span>().reshape((<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(X)        <span class="co"># Now we can just call model!</span></span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> nn.functional.binary_cross_entropy(predictions, yfloat)</span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb85-9"><a href="#cb85-9" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb85-10"><a href="#cb85-10" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb85-11"><a href="#cb85-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.712141752243042
0.7091044783592224
0.7065528631210327
0.7044107913970947
0.7026132941246033
0.7011056542396545
0.6998415589332581
0.6987819671630859
0.6978939771652222
0.6971497535705566
0.696526288986206
0.6960040330886841
0.6955663561820984
0.6951996684074402
0.694892406463623
0.6946350336074829
0.6944191455841064
0.6942383646965027
0.6940867900848389
0.6939594745635986
0.6938527822494507
0.6937631964683533
0.6936879754066467
0.69362473487854
0.6935714483261108
0.6935266852378845
0.6934890151023865
0.6934571266174316
0.6934301853179932
0.6934073567390442
0.693388044834137
0.693371593952179
0.693357527256012
0.6933454871177673
0.6933351755142212
0.6933264136314392
0.6933186054229736
0.6933119297027588
0.693306028842926
0.6933008432388306
0.6932962536811829
0.6932921409606934
0.6932885050773621
0.6932851672172546
0.6932820081710815
0.6932792067527771
0.6932765245437622
0.6932740807533264
0.693271815776825
0.6932694911956787
0.6932673454284668
0.6932653784751892
0.6932634115219116
0.6932615637779236
0.6932596564292908
0.6932578086853027
0.6932560801506042
0.693254292011261
0.6932525634765625
0.693250834941864
0.6932492256164551
0.6932475566864014
0.6932458281517029
0.6932441592216492
0.6932425498962402
0.6932408809661865
0.6932392120361328
0.6932376027107239
0.6932359933853149
0.693234384059906
0.6932327747344971
0.6932311058044434
0.6932294964790344
0.6932278275489807
0.6932263374328613
0.6932246685028076
0.6932230591773987
0.6932214498519897
0.693219780921936
0.6932182312011719
0.6932166218757629
0.693215012550354
0.6932134032249451
0.6932117938995361
0.693210244178772
0.6932085156440735
0.6932069659233093
0.6932052373886108
0.6932037472724915
0.6932021379470825
0.6932004690170288
0.6931988596916199
0.6931973099708557
0.693195641040802
0.6931940317153931
0.6931924223899841
0.6931908130645752
0.693189263343811
0.6931875348091125
0.6931860446929932</code></pre>
</div>
</div>
<p>For convinience, let’s definie a wrapper class for our model.</p>
<div class="cell" data-execution_count="96">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LogisticRegressionNeuralNetwork(nn.Module):</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, network):</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.network <span class="op">=</span> network                   </span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.network(X).reshape((<span class="op">-</span><span class="dv">1</span>,))</span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_probability(<span class="va">self</span>, X):</span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X)</span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb87-13"><a href="#cb87-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X) <span class="op">&gt;</span> <span class="fl">0.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="evaluating-models" class="level2">
<h2 class="anchored" data-anchor-id="evaluating-models">Evaluating models</h2>
<p>We see that we have a <em>lot</em> of options when designing a neural network. So far the choices we’ve seen are: - The number of layers - The number of neurons in each layer - The activation function - The learning rate for gradient descent</p>
<p>And this is just the beginning! As we go on, we’ll learn about many more options that we have.</p>
<p>Let’s take a look at how to make some of these choices. In many real cases, our data will not be a cleanly separated into 2 classes as we’ve seen. For instance, we can look at a noisier version of the dataset we saw before.</p>
<div class="cell" data-execution_count="223">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(<span class="dv">300</span>, noise<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> torch.tensor(X).<span class="bu">float</span>(), torch.tensor(y)</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>plt.scatter(<span class="op">*</span>X.T, c<span class="op">=</span>y, edgecolor<span class="op">=</span><span class="st">"black"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="223">
<pre><code>&lt;matplotlib.collections.PathCollection at 0x2e043f070&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-55-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>In reality, this dataset was drawn from the distribution shown below! The optimal classifier would still have an “s-shaped” decision boundary</p>
<div class="cell" data-execution_count="221">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(<span class="dv">50000</span>, noise<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, alpha<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a><span class="cf">pass</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-56-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Let’s split this into training and test sets as we’ve seen.</p>
<div class="cell" data-execution_count="231">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a>inds <span class="op">=</span> np.arange(X.shape[<span class="dv">0</span>])</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a>np.random.shuffle(inds)</span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a>Xtrain, ytrain <span class="op">=</span> X[inds[:<span class="dv">150</span>]], y[inds[:<span class="dv">150</span>]]</span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a>Xtest, ytest <span class="op">=</span> X[inds[<span class="dv">150</span>:]], y[inds[<span class="dv">150</span>:]]</span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a><span class="co"># We can plot training and test data on the same plot</span></span>
<span id="cb91-7"><a href="#cb91-7" aria-hidden="true" tabindex="-1"></a>plt.scatter(<span class="op">*</span>Xtrain.T, c<span class="op">=</span>ytrain, edgecolor<span class="op">=</span><span class="st">"black"</span>, marker<span class="op">=</span><span class="st">'o'</span>, label<span class="op">=</span><span class="st">'training data'</span>)</span>
<span id="cb91-8"><a href="#cb91-8" aria-hidden="true" tabindex="-1"></a>plt.scatter(<span class="op">*</span>Xtest.T, c<span class="op">=</span>ytest, edgecolor<span class="op">=</span><span class="st">"black"</span>, marker<span class="op">=</span><span class="st">'^'</span>, label<span class="op">=</span><span class="st">'test data'</span>)</span>
<span id="cb91-9"><a href="#cb91-9" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="231">
<pre><code>&lt;matplotlib.legend.Legend at 0x2e0a7c670&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-57-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="underfittting" class="level2">
<h2 class="anchored" data-anchor-id="underfittting">Underfittting</h2>
<p>We’ll start by fitting a logistic regression model as we’ve seen. This time we’ll keep track of the loss on both the training data and the test data.</p>
<div class="cell" data-execution_count="233">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>network <span class="op">=</span> nn.Sequential(</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">1</span>),</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegressionNeuralNetwork(network)   </span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb93-8"><a href="#cb93-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-9"><a href="#cb93-9" aria-hidden="true" tabindex="-1"></a>train_losses, test_losses <span class="op">=</span> [], []</span>
<span id="cb93-10"><a href="#cb93-10" aria-hidden="true" tabindex="-1"></a>pbar <span class="op">=</span> tqdm.trange(<span class="dv">2500</span>)</span>
<span id="cb93-11"><a href="#cb93-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> pbar:</span>
<span id="cb93-12"><a href="#cb93-12" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(Xtrain)        <span class="co"># Now we can just call model!</span></span>
<span id="cb93-13"><a href="#cb93-13" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy(predictions, ytrain.flatten().<span class="bu">float</span>())</span>
<span id="cb93-14"><a href="#cb93-14" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb93-15"><a href="#cb93-15" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb93-16"><a href="#cb93-16" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb93-17"><a href="#cb93-17" aria-hidden="true" tabindex="-1"></a>    pbar.set_description(<span class="st">'Loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> loss.item())</span>
<span id="cb93-18"><a href="#cb93-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-19"><a href="#cb93-19" aria-hidden="true" tabindex="-1"></a>    train_losses.append(loss.item())</span>
<span id="cb93-20"><a href="#cb93-20" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy(model(Xtest), ytest.flatten().<span class="bu">float</span>())</span>
<span id="cb93-21"><a href="#cb93-21" aria-hidden="true" tabindex="-1"></a>    test_losses.append(test_loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loss: 0.501: 100%|██████████| 2500/2500 [00:02&lt;00:00, 892.89it/s] </code></pre>
</div>
</div>
<div class="cell" data-execution_count="234">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtrain, ytrain)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-59-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Let’s compute the accuracy on both the training and the test data</p>
<div class="cell" data-execution_count="148">
<div class="sourceCode cell-code" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a>train_acc <span class="op">=</span> (model.predict(Xtrain) <span class="op">==</span> ytrain).<span class="bu">float</span>().mean()</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a>test_acc <span class="op">=</span> (model.predict(Xtest) <span class="op">==</span> ytest).<span class="bu">float</span>().mean()</span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Training accuracy: </span><span class="sc">%.3f</span><span class="st">, Test accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> (train_acc, test_acc))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training accuracy: 0.793, Test accuracy: 0.793</code></pre>
</div>
</div>
<p>We can also look at how the loss on both the training and test data changes as we run gradient descent.</p>
<div class="cell" data-execution_count="236">
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>plt.plot(train_losses[:<span class="dv">15000</span>:<span class="dv">100</span>], label<span class="op">=</span><span class="st">'Training loss'</span>)</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a>plt.plot(test_losses[:<span class="dv">15000</span>:<span class="dv">100</span>], label<span class="op">=</span><span class="st">'Test loss'</span>)</span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Loss vs. training iteration'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="236">
<pre><code>Text(0.5, 1.0, 'Loss vs. training iteration')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-61-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Here wee see that both the training loss/accuracy and the test loss/accuracy are quite poor! From our descision boundary plot we can see quite clearly that this is a consequence of our choice of a linear model for this classification problem. We call this problem <strong>underfitting</strong>, meaning that our model is not expressive enough to capture all the intricacies of our data. As we’ve already seen we can address this by adding neural network layers to increase the expressivity of our model.</p>
</section>
<section id="overfitting" class="level2">
<h2 class="anchored" data-anchor-id="overfitting">Overfitting</h2>
<p>Let’s try creating a much more complex model; one with several large neural network layers and fitting it to our data.</p>
<div class="cell" data-execution_count="238">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>network <span class="op">=</span> nn.Sequential(</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">200</span>),</span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">200</span>, <span class="dv">200</span>),</span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb100-6"><a href="#cb100-6" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">200</span>, <span class="dv">200</span>),</span>
<span id="cb100-7"><a href="#cb100-7" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb100-8"><a href="#cb100-8" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">200</span>, <span class="dv">1</span>),</span>
<span id="cb100-9"><a href="#cb100-9" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb100-10"><a href="#cb100-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb100-11"><a href="#cb100-11" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegressionNeuralNetwork(network)   </span>
<span id="cb100-12"><a href="#cb100-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-13"><a href="#cb100-13" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.03</span>)</span>
<span id="cb100-14"><a href="#cb100-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-15"><a href="#cb100-15" aria-hidden="true" tabindex="-1"></a>train_losses, test_losses <span class="op">=</span> [], []</span>
<span id="cb100-16"><a href="#cb100-16" aria-hidden="true" tabindex="-1"></a>pbar <span class="op">=</span> tqdm.trange(<span class="dv">25000</span>)</span>
<span id="cb100-17"><a href="#cb100-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> pbar:</span>
<span id="cb100-18"><a href="#cb100-18" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(Xtrain)        <span class="co"># Now we can just call model!</span></span>
<span id="cb100-19"><a href="#cb100-19" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy(predictions, ytrain.flatten().<span class="bu">float</span>())</span>
<span id="cb100-20"><a href="#cb100-20" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb100-21"><a href="#cb100-21" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb100-22"><a href="#cb100-22" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb100-23"><a href="#cb100-23" aria-hidden="true" tabindex="-1"></a>    pbar.set_description(<span class="st">'Loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> loss.item())</span>
<span id="cb100-24"><a href="#cb100-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-25"><a href="#cb100-25" aria-hidden="true" tabindex="-1"></a>    train_losses.append(loss.item())</span>
<span id="cb100-26"><a href="#cb100-26" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy(model(Xtest), ytest.flatten().<span class="bu">float</span>())</span>
<span id="cb100-27"><a href="#cb100-27" aria-hidden="true" tabindex="-1"></a>    test_losses.append(test_loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loss: 0.073: 100%|██████████| 25000/25000 [00:41&lt;00:00, 602.24it/s]</code></pre>
</div>
</div>
<p>We can view the descision boundary and accuracy for this classifier.</p>
<div class="cell" data-execution_count="247">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>train_acc <span class="op">=</span> (model.predict(Xtrain) <span class="op">==</span> ytrain).<span class="bu">float</span>().mean()</span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtrain, ytrain, title<span class="op">=</span><span class="st">'Training accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> train_acc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-63-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>We see that our model is much more expressive and basically correctly classifies every observation in our training dataset. This is great! However the boundary is quite complex. Let’s see what happens when we evaluate on our test set.</p>
<div class="cell" data-execution_count="250">
<div class="sourceCode cell-code" id="cb103"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>test_acc <span class="op">=</span> (model.predict(Xtest) <span class="op">==</span> ytest).<span class="bu">float</span>().mean()</span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtest, ytest, title<span class="op">=</span><span class="st">'Test accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> test_acc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-64-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The accuracy is <em>much</em> worse. Rather than capturing the true distribution of classes, our model has captured the training set we happened to draw. This means if we draw a new dataset from the same distribution (like our test set), performance is poor. We call this issue <strong>overfitting</strong>.</p>
<p>Let’s see how the training and test loss change over gradient descent.</p>
<div class="cell" data-execution_count="242">
<div class="sourceCode cell-code" id="cb104"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>plt.plot(train_losses[:<span class="dv">15000</span>:<span class="dv">100</span>], label<span class="op">=</span><span class="st">'Training loss'</span>)</span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a>plt.plot(test_losses[:<span class="dv">15000</span>:<span class="dv">100</span>], label<span class="op">=</span><span class="st">'Test loss'</span>)</span>
<span id="cb104-3"><a href="#cb104-3" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb104-4"><a href="#cb104-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Loss vs. training iteration'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="242">
<pre><code>Text(0.5, 1.0, 'Loss vs. training iteration')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-65-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Much of the rest of this class will focus on how to meet the deicate balance of overfitting vs.&nbsp;underfitting!</p>
<p>It’s worth noting the the best solution to overfitting is to get more data. If we train with enough data we can avoid overfitting entirely.</p>
<div class="cell" data-execution_count="270">
<div class="sourceCode cell-code" id="cb106"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(<span class="dv">2000</span>, noise<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, alpha<span class="op">=</span><span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="270">
<pre><code>&lt;matplotlib.collections.PathCollection at 0x2b7bd5460&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-66-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="275">
<div class="sourceCode cell-code" id="cb108"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> torch.tensor(X).<span class="bu">float</span>(), torch.tensor(y)</span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a>network <span class="op">=</span> nn.Sequential(</span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">200</span>),</span>
<span id="cb108-4"><a href="#cb108-4" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb108-5"><a href="#cb108-5" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">200</span>, <span class="dv">200</span>),</span>
<span id="cb108-6"><a href="#cb108-6" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb108-7"><a href="#cb108-7" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">200</span>, <span class="dv">200</span>),</span>
<span id="cb108-8"><a href="#cb108-8" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb108-9"><a href="#cb108-9" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">200</span>, <span class="dv">1</span>),</span>
<span id="cb108-10"><a href="#cb108-10" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb108-11"><a href="#cb108-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb108-12"><a href="#cb108-12" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegressionNeuralNetwork(network)   </span>
<span id="cb108-13"><a href="#cb108-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-14"><a href="#cb108-14" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb108-15"><a href="#cb108-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-16"><a href="#cb108-16" aria-hidden="true" tabindex="-1"></a>train_losses, test_losses <span class="op">=</span> [], []</span>
<span id="cb108-17"><a href="#cb108-17" aria-hidden="true" tabindex="-1"></a>pbar <span class="op">=</span> tqdm.trange(<span class="dv">2500</span>)</span>
<span id="cb108-18"><a href="#cb108-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> pbar:</span>
<span id="cb108-19"><a href="#cb108-19" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(X)        <span class="co"># Now we can just call model!</span></span>
<span id="cb108-20"><a href="#cb108-20" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy(predictions, y.flatten().<span class="bu">float</span>())</span>
<span id="cb108-21"><a href="#cb108-21" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb108-22"><a href="#cb108-22" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb108-23"><a href="#cb108-23" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb108-24"><a href="#cb108-24" aria-hidden="true" tabindex="-1"></a>    pbar.set_description(<span class="st">'Loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> loss.item())</span>
<span id="cb108-25"><a href="#cb108-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-26"><a href="#cb108-26" aria-hidden="true" tabindex="-1"></a>    train_losses.append(loss.item())</span>
<span id="cb108-27"><a href="#cb108-27" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy(model(Xtest), ytest.flatten().<span class="bu">float</span>())</span>
<span id="cb108-28"><a href="#cb108-28" aria-hidden="true" tabindex="-1"></a>    test_losses.append(test_loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/var/folders/t9/fjsr8h1x7c7cg4vqw_xhjq2w0000gn/T/ipykernel_79454/2923296796.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X, y = torch.tensor(X).float(), torch.tensor(y)
Loss: 0.390: 100%|██████████| 2500/2500 [00:16&lt;00:00, 151.68it/s]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="276">
<div class="sourceCode cell-code" id="cb110"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a>train_acc <span class="op">=</span> (model.predict(Xtrain) <span class="op">==</span> ytrain).<span class="bu">float</span>().mean()</span>
<span id="cb110-2"><a href="#cb110-2" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtrain, ytrain, title<span class="op">=</span><span class="st">'Training accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> train_acc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-68-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="277">
<div class="sourceCode cell-code" id="cb111"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a>test_acc <span class="op">=</span> (model.predict(Xtest) <span class="op">==</span> ytest).<span class="bu">float</span>().mean()</span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtest, ytest, title<span class="op">=</span><span class="st">'Test accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> test_acc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-69-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="278">
<div class="sourceCode cell-code" id="cb112"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a>plt.plot(train_losses[:<span class="dv">15000</span>:<span class="dv">100</span>], label<span class="op">=</span><span class="st">'Training loss'</span>)</span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a>plt.plot(test_losses[:<span class="dv">15000</span>:<span class="dv">100</span>], label<span class="op">=</span><span class="st">'Test loss'</span>)</span>
<span id="cb112-3"><a href="#cb112-3" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb112-4"><a href="#cb112-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Loss vs. training iteration'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="278">
<pre><code>Text(0.5, 1.0, 'Loss vs. training iteration')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-70-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Unfortunately, this often isn’t realistic. Data is hard to collect and more data means our model is slower and more expensive to train.</p>
</section>
<section id="early-stopping" class="level2">
<h2 class="anchored" data-anchor-id="early-stopping">Early stopping</h2>
<p>Let’s return to the original case</p>
<div class="cell" data-execution_count="280">
<div class="sourceCode cell-code" id="cb114"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(<span class="dv">300</span>, noise<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> torch.tensor(X).<span class="bu">float</span>(), torch.tensor(y)</span>
<span id="cb114-3"><a href="#cb114-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-4"><a href="#cb114-4" aria-hidden="true" tabindex="-1"></a>inds <span class="op">=</span> np.arange(X.shape[<span class="dv">0</span>])</span>
<span id="cb114-5"><a href="#cb114-5" aria-hidden="true" tabindex="-1"></a>np.random.shuffle(inds)</span>
<span id="cb114-6"><a href="#cb114-6" aria-hidden="true" tabindex="-1"></a>Xtrain, ytrain <span class="op">=</span> X[inds[:<span class="dv">150</span>]], y[inds[:<span class="dv">150</span>]]</span>
<span id="cb114-7"><a href="#cb114-7" aria-hidden="true" tabindex="-1"></a>Xtest, ytest <span class="op">=</span> X[inds[<span class="dv">150</span>:]], y[inds[<span class="dv">150</span>:]]</span>
<span id="cb114-8"><a href="#cb114-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-9"><a href="#cb114-9" aria-hidden="true" tabindex="-1"></a><span class="co"># We can plot training and test data on the same plot</span></span>
<span id="cb114-10"><a href="#cb114-10" aria-hidden="true" tabindex="-1"></a>plt.scatter(<span class="op">*</span>Xtrain.T, c<span class="op">=</span>ytrain, edgecolor<span class="op">=</span><span class="st">"black"</span>, marker<span class="op">=</span><span class="st">'o'</span>, label<span class="op">=</span><span class="st">'training data'</span>)</span>
<span id="cb114-11"><a href="#cb114-11" aria-hidden="true" tabindex="-1"></a>plt.scatter(<span class="op">*</span>Xtest.T, c<span class="op">=</span>ytest, edgecolor<span class="op">=</span><span class="st">"black"</span>, marker<span class="op">=</span><span class="st">'^'</span>, label<span class="op">=</span><span class="st">'test data'</span>)</span>
<span id="cb114-12"><a href="#cb114-12" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="280">
<pre><code>&lt;matplotlib.legend.Legend at 0x2e0cedf40&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-71-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Let’s try a network somewhere in-between, with just a single hidden layer.</p>
<div class="cell" data-execution_count="251">
<div class="sourceCode cell-code" id="cb116"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a>network <span class="op">=</span> nn.Sequential(</span>
<span id="cb116-2"><a href="#cb116-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">100</span>),</span>
<span id="cb116-3"><a href="#cb116-3" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb116-4"><a href="#cb116-4" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">100</span>, <span class="dv">1</span>),</span>
<span id="cb116-5"><a href="#cb116-5" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb116-6"><a href="#cb116-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb116-7"><a href="#cb116-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegressionNeuralNetwork(network)   </span>
<span id="cb116-8"><a href="#cb116-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb116-9"><a href="#cb116-9" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.03</span>)</span>
<span id="cb116-10"><a href="#cb116-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb116-11"><a href="#cb116-11" aria-hidden="true" tabindex="-1"></a>train_losses, test_losses <span class="op">=</span> [], []</span>
<span id="cb116-12"><a href="#cb116-12" aria-hidden="true" tabindex="-1"></a>pbar <span class="op">=</span> tqdm.trange(<span class="dv">25000</span>)</span>
<span id="cb116-13"><a href="#cb116-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> pbar:</span>
<span id="cb116-14"><a href="#cb116-14" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(Xtrain)        <span class="co"># Now we can just call model!</span></span>
<span id="cb116-15"><a href="#cb116-15" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy(predictions, ytrain.flatten().<span class="bu">float</span>())</span>
<span id="cb116-16"><a href="#cb116-16" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb116-17"><a href="#cb116-17" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb116-18"><a href="#cb116-18" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb116-19"><a href="#cb116-19" aria-hidden="true" tabindex="-1"></a>    pbar.set_description(<span class="st">'Loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> loss.item())</span>
<span id="cb116-20"><a href="#cb116-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb116-21"><a href="#cb116-21" aria-hidden="true" tabindex="-1"></a>    train_losses.append(loss.item())</span>
<span id="cb116-22"><a href="#cb116-22" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy(model(Xtest), ytest.flatten().<span class="bu">float</span>())</span>
<span id="cb116-23"><a href="#cb116-23" aria-hidden="true" tabindex="-1"></a>    test_losses.append(test_loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loss: 0.366: 100%|██████████| 25000/25000 [00:28&lt;00:00, 870.32it/s] </code></pre>
</div>
</div>
<div class="cell" data-execution_count="252">
<div class="sourceCode cell-code" id="cb118"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a>train_acc <span class="op">=</span> (model.predict(Xtrain) <span class="op">==</span> ytrain).<span class="bu">float</span>().mean()</span>
<span id="cb118-2"><a href="#cb118-2" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtrain, ytrain, title<span class="op">=</span><span class="st">'Training accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> train_acc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-73-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="253">
<div class="sourceCode cell-code" id="cb119"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a>test_acc <span class="op">=</span> (model.predict(Xtest) <span class="op">==</span> ytest).<span class="bu">float</span>().mean()</span>
<span id="cb119-2"><a href="#cb119-2" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtest, ytest, title<span class="op">=</span><span class="st">'Test accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> test_acc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-74-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>We see that this network is more consistent between train and test, and now performs better on test data! Let’s take a look at the plot of training and test loss.</p>
<div class="cell" data-execution_count="254">
<div class="sourceCode cell-code" id="cb120"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a>plt.plot(train_losses[:<span class="dv">15000</span>:<span class="dv">100</span>], label<span class="op">=</span><span class="st">'Training loss'</span>)</span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true" tabindex="-1"></a>plt.plot(test_losses[:<span class="dv">15000</span>:<span class="dv">100</span>], label<span class="op">=</span><span class="st">'Test loss'</span>)</span>
<span id="cb120-3"><a href="#cb120-3" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb120-4"><a href="#cb120-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Loss vs. training iteration'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="254">
<pre><code>Text(0.5, 1.0, 'Loss vs. training iteration')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-75-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>We still see that both drop quickly, but that test loss increases after a point. How might we use this to pick a better model?</p>
<p>One option would be to just use the model where the test loss is lowest. After all, that is our ultimate goal. There are different ways we can think about implementing this. One way to to have gradient descent stop when the test loss begins to increase. We call this approach <strong>early stopping</strong></p>
<div class="cell" data-execution_count="256">
<div class="sourceCode cell-code" id="cb122"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a>network <span class="op">=</span> nn.Sequential(</span>
<span id="cb122-2"><a href="#cb122-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">100</span>),</span>
<span id="cb122-3"><a href="#cb122-3" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb122-4"><a href="#cb122-4" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">100</span>, <span class="dv">1</span>),</span>
<span id="cb122-5"><a href="#cb122-5" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb122-6"><a href="#cb122-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb122-7"><a href="#cb122-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegressionNeuralNetwork(network)   </span>
<span id="cb122-8"><a href="#cb122-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb122-9"><a href="#cb122-9" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.03</span>)</span>
<span id="cb122-10"><a href="#cb122-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb122-11"><a href="#cb122-11" aria-hidden="true" tabindex="-1"></a>train_losses, test_losses <span class="op">=</span> [], []</span>
<span id="cb122-12"><a href="#cb122-12" aria-hidden="true" tabindex="-1"></a>pbar <span class="op">=</span> tqdm.trange(<span class="dv">25000</span>)</span>
<span id="cb122-13"><a href="#cb122-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> pbar:</span>
<span id="cb122-14"><a href="#cb122-14" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(Xtrain)        <span class="co"># Now we can just call model!</span></span>
<span id="cb122-15"><a href="#cb122-15" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy(predictions, ytrain.flatten().<span class="bu">float</span>())</span>
<span id="cb122-16"><a href="#cb122-16" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb122-17"><a href="#cb122-17" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb122-18"><a href="#cb122-18" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb122-19"><a href="#cb122-19" aria-hidden="true" tabindex="-1"></a>    pbar.set_description(<span class="st">'Loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> loss.item())</span>
<span id="cb122-20"><a href="#cb122-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb122-21"><a href="#cb122-21" aria-hidden="true" tabindex="-1"></a>    train_losses.append(loss.item())</span>
<span id="cb122-22"><a href="#cb122-22" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy(model(Xtest), ytest.flatten().<span class="bu">float</span>())</span>
<span id="cb122-23"><a href="#cb122-23" aria-hidden="true" tabindex="-1"></a>    test_losses.append(test_loss.item())</span>
<span id="cb122-24"><a href="#cb122-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb122-25"><a href="#cb122-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&gt;</span> <span class="dv">50</span> <span class="kw">and</span> test_loss.item() <span class="op">&gt;</span> test_losses[<span class="op">-</span><span class="dv">2</span>]:</span>
<span id="cb122-26"><a href="#cb122-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loss: 0.481:   1%|▏         | 346/25000 [00:00&lt;00:30, 810.31it/s]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="257">
<div class="sourceCode cell-code" id="cb124"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a>plt.plot(train_losses[:<span class="dv">15000</span>:<span class="dv">100</span>], label<span class="op">=</span><span class="st">'Training loss'</span>)</span>
<span id="cb124-2"><a href="#cb124-2" aria-hidden="true" tabindex="-1"></a>plt.plot(test_losses[:<span class="dv">15000</span>:<span class="dv">100</span>], label<span class="op">=</span><span class="st">'Test loss'</span>)</span>
<span id="cb124-3"><a href="#cb124-3" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb124-4"><a href="#cb124-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Loss vs. training iteration'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="257">
<pre><code>Text(0.5, 1.0, 'Loss vs. training iteration')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-77-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="258">
<div class="sourceCode cell-code" id="cb126"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true" tabindex="-1"></a>train_acc <span class="op">=</span> (model.predict(Xtrain) <span class="op">==</span> ytrain).<span class="bu">float</span>().mean()</span>
<span id="cb126-2"><a href="#cb126-2" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtrain, ytrain, title<span class="op">=</span><span class="st">'Training accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> train_acc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-78-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="259">
<div class="sourceCode cell-code" id="cb127"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a>test_acc <span class="op">=</span> (model.predict(Xtest) <span class="op">==</span> ytest).<span class="bu">float</span>().mean()</span>
<span id="cb127-2"><a href="#cb127-2" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtest, ytest, title<span class="op">=</span><span class="st">'Test accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> test_acc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-79-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Here we see that actually we do the best so far with this approach!</p>
<p>We could also try our early-stopping approach with our more complex network.</p>
<div class="cell" data-execution_count="260">
<div class="sourceCode cell-code" id="cb128"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a>network <span class="op">=</span> nn.Sequential(</span>
<span id="cb128-2"><a href="#cb128-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">200</span>),</span>
<span id="cb128-3"><a href="#cb128-3" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb128-4"><a href="#cb128-4" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">200</span>, <span class="dv">200</span>),</span>
<span id="cb128-5"><a href="#cb128-5" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb128-6"><a href="#cb128-6" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">200</span>, <span class="dv">200</span>),</span>
<span id="cb128-7"><a href="#cb128-7" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb128-8"><a href="#cb128-8" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">200</span>, <span class="dv">1</span>),</span>
<span id="cb128-9"><a href="#cb128-9" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb128-10"><a href="#cb128-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb128-11"><a href="#cb128-11" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegressionNeuralNetwork(network)   </span>
<span id="cb128-12"><a href="#cb128-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb128-13"><a href="#cb128-13" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.03</span>)</span>
<span id="cb128-14"><a href="#cb128-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb128-15"><a href="#cb128-15" aria-hidden="true" tabindex="-1"></a>train_losses, test_losses <span class="op">=</span> [], []</span>
<span id="cb128-16"><a href="#cb128-16" aria-hidden="true" tabindex="-1"></a>pbar <span class="op">=</span> tqdm.trange(<span class="dv">25000</span>)</span>
<span id="cb128-17"><a href="#cb128-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> pbar:</span>
<span id="cb128-18"><a href="#cb128-18" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(Xtrain)        <span class="co"># Now we can just call model!</span></span>
<span id="cb128-19"><a href="#cb128-19" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy(predictions, ytrain.flatten().<span class="bu">float</span>())</span>
<span id="cb128-20"><a href="#cb128-20" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb128-21"><a href="#cb128-21" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb128-22"><a href="#cb128-22" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb128-23"><a href="#cb128-23" aria-hidden="true" tabindex="-1"></a>    pbar.set_description(<span class="st">'Loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> loss.item())</span>
<span id="cb128-24"><a href="#cb128-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb128-25"><a href="#cb128-25" aria-hidden="true" tabindex="-1"></a>    train_losses.append(loss.item())</span>
<span id="cb128-26"><a href="#cb128-26" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy(model(Xtest), ytest.flatten().<span class="bu">float</span>())</span>
<span id="cb128-27"><a href="#cb128-27" aria-hidden="true" tabindex="-1"></a>    test_losses.append(test_loss.item())</span>
<span id="cb128-28"><a href="#cb128-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb128-29"><a href="#cb128-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&gt;</span> <span class="dv">50</span> <span class="kw">and</span> test_loss.item() <span class="op">&gt;</span> test_losses[<span class="op">-</span><span class="dv">2</span>]:</span>
<span id="cb128-30"><a href="#cb128-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loss: 0.465:   2%|▏         | 382/25000 [00:00&lt;00:42, 574.88it/s]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="261">
<div class="sourceCode cell-code" id="cb130"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a>train_acc <span class="op">=</span> (model.predict(Xtrain) <span class="op">==</span> ytrain).<span class="bu">float</span>().mean()</span>
<span id="cb130-2"><a href="#cb130-2" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtrain, ytrain, title<span class="op">=</span><span class="st">'Training accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> train_acc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-81-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="262">
<div class="sourceCode cell-code" id="cb131"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a>test_acc <span class="op">=</span> (model.predict(Xtest) <span class="op">==</span> ytest).<span class="bu">float</span>().mean()</span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtest, ytest, title<span class="op">=</span><span class="st">'Test accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> test_acc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-82-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="263">
<div class="sourceCode cell-code" id="cb132"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a>plt.plot(train_losses[:<span class="dv">15000</span>:<span class="dv">100</span>], label<span class="op">=</span><span class="st">'Training loss'</span>)</span>
<span id="cb132-2"><a href="#cb132-2" aria-hidden="true" tabindex="-1"></a>plt.plot(test_losses[:<span class="dv">15000</span>:<span class="dv">100</span>], label<span class="op">=</span><span class="st">'Test loss'</span>)</span>
<span id="cb132-3"><a href="#cb132-3" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb132-4"><a href="#cb132-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Loss vs. training iteration'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="263">
<pre><code>Text(0.5, 1.0, 'Loss vs. training iteration')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-83-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="train-validation-and-test" class="level2">
<h2 class="anchored" data-anchor-id="train-validation-and-test">Train, validation and test</h2>
<p>There is an issue here though! We’ve now used out test set to (indirectly) train our model. Both by using it to choose the number of layers and by using it to determine how long to run our optimization! This means that our model choice will be biased by our choice of test set, so how can we trust that our test loss or accuracy will actually be a good measure of the real-world performance of our model?</p>
<p>To deal with this issue we will typically split our data into 3 parts that we’ll call <strong>training</strong>, <strong>validation</strong> and <strong>test</strong>. We’ll use the validation portion as the portion to fit the model and the test set as the portion we use to estimate how well it will do in practice.</p>
<div class="cell" data-execution_count="282">
<div class="sourceCode cell-code" id="cb134"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(<span class="dv">300</span>, noise<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb134-2"><a href="#cb134-2" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> torch.tensor(X).<span class="bu">float</span>(), torch.tensor(y)</span>
<span id="cb134-3"><a href="#cb134-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-4"><a href="#cb134-4" aria-hidden="true" tabindex="-1"></a>inds <span class="op">=</span> np.arange(X.shape[<span class="dv">0</span>])</span>
<span id="cb134-5"><a href="#cb134-5" aria-hidden="true" tabindex="-1"></a>np.random.shuffle(inds)</span>
<span id="cb134-6"><a href="#cb134-6" aria-hidden="true" tabindex="-1"></a>Xtrain, ytrain <span class="op">=</span> X[inds[:<span class="dv">150</span>]], y[inds[:<span class="dv">150</span>]]</span>
<span id="cb134-7"><a href="#cb134-7" aria-hidden="true" tabindex="-1"></a>Xvalid, yvalid <span class="op">=</span> X[inds[<span class="dv">150</span>:<span class="dv">225</span>]], y[inds[<span class="dv">150</span>:<span class="dv">225</span>]]</span>
<span id="cb134-8"><a href="#cb134-8" aria-hidden="true" tabindex="-1"></a>Xtest, ytest <span class="op">=</span> X[inds[<span class="dv">225</span>:]], y[inds[<span class="dv">225</span>:]]</span>
<span id="cb134-9"><a href="#cb134-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-10"><a href="#cb134-10" aria-hidden="true" tabindex="-1"></a><span class="co"># We can plot training and test data on the same plot</span></span>
<span id="cb134-11"><a href="#cb134-11" aria-hidden="true" tabindex="-1"></a>plt.scatter(<span class="op">*</span>Xtrain.T, c<span class="op">=</span>ytrain, edgecolor<span class="op">=</span><span class="st">"black"</span>, marker<span class="op">=</span><span class="st">'o'</span>, label<span class="op">=</span><span class="st">'training data'</span>)</span>
<span id="cb134-12"><a href="#cb134-12" aria-hidden="true" tabindex="-1"></a>plt.scatter(<span class="op">*</span>Xtest.T, c<span class="op">=</span>ytest, edgecolor<span class="op">=</span><span class="st">"black"</span>, marker<span class="op">=</span><span class="st">'^'</span>, label<span class="op">=</span><span class="st">'test data'</span>)</span>
<span id="cb134-13"><a href="#cb134-13" aria-hidden="true" tabindex="-1"></a>plt.scatter(<span class="op">*</span>Xvalid.T, c<span class="op">=</span>yvalid, edgecolor<span class="op">=</span><span class="st">"black"</span>, marker<span class="op">=</span><span class="st">'*'</span>, label<span class="op">=</span><span class="st">'valid data'</span>)</span>
<span id="cb134-14"><a href="#cb134-14" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="282">
<pre><code>&lt;matplotlib.legend.Legend at 0x2e0c664c0&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-84-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="283">
<div class="sourceCode cell-code" id="cb136"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a>network <span class="op">=</span> nn.Sequential(</span>
<span id="cb136-2"><a href="#cb136-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">200</span>),</span>
<span id="cb136-3"><a href="#cb136-3" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb136-4"><a href="#cb136-4" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">200</span>, <span class="dv">200</span>),</span>
<span id="cb136-5"><a href="#cb136-5" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb136-6"><a href="#cb136-6" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">200</span>, <span class="dv">200</span>),</span>
<span id="cb136-7"><a href="#cb136-7" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb136-8"><a href="#cb136-8" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">200</span>, <span class="dv">1</span>),</span>
<span id="cb136-9"><a href="#cb136-9" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb136-10"><a href="#cb136-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb136-11"><a href="#cb136-11" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegressionNeuralNetwork(network)   </span>
<span id="cb136-12"><a href="#cb136-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-13"><a href="#cb136-13" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.03</span>)</span>
<span id="cb136-14"><a href="#cb136-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-15"><a href="#cb136-15" aria-hidden="true" tabindex="-1"></a>train_losses, valid_losses <span class="op">=</span> [], []</span>
<span id="cb136-16"><a href="#cb136-16" aria-hidden="true" tabindex="-1"></a>pbar <span class="op">=</span> tqdm.trange(<span class="dv">25000</span>)</span>
<span id="cb136-17"><a href="#cb136-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> pbar:</span>
<span id="cb136-18"><a href="#cb136-18" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(Xtrain)        <span class="co"># Now we can just call model!</span></span>
<span id="cb136-19"><a href="#cb136-19" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy(predictions, ytrain.flatten().<span class="bu">float</span>())</span>
<span id="cb136-20"><a href="#cb136-20" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb136-21"><a href="#cb136-21" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb136-22"><a href="#cb136-22" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb136-23"><a href="#cb136-23" aria-hidden="true" tabindex="-1"></a>    pbar.set_description(<span class="st">'Loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> loss.item())</span>
<span id="cb136-24"><a href="#cb136-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-25"><a href="#cb136-25" aria-hidden="true" tabindex="-1"></a>    train_losses.append(loss.item())</span>
<span id="cb136-26"><a href="#cb136-26" aria-hidden="true" tabindex="-1"></a>    valid_loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy(model(Xvalid), yvalid.flatten().<span class="bu">float</span>())</span>
<span id="cb136-27"><a href="#cb136-27" aria-hidden="true" tabindex="-1"></a>    valid_losses.append(valid_loss.item())</span>
<span id="cb136-28"><a href="#cb136-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-29"><a href="#cb136-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&gt;</span> <span class="dv">50</span> <span class="kw">and</span> valid_loss.item() <span class="op">&gt;</span> valid_losses[<span class="op">-</span><span class="dv">2</span>]:</span>
<span id="cb136-30"><a href="#cb136-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loss: 0.406:   4%|▍         | 1060/25000 [00:02&lt;00:46, 513.32it/s]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="284">
<div class="sourceCode cell-code" id="cb138"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a>train_acc <span class="op">=</span> (model.predict(Xtrain) <span class="op">==</span> ytrain).<span class="bu">float</span>().mean()</span>
<span id="cb138-2"><a href="#cb138-2" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtrain, ytrain, title<span class="op">=</span><span class="st">'Training accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> train_acc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-86-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="287">
<div class="sourceCode cell-code" id="cb139"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb139-1"><a href="#cb139-1" aria-hidden="true" tabindex="-1"></a>test_acc <span class="op">=</span> (model.predict(Xvalid) <span class="op">==</span> yvalid).<span class="bu">float</span>().mean()</span>
<span id="cb139-2"><a href="#cb139-2" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xvalid, yvalid, title<span class="op">=</span><span class="st">'Validation accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> test_acc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-87-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="285">
<div class="sourceCode cell-code" id="cb140"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true" tabindex="-1"></a>test_acc <span class="op">=</span> (model.predict(Xtest) <span class="op">==</span> ytest).<span class="bu">float</span>().mean()</span>
<span id="cb140-2"><a href="#cb140-2" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtest, ytest, title<span class="op">=</span><span class="st">'Test accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> test_acc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook copy_files/figure-html/cell-88-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="cross-validation" class="level2">
<h2 class="anchored" data-anchor-id="cross-validation">Cross validation</h2>
<p>Another important approach is <strong>cross validation</strong>. In this setting, rather than using a single validation set, we will split our training set many times!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="image-3.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Alt text</figcaption><p></p>
</figure>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>