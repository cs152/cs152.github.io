[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course Syllabus",
    "section": "",
    "text": "In this course, we will introduce neural networks as a tool for machine learning and function approximation. We will start with the fundamentals of how to build and train a neural network from scratch. We will work our way towards discussing state-of-the-art neural networks including networks that power applications like object recognition, image generation and large language models. Throughout the course we will keep a strong focus on the implications of these models and how to apply them responsibly.\nCourse Slack: https://join.slack.com/t/cs152neuralne-fop9003/shared_invite/zt-2ax94g9q8-Av_OBLyv2Lh63Om2WGWYRw\nCourse Survey: https://forms.gle/QaaKDcqpZL8ZvTAB7\n\n\n\n\n\n\n\nProf. Gabe Hope (he/him)\nEmail: ghope@g.hmc.edu\nOffice: MacGregor 322\nOffice hours: Thursdays 3:30-5pm MacGregor 322\n\n\n\nYou can call me any combination of Prof./Professor/Dr. and Hope/Gabe. My full name is actually John Gabriel Hope.\nI am originally from New York City\nI have a Bachelor of Science (BS) in computer science and finance from Washington University in St. Louis.\nI have a Master of Science (MS) from Brown University (This was actually the start of my Ph.D.)\nI earned my Ph.D. from the University of California, Irvine advised by Erik Sudderth.\nMy research focuses on using neural networks to find interpretable structures in data. I mainly focus on image data, though I have also worked on analyzing motion-capture, audio and climate data among other things!\n\n\n\n\n\n\n\n\n\nLinear and logistic regression\nGradient descent and optimization\nFeature transforms and feed-forward networks\nPerformance tuning and analysis for neural networks\nConvolutional neural networks\nRegularization and normalization\nResidual networks and large-scale architectures\nAttention and transformers\nBiases and fairness in machine learning\n\n\n\n\nProbabilistic Machine Learning by Kevin Murphy. Full text for book 1 and book 2 are available for free online.\nOpen door policy: If my door is open, feel free to stop in, say hi and ask questions about the course, research or any other academic issues. If the door is closed, feel free to knock. I often like to close my door to focus, but it does not always mean I am unable to talk. If I don’t answer promptly I am either out of office or in a meeting and am unable to talk. If in doubt, feel free contact me on slack. Note that I generally prefer to keep longer discussion of course materials to designated office hours."
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "Course Syllabus",
    "section": "",
    "text": "Prof. Gabe Hope (he/him)\nEmail: ghope@g.hmc.edu\nOffice: MacGregor 322\nOffice hours: Thursdays 3:30-5pm MacGregor 322\n\n\n\nYou can call me any combination of Prof./Professor/Dr. and Hope/Gabe. My full name is actually John Gabriel Hope.\nI am originally from New York City\nI have a Bachelor of Science (BS) in computer science and finance from Washington University in St. Louis.\nI have a Master of Science (MS) from Brown University (This was actually the start of my Ph.D.)\nI earned my Ph.D. from the University of California, Irvine advised by Erik Sudderth.\nMy research focuses on using neural networks to find interpretable structures in data. I mainly focus on image data, though I have also worked on analyzing motion-capture, audio and climate data among other things!"
  },
  {
    "objectID": "index.html#topics-covered",
    "href": "index.html#topics-covered",
    "title": "Course Syllabus",
    "section": "",
    "text": "Linear and logistic regression\nGradient descent and optimization\nFeature transforms and feed-forward networks\nPerformance tuning and analysis for neural networks\nConvolutional neural networks\nRegularization and normalization\nResidual networks and large-scale architectures\nAttention and transformers\nBiases and fairness in machine learning"
  },
  {
    "objectID": "index.html#textbook",
    "href": "index.html#textbook",
    "title": "Course Syllabus",
    "section": "",
    "text": "Probabilistic Machine Learning by Kevin Murphy. Full text for book 1 and book 2 are available for free online.\nOpen door policy: If my door is open, feel free to stop in, say hi and ask questions about the course, research or any other academic issues. If the door is closed, feel free to knock. I often like to close my door to focus, but it does not always mean I am unable to talk. If I don’t answer promptly I am either out of office or in a meeting and am unable to talk. If in doubt, feel free contact me on slack. Note that I generally prefer to keep longer discussion of course materials to designated office hours."
  },
  {
    "objectID": "index.html#vscode-optional",
    "href": "index.html#vscode-optional",
    "title": "Course Syllabus",
    "section": "VSCode (Optional)",
    "text": "VSCode (Optional)\nVisual Studio Code is a free development environment developed by Microsoft. It is available for Mac, Windows and Linux, and provides convenient tools for working with Python, Git and Jupyter. It is what I use to develop the materials for this course, and it is what I would recommend using for homework assignments. This is completely optional however. You are welcome to use whatever environment you feel most comfortable with.\nHere are resources for getting started:\n\nRecommended extensions for data science and working with Jupyter notebooks are listed here.\nInstructions for setting up Python in VSCode are here.\nInstructions for working with Jupyter notebooks in VSCode are here.\nInstructions for setting up Git in VSCode are here."
  },
  {
    "objectID": "index.html#python",
    "href": "index.html#python",
    "title": "Course Syllabus",
    "section": "Python",
    "text": "Python\nAssignments and projects in this course will be based on Python 3. We will be using the following packages throughout this course:\n\nNumpy: The industry-standard Python library for working with vectors, matrices and general numerical computing.\nMatplotlib: The most popular and widely supported library for data visualization in Python.\nSciKit-Learn: A popular library for basic machine learning.\nPyTorch: A deep learning library. Currently the most popular library for neural networks research and competitive with TensorFlow in terms of industry deployment.\n\nYou can find this course’s requirements file here. It will also be included in homework distributions. You can install the full set of packages using the command:\npip install -r requirements.txt"
  },
  {
    "objectID": "index.html#jupyter",
    "href": "index.html#jupyter",
    "title": "Course Syllabus",
    "section": "Jupyter",
    "text": "Jupyter\nMost homework assignments will be distributed and submitted as Jupyter notebooks. Jupyterlab is included in the course requirements.txt file, but instructions for installing it are also available here. Once installed, you can launch a JupyterLab server locally on you computer using the command:\njupyter lab\nThis will open the Jupyter Lab application in a web browser. From there you can navigate to the homework’s main.ipynb file. Resources and documentation for working with Jupyter notebooks are available here."
  },
  {
    "objectID": "index.html#latex-style-equations",
    "href": "index.html#latex-style-equations",
    "title": "Course Syllabus",
    "section": "Latex (style) equations",
    "text": "Latex (style) equations\nHomework assignments will ask you to submit answers requiring mathematical derivations as typeset Latex-style equations. Latex equations are supported directly within Jupyter. To write an equation in a text/markdown cell, simply surround the equation with $ symbols as: $y = x^2 + 1$, which produces the output: \\(y=x^2 +1\\). You can write block equation using double dollar-signs as $$y = x^2 + 1$$, which puts the equation on its own centered line.\nAn extensive reference for Latex equations is available here.\nIn general, only the final answer to such problems will be required to be submitted in this way, intermediate steps in derivations can be submitted separately as handwritten notes. To do this, scan or photograph (clearly!) the handwritten derivations and include them (ideally) as images in your notebook or as a separeate derivations.pdf file in the homework repository. You may also omit intermediate steps altogether, but this is not recommended as it may limit your ability to earn partial credit if your answer is incorrect."
  },
  {
    "objectID": "index.html#submitting-pdfs-to-gradescope",
    "href": "index.html#submitting-pdfs-to-gradescope",
    "title": "Course Syllabus",
    "section": "Submitting PDFs to Gradescope",
    "text": "Submitting PDFs to Gradescope\nEach complete homework notebook should be submitted as a PDF file to Gradescope. This primarily where homeworks will be graded and where grades and regrade requests will be handeled. Jupyter notebooks can be converted to PDFs either through VSCode (instructions here), or via online tools such as this one."
  },
  {
    "objectID": "index.html#course-gpu-server",
    "href": "index.html#course-gpu-server",
    "title": "Course Syllabus",
    "section": "Course GPU Server",
    "text": "Course GPU Server\nWe have a GPU server for this course that will be available to you for your final projects. (Thank you to our system administrator Tim Buchheim for setting this up!). The server is located at teapot.cs.hmc.edu (named for the Utah teapot). We will discuss how to allocate resources on this machine at the start of the course project. You will not need GPU access for most homework assignments."
  },
  {
    "objectID": "index.html#homework-assignments-60-of-course-grade",
    "href": "index.html#homework-assignments-60-of-course-grade",
    "title": "Course Syllabus",
    "section": "Homework assignments (60% of course grade)",
    "text": "Homework assignments (60% of course grade)\nFrequency and deadlines: Homeworks will be assigned on a weekly basis throughout the semester, with the exception of weeks where final project checkpoints are due. Homeworks are assigned on Wednesdays and must be submitted on gradescope and Github by the end of the following Thursday (11:59pm Thursday).\nSubmission Homework assignments are submitted by uploading a solution PDF to Gradescope. You must convert your completed notebook to PDF using the CS152 conversion tool. This page will extract and separate the answers for each question. Please make sure to check that all answers are included before submitting the PDF to Gradescope. Incorrectly formatted submissions are subject to a 10% penalty. See further instructions under software and tools!\nLate policy: Homeworks may be submitted up to 1 day late with no penalty (to Friday 11:59pm). Assignments submitted on time will recieve a 5 point bonus (up to 100 total points).\nDrops: The lowest 2 homework scores will be dropped unconditionally. Your homework score will be computed as the average of the remaining scores. Please note that you should still try to complete every homework in this class. These drops are intended to cover normal unforseen circumstances such as minor illness, conference or clinic travel and job interviews, as well as to allow you to balance your work with other classes and take care of your mental health.\nExtension policy: For situations such as extended illnesses, family emergencies, or other significant and unforeseen circumstances preventing you from working, I’m happy to work with you to try to find a way to give you the space you need while being able to come back to the course when you’re able. The best way to start this conversation is by directly emailing me (ghope@g.hmc.edu). To protect your privacy about the reasons for these circumstances and to ensure you get the support you need, I may ask you to reach out about this to your campus’ Division of Student Affairs, Dean of Students office, or your Student Disability Services coordinator to verify what kind of accommodation makes sense and to ensure you’re being supported across your other courses. If it’s more comfortable, you may also choose to reach out to any of these groups to reach out to me before contacting me yourself."
  },
  {
    "objectID": "index.html#participation-10-of-course-grade",
    "href": "index.html#participation-10-of-course-grade",
    "title": "Course Syllabus",
    "section": "Participation (10% of course grade)",
    "text": "Participation (10% of course grade)\nThis course is not generally a discussion-based class, however there will be certain lectures with open-ended discussions throughout the semester. The participation grade will be based on the following factors:\n\nParticipation in open-ended discussion sessions during class\nContriubting to the learning environment by asking or answering questions during regular lectures\nFollowing the guidelines for respectful discussion (as outlined in course policies)\nClass attendance\nAttending office hours outside class\nSupport provided to peers, e.g. by helping others debug or answering questions on Slack.\n\nEarning a perfect participation grade will not require full marks for all of these criteria, and it is expected that most students will earn full credit for participation. A perfect participation grade will be earned by any student who: attends class regularly (&gt; 80% of the time) and at least occasionally participates respectfully in class. Attending office hours is not strictly required, but if you miss class or are struggling to participate, I will assign bonus points to your participation grade for attending. If you have questions about your participation grade at any point, please contact me."
  },
  {
    "objectID": "index.html#final-project-30-of-course-grade",
    "href": "index.html#final-project-30-of-course-grade",
    "title": "Course Syllabus",
    "section": "Final Project (30% of course grade)",
    "text": "Final Project (30% of course grade)\nThe culmination of this course will be a final project completed in teams of 2-4 students. Full project description to follow. Your grade for the final project will depend on:\n\nThe strength of your team’s final presentation and write-up\nYour strength as a team-member (determined by self, peer and instructor evaluations)\nInitial project proposal\nMid-project check-ins\n\nStudents enter this class with highly varying backgrounds and prior experiences with neural networks, so I will help each team determine an appropriate scope for their project. Grades will be evaluated for each team individually based on how the team approached, analyzed and executed on the goals of the project. The relative technical sophistication of other teams projects will not be considered."
  },
  {
    "objectID": "index.html#letter-grade-assignments",
    "href": "index.html#letter-grade-assignments",
    "title": "Course Syllabus",
    "section": "Letter grade assignments",
    "text": "Letter grade assignments\nAs this course is still under active development I cannot yet guarantee exact cutoffs for each grade. Harvey Mudd does not impose expectations for the grade distribution, so every student that meets the requirements for a given grade will earn it. The following is the maximum cutoff of each letter grade, the actual cutoff for each grade may be lower that what is listed below:\n\n&gt;90%: A\n&gt;80%: B\n&gt;70%: C\n&gt;60%: D\n\nAs the semester progresses, I will update this guide to provide a clearer picture of how grades will be assigned."
  },
  {
    "objectID": "index.html#course-feedback",
    "href": "index.html#course-feedback",
    "title": "Course Syllabus",
    "section": "Course feedback",
    "text": "Course feedback\nThis is my first time teaching a college course, so I will need your help! I want to make sure that we go through the material at an appropriate pace and that concepts are presented in a clear and understandable way. To do this, I will be continuously soliciting feedback from you throughout the semester on both the lectures and assignments. I ask that you provide feedback honestly, but kindly. There are three mechanisms I will use for feedback:\nIn-class: In class we will use a thumbs-up, thumbs down system. When I ask if people are following the lecture you can put your thumb up to indicate that you feel you are understanding the material being presented, down to indicate that you are lost or the lecture is confusing, and sideways to indicate that you followed some parts, but not all. You are, of course, also encouraged to give verbal feedback if appropriate.\nWith homework: Each homework will include a link to a survey to give feedback on that week’s assignment and lectures. Submitting this form is a required part of the homework, but your answers will not be tracked or accounted for in grades. This gives you a chance to indicate any issues (or things you like) with the class.\nGeneral anonymous feedback: If you have an issue with the course that you would like me to be aware of, but do not want your name to be associated with, you can use this form to submit an anonymous issue. Please continue to remain constructive and kind when submitting feedback in this way."
  },
  {
    "objectID": "index.html#academic-issues-and-wellness",
    "href": "index.html#academic-issues-and-wellness",
    "title": "Course Syllabus",
    "section": "Academic issues and wellness",
    "text": "Academic issues and wellness\nMy primary goal is for every student to understand the material to the best extent possible and hopefully enjoy learning the material at the same time. If at any point you are concerned about your grade or feel you are struggling for any reason I encourage you to reach out to me either via slack/email or during office hours. I will also try to reach out to you if I notice you are struggling with the material or are not on track to pass the class.\nI also want you to prioritize your mental and physical well-being. The college has several resources that can help you with this. The academic deans (academicdeans@g.hmc.edu) can help you keep on top of your academic progress. The office of health and wellness (https://www.hmc.edu/health-wellness/) can help you with a wide range of physical and metal health issues. I encourage you to be proactive, if you are starting to feel anxious, overwhelmed or depressed seeking help early can be extremely valuable. If you are unsure where to go, I can help guide you to the appropriate resource. The Claremont Care Guide, provides a useful guide if you or someone you know is in urgent distress."
  },
  {
    "objectID": "index.html#accommodations",
    "href": "index.html#accommodations",
    "title": "Course Syllabus",
    "section": "Accommodations",
    "text": "Accommodations\nIf you have a disability (for example, mental health, learning, chronic health, physical, hearing, vision, neurological, etc.) and expect barriers related to this course, it is important to request accommodations and establish a plan. Requests for accommodations must go through the Office of Accessible Education. I am happy to work with them to establish an appropriate plan for this course. I also encourage reaching out to them if you are unsure of your eligibility for accommodations, they can help determine what is appropriate for you.\nRemember that requests for accommodations must be made each semester. If you are not already registered this process can take time and accommodations cannot be applied retroactively, so starting the process early is important."
  },
  {
    "objectID": "index.html#attendence",
    "href": "index.html#attendence",
    "title": "Course Syllabus",
    "section": "Attendence",
    "text": "Attendence\nAttendence is strongly encouraged as it is beneficial for both your own learning and that of your peers who may learn from your knowledge and viewpoints. Not only is attendance reflected in your participation grade, it is also highly correlated with performance on exams and homework. That said, I understand that there are times where student may miss class for a variety of reasons. If you miss a class (or several) please contact me by email or slack and we can work out a plan to catch you up on the material. Up to 1 unexcused absence per month will not affect your participation grade, neither will excused absences due to illness, injury, etc."
  },
  {
    "objectID": "index.html#guidelines-for-respectful-class-discussion",
    "href": "index.html#guidelines-for-respectful-class-discussion",
    "title": "Course Syllabus",
    "section": "Guidelines for respectful class discussion",
    "text": "Guidelines for respectful class discussion\nThe goal of in-class discussions to understand each others perspectives and to contribute to both our own learning and that of our peers. To make sure that in-class discussions are aligned with these goals please be mindful of the following guidelines:\n\nAvoid judgment: Students enter this class with a variety of backgrounds, experience and viewpoints. Be positive and encouraging to your peers even if you feel they are incorrect. Strive to make sure those around you feel comfortable answering questions even if they are not completely sure of their answer and give opinions that they are not sure others will agree with. Remember that giving an answer different from what the instructor was looking for can lead to productive and informative discussions.\nAllow everyone a chance to speak: We want to give every student a chance to participate in the class and in discussions. If you find yourself speaking, answering or asking questions far more than your peers, consider encouraging others to speak instead. Remember that in-class time is not your only opportunity to discuss this material and you are welcome to ask more questions in office hours.\nPractice active listening: When having in-class discussions make sure to acknowledge the answers and opinions of others before offering your own. Avoid interrupting others. Your thoughts deserve to be heard and understood, so it’s important that we work together to make sure everyone’s contributions are considered.\nBe kind: Do not use harsh or disparaging language in class. Avoid blame or speculation about other students. Aim to be charitable in your interpretations of other peoples words. Respect the boundaries set by others.\nBe inclusive: Be respectful of everyone’s background and identity. Avoid making assumptions or generalizations based on someone’s (perceived) social group. Do not ask individuals to speak for their social group."
  },
  {
    "objectID": "index.html#collaboration-policy",
    "href": "index.html#collaboration-policy",
    "title": "Course Syllabus",
    "section": "Collaboration policy",
    "text": "Collaboration policy\nYou are encouraged to discuss and collaborate on homework assignments with other students, but you must write-up all final answers on your own. This means you may:\n\nDiscuss published course materials and topics\nDiscuss approaches to problems with other students, including while working on the assignments (work in small groups).\nShare helpful examples and resources with other students\nHelp other students with technical issues such as setting up GitHub and Python environments.\nView another student’s code for the purpose of debugging small technical issues (exceptions, syntax errors etc.)\n\nYou may not:\n\nCopy/paste another student’s completed answers to any problem or allow another student to copy/paste your answers\nShare answers to completed problems with other students\nDistribute or post online any assignments, problems and/or solutions.\nFail to acknowledge collaborators.\n\nThis collaboration policy is covered by the Harvey Mudd honor code and violations will be referred to the honor code board.\nEach homework will have space for you to indicate who you discussed the assignment with. Please use the #team-matching channel on the course Slack to find study partners."
  },
  {
    "objectID": "index.html#ai-policy",
    "href": "index.html#ai-policy",
    "title": "Course Syllabus",
    "section": "AI Policy",
    "text": "AI Policy\nIn this course we will be learning the fundamental tools for building large language models and chat AIs, such as ChatGPT. Therefore I encourage you to experiment with ChatGPT and it’s competitors during this course, but only for learning about concepts. You may not share any assignment materials with large language models including assignment questions, support code and your own answers. This includes AI-assisted code-helpers such as Github co-pilot and Google Gemini in Colab. Please disable these tools when working on class assignments.\nHomeworks in this class are highly structured, in order to guide you through the often complex tools at the heart of deep learning. While I believe this structure is helpful for learning This makes the assignments particularly susceptible to AI-assisted academic dishonesty."
  },
  {
    "objectID": "index.html#covid-safety",
    "href": "index.html#covid-safety",
    "title": "Course Syllabus",
    "section": "COVID Safety",
    "text": "COVID Safety\nCollege policy states that masks are no longer required indoors for the upcoming semester. I will not require masks in class, but students who prefer to continue wearing masks are should do so. If you are feeling sick please stay home and let me know so that I can provide you with the relevant course materials."
  },
  {
    "objectID": "index.html#courses",
    "href": "index.html#courses",
    "title": "Course Syllabus",
    "section": "Courses",
    "text": "Courses\n\nfastai (website)\nDeep Learning Specialization (Coursera MOOC)\nDeep Learning (Stanford CS230 course)\nConvolutional Neural Networks for Visual Recognition (Stanford CS231n course)\nIntroduction to Deep Learning (MIT 6.S191 course)\nMIT Deep Learning and Artificial Intelligence Lectures (MIT course)\nDeep Reinforcement Learning (Berkeley CS 285 course)\nDeep Reinforcement Learning (free online course)\nDeep Learning Systems"
  },
  {
    "objectID": "index.html#books",
    "href": "index.html#books",
    "title": "Course Syllabus",
    "section": "Books",
    "text": "Books\n\nDive into Deep Learning (UC Berkeley book)\nDeep Learning (free book)\nFirst steps towards Deep Learning with PyTorch (free book)\nNeural Networks and Deep Learning (free book)\nDeep Learning With PyTorch (pdf)\nAnnotated Algorithms in Python (free book)\nLearn Python the Right way (free book)\nThe Linux Command Line by William Shotts"
  },
  {
    "objectID": "index.html#math",
    "href": "index.html#math",
    "title": "Course Syllabus",
    "section": "Math",
    "text": "Math\n\nThe Matrix Calculus You Need For Deep Learning (website)\nThe Mechanics of Machine Learning (free book)\nMathematics for Machine Learning (free book)\nSeeing Theory: A Visual Introduction To Probability And Statistics (free book)"
  },
  {
    "objectID": "index.html#extras",
    "href": "index.html#extras",
    "title": "Course Syllabus",
    "section": "Extras",
    "text": "Extras\n\nCheatsheet (website)\nTensorFlow 2.0 Complete Course - Python Neural Networks for Beginners Tutorial (YouTube)\nNeural Networks (3blue1brown YouTube)\nMachine Learning From Scratch (website)\nA visual introduction to machine learning (website)"
  },
  {
    "objectID": "index.html#python-1",
    "href": "index.html#python-1",
    "title": "Course Syllabus",
    "section": "Python",
    "text": "Python\n\nGoogle’s Python Class\nIntroduction to Python | Microsoft Learn\nList of free free Python books\nPython Programming Tutorials\nLearn Python - Full Course for Beginners (YouTube)\nPython In 1 Minute (YouTube)\nAutomate the Boring Stuff With Python (book)\nIntroduction to Python Programming (free course)\nA Whirlwind Tour of Python (Jupyter Notebooks)\nPython for Everybody Specialization (free course)\nIntroduction to Computer Science and Programming Using Python (MIT course)"
  },
  {
    "objectID": "index.html#ethics",
    "href": "index.html#ethics",
    "title": "Course Syllabus",
    "section": "Ethics",
    "text": "Ethics\n\nAwful AI\nLearning from the past to create Responsible AI\nPractical Data Ethics\nFair ML Book\nMachine Ethics Podcast\nACM Code of Ethics and Professional Conduct\nIEEE Code of Ethics\nCode of Conduct for Professional Data Scientists"
  },
  {
    "objectID": "index.html#librariesframeworkstools",
    "href": "index.html#librariesframeworkstools",
    "title": "Course Syllabus",
    "section": "Libraries/Frameworks/Tools",
    "text": "Libraries/Frameworks/Tools\n\nMlxtend (machine learning extensions)\nStreamlit (Turn data scripts into sharable web apps in minutes)\nDeepnote (The notebook you’ll love to use)"
  },
  {
    "objectID": "calendar/calendar.html",
    "href": "calendar/calendar.html",
    "title": "Course Calendar",
    "section": "",
    "text": "This calendar is subject to change depending on the pace of the class and student interest.\n\n\n\n\n\n\n\n\nDate\nEvent\nTopics\nReadings\nMaterials\n\n\n\n\nWeek 1\n\n\nJan 20, 2025\nMartin Luther King Jr. Day\n\n\n\n\n\nJan 22, 2025\nLecture 1\nCourse introduction, goals of neural networks\nBook 1: 1.2.1-1.2.2, 7.1.1-7.2.4, 7.8.1, 7.8.2\nNotes\n\n\nWeek 2\n\n\nJan 27, 2025\nLecture 2\nLinear regression, gradient descent\nBook 1: 4.2.1, 4.2.2, 11.1-11.2.4\nNotes Slides\n\n\nJan 29, 2025\nLecture 3\nLinear regression, maximum likelihood\nBook 1: 4.2.1, 4.2.2, 11.1-11.2.4\nNotes\n\n\nJan 30, 2025\nHomework 1 Due\n\n\nAssignment\n\n\nWeek 3\n\n\nFeb 3, 2025\nLecture 4\nLogistic regression\nBook 1: 10.1, 10.2.1, 10.2.3, 10.3.1-10.3.3\nNotes\n\n\nFeb 5, 2025\nLecture 5\nMultinomial logistic regression\nBook 1: 10.2.2\nNotes\n\n\nFeb 6, 2025\nHomework 2 Due\n\n\nAssignment\n\n\nWeek 4\n\n\nFeb 10, 2025\nLecture 6\nFeature transforms\nBook 1: 13.1, 13.2\nNotes\n\n\nFeb 12, 2025\nlecture 7\nNeural networks\nBook 1: 13.1, 13.2\nNotes\n\n\nFeb 13, 2025\nHomework 3 Due\n\n\nAssignment\n\n\nWeek 5\n\n\nFeb 17, 2025\nLecture 8\nDeep neural networks\nBook 1: 13.1, 13.2\nNotes\n\n\nFeb 19, 2025\nLecture 9\nAutomatic Differentiation\nBook 1: 13.3\nNotes\n\n\nFeb 20, 2025\nHomework 4 Due\n\n\nAssignment\n\n\nWeek 6\n\n\nFeb 24, 2025\nLecture 10\nReverse-mode Automatic Differentiation\nBook 1: 13.4.5, 13.5\nNotes\n\n\nFeb 26, 2025\nLecture 11\nPyTorch\nBook 1: 13.5\nNotebook\n\n\nFeb 27, 2025\nHomework 5 Due\n\n\nAssignment\n\n\nWeek 7\n\n\nMar 3, 2025\nLecture 12\nL1 & L2 regularization\nBook 1: 8.4\nNotes\n\n\nMar 5, 2025\nLecture 13\nRegularization (Cont.), Dropout\nBook 1: 13.4.1-13.4.2\nNotes\n\n\nMar 6, 2025\nHomework 6 Due\n\n\nAssignment\n\n\nWeek 8\n\n\nMar 10, 2025\nLecture 14\nStochastic gradient descent\nBook 1: 14.1-14.3\nNotes\n\n\nMar 12, 2025\nLecture 15\nStochastic gradient descent\nBook 1: 14.1-14.3\nNotes\n\n\nMar 13, 2025\nHomework 7 Due\n\n\nAssignment\n\n\nWeek 9\n\n\nMar 17, 2025\nSpring break\n\n\n\n\n\nWeek 10\n\n\nMar 24, 2025\nLecture 16\nResidual networks and Normalization\nBook 1: 15.1, 15.2.1-15.2.3\nSlides\n\n\nMar 26, 2025\nLecture 17\nResidual networks and Normalization (cont.)\nBook 1: 15.1, 15.2.1-15.2.3\nSlides\n\n\nMar 27, 2025\nEthics Warm-up Due\n\n\nAssignment\n\n\nWeek 11\n\n\nMar 31, 2025\nLecture 18\nConvolutional Networks\nBook 1: 15.2.5-15.2.7\nSlides\n\n\nApr 2, 2025\nLecture 19\nConvolutional Networks (cont.)\nBook 1: 15.2.5-15.2.7\nSlides\n\n\nApr 3, 2025\nHomework 8 Due\n\n\nAssignment\n\n\nWeek 12\n\n\nApr 7, 2025\nLecture 20\nAutoencoders & U-Nets\nBook 1: 15.2.1-15.2.6\n\n\n\nApr 9, 2025\nLecture 21\nImage models\n\n\n\n\nApr 10, 2025\nProject Proposal Due\n\n\nAssignment\n\n\nWeek 13\n\n\nApr 14, 2025\nLecture 22\nLanguage models, RNNs\nBook 1: 15.2.1-15.2.6\nSlides\n\n\nApr 16, 2025\nLecture 23\nAttention layers\n\n\n\n\nApr 17, 2025\nHomework 9 Due\n\n\nAssignment\n\n\nWeek 14\n\n\nApr 21, 2025\nLecture 24\nTransformers\n\n\n\n\nApr 23, 2025\nLecture 25\nTransformers (cont.), Large language models\n\n\n\n\nApr 24, 2025\nProject check-in due\n\n\nAssignment\n\n\nWeek 15\n\n\nApr 28, 2025\nLecture 26\nAddressing bias in machine learning\n\n\n\n\nApr 29, 2025\nLecture 27\nImpacts of machine learning\n\n\n\n\nApr 30, 2025\nHomework 10 Due\n\n\nAssignment\n\n\nFinals\n\n\nMay 9, 2025\nFinal project due\n\n\nAssignment\n\n\nMay 12, 2025\nFinal project feedback due"
  },
  {
    "objectID": "assignments/homeworks/homeworks.html",
    "href": "assignments/homeworks/homeworks.html",
    "title": "Homeworks",
    "section": "",
    "text": "Assignment\nRequirements.txt\nNotebook"
  },
  {
    "objectID": "assignments/homeworks/homeworks.html#homework-1",
    "href": "assignments/homeworks/homeworks.html#homework-1",
    "title": "Homeworks",
    "section": "",
    "text": "Assignment\nRequirements.txt\nNotebook"
  },
  {
    "objectID": "assignments/homeworks/homeworks.html#homework-2",
    "href": "assignments/homeworks/homeworks.html#homework-2",
    "title": "Homeworks",
    "section": "Homework 2",
    "text": "Homework 2\n\nAssignment\nRequirements.txt\nNotebook"
  },
  {
    "objectID": "assignments/homeworks/homeworks.html#homework-3",
    "href": "assignments/homeworks/homeworks.html#homework-3",
    "title": "Homeworks",
    "section": "Homework 3",
    "text": "Homework 3\n\nAssignment\nRequirements.txt\nNotebook\nSupport code"
  },
  {
    "objectID": "assignments/homeworks/homeworks.html#homework-4",
    "href": "assignments/homeworks/homeworks.html#homework-4",
    "title": "Homeworks",
    "section": "Homework 4",
    "text": "Homework 4\n\nAssignment\nRequirements.txt\nNotebook\nSupport code"
  },
  {
    "objectID": "assignments/homeworks/homeworks.html#homework-5",
    "href": "assignments/homeworks/homeworks.html#homework-5",
    "title": "Homeworks",
    "section": "Homework 5",
    "text": "Homework 5\n\nAssignment\nRequirements.txt\nNotebook\nSupport code"
  },
  {
    "objectID": "assignments/homeworks/homeworks.html#homework-6",
    "href": "assignments/homeworks/homeworks.html#homework-6",
    "title": "Homeworks",
    "section": "Homework 6",
    "text": "Homework 6\n\nAssignment\nRequirements.txt\nNotebook\nSupport code"
  },
  {
    "objectID": "assignments/homeworks/homeworks.html#homework-7",
    "href": "assignments/homeworks/homeworks.html#homework-7",
    "title": "Homeworks",
    "section": "Homework 7",
    "text": "Homework 7\n\nAssignment\nRequirements.txt\nNotebook\nSupport code"
  },
  {
    "objectID": "assignments/homeworks/homeworks.html#homework-8",
    "href": "assignments/homeworks/homeworks.html#homework-8",
    "title": "Homeworks",
    "section": "Homework 8",
    "text": "Homework 8\n\nAssignment\nRequirements.txt\nNotebook\nSupport code"
  },
  {
    "objectID": "assignments/homeworks/homeworks.html#homework-9",
    "href": "assignments/homeworks/homeworks.html#homework-9",
    "title": "Homeworks",
    "section": "Homework 9",
    "text": "Homework 9\n\nAssignment\nRequirements.txt\nNotebook\nSupport code"
  },
  {
    "objectID": "assignments/homeworks/homeworks.html#homework-10",
    "href": "assignments/homeworks/homeworks.html#homework-10",
    "title": "Homeworks",
    "section": "Homework 10",
    "text": "Homework 10\n\nAssignment\nRequirements.txt\nNotebook\nSupport code"
  },
  {
    "objectID": "assignments/homeworks/Homework 1/main.html",
    "href": "assignments/homeworks/Homework 1/main.html",
    "title": "Homework 1: Introduction to Numpy",
    "section": "",
    "text": "# Run me first!\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')"
  },
  {
    "objectID": "assignments/homeworks/Homework 1/main.html#part-1-numpy-basics",
    "href": "assignments/homeworks/Homework 1/main.html#part-1-numpy-basics",
    "title": "Homework 1: Introduction to Numpy",
    "section": "Part 1: Numpy basics",
    "text": "Part 1: Numpy basics\nAs discussed in class, a square matrix \\(A\\) defines a linear mapping: \\(\\mathbb{R}^n\\rightarrow \\mathbb{R}^n\\). Given a vector \\(\\textbf{x}\\), we can find the corresponding output of this mapping \\(\\textbf{b}\\) using matrix-vector multiplication: \\(\\textbf{b}=A \\textbf{x}\\). We can write an example matrix-multiplication using matrix notation as:\n\\[\\begin{bmatrix}\n4 & -3 & 2 \\\\\n6 & 5  & 1 \\\\\n-4 & -1 & 2\n\\end{bmatrix} \\cdot \\begin{bmatrix}\n1 \\\\\n-2 \\\\\n1\n\\end{bmatrix}\n= \\begin{bmatrix}\n? \\\\\n? \\\\\n?\n\\end{bmatrix}\n\\]\n\nQ1 (5 points)\nPerform this matrix-vector multiplication by hand and write the answer in the cell below.\nWRITE ANSWER HERE\n\n\n\n\n\nQ2 (5 points)\nIn the code cell below, create the matrix \\(A\\) and the vector \\(\\textbf{x}\\) shown above, using Numpy. Then use the np.dot function to find the output of the mapping \\(\\textbf{b} = A\\textbf{x}\\). Verify that the answer matches what you derived above.\n\n# Fill answers here\nA =\nx =\nb =\n\nprint(b)\n\nOften we will have access to the transformed vector \\(\\textbf{b}\\) and need to find the orginal vector \\(\\textbf{x}\\). To do this we need to solve the system of linear equations \\(A\\textbf{x}=\\textbf{b}\\) for \\(\\textbf{x}\\). \\[\\begin{bmatrix}\n4 & -3 & 2 \\\\\n6 & 5  & 1 \\\\\n-4 & -1 & 2\n\\end{bmatrix} \\cdot \\begin{bmatrix}\n? \\\\\n? \\\\\n?\n\\end{bmatrix} = \\begin{bmatrix}\n2 \\\\\n-1 \\\\\n3\n\\end{bmatrix}\n\\]\n\n\nQ3 (5 points)\nFind the missing \\(\\textbf{x}\\) in the equation above using the np.linalg.solve function and verify that \\(A\\textbf{x}=\\textbf{b}\\).\n\n# Fill answer here (A is the same matrix from above)\nb =\nx =\n\nIn linear algebra you may have learned how to solve a system of linear equations using Gaussian elimination. Here we will implement an alternative approach known as Richardson iteration. In this method we start with an inital guess for the solution: \\(\\textbf{x}^{(0)}\\), then we will iteratively update this guess until the solution is reached. Given a matrix \\(A\\), a target \\(\\textbf{b}\\) and a current guess \\(\\textbf{x}^{(k)}\\), we can compute the Richardson update as:\n\\[\\textbf{x}^{(k+1)} \\leftarrow \\textbf{x}^{(k)} + \\omega \\left(\\textbf{b} - A\\textbf{x}^{(k)}\\right)\\]\nHere \\(\\omega\\) is a constant that we can choose to adjust the algorithm. We will set \\(\\omega = 0.1\\).\n\n\nQ4 (5 points)\nFill in the Richardson iteration function below and apply it to the system of linear equations from above using 100 updates. Verify that if gives a similar answer to np.linalg.solve.\n\n# Fill in function below\ndef richardson_iter(x_guess, A, b, omega=0.1):\n\n    return new_x_guess\n\nx_guess = np.zeros(3)\nfor i in range(100):\n    x_guess = richardson_iter(x_guess, A, b)\n\nprint(x_guess, x)\n\nRecall that the length of a vector is given by it’s two-norm, which is defined as: \\[\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}.\\]\nCorrespondingly, the (Euclidian) distance between two points \\(\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^n\\) can be written as \\(\\|\\mathbf{a} - \\mathbf{b}\\|_2\\). As a convenient measure of error for our Richardson iteration algorithm, we will use the squared Euclidean distance. For a guess \\(\\mathbf{x}^{(k)}\\) we will compute the error \\(e^{(k)}\\) as: \\[e^{(k)} = \\|A\\mathbf{x}^{(k)} - \\mathbf{b}\\|_2^2\\]\nIn expanded form, this would be written as: \\[e^{(k)} = \\sum_{i=1}^n \\left(\\sum_{j=1}^n A_{ij}x^{(k)}_j - b_i\\right)^2\\]\n\n\nQ5 (10 points)\nWrite a function to compute the error of a given guess. Then run Richardson iteration again for 100 steps, computing the error at each step. Finally create a plot of the error for each step (error vs. step). Plot reference\nHint: recall that basic operations in numpy (addition, subtraction, powers) are performed element-wise.\n\n# Fill in function below\ndef error(x_guess, A, b):\n\n    return err\n\n# Add code to plot the error over time\n\nx_guess = np.zeros(3)\nfor step in range(100):\n\n    x_guess = richardson_iter(x_guess, A, b)\n\n\n\n\nQ6 (10 points)\nDerive the partial derivative of the error with respect to a single entry of \\(\\mathbf{x}^{(k)}\\) (without loss of generality, we will say \\(x^{(k)}_1\\)). Work in the expanded form as in the equation above, writing your answer in the markdown cell below.\nHint: You may find it helpful to refer to the latex equation cheatsheet on the course website. You may show intermediate steps here or as handwritten work as a separate file in the repository. The final answer should be filled in here.\nEDIT THIS CELL WITH YOUR ANSWER\n\\[\\frac{\\partial e^{(k)}}{\\partial x^{(k)}_1}= \\]\nYOU MAY ADD WORK HERE\nIn practice, we will likely want to compute the derivative with respect to all entries of \\(\\mathbf{x}\\): \\[\\frac{\\partial e^{(k)}}{\\partial \\mathbf{x}^{(k)}} = \\begin{bmatrix}\n\\frac{\\partial e^{(k)}}{\\partial x^{(k)}_1} \\\\\n\\vdots \\\\\n\\frac{\\partial e^{(k)}}{\\partial x^{(k)}_n}\n\\end{bmatrix}\\]\n\n\nQ7 (10 points)\nUsing the formula you just derived, write the formula for the vector of all partial derivatives in the compact matrix/vector notation (e.g. \\(A\\mathbf{x}=\\mathbf{b}\\)).\nEDIT THIS CELL WITH YOUR ANSWER\n\\[\\frac{\\partial e^{(k)}}{\\partial \\mathbf{x}^{(k)}}= \\]\n\n\nQ8 (5 points)\nDo you notice any relationship between this result and the Richardson iteration algorithm above? (1-2 sentences) We will discuss this more in class!\nWRITE YOUR ANSWER HERE"
  },
  {
    "objectID": "assignments/homeworks/Homework 1/main.html#part-2-working-with-batches-of-vectors",
    "href": "assignments/homeworks/Homework 1/main.html#part-2-working-with-batches-of-vectors",
    "title": "Homework 1: Introduction to Numpy",
    "section": "Part 2: Working with batches of vectors",
    "text": "Part 2: Working with batches of vectors\nRecall that a vector can also be seen as either an \\(n \\times 1\\) matrix (column vector) or a \\(1 \\times n\\) matrix (row vector).\n\\[\\text{Column vector: } \\mathbf{x} = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3\n\\end{bmatrix}, \\quad \\text{Row vector: } \\mathbf{x}^T = \\begin{bmatrix}\nx_1 & x_2 & x_3\n\\end{bmatrix}\\]\nNote that we use the same notation for both as they refer to the same concept (a vector). The difference becomes relevant when we consider matrix-vector multiplication. We can write matrix-vector multiplication in two ways: \\[\\text{Matrix-vector: }A\\mathbf{x} = \\mathbf{b}, \\quad \\text{Vector-matrix: }\\mathbf{x}^TA^T= \\mathbf{b}^T\\] In matrix-vector multiplication we treat \\(\\textbf{x}\\) as a column vector (\\(n \\times 1\\) matrix), while in vector-matrix multiplication we treat it as a row vector (\\(n \\times 1\\) matrix). Transposing \\(A\\) for left multiplication ensures that the two forms give the same answer.\n\nQ9 (5 points)\nUsing the previously defined \\(\\mathbf{x}\\), create an explicit column vector and row vector. Then using the previously defined \\(A\\), verify that the matrix-vector and vector-matrix multiplications shown above do produce the same resultant vector \\(\\mathbf{b}\\).\nHint: Recall that np.dot is also used for matrix-matrix multiplication. The values of \\(\\mathbf{x}\\) and \\(A\\) are: \\[\\mathbf{x} = \\begin{bmatrix}\n1 \\\\\n-2 \\\\\n1\n\\end{bmatrix}, \\quad A = \\begin{bmatrix}\n4 & -3 & 2 \\\\\n6 & 5  & 1 \\\\\n-4 & -1 & 2\n\\end{bmatrix}\\]\n\n# Fill in code here\nx_col =\nx_row =\n\nThe distinction between row and column vectors also affects the behaivior of other operations on np.array objects, particularly through the concept of broadcasting.\n\n\nQ10 (5 points)\nConsider a \\(3 \\times 3\\) matrix of all ones as defined in code below, along with the 1-d vector \\(\\mathbf{x}\\).\n\nones = np.ones((3, 3))\nx = np.array([1, -2, 1])\nones\n\narray([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]])\n\n\nComplete the line of code below such that the result of the operation is:\n\\[\\begin{bmatrix}\n1 & -2 & 1 \\\\\n1 & -2  & 1 \\\\\n1 & -2 & 1\n\\end{bmatrix}\\]\nHint: You should replace SHAPE with an appropriate shape for broadcasting and OPERATION with an appropriate operation (e.g. +, -, *, /)\n\n# Fill in code here\n\nones OPERATION x.reshape( SHAPE )\n\nThroughout this course we will typically use row vectors and vector-matrix multiplication, as this is more conventional in neural-network literature. The concept of row and column vectors becomes handy when transforming collections of vectors.\nRecall that a matrix can be seen as a collection of vectors. In numpy we can create a matrix from a list of (1- dimensional) vectors using the np.stack function. This function assumes that the vectors are row vectors creating the matrix as follows: \\[\\begin{bmatrix}\n3 & 1 & -2\n\\end{bmatrix},\\ \\begin{bmatrix}\n4 & 5 & 3\n\\end{bmatrix},\\ \\begin{bmatrix}\n-2 & -1 & 5\n\\end{bmatrix}\\quad \\overset{\\text{np.stack}}{\\longrightarrow} \\begin{bmatrix}\n3 & 1 & -2 \\\\\n4 & 5 & 3 \\\\\n-2 & -1 & 5\n\\end{bmatrix} \\]\nWe will call this matrix \\(X\\) to denote that it is a collection of vectors, rather than a single vector (\\(\\mathbf{x}\\)).\n\n\nQ11 (5 points)\nCreate this matrix in numpy using th np.stack function.\n\n# Fill code here\n\nX =\nprint(X)\n\nWhen taken together as a matrix in this way, we can apply the linear mapping \\(A\\) to all vectors using matrix-matrix multiplication: \\[B=XA^T\\]\nLet’s put this into practice with a visual example.\n\n\nQ12 (5 points)\nCreate a \\(20 \\times 3\\) matrix, circle, in numpy of the following form\n\\[ \\begin{bmatrix} \\sin(\\theta_1) & \\cos(\\theta_1)  & 1 \\\\\n\\sin(\\theta_2) & \\cos(\\theta_2)  & 1 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n\\sin(\\theta_{20}) & \\cos(\\theta_{20})  & 1 \\\\\n\\end{bmatrix}\\] Where \\(\\theta_1...\\theta_{20}\\) are evenly spaced between \\(0\\) and \\(2\\pi\\).\n\ntheta = np.linspace(0, 2 * np.pi, 20) # Generates 20 evenly-spaced numbers between 0 and 2π\n\n# Fill in your code here\ncircle =\n\nThe code we just wrote creates a matrix corresponding to a collection of \\(20\\) row vectors of length 3. Each vector represents a point on the unit circle where the first entry is the x-coordinate, the second entry is the y-coordinate and the third entry is always \\(1\\): \\[ \\begin{bmatrix} x & y & 1 \\end{bmatrix}\\]\n\n\nQ13 (5 points)\nPlot the set of 20 points in circle using the plt.plot function. Use only the x and y coordinates, ignoring the column of 1s.\n\nplt.figure(figsize=(4, 4))\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\n\n# Fill your code here\n\n\n\nQ14 (5 points)\nTransform all the vectors in circle with the matrix \\(A\\) using a single call to np.dot. Then plot the original set of points in black and the transformed points in red using the plt.plot function.\nYou might also consider why we added the extra column of 1s! We will discuss the answer to that in class. \\(A\\) is the same matrix from q1.\n\n# Fill your code here\ntransformed_circle =\n\n\nplt.figure(figsize=(4, 4))\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\n\n# Fill your code here\n\n\n\nQ15 (5 points)\nFinally, shift all the vectors in transformed_circle by the vector \\(\\mathbf{b}\\) defined below, that is, add \\(\\mathbf{b}\\) to every row of the output matrix. Again plot the original set of points in black and the transformed points in red using the plt.plot function.\nHint: the solution to this question should not involve any loops, instead use broadcasting.\n\n# Fill your code here\nb = np.array([-0.5, 1.2, 0])\ntransformed_circle =\n\n\nplt.figure(figsize=(4, 4))\nplt.xlim(-10, 10)\nplt.ylim(-10, 10)\n\n# Fill your code here"
  },
  {
    "objectID": "assignments/homeworks/Homework 1/main.html#part-3-loading-and-visualizing-data",
    "href": "assignments/homeworks/Homework 1/main.html#part-3-loading-and-visualizing-data",
    "title": "Homework 1: Introduction to Numpy",
    "section": "Part 3: Loading and visualizing data",
    "text": "Part 3: Loading and visualizing data\nFor most of this class we will be working with real-world data. A very well-known dataset in statistics is the Iris flower dataset collected by Edgar Anderson in 1929. The dataset consists of measurments of iris flowers. Each flower has 4 collected measurements: sepal length, sepal width, petal length, petal width, as well as a classification into one of 3 species: Iris setosa, Iris versicolor and Iris virginica. We will return to this dataset in the next homework.\nWe can load this dataset as Numpy objects using the Scikit-Learn library. Below we’ve extrated 4 relevant arrays: - features: a matrix which has one row per observed flower and one column per measurement. - targets: An array that specifies the species of each flower as a number 0-2. - feature_names: a list of strings with the name of each measurement. - target_names: a list of strings with the name of each species.\nIn this homework, we will only visualize this dataset, which is typically a good first step in working with a new type of data. We’ll start by just looking at 2 measurements sepal length and petal length, along with the species.\n\nQ16 (10 points)\nBased on the Iris dataset loaded below, how many flowers did Edgar Anderson measure?\nIn other words, how many observations are in this dataset?\n\nimport sklearn.datasets as datasets\ndataset = datasets.load_iris()\nfeatures = dataset['data']\ntargets = dataset['target']\nfeature_names = dataset['feature_names']\ntarget_names = dataset['target_names']\n\n# Write any code to determine the number of observations here\n\nFill in the code to create a scatterplot for the Iris dataset below. Plot sepal length on the x-axis and petal length on the y-axis. Set the color to correspond to the species.\n\n# Fill your code here\n\nplt.figure(figsize=(4, 4))"
  }
]