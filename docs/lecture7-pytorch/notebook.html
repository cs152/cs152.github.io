<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.319">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>CS 152: Neural Networks - Lecture 7: PyTorch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">CS 152: Neural Networks</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../calendar/calendar.html" rel="" target="">
 <span class="menu-text">Calendar</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/orgs/CS152-Neural-Networks-Fall-2023/repositories" rel="" target="">
 <span class="menu-text">Homeworks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://harveymuddcollege.instructure.com/courses/615/" rel="" target="">
 <span class="menu-text">Canvas</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/orgs/CS152-Neural-Networks-Fall-2023/" rel="" target="">
 <span class="menu-text">Github</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-to-pytorch" id="toc-introduction-to-pytorch" class="nav-link active" data-scroll-target="#introduction-to-pytorch">Introduction to PyTorch</a></li>
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link" data-scroll-target="#gradient-descent">Gradient descent</a></li>
  <li><a href="#torch.nn" id="toc-torch.nn" class="nav-link" data-scroll-target="#torch.nn">torch.nn</a></li>
  <li><a href="#evaluating-models" id="toc-evaluating-models" class="nav-link" data-scroll-target="#evaluating-models">Evaluating models</a></li>
  <li><a href="#underfittting" id="toc-underfittting" class="nav-link" data-scroll-target="#underfittting">Underfittting</a></li>
  <li><a href="#overfitting" id="toc-overfitting" class="nav-link" data-scroll-target="#overfitting">Overfitting</a></li>
  <li><a href="#early-stopping" id="toc-early-stopping" class="nav-link" data-scroll-target="#early-stopping">Early stopping</a></li>
  <li><a href="#train-validation-and-test" id="toc-train-validation-and-test" class="nav-link" data-scroll-target="#train-validation-and-test">Train, validation and test</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Lecture 7: PyTorch</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tqdm</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.inspection <span class="im">import</span> DecisionBoundaryDisplay</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_boundary(model, X, y, alpha<span class="op">=</span><span class="dv">1</span>, title<span class="op">=</span><span class="st">''</span>):</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">xrange</span> <span class="op">=</span> (<span class="op">-</span>X[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">+</span> X[:, <span class="dv">0</span>].<span class="bu">max</span>()) <span class="op">/</span> <span class="dv">10</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    yrange <span class="op">=</span> (<span class="op">-</span>X[:, y].<span class="bu">min</span>() <span class="op">+</span> X[:, y].<span class="bu">max</span>()) <span class="op">/</span> <span class="dv">10</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    feature_1, feature_2 <span class="op">=</span> np.meshgrid(</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        np.linspace(X[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="bu">xrange</span>, X[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="bu">xrange</span>, <span class="dv">250</span>),</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        np.linspace(X[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> yrange, X[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> yrange, <span class="dv">250</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    grid <span class="op">=</span> np.vstack([feature_1.ravel(), feature_2.ravel()]).T</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> np.reshape(model.predict(torch.tensor(grid).<span class="bu">float</span>()).detach().numpy(), feature_1.shape)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    display <span class="op">=</span> DecisionBoundaryDisplay(</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        xx0<span class="op">=</span>feature_1, xx1<span class="op">=</span>feature_2, response<span class="op">=</span>y_pred</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    display.plot()</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    display.ax_.scatter(</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, alpha<span class="op">=</span>alpha, edgecolor<span class="op">=</span><span class="st">"black"</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    plt.title(title)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="introduction-to-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-pytorch">Introduction to PyTorch</h2>
<p>The most basic object in PyTorch is a <code>tensor</code>. Tensor objects behave much like the <code>AutogradValue</code> objects we are creating in the homework! We can create a <code>tensor</code> object with a given value as follows</p>
<div class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor(<span class="fl">4.</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="45">
<pre><code>tensor(4.)</code></pre>
</div>
</div>
<p>Performing basic operations on <code>tensor</code> objects gives tensor objects.</p>
<div class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> x <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">5</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>a</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="46">
<pre><code>tensor(21.)</code></pre>
</div>
</div>
<p><code>tensor</code> objects also support reverse-mode automatic differentiation! To use this, we must specify that we will want to compute the derivative with respect to a given <code>tensor</code>. We can do this with the <code>requires_grad</code> argument.</p>
<div class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor(<span class="fl">4.</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Once we have a <code>tensor</code> that <code>requires_grad</code>, we can perform operations on it to compute a loss.</p>
<div class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> x <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">5</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> torch.log(a) <span class="co"># Functions like log must be called through torch</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>L</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="48">
<pre><code>tensor(3.0445, grad_fn=&lt;LogBackward0&gt;)</code></pre>
</div>
</div>
<p>Once we have a loss running the backward pass is done exactly as in the homework. First we call <code>backward()</code> on the loss <code>tensor</code> object, then we can access the derivative through the <code>grad</code> property of <code>x</code>.</p>
<div class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>L.backward()</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>x.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="49">
<pre><code>tensor(0.3810)</code></pre>
</div>
</div>
<p>We can also create <code>tensor</code> objects that wrap arrays.</p>
<div class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor(np.array([<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>]))</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="50">
<pre><code>tensor([3, 4, 5])</code></pre>
</div>
</div>
<p>We can also just directly create tensors as we would numpy arrays</p>
<div class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor([<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>])</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="51">
<pre><code>tensor([3, 4, 5])</code></pre>
</div>
</div>
<p>Including convienience constructors.</p>
<div class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torch.ones((<span class="dv">5</span>,)))</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torch.zeros((<span class="dv">2</span>, <span class="dv">3</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([1., 1., 1., 1., 1.])
tensor([[0., 0., 0.],
        [0., 0., 0.]])</code></pre>
</div>
</div>
<p>Automatic differentiation still works for arrays. In this case it gives use the gradient of the loss (hence the <code>grad</code> property).</p>
<div class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor([<span class="fl">3.</span>, <span class="fl">4.</span>, <span class="fl">5.</span>], requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> torch.<span class="bu">sum</span>(x <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>L</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="53">
<pre><code>tensor(50., grad_fn=&lt;SumBackward0&gt;)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>L.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/var/folders/t9/fjsr8h1x7c7cg4vqw_xhjq2w0000gn/T/ipykernel_79454/323164392.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:491.)
  L.grad</code></pre>
</div>
</div>
<div class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>L.backward()</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>x.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="55">
<pre><code>tensor([ 6.,  8., 10.])</code></pre>
</div>
</div>
<p>We can convert <code>tensor</code> objects back to numpy by calling <code>x.detach().numpy()</code>. (<code>detach</code> removes the variable from any automatic differentiation computations)</p>
<div class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>x.detach().numpy()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="56">
<pre><code>array([3., 4., 5.], dtype=float32)</code></pre>
</div>
</div>
<p>At this point it’s probably worth remarking on where the name <code>tensor</code> comes from.</p>
<p>So far we’ve discussed 3 kinds of array objects - <strong>Scalars:</strong> which are just single values (0-dimensional) - <strong>Vectors:</strong> 1-dimensional arrays of numbers - <strong>Matrices:</strong> 2-dimensional arrays of numbers</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="image.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Alt text</figcaption><p></p>
</figure>
</div>
<p>A <strong>tensor</strong> is the generalization of a vector or matrix to <em>any</em> number of dimensions. For example, a 3-dimensional tensor can be seen in multiple ways.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="image-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Alt text</figcaption><p></p>
</figure>
</div>
<p>A <code>tensor</code> object can be created with any number of dimensions. For example, we could create a 2x2x2 tensor as:</p>
<div class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> torch.tensor([[[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>]], [[<span class="dv">5</span>, <span class="dv">6</span>], [<span class="dv">7</span>, <span class="dv">8</span>]]])</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>t</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="57">
<pre><code>tensor([[[1, 2],
         [3, 4]],

        [[5, 6],
         [7, 8]]])</code></pre>
</div>
</div>
<p>Or we could create the tensor in the image using <code>arange</code> and <code>reshape</code>.</p>
<div class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> torch.arange(<span class="dv">30</span>).reshape((<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">5</span>))</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>t</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="58">
<pre><code>tensor([[[ 0,  1,  2,  3,  4],
         [ 5,  6,  7,  8,  9]],

        [[10, 11, 12, 13, 14],
         [15, 16, 17, 18, 19]],

        [[20, 21, 22, 23, 24],
         [25, 26, 27, 28, 29]]])</code></pre>
</div>
</div>
<p>4-dimensional tensors can also be visualized</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="image-2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Alt text</figcaption><p></p>
</figure>
</div>
<div class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> torch.ones((<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">5</span>))</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>t.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="59">
<pre><code>torch.Size([3, 2, 4, 5])</code></pre>
</div>
</div>
<p>There are some notable differences between torch and numpy when it comes to operations. The important one to watch out for at this point is matrix multiplation. In numpy we accomplished with with <code>np.dot</code>:</p>
<div class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.ones((<span class="dv">4</span>, <span class="dv">5</span>))</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.ones((<span class="dv">5</span>, <span class="dv">2</span>))</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>np.dot(x, w)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="60">
<pre><code>array([[5., 5.],
       [5., 5.],
       [5., 5.],
       [5., 5.]])</code></pre>
</div>
</div>
<p>In PyTorch <code>torch.dot</code> only does vector dot products and thus only applies to 1-dimensional <code>tensor</code> objects:</p>
<div class="cell" data-execution_count="61">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.ones((<span class="dv">4</span>, <span class="dv">5</span>))</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> torch.ones((<span class="dv">5</span>, <span class="dv">2</span>))</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>torch.dot(x, w)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>RuntimeError: 1D tensors expected, but got 2D and 2D tensors</code></pre>
</div>
</div>
<p>Instead we use the <code>torch.matmul</code> function for this purpose</p>
<div class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>torch.matmul(x, w)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="62">
<pre><code>tensor([[5., 5.],
        [5., 5.],
        [5., 5.],
        [5., 5.]])</code></pre>
</div>
</div>
<p>PyTorch also has many handy built-in functions that numpy doesn’t have, such as <code>sigmoid</code>.</p>
<div class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">50</span>)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> torch.sigmoid(x)</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>plt.plot(x, s)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-21-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>This makes it very easy to implement something like logistic regression.</p>
<div class="cell" data-execution_count="64">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LogisticRegression:</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dims):</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> torch.ones((dims,), requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> torch.zeros((), requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_probability(<span class="va">self</span>, X):</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>        f_X <span class="op">=</span> torch.matmul(X, <span class="va">self</span>.weights) <span class="op">+</span> <span class="va">self</span>.bias</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sigmoid(f_X)</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.predict_probability(X) <span class="op">&gt;</span> <span class="fl">0.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s try loading a dataset, converting it to <code>tensor</code> and making predictions</p>
<div class="cell" data-execution_count="65">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(noise<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> torch.tensor(X).<span class="bu">float</span>(), torch.tensor(y)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(<span class="dv">2</span>)</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-23-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>When working with PyTorch, it is convention to separate the loss function from the model, where the loss function will just take predictions and labels.</p>
<div class="cell" data-execution_count="66">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> NLL(pred, y):</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    LL <span class="op">=</span> y <span class="op">*</span> torch.log(pred) <span class="op">+</span> (<span class="fl">1.</span> <span class="op">-</span> y) <span class="op">*</span> torch.log(<span class="fl">1.</span> <span class="op">-</span> pred)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>LL.<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="gradient-descent" class="level2">
<h2 class="anchored" data-anchor-id="gradient-descent">Gradient descent</h2>
<p>Gradient descent is also implemented in PyTorch in the <code>optim</code> module.</p>
<div class="cell" data-execution_count="67">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> optim</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Gradient descent works a bit differently in PyTorch than what we’ve seen. We first need to construct a gradient descent <em>object</em> which specifies which values we’re optimizing and what the learning rate will be. We specify the values to optimize by simply passing a list of weights/parameters to the constructor.</p>
<p>In PyTorch, basic gradient descent is encapsulated in the <code>optim.SGD</code> class (<code>SGD</code> stands for <em>stochastic gradient descent</em>, we’ll talk about what <em>stochastic</em> means in this context next week.)</p>
<div class="cell" data-execution_count="68">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD([model.weights, model.bias], lr<span class="op">=</span><span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Notice that this object doesn’t even take in the function we’re trying to optimize, only the inputs. We need to call the function ourselves <em>and</em> run <code>backward()</code> to compute the gradients.</p>
<div class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> model.predict_probability(X)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> NLL(predictions, y)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>loss.backward()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s look at our model <code>weights</code></p>
<div class="cell" data-execution_count="70">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>model.weights</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>model.weights.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="70">
<pre><code>tensor([-5.6526, 24.1355])</code></pre>
</div>
</div>
<p>We can take a single step of gradient descent using the <code>step</code> method of the optimizer.</p>
<div class="cell" data-execution_count="71">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>optimizer.step()</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>model.weights</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="71">
<pre><code>tensor([ 1.5653, -1.4135], requires_grad=True)</code></pre>
</div>
</div>
<p>We see that this actually updates the weights themselves!</p>
<p>It’s important to note that in PyTorch, calling <code>backward</code> does <strong>not</strong> clear the value stored in grad. So computing the gradient multiple times will result in updates to the gradient.</p>
<div class="cell" data-execution_count="72">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.weights.grad)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>NLL(model.predict_probability(X), y).backward()</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.weights.grad)</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>NLL(model.predict_probability(X), y).backward()</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.weights.grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([-5.6526, 24.1355])
tensor([-14.5242,  28.7704])
tensor([-23.3958,  33.4053])</code></pre>
</div>
</div>
<p>We can clear the stored gradients using the optimizer.</p>
<div class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>optimizer.zero_grad()</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.weights.grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>None</code></pre>
</div>
</div>
<p>So far we’ve only taking a single step of gradient descent. In order to run many steps, we need to write a loop to do everything we just saw.</p>
<div class="cell" data-execution_count="74">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model.predict_probability(X)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> NLL(predictions, y)</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>45.934814453125
40.60100555419922
32.82542037963867
30.3773136138916
28.918834686279297
28.118915557861328
27.602989196777344
27.248106002807617
26.989551544189453
26.794662475585938</code></pre>
</div>
</div>
<p>We should now see that our model has been optimized!</p>
<div class="cell" data-execution_count="75">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-33-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="torch.nn" class="level2">
<h2 class="anchored" data-anchor-id="torch.nn">torch.nn</h2>
<p>While PyTorch as a tool for automatic differentiation and optimization would be useful by itself. It actually gives us a lot more than that!</p>
<p>On of the most important features of PyTorch is its model-building tools in the <code>torch.nn</code> module. This gives us a lot of powerful features that we can use to build complex neural networks!</p>
<div class="cell" data-execution_count="76">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s start by building out logistic regression model in the <code>torch.nn</code> framwork. In order for a model to benefit from <code>torch.nn</code> our model class needs to inheret from <code>nn.Module</code></p>
<div class="cell" data-execution_count="77">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LogisticRegression(nn.Module):</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dims):</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> nn.Parameter(torch.ones((dims,)))</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> nn.Parameter(torch.zeros(()))</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sigmoid(torch.matmul(X, <span class="va">self</span>.weights) <span class="op">+</span> <span class="va">self</span>.bias)</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_probability(<span class="va">self</span>, X):</span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X)</span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X) <span class="op">&gt;</span> <span class="fl">0.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>There are 2 changes to note here. The first is that we wrapped our <code>weights</code> and <code>bias</code> terms in <code>nn.Parameter</code>. This tells PyTorch that these are the parameters we will want to optimize. We don’t need to specify <code>requires_grad</code> for parameters, PyTorch will take care of that for us.</p>
<p>The second is that we moved the implmentation of <code>predict_probability</code> to <code>forward</code>. In PyTorch models the <code>forward</code> method is special, it defines the model as a function. If we call the model as a function <code>forward</code> will be called internally.</p>
<div class="cell" data-execution_count="78">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(<span class="dv">2</span>)</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>model(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="78">
<pre><code>tensor([0.5652, 0.7431, 0.8238, 0.5468, 0.8991, 0.5719, 0.8651, 0.7784, 0.6153,
        0.7053, 0.7419, 0.3129, 0.4502, 0.5031, 0.4508, 0.7075, 0.7626, 0.4756,
        0.4158, 0.4693, 0.7904, 0.7241, 0.2430, 0.5297, 0.7944, 0.8496, 0.6825,
        0.6149, 0.6730, 0.5231, 0.6151, 0.6108, 0.5509, 0.7476, 0.6634, 0.8512,
        0.8117, 0.7527, 0.5092, 0.7742, 0.8012, 0.7604, 0.6411, 0.3242, 0.2805,
        0.4016, 0.6296, 0.3000, 0.7045, 0.7307, 0.7442, 0.8091, 0.5104, 0.8006,
        0.6166, 0.5173, 0.6404, 0.5779, 0.8199, 0.9030, 0.7872, 0.6059, 0.8091,
        0.9367, 0.5806, 0.7576, 0.8079, 0.3847, 0.4926, 0.4909, 0.8605, 0.6233,
        0.5605, 0.5980, 0.3629, 0.6874, 0.8761, 0.7593, 0.8267, 0.7912, 0.7147,
        0.4980, 0.4396, 0.9295, 0.8956, 0.5096, 0.7148, 0.7788, 0.7961, 0.2600,
        0.4964, 0.6008, 0.5710, 0.7882, 0.9379, 0.8573, 0.7131, 0.8105, 0.6835,
        0.5392], grad_fn=&lt;SigmoidBackward0&gt;)</code></pre>
</div>
</div>
<p>This means that we can use instances of <code>nn.Module</code> as parameterized functions. For example, we might create a general linear (technically affine) function in the same way.</p>
<p><span class="math display">\[f(\mathbf{x}) = \mathbf{x}^T\mathbf{W}^T + \mathbf{b},  \quad f: \mathbb{R}^i \rightarrow \mathbb{R}^o\]</span></p>
<p>Note that here we are <strong>not</strong> assuming an augmented representation of <span class="math inline">\(\mathbf{x}\)</span>.</p>
<div class="cell" data-execution_count="79">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Linear(nn.Module):</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, inputs, outputs):</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weightsT <span class="op">=</span> nn.Parameter(torch.ones((inputs, outputs)))</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> nn.Parameter(torch.zeros((outputs,)))</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.matmul(X, <span class="va">self</span>.weightsT) <span class="op">+</span> <span class="va">self</span>.bias</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can use this module to implement out logistic regression model above.</p>
<div class="cell" data-execution_count="80">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LogisticRegression(nn.Module):</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dims):</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear <span class="op">=</span> Linear(dims, <span class="dv">1</span>)                       <span class="co"># Dims input 1 output</span></span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sigmoid(<span class="va">self</span>.linear(X)).reshape((<span class="op">-</span><span class="dv">1</span>,)) <span class="co"># Turn output into a vector</span></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_probability(<span class="va">self</span>, X):</span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X)</span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X) <span class="op">&gt;</span> <span class="fl">0.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="81">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(<span class="dv">2</span>)</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>model(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="81">
<pre><code>tensor([0.5652, 0.7431, 0.8238, 0.5468, 0.8991, 0.5719, 0.8651, 0.7784, 0.6153,
        0.7053, 0.7419, 0.3129, 0.4502, 0.5031, 0.4508, 0.7075, 0.7626, 0.4756,
        0.4158, 0.4693, 0.7904, 0.7241, 0.2430, 0.5297, 0.7944, 0.8496, 0.6825,
        0.6149, 0.6730, 0.5231, 0.6151, 0.6108, 0.5509, 0.7476, 0.6634, 0.8512,
        0.8117, 0.7527, 0.5092, 0.7742, 0.8012, 0.7604, 0.6411, 0.3242, 0.2805,
        0.4016, 0.6296, 0.3000, 0.7045, 0.7307, 0.7442, 0.8091, 0.5104, 0.8006,
        0.6166, 0.5173, 0.6404, 0.5779, 0.8199, 0.9030, 0.7872, 0.6059, 0.8091,
        0.9367, 0.5806, 0.7576, 0.8079, 0.3847, 0.4926, 0.4909, 0.8605, 0.6233,
        0.5605, 0.5980, 0.3629, 0.6874, 0.8761, 0.7593, 0.8267, 0.7912, 0.7147,
        0.4980, 0.4396, 0.9295, 0.8956, 0.5096, 0.7148, 0.7788, 0.7961, 0.2600,
        0.4964, 0.6008, 0.5710, 0.7882, 0.9379, 0.8573, 0.7131, 0.8105, 0.6835,
        0.5392], grad_fn=&lt;ReshapeAliasBackward0&gt;)</code></pre>
</div>
</div>
<p>The power here is that because <code>Linear</code> is also an instance of <code>nn.Module</code>, PyTorch knows that it’s weights should also be considered part of our models weights. We can access the weights of a model using the <code>parameters()</code> method.</p>
<div class="cell" data-execution_count="82">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(model.parameters())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="82">
<pre><code>[Parameter containing:
 tensor([[1.],
         [1.]], requires_grad=True),
 Parameter containing:
 tensor([0.], requires_grad=True)]</code></pre>
</div>
</div>
<p>This lets us easily apply gradient descent:</p>
<div class="cell" data-execution_count="83">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(X)        <span class="co"># Now we can just call model!</span></span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> NLL(predictions, y)</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>76.53839111328125
45.934814453125
40.60100555419922
32.82542037963867
30.3773136138916
28.918834686279297
28.118915557861328
27.602989196777344
27.248106002807617
26.989551544189453</code></pre>
</div>
</div>
<div class="cell" data-execution_count="84">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-42-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>PyTorch unsurprisingly also provides a built-in <code>Linear</code> module. As <code>nn.Linear</code>.</p>
<div class="cell" data-execution_count="85">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>nn.Linear(<span class="dv">2</span>, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="85">
<pre><code>Linear(in_features=2, out_features=1, bias=True)</code></pre>
</div>
</div>
<p>Knowing how to make a parameterized function in PyTorch, let’s consider making a neural network layer with a sigmoid activation function.</p>
<p><span class="math display">\[f(\mathbf{x}) = \sigma(\mathbf{x}^T\mathbf{W}^T + \mathbf{b})\]</span></p>
<div class="cell" data-execution_count="86">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SigmoidLayer(nn.Module):</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, inputs, outputs):</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear <span class="op">=</span> nn.Linear(inputs, outputs)</span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sigmoid(<span class="va">self</span>.linear(X))</span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>        </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s create a layer with 10 neurons. (So <span class="math inline">\(\mathbf{W}:\ (10 \times 2)\)</span>)</p>
<div class="cell" data-execution_count="87">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>layer <span class="op">=</span> SigmoidLayer(<span class="dv">2</span>, <span class="dv">10</span>)</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X.shape)</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>layer(X).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([100, 2])</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="87">
<pre><code>torch.Size([100, 10])</code></pre>
</div>
</div>
<p>Let’s use this to create a neural network class for binary classification!</p>
<div class="cell" data-execution_count="88">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NeuralNetwork(nn.Module):</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dims, hidden_size):</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer <span class="op">=</span> SigmoidLayer(dims, hidden_size)</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear <span class="op">=</span> Linear(hidden_size, <span class="dv">1</span>)                       </span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a>        hidden_neurons <span class="op">=</span> <span class="va">self</span>.layer(X)</span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.linear(hidden_neurons)</span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sigmoid(output).reshape((<span class="op">-</span><span class="dv">1</span>,))</span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_probability(<span class="va">self</span>, X):</span>
<span id="cb74-13"><a href="#cb74-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X)</span>
<span id="cb74-14"><a href="#cb74-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-15"><a href="#cb74-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb74-16"><a href="#cb74-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X) <span class="op">&gt;</span> <span class="fl">0.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We see that PyTorch recognizes both the parameters of the logistic regression and the parameters of our neural network feature transform:</p>
<div class="cell" data-execution_count="89">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> NeuralNetwork(<span class="dv">2</span>, <span class="dv">10</span>)</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(model.parameters())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="89">
<pre><code>[Parameter containing:
 tensor([[-0.0632, -0.0324],
         [-0.4227, -0.0076],
         [ 0.5793, -0.4920],
         [ 0.1640, -0.5442],
         [ 0.2326,  0.3651],
         [-0.0184,  0.6859],
         [ 0.0893,  0.1236],
         [-0.4774, -0.3702],
         [-0.0048,  0.4830],
         [-0.4720,  0.5458]], requires_grad=True),
 Parameter containing:
 tensor([-0.2695, -0.1038, -0.7001,  0.3398, -0.0591,  0.6680,  0.3601, -0.3093,
          0.0831, -0.4315], requires_grad=True),
 Parameter containing:
 tensor([[1.],
         [1.],
         [1.],
         [1.],
         [1.],
         [1.],
         [1.],
         [1.],
         [1.],
         [1.]], requires_grad=True),
 Parameter containing:
 tensor([0.], requires_grad=True)]</code></pre>
</div>
</div>
<p>This means that we can easily run our optimization as before.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(X)        <span class="co"># Now we can just call model!</span></span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> NLL(predictions, y)</span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="91">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-49-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>PyTorch also gives us an easier (but less flexible) way to define a composition of modules like this. In PyTorch we can define this simple network using <code>nn.Sequential</code></p>
<div class="cell" data-execution_count="92">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">10</span>),</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">10</span>, <span class="dv">1</span>),</span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a>model(X).reshape((<span class="op">-</span><span class="dv">1</span>,))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="92">
<pre><code>tensor([0.3653, 0.3638, 0.3685, 0.3575, 0.3788, 0.3567, 0.3749, 0.3824, 0.3716,
        0.3777, 0.3612, 0.3494, 0.3608, 0.3496, 0.3610, 0.3802, 0.3737, 0.3504,
        0.3581, 0.3621, 0.3859, 0.3814, 0.3410, 0.3605, 0.3768, 0.3890, 0.3599,
        0.3586, 0.3567, 0.3529, 0.3723, 0.3711, 0.3682, 0.3644, 0.3774, 0.3742,
        0.3804, 0.3808, 0.3526, 0.3757, 0.3669, 0.3735, 0.3731, 0.3516, 0.3461,
        0.3566, 0.3558, 0.3496, 0.3594, 0.3775, 0.3755, 0.3808, 0.3586, 0.3792,
        0.3543, 0.3657, 0.3559, 0.3642, 0.3848, 0.3816, 0.3824, 0.3546, 0.3808,
        0.3906, 0.3661, 0.3723, 0.3823, 0.3552, 0.3561, 0.3543, 0.3730, 0.3723,
        0.3523, 0.3669, 0.3531, 0.3590, 0.3764, 0.3702, 0.3680, 0.3841, 0.3798,
        0.3555, 0.3480, 0.3861, 0.3814, 0.3495, 0.3793, 0.3814, 0.3839, 0.3427,
        0.3540, 0.3623, 0.3700, 0.3661, 0.3915, 0.3718, 0.3660, 0.3685, 0.3601,
        0.3597], grad_fn=&lt;ReshapeAliasBackward0&gt;)</code></pre>
</div>
</div>
<p>Here <code>nn.Sigmoid</code> is a built-in module that just applies the sigmoid function. Its implementation would look like:</p>
<div class="cell" data-execution_count="93">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SigmoidLayer(nn.Module):</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sigmoid(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We could use this to create a network with several hidden layers:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">10</span>),</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">10</span>, <span class="dv">10</span>),</span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">10</span>, <span class="dv">10</span>),</span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">10</span>, <span class="dv">1</span>),</span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb82-10"><a href="#cb82-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb82-11"><a href="#cb82-11" aria-hidden="true" tabindex="-1"></a>model(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>PyTorch also provides built-in loss functions. The PyTorch function for the negative log-likelihood for logistic regression is called <code>nn.functional.binary_cross_entropy</code>. It has some sharp edges though.</p>
<p>For one, it expects <code>y</code> to be a float type. We can convert a PyTorch <code>int</code> tensor into a <code>float</code> one by calling the <code>float</code> method.</p>
<p>We also see that our sequential model returns a column vector, so <code>y</code> should match that as well.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>yfloat <span class="op">=</span> y.<span class="bu">float</span>().reshape((<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(X)        <span class="co"># Now we can just call model!</span></span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> nn.functional.binary_cross_entropy(predictions, yfloat)</span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb83-11"><a href="#cb83-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For convinience, let’s definie a wrapper class for our model.</p>
<div class="cell" data-execution_count="96">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LogisticRegressionNeuralNetwork(nn.Module):</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, network):</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.network <span class="op">=</span> network                   </span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.network(X).reshape((<span class="op">-</span><span class="dv">1</span>,))</span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_probability(<span class="va">self</span>, X):</span>
<span id="cb84-10"><a href="#cb84-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X)</span>
<span id="cb84-11"><a href="#cb84-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-12"><a href="#cb84-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb84-13"><a href="#cb84-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X) <span class="op">&gt;</span> <span class="fl">0.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="evaluating-models" class="level2">
<h2 class="anchored" data-anchor-id="evaluating-models">Evaluating models</h2>
<p>We see that we have a <em>lot</em> of options when designing a neural network. So far the choices we’ve seen are: - The number of layers - The number of neurons in each layer - The activation function - The learning rate for gradient descent</p>
<p>And this is just the beginning! As we go on, we’ll learn about many more options that we have.</p>
<p>Let’s take a look at how to make some of these choices. In many real cases, our data will not be a cleanly separated into 2 classes as we’ve seen. For instance, we can look at a noisier version of the dataset we saw before.</p>
<div class="cell" data-execution_count="223">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(<span class="dv">300</span>, noise<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> torch.tensor(X).<span class="bu">float</span>(), torch.tensor(y)</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>plt.scatter(<span class="op">*</span>X.T, c<span class="op">=</span>y, edgecolor<span class="op">=</span><span class="st">"black"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="223">
<pre><code>&lt;matplotlib.collections.PathCollection at 0x2e043f070&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-55-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>In reality, this dataset was drawn from the distribution shown below! The optimal classifier would still have an “s-shaped” decision boundary</p>
<div class="cell" data-execution_count="221">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(<span class="dv">50000</span>, noise<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, alpha<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a><span class="cf">pass</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-56-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Let’s split this into training and test sets as we’ve seen.</p>
<div class="cell" data-execution_count="231">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>inds <span class="op">=</span> np.arange(X.shape[<span class="dv">0</span>])</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>np.random.shuffle(inds)</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>Xtrain, ytrain <span class="op">=</span> X[inds[:<span class="dv">150</span>]], y[inds[:<span class="dv">150</span>]]</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>Xtest, ytest <span class="op">=</span> X[inds[<span class="dv">150</span>:]], y[inds[<span class="dv">150</span>:]]</span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a><span class="co"># We can plot training and test data on the same plot</span></span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a>plt.scatter(<span class="op">*</span>Xtrain.T, c<span class="op">=</span>ytrain, edgecolor<span class="op">=</span><span class="st">"black"</span>, marker<span class="op">=</span><span class="st">'o'</span>, label<span class="op">=</span><span class="st">'training data'</span>)</span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a>plt.scatter(<span class="op">*</span>Xtest.T, c<span class="op">=</span>ytest, edgecolor<span class="op">=</span><span class="st">"black"</span>, marker<span class="op">=</span><span class="st">'^'</span>, label<span class="op">=</span><span class="st">'test data'</span>)</span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="231">
<pre><code>&lt;matplotlib.legend.Legend at 0x2e0a7c670&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-57-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="underfittting" class="level2">
<h2 class="anchored" data-anchor-id="underfittting">Underfittting</h2>
<p>We’ll start by fitting a logistic regression model as we’ve seen. This time we’ll keep track of the loss on both the training data and the test data.</p>
<div class="cell" data-execution_count="233">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>network <span class="op">=</span> nn.Sequential(</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">1</span>),</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegressionNeuralNetwork(network)   </span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb90-8"><a href="#cb90-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-9"><a href="#cb90-9" aria-hidden="true" tabindex="-1"></a>train_losses, test_losses <span class="op">=</span> [], []</span>
<span id="cb90-10"><a href="#cb90-10" aria-hidden="true" tabindex="-1"></a>pbar <span class="op">=</span> tqdm.trange(<span class="dv">2500</span>)</span>
<span id="cb90-11"><a href="#cb90-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> pbar:</span>
<span id="cb90-12"><a href="#cb90-12" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(Xtrain)        <span class="co"># Now we can just call model!</span></span>
<span id="cb90-13"><a href="#cb90-13" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy(predictions, ytrain.flatten().<span class="bu">float</span>())</span>
<span id="cb90-14"><a href="#cb90-14" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb90-15"><a href="#cb90-15" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb90-16"><a href="#cb90-16" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb90-17"><a href="#cb90-17" aria-hidden="true" tabindex="-1"></a>    pbar.set_description(<span class="st">'Loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> loss.item())</span>
<span id="cb90-18"><a href="#cb90-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-19"><a href="#cb90-19" aria-hidden="true" tabindex="-1"></a>    train_losses.append(loss.item())</span>
<span id="cb90-20"><a href="#cb90-20" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy(model(Xtest), ytest.flatten().<span class="bu">float</span>())</span>
<span id="cb90-21"><a href="#cb90-21" aria-hidden="true" tabindex="-1"></a>    test_losses.append(test_loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loss: 0.501: 100%|██████████| 2500/2500 [00:02&lt;00:00, 892.89it/s] </code></pre>
</div>
</div>
<div class="cell" data-execution_count="234">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtrain, ytrain)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-59-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Let’s compute the accuracy on both the training and the test data</p>
<div class="cell" data-execution_count="148">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>train_acc <span class="op">=</span> (model.predict(Xtrain) <span class="op">==</span> ytrain).<span class="bu">float</span>().mean()</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>test_acc <span class="op">=</span> (model.predict(Xtest) <span class="op">==</span> ytest).<span class="bu">float</span>().mean()</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Training accuracy: </span><span class="sc">%.3f</span><span class="st">, Test accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> (train_acc, test_acc))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training accuracy: 0.793, Test accuracy: 0.793</code></pre>
</div>
</div>
<p>We can also look at how the loss on both the training and test data changes as we run gradient descent.</p>
<div class="cell" data-execution_count="288">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>plt.plot(train_losses[:<span class="dv">15000</span>:<span class="dv">100</span>], label<span class="op">=</span><span class="st">'Training loss'</span>)</span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a><span class="co">#plt.plot(test_losses[:15000:100], label='Test loss')</span></span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Loss vs. training iteration'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="288">
<pre><code>Text(0.5, 1.0, 'Loss vs. training iteration')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-61-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Here wee see that both the training loss/accuracy and the test loss/accuracy are quite poor! From our descision boundary plot we can see quite clearly that this is a consequence of our choice of a linear model for this classification problem. We call this problem <strong>underfitting</strong>, meaning that our model is not expressive enough to capture all the intricacies of our data. As we’ve already seen we can address this by adding neural network layers to increase the expressivity of our model.</p>
</section>
<section id="overfitting" class="level2">
<h2 class="anchored" data-anchor-id="overfitting">Overfitting</h2>
<p>Let’s try creating a much more complex model; one with several large neural network layers and fitting it to our data.</p>
<div class="cell" data-execution_count="238">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>network <span class="op">=</span> nn.Sequential(</span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">200</span>),</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">200</span>, <span class="dv">200</span>),</span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">200</span>, <span class="dv">200</span>),</span>
<span id="cb97-7"><a href="#cb97-7" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb97-8"><a href="#cb97-8" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">200</span>, <span class="dv">1</span>),</span>
<span id="cb97-9"><a href="#cb97-9" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb97-10"><a href="#cb97-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb97-11"><a href="#cb97-11" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegressionNeuralNetwork(network)   </span>
<span id="cb97-12"><a href="#cb97-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-13"><a href="#cb97-13" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.03</span>)</span>
<span id="cb97-14"><a href="#cb97-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-15"><a href="#cb97-15" aria-hidden="true" tabindex="-1"></a>train_losses, test_losses <span class="op">=</span> [], []</span>
<span id="cb97-16"><a href="#cb97-16" aria-hidden="true" tabindex="-1"></a>pbar <span class="op">=</span> tqdm.trange(<span class="dv">25000</span>)</span>
<span id="cb97-17"><a href="#cb97-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> pbar:</span>
<span id="cb97-18"><a href="#cb97-18" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(Xtrain)        <span class="co"># Now we can just call model!</span></span>
<span id="cb97-19"><a href="#cb97-19" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy(predictions, ytrain.flatten().<span class="bu">float</span>())</span>
<span id="cb97-20"><a href="#cb97-20" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb97-21"><a href="#cb97-21" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb97-22"><a href="#cb97-22" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb97-23"><a href="#cb97-23" aria-hidden="true" tabindex="-1"></a>    pbar.set_description(<span class="st">'Loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> loss.item())</span>
<span id="cb97-24"><a href="#cb97-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-25"><a href="#cb97-25" aria-hidden="true" tabindex="-1"></a>    train_losses.append(loss.item())</span>
<span id="cb97-26"><a href="#cb97-26" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy(model(Xtest), ytest.flatten().<span class="bu">float</span>())</span>
<span id="cb97-27"><a href="#cb97-27" aria-hidden="true" tabindex="-1"></a>    test_losses.append(test_loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loss: 0.073: 100%|██████████| 25000/25000 [00:41&lt;00:00, 602.24it/s]</code></pre>
</div>
</div>
<p>We can view the descision boundary and accuracy for this classifier.</p>
<div class="cell" data-execution_count="247">
<div class="sourceCode cell-code" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>train_acc <span class="op">=</span> (model.predict(Xtrain) <span class="op">==</span> ytrain).<span class="bu">float</span>().mean()</span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtrain, ytrain, title<span class="op">=</span><span class="st">'Training accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> train_acc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-63-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>We see that our model is much more expressive and basically correctly classifies every observation in our training dataset. This is great! However the boundary is quite complex. Let’s see what happens when we evaluate on our test set.</p>
<div class="cell" data-execution_count="250">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>test_acc <span class="op">=</span> (model.predict(Xtest) <span class="op">==</span> ytest).<span class="bu">float</span>().mean()</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtest, ytest, title<span class="op">=</span><span class="st">'Test accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> test_acc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-64-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The accuracy is <em>much</em> worse. Rather than capturing the true distribution of classes, our model has captured the training set we happened to draw. This means if we draw a new dataset from the same distribution (like our test set), performance is poor. We call this issue <strong>overfitting</strong>.</p>
<p>Let’s see how the training and test loss change over gradient descent.</p>
<div class="cell" data-execution_count="242">
<div class="sourceCode cell-code" id="cb101"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>plt.plot(train_losses[:<span class="dv">15000</span>:<span class="dv">100</span>], label<span class="op">=</span><span class="st">'Training loss'</span>)</span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a>plt.plot(test_losses[:<span class="dv">15000</span>:<span class="dv">100</span>], label<span class="op">=</span><span class="st">'Test loss'</span>)</span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Loss vs. training iteration'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="242">
<pre><code>Text(0.5, 1.0, 'Loss vs. training iteration')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-65-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Much of the rest of this class will focus on how to meet the deicate balance of overfitting vs.&nbsp;underfitting!</p>
<p>It’s worth noting the the best solution to overfitting is to get more data. If we train with enough data we can avoid overfitting entirely.</p>
<div class="cell" data-execution_count="270">
<div class="sourceCode cell-code" id="cb103"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(<span class="dv">2000</span>, noise<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, alpha<span class="op">=</span><span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="270">
<pre><code>&lt;matplotlib.collections.PathCollection at 0x2b7bd5460&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-66-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="275">
<div class="sourceCode cell-code" id="cb105"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> torch.tensor(X).<span class="bu">float</span>(), torch.tensor(y)</span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a>network <span class="op">=</span> nn.Sequential(</span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">200</span>),</span>
<span id="cb105-4"><a href="#cb105-4" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb105-5"><a href="#cb105-5" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">200</span>, <span class="dv">200</span>),</span>
<span id="cb105-6"><a href="#cb105-6" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb105-7"><a href="#cb105-7" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">200</span>, <span class="dv">200</span>),</span>
<span id="cb105-8"><a href="#cb105-8" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb105-9"><a href="#cb105-9" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">200</span>, <span class="dv">1</span>),</span>
<span id="cb105-10"><a href="#cb105-10" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb105-11"><a href="#cb105-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb105-12"><a href="#cb105-12" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegressionNeuralNetwork(network)   </span>
<span id="cb105-13"><a href="#cb105-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-14"><a href="#cb105-14" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb105-15"><a href="#cb105-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-16"><a href="#cb105-16" aria-hidden="true" tabindex="-1"></a>train_losses, test_losses <span class="op">=</span> [], []</span>
<span id="cb105-17"><a href="#cb105-17" aria-hidden="true" tabindex="-1"></a>pbar <span class="op">=</span> tqdm.trange(<span class="dv">2500</span>)</span>
<span id="cb105-18"><a href="#cb105-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> pbar:</span>
<span id="cb105-19"><a href="#cb105-19" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(X)        <span class="co"># Now we can just call model!</span></span>
<span id="cb105-20"><a href="#cb105-20" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy(predictions, y.flatten().<span class="bu">float</span>())</span>
<span id="cb105-21"><a href="#cb105-21" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb105-22"><a href="#cb105-22" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb105-23"><a href="#cb105-23" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb105-24"><a href="#cb105-24" aria-hidden="true" tabindex="-1"></a>    pbar.set_description(<span class="st">'Loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> loss.item())</span>
<span id="cb105-25"><a href="#cb105-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-26"><a href="#cb105-26" aria-hidden="true" tabindex="-1"></a>    train_losses.append(loss.item())</span>
<span id="cb105-27"><a href="#cb105-27" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy(model(Xtest), ytest.flatten().<span class="bu">float</span>())</span>
<span id="cb105-28"><a href="#cb105-28" aria-hidden="true" tabindex="-1"></a>    test_losses.append(test_loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/var/folders/t9/fjsr8h1x7c7cg4vqw_xhjq2w0000gn/T/ipykernel_79454/2923296796.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X, y = torch.tensor(X).float(), torch.tensor(y)
Loss: 0.390: 100%|██████████| 2500/2500 [00:16&lt;00:00, 151.68it/s]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="276">
<div class="sourceCode cell-code" id="cb107"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a>train_acc <span class="op">=</span> (model.predict(Xtrain) <span class="op">==</span> ytrain).<span class="bu">float</span>().mean()</span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtrain, ytrain, title<span class="op">=</span><span class="st">'Training accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> train_acc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-68-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="277">
<div class="sourceCode cell-code" id="cb108"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a>test_acc <span class="op">=</span> (model.predict(Xtest) <span class="op">==</span> ytest).<span class="bu">float</span>().mean()</span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtest, ytest, title<span class="op">=</span><span class="st">'Test accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> test_acc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-69-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="278">
<div class="sourceCode cell-code" id="cb109"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>plt.plot(train_losses[:<span class="dv">15000</span>:<span class="dv">100</span>], label<span class="op">=</span><span class="st">'Training loss'</span>)</span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a>plt.plot(test_losses[:<span class="dv">15000</span>:<span class="dv">100</span>], label<span class="op">=</span><span class="st">'Test loss'</span>)</span>
<span id="cb109-3"><a href="#cb109-3" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb109-4"><a href="#cb109-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Loss vs. training iteration'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="278">
<pre><code>Text(0.5, 1.0, 'Loss vs. training iteration')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-70-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Unfortunately, this often isn’t realistic. Data is hard to collect and more data means our model is slower and more expensive to train.</p>
</section>
<section id="early-stopping" class="level2">
<h2 class="anchored" data-anchor-id="early-stopping">Early stopping</h2>
<p>Let’s return to the original case</p>
<div class="cell" data-execution_count="280">
<div class="sourceCode cell-code" id="cb111"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(<span class="dv">300</span>, noise<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> torch.tensor(X).<span class="bu">float</span>(), torch.tensor(y)</span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-4"><a href="#cb111-4" aria-hidden="true" tabindex="-1"></a>inds <span class="op">=</span> np.arange(X.shape[<span class="dv">0</span>])</span>
<span id="cb111-5"><a href="#cb111-5" aria-hidden="true" tabindex="-1"></a>np.random.shuffle(inds)</span>
<span id="cb111-6"><a href="#cb111-6" aria-hidden="true" tabindex="-1"></a>Xtrain, ytrain <span class="op">=</span> X[inds[:<span class="dv">150</span>]], y[inds[:<span class="dv">150</span>]]</span>
<span id="cb111-7"><a href="#cb111-7" aria-hidden="true" tabindex="-1"></a>Xtest, ytest <span class="op">=</span> X[inds[<span class="dv">150</span>:]], y[inds[<span class="dv">150</span>:]]</span>
<span id="cb111-8"><a href="#cb111-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-9"><a href="#cb111-9" aria-hidden="true" tabindex="-1"></a><span class="co"># We can plot training and test data on the same plot</span></span>
<span id="cb111-10"><a href="#cb111-10" aria-hidden="true" tabindex="-1"></a>plt.scatter(<span class="op">*</span>Xtrain.T, c<span class="op">=</span>ytrain, edgecolor<span class="op">=</span><span class="st">"black"</span>, marker<span class="op">=</span><span class="st">'o'</span>, label<span class="op">=</span><span class="st">'training data'</span>)</span>
<span id="cb111-11"><a href="#cb111-11" aria-hidden="true" tabindex="-1"></a>plt.scatter(<span class="op">*</span>Xtest.T, c<span class="op">=</span>ytest, edgecolor<span class="op">=</span><span class="st">"black"</span>, marker<span class="op">=</span><span class="st">'^'</span>, label<span class="op">=</span><span class="st">'test data'</span>)</span>
<span id="cb111-12"><a href="#cb111-12" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="280">
<pre><code>&lt;matplotlib.legend.Legend at 0x2e0cedf40&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-71-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Let’s try a network somewhere in-between, with just a single hidden layer.</p>
<div class="cell" data-execution_count="251">
<div class="sourceCode cell-code" id="cb113"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a>network <span class="op">=</span> nn.Sequential(</span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">100</span>),</span>
<span id="cb113-3"><a href="#cb113-3" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb113-4"><a href="#cb113-4" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">100</span>, <span class="dv">1</span>),</span>
<span id="cb113-5"><a href="#cb113-5" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb113-6"><a href="#cb113-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb113-7"><a href="#cb113-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegressionNeuralNetwork(network)   </span>
<span id="cb113-8"><a href="#cb113-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-9"><a href="#cb113-9" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.03</span>)</span>
<span id="cb113-10"><a href="#cb113-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-11"><a href="#cb113-11" aria-hidden="true" tabindex="-1"></a>train_losses, test_losses <span class="op">=</span> [], []</span>
<span id="cb113-12"><a href="#cb113-12" aria-hidden="true" tabindex="-1"></a>pbar <span class="op">=</span> tqdm.trange(<span class="dv">25000</span>)</span>
<span id="cb113-13"><a href="#cb113-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> pbar:</span>
<span id="cb113-14"><a href="#cb113-14" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(Xtrain)        <span class="co"># Now we can just call model!</span></span>
<span id="cb113-15"><a href="#cb113-15" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy(predictions, ytrain.flatten().<span class="bu">float</span>())</span>
<span id="cb113-16"><a href="#cb113-16" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb113-17"><a href="#cb113-17" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb113-18"><a href="#cb113-18" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb113-19"><a href="#cb113-19" aria-hidden="true" tabindex="-1"></a>    pbar.set_description(<span class="st">'Loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> loss.item())</span>
<span id="cb113-20"><a href="#cb113-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-21"><a href="#cb113-21" aria-hidden="true" tabindex="-1"></a>    train_losses.append(loss.item())</span>
<span id="cb113-22"><a href="#cb113-22" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy(model(Xtest), ytest.flatten().<span class="bu">float</span>())</span>
<span id="cb113-23"><a href="#cb113-23" aria-hidden="true" tabindex="-1"></a>    test_losses.append(test_loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loss: 0.366: 100%|██████████| 25000/25000 [00:28&lt;00:00, 870.32it/s] </code></pre>
</div>
</div>
<div class="cell" data-execution_count="252">
<div class="sourceCode cell-code" id="cb115"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a>train_acc <span class="op">=</span> (model.predict(Xtrain) <span class="op">==</span> ytrain).<span class="bu">float</span>().mean()</span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtrain, ytrain, title<span class="op">=</span><span class="st">'Training accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> train_acc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-73-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="253">
<div class="sourceCode cell-code" id="cb116"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a>test_acc <span class="op">=</span> (model.predict(Xtest) <span class="op">==</span> ytest).<span class="bu">float</span>().mean()</span>
<span id="cb116-2"><a href="#cb116-2" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtest, ytest, title<span class="op">=</span><span class="st">'Test accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> test_acc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-74-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>We see that this network is more consistent between train and test, and now performs better on test data! Let’s take a look at the plot of training and test loss.</p>
<div class="cell" data-execution_count="254">
<div class="sourceCode cell-code" id="cb117"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a>plt.plot(train_losses[:<span class="dv">15000</span>:<span class="dv">100</span>], label<span class="op">=</span><span class="st">'Training loss'</span>)</span>
<span id="cb117-2"><a href="#cb117-2" aria-hidden="true" tabindex="-1"></a>plt.plot(test_losses[:<span class="dv">15000</span>:<span class="dv">100</span>], label<span class="op">=</span><span class="st">'Test loss'</span>)</span>
<span id="cb117-3"><a href="#cb117-3" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb117-4"><a href="#cb117-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Loss vs. training iteration'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="254">
<pre><code>Text(0.5, 1.0, 'Loss vs. training iteration')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-75-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>We still see that both drop quickly, but that test loss increases after a point. How might we use this to pick a better model?</p>
<p>One option would be to just use the model where the test loss is lowest. After all, that is our ultimate goal. There are different ways we can think about implementing this. One way to to have gradient descent stop when the test loss begins to increase. We call this approach <strong>early stopping</strong></p>
<div class="cell" data-execution_count="256">
<div class="sourceCode cell-code" id="cb119"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a>network <span class="op">=</span> nn.Sequential(</span>
<span id="cb119-2"><a href="#cb119-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">100</span>),</span>
<span id="cb119-3"><a href="#cb119-3" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb119-4"><a href="#cb119-4" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">100</span>, <span class="dv">1</span>),</span>
<span id="cb119-5"><a href="#cb119-5" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb119-6"><a href="#cb119-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb119-7"><a href="#cb119-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegressionNeuralNetwork(network)   </span>
<span id="cb119-8"><a href="#cb119-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-9"><a href="#cb119-9" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.03</span>)</span>
<span id="cb119-10"><a href="#cb119-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-11"><a href="#cb119-11" aria-hidden="true" tabindex="-1"></a>train_losses, test_losses <span class="op">=</span> [], []</span>
<span id="cb119-12"><a href="#cb119-12" aria-hidden="true" tabindex="-1"></a>pbar <span class="op">=</span> tqdm.trange(<span class="dv">25000</span>)</span>
<span id="cb119-13"><a href="#cb119-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> pbar:</span>
<span id="cb119-14"><a href="#cb119-14" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(Xtrain)        <span class="co"># Now we can just call model!</span></span>
<span id="cb119-15"><a href="#cb119-15" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy(predictions, ytrain.flatten().<span class="bu">float</span>())</span>
<span id="cb119-16"><a href="#cb119-16" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb119-17"><a href="#cb119-17" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb119-18"><a href="#cb119-18" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb119-19"><a href="#cb119-19" aria-hidden="true" tabindex="-1"></a>    pbar.set_description(<span class="st">'Loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> loss.item())</span>
<span id="cb119-20"><a href="#cb119-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-21"><a href="#cb119-21" aria-hidden="true" tabindex="-1"></a>    train_losses.append(loss.item())</span>
<span id="cb119-22"><a href="#cb119-22" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy(model(Xtest), ytest.flatten().<span class="bu">float</span>())</span>
<span id="cb119-23"><a href="#cb119-23" aria-hidden="true" tabindex="-1"></a>    test_losses.append(test_loss.item())</span>
<span id="cb119-24"><a href="#cb119-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-25"><a href="#cb119-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&gt;</span> <span class="dv">50</span> <span class="kw">and</span> test_loss.item() <span class="op">&gt;</span> test_losses[<span class="op">-</span><span class="dv">2</span>]:</span>
<span id="cb119-26"><a href="#cb119-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loss: 0.481:   1%|▏         | 346/25000 [00:00&lt;00:30, 810.31it/s]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="257">
<div class="sourceCode cell-code" id="cb121"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a>plt.plot(train_losses[:<span class="dv">15000</span>:<span class="dv">100</span>], label<span class="op">=</span><span class="st">'Training loss'</span>)</span>
<span id="cb121-2"><a href="#cb121-2" aria-hidden="true" tabindex="-1"></a>plt.plot(test_losses[:<span class="dv">15000</span>:<span class="dv">100</span>], label<span class="op">=</span><span class="st">'Test loss'</span>)</span>
<span id="cb121-3"><a href="#cb121-3" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb121-4"><a href="#cb121-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Loss vs. training iteration'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="257">
<pre><code>Text(0.5, 1.0, 'Loss vs. training iteration')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-77-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="258">
<div class="sourceCode cell-code" id="cb123"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a>train_acc <span class="op">=</span> (model.predict(Xtrain) <span class="op">==</span> ytrain).<span class="bu">float</span>().mean()</span>
<span id="cb123-2"><a href="#cb123-2" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtrain, ytrain, title<span class="op">=</span><span class="st">'Training accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> train_acc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-78-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="259">
<div class="sourceCode cell-code" id="cb124"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a>test_acc <span class="op">=</span> (model.predict(Xtest) <span class="op">==</span> ytest).<span class="bu">float</span>().mean()</span>
<span id="cb124-2"><a href="#cb124-2" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtest, ytest, title<span class="op">=</span><span class="st">'Test accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> test_acc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-79-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Here we see that actually we do the best so far with this approach!</p>
<p>We could also try our early-stopping approach with our more complex network.</p>
<div class="cell" data-execution_count="260">
<div class="sourceCode cell-code" id="cb125"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a>network <span class="op">=</span> nn.Sequential(</span>
<span id="cb125-2"><a href="#cb125-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">200</span>),</span>
<span id="cb125-3"><a href="#cb125-3" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb125-4"><a href="#cb125-4" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">200</span>, <span class="dv">200</span>),</span>
<span id="cb125-5"><a href="#cb125-5" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb125-6"><a href="#cb125-6" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">200</span>, <span class="dv">200</span>),</span>
<span id="cb125-7"><a href="#cb125-7" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb125-8"><a href="#cb125-8" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">200</span>, <span class="dv">1</span>),</span>
<span id="cb125-9"><a href="#cb125-9" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb125-10"><a href="#cb125-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb125-11"><a href="#cb125-11" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegressionNeuralNetwork(network)   </span>
<span id="cb125-12"><a href="#cb125-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-13"><a href="#cb125-13" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.03</span>)</span>
<span id="cb125-14"><a href="#cb125-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-15"><a href="#cb125-15" aria-hidden="true" tabindex="-1"></a>train_losses, test_losses <span class="op">=</span> [], []</span>
<span id="cb125-16"><a href="#cb125-16" aria-hidden="true" tabindex="-1"></a>pbar <span class="op">=</span> tqdm.trange(<span class="dv">25000</span>)</span>
<span id="cb125-17"><a href="#cb125-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> pbar:</span>
<span id="cb125-18"><a href="#cb125-18" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(Xtrain)        <span class="co"># Now we can just call model!</span></span>
<span id="cb125-19"><a href="#cb125-19" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy(predictions, ytrain.flatten().<span class="bu">float</span>())</span>
<span id="cb125-20"><a href="#cb125-20" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb125-21"><a href="#cb125-21" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb125-22"><a href="#cb125-22" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb125-23"><a href="#cb125-23" aria-hidden="true" tabindex="-1"></a>    pbar.set_description(<span class="st">'Loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> loss.item())</span>
<span id="cb125-24"><a href="#cb125-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-25"><a href="#cb125-25" aria-hidden="true" tabindex="-1"></a>    train_losses.append(loss.item())</span>
<span id="cb125-26"><a href="#cb125-26" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy(model(Xtest), ytest.flatten().<span class="bu">float</span>())</span>
<span id="cb125-27"><a href="#cb125-27" aria-hidden="true" tabindex="-1"></a>    test_losses.append(test_loss.item())</span>
<span id="cb125-28"><a href="#cb125-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-29"><a href="#cb125-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&gt;</span> <span class="dv">50</span> <span class="kw">and</span> test_loss.item() <span class="op">&gt;</span> test_losses[<span class="op">-</span><span class="dv">2</span>]:</span>
<span id="cb125-30"><a href="#cb125-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loss: 0.465:   2%|▏         | 382/25000 [00:00&lt;00:42, 574.88it/s]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="261">
<div class="sourceCode cell-code" id="cb127"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a>train_acc <span class="op">=</span> (model.predict(Xtrain) <span class="op">==</span> ytrain).<span class="bu">float</span>().mean()</span>
<span id="cb127-2"><a href="#cb127-2" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtrain, ytrain, title<span class="op">=</span><span class="st">'Training accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> train_acc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-81-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="262">
<div class="sourceCode cell-code" id="cb128"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a>test_acc <span class="op">=</span> (model.predict(Xtest) <span class="op">==</span> ytest).<span class="bu">float</span>().mean()</span>
<span id="cb128-2"><a href="#cb128-2" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtest, ytest, title<span class="op">=</span><span class="st">'Test accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> test_acc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-82-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="263">
<div class="sourceCode cell-code" id="cb129"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a>plt.plot(train_losses[:<span class="dv">15000</span>:<span class="dv">100</span>], label<span class="op">=</span><span class="st">'Training loss'</span>)</span>
<span id="cb129-2"><a href="#cb129-2" aria-hidden="true" tabindex="-1"></a>plt.plot(test_losses[:<span class="dv">15000</span>:<span class="dv">100</span>], label<span class="op">=</span><span class="st">'Test loss'</span>)</span>
<span id="cb129-3"><a href="#cb129-3" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb129-4"><a href="#cb129-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Loss vs. training iteration'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="263">
<pre><code>Text(0.5, 1.0, 'Loss vs. training iteration')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-83-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="train-validation-and-test" class="level2">
<h2 class="anchored" data-anchor-id="train-validation-and-test">Train, validation and test</h2>
<p>There is an issue here though! We’ve now used out test set to (indirectly) train our model. Both by using it to choose the number of layers and by using it to determine how long to run our optimization! This means that our model choice will be biased by our choice of test set, so how can we trust that our test loss or accuracy will actually be a good measure of the real-world performance of our model?</p>
<p>To deal with this issue we will typically split our data into 3 parts that we’ll call <strong>training</strong>, <strong>validation</strong> and <strong>test</strong>. We’ll use the validation portion as the portion to fit the model and the test set as the portion we use to estimate how well it will do in practice.</p>
<div class="cell" data-execution_count="282">
<div class="sourceCode cell-code" id="cb131"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(<span class="dv">300</span>, noise<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> torch.tensor(X).<span class="bu">float</span>(), torch.tensor(y)</span>
<span id="cb131-3"><a href="#cb131-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-4"><a href="#cb131-4" aria-hidden="true" tabindex="-1"></a>inds <span class="op">=</span> np.arange(X.shape[<span class="dv">0</span>])</span>
<span id="cb131-5"><a href="#cb131-5" aria-hidden="true" tabindex="-1"></a>np.random.shuffle(inds)</span>
<span id="cb131-6"><a href="#cb131-6" aria-hidden="true" tabindex="-1"></a>Xtrain, ytrain <span class="op">=</span> X[inds[:<span class="dv">150</span>]], y[inds[:<span class="dv">150</span>]]</span>
<span id="cb131-7"><a href="#cb131-7" aria-hidden="true" tabindex="-1"></a>Xvalid, yvalid <span class="op">=</span> X[inds[<span class="dv">150</span>:<span class="dv">225</span>]], y[inds[<span class="dv">150</span>:<span class="dv">225</span>]]</span>
<span id="cb131-8"><a href="#cb131-8" aria-hidden="true" tabindex="-1"></a>Xtest, ytest <span class="op">=</span> X[inds[<span class="dv">225</span>:]], y[inds[<span class="dv">225</span>:]]</span>
<span id="cb131-9"><a href="#cb131-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-10"><a href="#cb131-10" aria-hidden="true" tabindex="-1"></a><span class="co"># We can plot training and test data on the same plot</span></span>
<span id="cb131-11"><a href="#cb131-11" aria-hidden="true" tabindex="-1"></a>plt.scatter(<span class="op">*</span>Xtrain.T, c<span class="op">=</span>ytrain, edgecolor<span class="op">=</span><span class="st">"black"</span>, marker<span class="op">=</span><span class="st">'o'</span>, label<span class="op">=</span><span class="st">'training data'</span>)</span>
<span id="cb131-12"><a href="#cb131-12" aria-hidden="true" tabindex="-1"></a>plt.scatter(<span class="op">*</span>Xtest.T, c<span class="op">=</span>ytest, edgecolor<span class="op">=</span><span class="st">"black"</span>, marker<span class="op">=</span><span class="st">'^'</span>, label<span class="op">=</span><span class="st">'test data'</span>)</span>
<span id="cb131-13"><a href="#cb131-13" aria-hidden="true" tabindex="-1"></a>plt.scatter(<span class="op">*</span>Xvalid.T, c<span class="op">=</span>yvalid, edgecolor<span class="op">=</span><span class="st">"black"</span>, marker<span class="op">=</span><span class="st">'*'</span>, label<span class="op">=</span><span class="st">'valid data'</span>)</span>
<span id="cb131-14"><a href="#cb131-14" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="282">
<pre><code>&lt;matplotlib.legend.Legend at 0x2e0c664c0&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-84-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="283">
<div class="sourceCode cell-code" id="cb133"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a>network <span class="op">=</span> nn.Sequential(</span>
<span id="cb133-2"><a href="#cb133-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">200</span>),</span>
<span id="cb133-3"><a href="#cb133-3" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb133-4"><a href="#cb133-4" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">200</span>, <span class="dv">200</span>),</span>
<span id="cb133-5"><a href="#cb133-5" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb133-6"><a href="#cb133-6" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">200</span>, <span class="dv">200</span>),</span>
<span id="cb133-7"><a href="#cb133-7" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb133-8"><a href="#cb133-8" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">200</span>, <span class="dv">1</span>),</span>
<span id="cb133-9"><a href="#cb133-9" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb133-10"><a href="#cb133-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb133-11"><a href="#cb133-11" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegressionNeuralNetwork(network)   </span>
<span id="cb133-12"><a href="#cb133-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-13"><a href="#cb133-13" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.03</span>)</span>
<span id="cb133-14"><a href="#cb133-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-15"><a href="#cb133-15" aria-hidden="true" tabindex="-1"></a>train_losses, valid_losses <span class="op">=</span> [], []</span>
<span id="cb133-16"><a href="#cb133-16" aria-hidden="true" tabindex="-1"></a>pbar <span class="op">=</span> tqdm.trange(<span class="dv">25000</span>)</span>
<span id="cb133-17"><a href="#cb133-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> pbar:</span>
<span id="cb133-18"><a href="#cb133-18" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(Xtrain)        <span class="co"># Now we can just call model!</span></span>
<span id="cb133-19"><a href="#cb133-19" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy(predictions, ytrain.flatten().<span class="bu">float</span>())</span>
<span id="cb133-20"><a href="#cb133-20" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb133-21"><a href="#cb133-21" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb133-22"><a href="#cb133-22" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb133-23"><a href="#cb133-23" aria-hidden="true" tabindex="-1"></a>    pbar.set_description(<span class="st">'Loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> loss.item())</span>
<span id="cb133-24"><a href="#cb133-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-25"><a href="#cb133-25" aria-hidden="true" tabindex="-1"></a>    train_losses.append(loss.item())</span>
<span id="cb133-26"><a href="#cb133-26" aria-hidden="true" tabindex="-1"></a>    valid_loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy(model(Xvalid), yvalid.flatten().<span class="bu">float</span>())</span>
<span id="cb133-27"><a href="#cb133-27" aria-hidden="true" tabindex="-1"></a>    valid_losses.append(valid_loss.item())</span>
<span id="cb133-28"><a href="#cb133-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-29"><a href="#cb133-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&gt;</span> <span class="dv">50</span> <span class="kw">and</span> valid_loss.item() <span class="op">&gt;</span> valid_losses[<span class="op">-</span><span class="dv">2</span>]:</span>
<span id="cb133-30"><a href="#cb133-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loss: 0.406:   4%|▍         | 1060/25000 [00:02&lt;00:46, 513.32it/s]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="284">
<div class="sourceCode cell-code" id="cb135"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb135-1"><a href="#cb135-1" aria-hidden="true" tabindex="-1"></a>train_acc <span class="op">=</span> (model.predict(Xtrain) <span class="op">==</span> ytrain).<span class="bu">float</span>().mean()</span>
<span id="cb135-2"><a href="#cb135-2" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtrain, ytrain, title<span class="op">=</span><span class="st">'Training accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> train_acc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-86-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="287">
<div class="sourceCode cell-code" id="cb136"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a>test_acc <span class="op">=</span> (model.predict(Xvalid) <span class="op">==</span> yvalid).<span class="bu">float</span>().mean()</span>
<span id="cb136-2"><a href="#cb136-2" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xvalid, yvalid, title<span class="op">=</span><span class="st">'Validation accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> test_acc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-87-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="285">
<div class="sourceCode cell-code" id="cb137"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb137-1"><a href="#cb137-1" aria-hidden="true" tabindex="-1"></a>test_acc <span class="op">=</span> (model.predict(Xtest) <span class="op">==</span> ytest).<span class="bu">float</span>().mean()</span>
<span id="cb137-2"><a href="#cb137-2" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtest, ytest, title<span class="op">=</span><span class="st">'Test accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> test_acc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-88-output-1.png" class="img-fluid"></p>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>