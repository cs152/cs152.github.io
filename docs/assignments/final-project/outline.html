<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Final project – CS 152: Neural Networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">CS 152: Neural Networks</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../calendar/calendar.html"> 
<span class="menu-text">Calendar</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://drive.google.com/drive/folders/1JFhwmcFBTHiRbfhQ0VRDj9xIPxFpHuWj?usp=drive_link"> 
<span class="menu-text">Homeworks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="../../assignments/final-project/outline.html" aria-current="page"> 
<span class="menu-text">Project</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-resources" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Resources</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-resources">    
        <li>
    <a class="dropdown-item" href="https://probml.github.io/pml-book/book1.html">
 <span class="dropdown-text">Textbook</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://blank-app-ufu2uvdeosc.streamlit.app/">
 <span class="dropdown-text">Notebook conversion</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://www.gradescope.com/courses/710173">
 <span class="dropdown-text">Gradescope</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-solutions" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Solutions</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-solutions">    
        <li class="dropdown-header">Homework 1 Solutions</li>
        <li class="dropdown-header">Homework 2 Solutions</li>
        <li class="dropdown-header">Homework 3 Solutions</li>
        <li class="dropdown-header">Homework 4 Solutions</li>
        <li class="dropdown-header">Homework 5 Solutions</li>
        <li class="dropdown-header">Homework 6 Solutions</li>
        <li class="dropdown-header">Homework 7 Solutions</li>
        <li class="dropdown-header">Homework 8 Solutions</li>
        <li class="dropdown-header">Homework 9 Solutions</li>
        <li class="dropdown-header">Homework 10 Solutions</li>
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#ethics-discussion-due-wednesday-28-1159pm" id="toc-ethics-discussion-due-wednesday-28-1159pm" class="nav-link active" data-scroll-target="#ethics-discussion-due-wednesday-28-1159pm">Ethics Discussion (<strong>Due Wednesday 2/8 11:59pm</strong>)</a></li>
  <li><a href="#proposal-due-thursday-44-1159pm" id="toc-proposal-due-thursday-44-1159pm" class="nav-link" data-scroll-target="#proposal-due-thursday-44-1159pm">Proposal (<strong>Due Thursday 4/4 11:59pm</strong>)</a></li>
  <li><a href="#check-in-due-monday-422-1159pm" id="toc-check-in-due-monday-422-1159pm" class="nav-link" data-scroll-target="#check-in-due-monday-422-1159pm">Check-in (<strong>Due Monday 4/22 11:59pm</strong>)</a></li>
  <li><a href="#final-deliverables-due-53-1159pm" id="toc-final-deliverables-due-53-1159pm" class="nav-link" data-scroll-target="#final-deliverables-due-53-1159pm">Final deliverables (<strong>Due 5/3, 11:59pm</strong>)</a></li>
  <li><a href="#course-server" id="toc-course-server" class="nav-link" data-scroll-target="#course-server">Course server</a></li>
  <li><a href="#examples-from-previous-semesters" id="toc-examples-from-previous-semesters" class="nav-link" data-scroll-target="#examples-from-previous-semesters">Examples from previous semesters</a></li>
  <li><a href="#possible-projects" id="toc-possible-projects" class="nav-link" data-scroll-target="#possible-projects">Possible projects</a>
  <ul class="collapse">
  <li><a href="#neural-style-transfer" id="toc-neural-style-transfer" class="nav-link" data-scroll-target="#neural-style-transfer">Neural style transfer</a></li>
  <li><a href="#semi-supervised-learning-with-mixmatch" id="toc-semi-supervised-learning-with-mixmatch" class="nav-link" data-scroll-target="#semi-supervised-learning-with-mixmatch">Semi-supervised learning with MixMatch</a></li>
  <li><a href="#audio-generation-and-classification-with-wavenet" id="toc-audio-generation-and-classification-with-wavenet" class="nav-link" data-scroll-target="#audio-generation-and-classification-with-wavenet">Audio generation and classification with WaveNet</a></li>
  <li><a href="#u-nets-for-segmentation-depth-prediction-colorization-or-super-resolution" id="toc-u-nets-for-segmentation-depth-prediction-colorization-or-super-resolution" class="nav-link" data-scroll-target="#u-nets-for-segmentation-depth-prediction-colorization-or-super-resolution">U-Nets for segmentation, depth-prediction, colorization or super-resolution</a></li>
  <li><a href="#object-detection-with-yolo" id="toc-object-detection-with-yolo" class="nav-link" data-scroll-target="#object-detection-with-yolo">Object detection with YOLO</a></li>
  <li><a href="#image-generation-with-generative-adversarial-networks" id="toc-image-generation-with-generative-adversarial-networks" class="nav-link" data-scroll-target="#image-generation-with-generative-adversarial-networks">Image generation with Generative Adversarial Networks</a></li>
  <li><a href="#image-classification-with-visual-transformers" id="toc-image-classification-with-visual-transformers" class="nav-link" data-scroll-target="#image-classification-with-visual-transformers">Image classification with visual transformers</a></li>
  <li><a href="#text-generation-or-classification-with-gpt" id="toc-text-generation-or-classification-with-gpt" class="nav-link" data-scroll-target="#text-generation-or-classification-with-gpt">Text generation or classification with GPT</a></li>
  <li><a href="#other-possible-projects" id="toc-other-possible-projects" class="nav-link" data-scroll-target="#other-possible-projects">Other possible projects:</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Final project</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="ethics-discussion-due-wednesday-28-1159pm" class="level2">
<h2 class="anchored" data-anchor-id="ethics-discussion-due-wednesday-28-1159pm">Ethics Discussion (<strong>Due Wednesday 2/8 11:59pm</strong>)</h2>
<p>The first step for the project will be to form a group and participate in a discussion about the potential ethical issues that arise when applying machine learning models in the real world. For this stage you should:</p>
<ul>
<li><p>Form a group of 2-4 students and choose a cool team name. If you would like help finding a group, please email me! If you would like to work individually, you must get prior approval from the professor.</p></li>
<li><p>Complete the <a href="../../assignments/final-project/ethics.html">ethics and AI assignment</a> as a group.</p></li>
</ul>
</section>
<section id="proposal-due-thursday-44-1159pm" class="level2">
<h2 class="anchored" data-anchor-id="proposal-due-thursday-44-1159pm">Proposal (<strong>Due Thursday 4/4 11:59pm</strong>)</h2>
<p>The next step for the project will be to formally propose a project topic, for this milestone you should:</p>
<ul>
<li><p>Choose a project topic. You may choose a topic from the list below or you may propose your own topic.</p></li>
<li><p>With your team, write a short (up to 1 page) proposal for your project and submit it on gradescope. Your proposal should include:</p></li>
<li><p>The names of the team members</p></li>
<li><p>A one paragraph high-level description of the goal(s) of the project (e.g.&nbsp;object detection in images, classifying text data, etc.), how this goal could be useful in real-world applications and why this topic interests you.</p></li>
<li><p>A one paragraph description of how you intend to approach this problem and any challenges you forsee. This should include identifying datasets that you might use, identifying at least one referance (academic paper or website) for a technique you intend to try.</p></li>
<li><p>A one paragraph description of how you will evaluate success for your application.</p></li>
</ul>
</section>
<section id="check-in-due-monday-422-1159pm" class="level2">
<h2 class="anchored" data-anchor-id="check-in-due-monday-422-1159pm">Check-in (<strong>Due Monday 4/22 11:59pm</strong>)</h2>
<p>As a progress report you and your team will submit a short (1 page) summery of progress on Gradescope. This summary will include:</p>
<ul>
<li><p>A high-level description of any updates you have made to the goals of your project.</p></li>
<li><p>A description of what methods you have tried and any preliminary results.</p></li>
<li><p>A timeline for finishing the remaining goals of your project.</p></li>
<li><p>A brief description of the contributions made by each team member.</p></li>
</ul>
</section>
<section id="final-deliverables-due-53-1159pm" class="level2">
<h2 class="anchored" data-anchor-id="final-deliverables-due-53-1159pm">Final deliverables (<strong>Due 5/3, 11:59pm</strong>)</h2>
<p>Please see the final project template <a href="./CS152_Project_Template.pdf">here</a>, which includes instructions and guidelines for what to submit. Submissions will be through gradescope.</p>
<p><strong>Rubric:</strong> <a href="../../assignments/final-project/rubric.html">link</a></p>
<p><strong>Overleaf template:</strong> <a href="https://www.overleaf.com/read/sgrzqjprrcqv#f41b88">link</a></p>
</section>
<section id="course-server" class="level2">
<h2 class="anchored" data-anchor-id="course-server">Course server</h2>
<p>There is a GPU server allocated to this class at <code>teapot.cs.hmc.edu</code>. Please follow the instructions in homework 6 or 7 to setup a Jupyter session for project work.</p>
<p><strong>Data directory</strong> If you are working with a large dataset on the server, please do not save it to your home directory as this may cause you to exceed your account’s allocated limit. Instead, please create a sub-directory of <code>/cs/cs152/shared</code> for your project and save your data there.</p>
</section>
<section id="examples-from-previous-semesters" class="level2">
<h2 class="anchored" data-anchor-id="examples-from-previous-semesters">Examples from previous semesters</h2>
<p>Here are examples of great projects kindly shared by students from previous semesters. Note that these may be longer than what is expected for a perfect grade. 4-6 pages should be sufficient.</p>
<ul>
<li><a href="./Examples/CS152_Project_example_1.pdf">Explainable Transfer Learning for CNNs and Transformers</a></li>
<li><a href="./Examples/CS152_Project_example_2.pdf">Recurrent Neural Network Variants for Generating Opinion Distributions</a></li>
</ul>
</section>
<section id="possible-projects" class="level2">
<h2 class="anchored" data-anchor-id="possible-projects">Possible projects</h2>
<section id="neural-style-transfer" class="level3">
<h3 class="anchored" data-anchor-id="neural-style-transfer">Neural style transfer</h3>
<p><strong>Link:</strong> <a href="https://arxiv.org/pdf/1508.06576.pdf">https://arxiv.org/pdf/1508.06576.pdf</a></p>
<p><strong>Summary:</strong> Style transfer is the process of taking an existing image and applying an artistic style to it, such as making a photograph look like a painting or a drawing (check out the examples in the linked paper!). This can be accomplished with neural networks. For this project you could: implement the neural style transfer algorithm, evaluate it with different kinds of images and compare it to other methods for restyling images.</p>
<p><strong>Suggested datasets:</strong> Any artistic images you’d like!</p>
</section>
<section id="semi-supervised-learning-with-mixmatch" class="level3">
<h3 class="anchored" data-anchor-id="semi-supervised-learning-with-mixmatch">Semi-supervised learning with MixMatch</h3>
<p><strong>Link:</strong> <a href="https://arxiv.org/pdf/1905.02249.pdf" class="uri">https://arxiv.org/pdf/1905.02249.pdf</a></p>
<p><strong>Summary:</strong> Semi-supervised learning is the problem of learning when we don’t have all the labels for our training data. MixMatch is a state-of-the-art approach for semi-supervised image classification. In this project you could: implement the Mix-Match algorithm, compare the different versions of it discussed in the paper and evaluate it on several different datasets. You could also test it on your own proposed semi-supervised learning task.</p>
<p><strong>Suggested datasets:</strong> <a href="https://pytorch.org/vision/stable/generated/torchvision.datasets.SVHN.html#torchvision.datasets.SVHN">Street view house numbers</a>, <a href="https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html">CIFAR-10</a>, <a href="https://pytorch.org/vision/stable/generated/torchvision.datasets.STL10.html#torchvision.datasets.STL10">STL-10</a></p>
</section>
<section id="audio-generation-and-classification-with-wavenet" class="level3">
<h3 class="anchored" data-anchor-id="audio-generation-and-classification-with-wavenet">Audio generation and classification with WaveNet</h3>
<p><strong>Link:</strong> <a href="https://arxiv.org/pdf/1609.03499.pdf" class="uri">https://arxiv.org/pdf/1609.03499.pdf</a></p>
<p><strong>Summary:</strong> WaveNet is at network that forms the basis for many text-to-speech systems (think Alexa or Siri) it also allows for classifying audio. For this project you could: implement WaveNet, train it to generate speech (or other audio like music!) and evaluate it compared to existing tools for generation. You could also try to use it to classify speech or music.</p>
<p><strong>Suggested datasets:</strong> <a href="https://www.tensorflow.org/datasets/catalog/spoken_digit">Spoken digits</a>, <a href="https://pytorch.org/audio/stable/generated/torchaudio.datasets.SPEECHCOMMANDS.html#torchaudio.datasets.SPEECHCOMMANDS">Speech commands</a>, <a href="https://www.tensorflow.org/datasets/catalog/crema_d">Crema-D</a>, <a href="https://www.tensorflow.org/datasets/catalog/gtzan">GTZAN</a></p>
</section>
<section id="u-nets-for-segmentation-depth-prediction-colorization-or-super-resolution" class="level3">
<h3 class="anchored" data-anchor-id="u-nets-for-segmentation-depth-prediction-colorization-or-super-resolution">U-Nets for segmentation, depth-prediction, colorization or super-resolution</h3>
<p><strong>Link:</strong> <a href="https://arxiv.org/pdf/1505.04597.pdf" class="uri">https://arxiv.org/pdf/1505.04597.pdf</a></p>
<p><strong>Summary:</strong> U-Nets are a very flexible type of neural network used for many computer vision tasks. They were originally introduced for segmenting different parts of medical images, but can used for everything from colorizing images to upscaling images to predicting depth in am image. For this project you could: implement the U-Net architecture, train a U-Net on one or more of the above tasks and evaluate its performance.</p>
<p><strong>Suggested datasets:</strong> <a href="https://www.tensorflow.org/datasets/catalog/oxford_flowers102">Oxford flowers</a> <a href="https://www.tensorflow.org/datasets/catalog/imagenet_resized">ImageNet</a>, <a href="https://www.tensorflow.org/datasets/catalog/nyu_depth_v2">NYU Depth</a></p>
</section>
<section id="object-detection-with-yolo" class="level3">
<h3 class="anchored" data-anchor-id="object-detection-with-yolo">Object detection with YOLO</h3>
<p><strong>Introduction:</strong> <a href="https://pyimagesearch.com/2022/04/11/understanding-a-real-time-object-detection-network-you-only-look-once-yolov1/" class="uri">https://pyimagesearch.com/2022/04/11/understanding-a-real-time-object-detection-network-you-only-look-once-yolov1/</a></p>
<p><strong>Original paper:</strong> <a href="https://arxiv.org/pdf/1506.02640v5.pdf" class="uri">https://arxiv.org/pdf/1506.02640v5.pdf</a></p>
<p><strong>Summary:</strong> Object detection is the task of locating objects within an image. This is a step more difficult than just classifying images, but is very useful in practice. For this project you could: implement the YOLO object detection model, test different variations of the model and evaluate it on new data.</p>
<p><strong>Suggested datasets:</strong> <a href="https://www.tensorflow.org/datasets/catalog/voc">VOC</a>, <a href="https://www.tensorflow.org/datasets/catalog/wider_face">Wider face</a>, <a href="https://www.tensorflow.org/datasets/catalog/kitti">Kitti</a></p>
</section>
<section id="image-generation-with-generative-adversarial-networks" class="level3">
<h3 class="anchored" data-anchor-id="image-generation-with-generative-adversarial-networks">Image generation with Generative Adversarial Networks</h3>
<p><strong>Introduction:</strong> <a href="https://developers.google.com/machine-learning/gan" class="uri">https://developers.google.com/machine-learning/gan</a></p>
<p><strong>Original paper:</strong> <a href="https://arxiv.org/pdf/1406.2661.pdf" class="uri">https://arxiv.org/pdf/1406.2661.pdf</a></p>
<p><strong>Summary:</strong> Generative adversarial networks (GANs for short) are an effective way to generate realistic looking images using neural networks. They have caused a considerable amount of excitement and concern for their performance in generating realistic images of humans. For this project you could: implement a generative adversarial network, explore reasonable ways to evaluate the performance of GANs and dive into the ethical implications.</p>
<p><strong>Suggested datasets:</strong> <a href="https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST">MNIST</a>, <a href="https://pytorch.org/vision/stable/generated/torchvision.datasets.Omniglot.html#torchvision.datasets.Omniglot">Omniglot</a>, <a href="https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html">CIFAR-10</a>, <a href="https://github.com/NVlabs/ffhq-dataset">FFHQ</a></p>
</section>
<section id="image-classification-with-visual-transformers" class="level3">
<h3 class="anchored" data-anchor-id="image-classification-with-visual-transformers">Image classification with visual transformers</h3>
<p><strong>Link:</strong> <a href="https://arxiv.org/pdf/2010.11929.pdf" class="uri">https://arxiv.org/pdf/2010.11929.pdf</a></p>
<p><strong>Summary:</strong> Transformer-based neural networks have transformed the field of natural language processing in recent years, as evidenced by the performance of models such as ChatGPT. There is growing evidence that they are also extremely useful for classifying images. For this project you might: implement the visual transformer architecture, compare it to convolutional neural network based architecture for image classification and visualize features to understand the differences in the approaches. You might also consider applying it to your own dataset.</p>
<p><strong>Suggested datasets:</strong> <a href="https://www.tensorflow.org/datasets/catalog/oxford_flowers102">Oxford flowers</a>, <a href="https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html">CIFAR-10</a>, <a href="https://www.tensorflow.org/datasets/catalog/imagenet_resized">ImageNet</a></p>
</section>
<section id="text-generation-or-classification-with-gpt" class="level3">
<h3 class="anchored" data-anchor-id="text-generation-or-classification-with-gpt">Text generation or classification with GPT</h3>
<p><strong>Link:</strong> <a href="https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf" class="uri">https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf</a></p>
<p><strong>Summary:</strong> Large language models, such at GPT-3 and GPT-4 have gained a lot of attention recently, as their performance in generating plausible text is (debatably) approaching human levels. The GPT model is used by Chat-GPT and many other applications to model language. For this project you could implement and train your own version of the original (GPT-1) model, compare it against available tools such as Chat-GPT and explore how to distinguish generated text from real human writing.</p>
<p><strong>Suggested datasets:</strong> <a href="https://www.tensorflow.org/datasets/catalog/amazon_us_reviews">Amazon reviews</a> <a href="https://www.tensorflow.org/datasets/catalog/imdb_reviews">IMDB reviews</a></p>
</section>
<section id="other-possible-projects" class="level3">
<h3 class="anchored" data-anchor-id="other-possible-projects">Other possible projects:</h3>
<section id="machine-learning-with-differential-privacy" class="level4">
<h4 class="anchored" data-anchor-id="machine-learning-with-differential-privacy">Machine learning with differential privacy</h4>
<p><a href="https://arxiv.org/pdf/1607.00133.pdf" class="uri">https://arxiv.org/pdf/1607.00133.pdf</a></p>
</section>
<section id="graph-neural-networks" class="level4">
<h4 class="anchored" data-anchor-id="graph-neural-networks">Graph neural networks</h4>
<p><a href="https://arxiv.org/pdf/1810.00826.pdf" class="uri">https://arxiv.org/pdf/1810.00826.pdf</a></p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>