<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.319">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>CS 152: Neural Networks - Homework 4: Automatic Differentiation and Neural Networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">CS 152: Neural Networks</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../calendar/calendar.html" rel="" target="">
 <span class="menu-text">Calendar</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://drive.google.com/drive/folders/1JFhwmcFBTHiRbfhQ0VRDj9xIPxFpHuWj?usp=drive_link" rel="" target="">
 <span class="menu-text">Homeworks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../assignments/final-project/outline.html" rel="" target="">
 <span class="menu-text">Project</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/orgs/CS152/" rel="" target="">
 <span class="menu-text">Github</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://www.gradescope.com/courses/710173" rel="" target="">
 <span class="menu-text">Gradescope</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#collaborators" id="toc-collaborators" class="nav-link active" data-scroll-target="#collaborators">Collaborators</a></li>
  <li><a href="#course-feedback" id="toc-course-feedback" class="nav-link" data-scroll-target="#course-feedback">Course feedback</a></li>
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#part-1-autograd" id="toc-part-1-autograd" class="nav-link" data-scroll-target="#part-1-autograd">Part 1: Autograd</a></li>
  <li><a href="#part-2-implementing-a-neural-network" id="toc-part-2-implementing-a-neural-network" class="nav-link" data-scroll-target="#part-2-implementing-a-neural-network">Part 2: Implementing a neural network</a></li>
  <li><a href="#part-3-forward-mode-automatic-differentiation" id="toc-part-3-forward-mode-automatic-differentiation" class="nav-link" data-scroll-target="#part-3-forward-mode-automatic-differentiation">Part 3: Forward-mode automatic differentiation</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><strong>Homework 4:</strong> Automatic Differentiation and Neural Networks</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="collaborators" class="level3">
<h3 class="anchored" data-anchor-id="collaborators">Collaborators</h3>
<p>Please list anyone you discussed or collaborated on this assignment with below.</p>
<p>LIST COLLABORATORS HERE</p>
</section>
<section id="course-feedback" class="level3">
<h3 class="anchored" data-anchor-id="course-feedback">Course feedback</h3>
<p>Please submit this week’s course survey here: https://forms.gle/PJjGy8wrhhQnAhbb6</p>
</section>
<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<p>In this homework we will build a tiny neural network libarary from scratch and start on an implementation of automatic differentiation!</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Uncomment and run if using Colab!</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">#import urllib.request</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">#remote_url = 'https://gist.githubusercontent.com/gabehope/cb9e69f642104f107f25826a0931629a/raw/163f9cf5325db28826f4103d0f168702c77dfca1/hw4_support.py'</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">#with urllib.request.urlopen(remote_url) as remote, open('hw4_support.py', 'w') as local:</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">#  [local.write(str(line, encoding='utf-8')) for line in remote]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Run me first!</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> hw4_support <span class="im">import</span> <span class="op">*</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="python-features" class="level4">
<h4 class="anchored" data-anchor-id="python-features">Python features</h4>
<p>This homework makes use of a few fancy features in Python that are worth knowing about if you are unfamiliar. - <a href="https://book.pythontips.com/en/latest/args_and_kwargs.html">Variable length arguments</a> (e.g.&nbsp;<code>*args</code>) - <a href="https://book.pythontips.com/en/latest/comprehensions.html#list-comprehensions">List comprehensions</a> (e.g.&nbsp;<code>[a**2 for a in range(5)]</code>) - <a href="https://rszalski.github.io/magicmethods/">Magic methods</a> (e.g.&nbsp;<code>__add__</code>)</p>
</section>
</section>
<section id="part-1-autograd" class="level2">
<h2 class="anchored" data-anchor-id="part-1-autograd">Part 1: Autograd</h2>
<p>In this homework we will be using a special version of Numpy from a package called <code>Autograd</code>. Assuming it is installed (<code>pip install autograd</code>), we can import it as follows:</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> autograd.numpy <span class="im">as</span> np</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This special version of Numpy behaives exactly like normal numpy. We can create and do calculations with arrays just like we would before:</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([<span class="fl">3.</span>, <span class="fl">2.</span>, <span class="dv">1</span>])</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'x:</span><span class="ch">\t</span><span class="st">'</span>, x)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'x^2:</span><span class="ch">\t</span><span class="st">'</span>, x <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'sum(x):</span><span class="ch">\t</span><span class="st">'</span>, np.<span class="bu">sum</span>(x))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>x:   [3. 2. 1.]
x^2:     [9. 4. 1.]
sum(x):  6.0</code></pre>
</div>
</div>
<p>However, Autograd also has a very important trick up its sleeve: it can take derivatives (and gradients) for us! This functionality can be accessed through the <code>grad</code> function. Let’s start by seeing it in action with a very simple example, where we know the correct answer. The square function and its derivative can be written as:</p>
<p><span class="math inline">\(f(x) = x^2, \quad f'(x) = 2x\)</span></p>
<p>The following code uses Autograd to compute this derivative automatically:</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> autograd <span class="im">import</span> grad</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x):</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Use 'grad' to compute the derivative function</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>f_prime <span class="op">=</span> grad(f)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify that we get the correct answer</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="fl">5.</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'x:</span><span class="ch">\t</span><span class="st">'</span>, x)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'f(x):</span><span class="ch">\t</span><span class="st">'</span>, f(x))</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"f'(x):</span><span class="ch">\t</span><span class="st">"</span>, f_prime(x))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>x:   5.0
f(x):    25.0
f'(x):   10.0</code></pre>
</div>
</div>
<p>We can start to see how <code>grad</code> operates. <code>grad</code> takes as input a function (e.g.&nbsp;<span class="math inline">\(f(x)\)</span>) and returns a new function that computes the derivative of <span class="math inline">\(f\)</span> at <span class="math inline">\(x\)</span>. (<span class="math inline">\(f'(x)\)</span>). So:</p>
<p><span class="math inline">\(\text{grad}(f) \longrightarrow f'\)</span></p>
<section id="q1" class="level4">
<h4 class="anchored" data-anchor-id="q1">Q1</h4>
<p>Define the following function in python:</p>
<p><span class="math inline">\(f(x) = \log(\sin(x^3) + 3 x)\)</span></p>
<p>Use <code>grad</code> to compute the derivative of <span class="math inline">\(f\)</span> at <span class="math inline">\(1.5\)</span> (i.e.&nbsp;compute <span class="math inline">\(f'(1.5)\)</span>)</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.log(np.sin(x <span class="op">**</span> <span class="dv">3</span>) <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> x)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>answer <span class="op">=</span> grad(f)(<span class="fl">1.5</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"f'(1.5)="</span>, answer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>f'(1.5)= -1.6494948636364284</code></pre>
</div>
</div>
<p>As the name would imply, <code>grad</code> can more generally be used to compute the <em>gradient</em> of a function of the form <span class="math inline">\(f(\mathbf{x}): \mathbb{R}^d\rightarrow \mathbb{R}\)</span>. Remember that for a function that takes in a vector and outputs a scalar, the gradient is vector of all partial derivatives of the output with respect to each input. For example, consider a function that gives the square of the 2-norm of a vector:</p>
<p><span class="math inline">\(f(\mathbf{x}) = ||\mathbf{x}||^2_2 = \mathbf{x}^T\mathbf{x} = \sum_{i=1}^d x_i^2\)</span></p>
<p><em>Think about why these expressions are equivalent!</em></p>
<p>As we’ve seen, the gradient of this function can be written as:</p>
<p><span class="math inline">\(\nabla f(\mathbf{x}) = 2\mathbf{x} = \begin{bmatrix}2x_1 \\ 2x_2 \\ \vdots \\ 2x_d \end{bmatrix}\)</span></p>
<p>Let’s see what Autograd gives us in this case:</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x):</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.<span class="bu">sum</span>(x <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Use 'grad' to compute the derivative function</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>grad_f <span class="op">=</span> grad(f)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify that we get the correct answer</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([<span class="fl">1.</span>, <span class="fl">2.</span>, <span class="dv">3</span>])</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'x:</span><span class="ch">\t</span><span class="st">'</span>, x)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'f(x):</span><span class="ch">\t</span><span class="st">'</span>, f(x))</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"f'(x):</span><span class="ch">\t</span><span class="st">"</span>, grad_f(x))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>x:   [1. 2. 3.]
f(x):    14.0
f'(x):   [2. 4. 6.]</code></pre>
</div>
</div>
<p>We see that the gradient has the same shape as the input. So the gradient function is of the form: <span class="math inline">\(\mathbb{R}^d \rightarrow \mathbb{R}^d\)</span></p>
<p>This makes sense as the gradient should have exactly one partial derivative for each entry in the input to the function. As discussed, this even extends beyond vectors! We could have a function that takes in any datastructure and computes the set of partial derivatives with respect to each entry.</p>
</section>
<section id="q2" class="level4">
<h4 class="anchored" data-anchor-id="q2">Q2</h4>
<p>Write a function that takes a <code>list</code> of vectors and computes the sum of the squared 2-norm for each vector. That is:</p>
<p><span class="math inline">\(f([\mathbf{a}, \mathbf{b}, \mathbf{c}...]) = ||\mathbf{a}||^2 + ||\mathbf{b}||^2 + ||\mathbf{c}||^2+...\)</span></p>
<p><em>Recall from above how we can compute each term in this sum!</em></p>
<p>Then use <code>grad</code> to compute the gradient of this function with respect to the given input.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x):</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute the sum of squared 2-norms for a list of vectors</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co">        x (list of arrays): A list of 1-dimensional arrays</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="co">        output (float): The result</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> xi <span class="kw">in</span> x:</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>        s <span class="op">+=</span> np.dot(xi, xi)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> s</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Use 'grad' to compute the derivative function</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>grad_f <span class="op">=</span> grad(f)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify that we get the correct answer</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> [np.array([<span class="fl">1.</span>, <span class="fl">2.</span>, <span class="dv">3</span>]), np.array([<span class="fl">7.</span>, <span class="fl">2.</span>]), np.array([<span class="fl">6.</span>])]</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'x:</span><span class="ch">\t</span><span class="st">'</span>, x)</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'f(x):</span><span class="ch">\t</span><span class="st">'</span>, f(x))</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"f'(x):</span><span class="ch">\t</span><span class="st">"</span>, grad_f(x))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>x:   [array([1., 2., 3.]), array([7., 2.]), array([6.])]
f(x):    103.0
f'(x):   [array([2., 4., 6.]), array([14.,  4.]), array([12.])]</code></pre>
</div>
</div>
<p>A useful argument that we can give to <code>grad</code> is <code>argnum</code>. If our function takes more than one argument <code>argnum</code> lets us specify which one to take the gradient with respect to. For example, if we have the function:</p>
<p><span class="math inline">\(f(x, y) = x^2y\)</span></p>
<p>Then:</p>
<p><span class="math inline">\(f'_x(x,y)=2xy, \quad f'_y(x, y)=x^2\)</span></p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x, y):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x <span class="op">**</span> <span class="dv">2</span> <span class="op">*</span> y</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'f(3, 5) = '</span>, f(<span class="fl">3.</span>, <span class="fl">5.</span>))</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>df_dx <span class="op">=</span> grad(f, argnum<span class="op">=</span><span class="dv">0</span>)(<span class="fl">3.</span>, <span class="fl">5.</span>)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>df_dy <span class="op">=</span> grad(f, argnum<span class="op">=</span><span class="dv">1</span>)(<span class="fl">3.</span>, <span class="fl">5.</span>)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'df_dx = '</span>, df_dx)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'df_dy = '</span>, df_dy)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>f(3, 5) =  45.0
df_dx =  30.0
df_dy =  9.0</code></pre>
</div>
</div>
</section>
</section>
<section id="part-2-implementing-a-neural-network" class="level2">
<h2 class="anchored" data-anchor-id="part-2-implementing-a-neural-network">Part 2: Implementing a neural network</h2>
<p>Now that we have everything we need to apply automatic differentiation to train a neural network!</p>
<p>Before we do that though, let’s try out our automatic differentiation for logistic regression. Below is a slight modification of LogisticRegression implementation we saw in the last homework.</p>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x):</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Computes the sigmoid function</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">1.</span> <span class="op">/</span> (<span class="fl">1.</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LogisticRegression:</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dims):</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">'''</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="co">            dims (int): d, the dimension of each input</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="co">        '''</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> np.zeros((dims <span class="op">+</span> <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> prediction_function(<span class="va">self</span>, X, w):</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>        <span class="co">'''</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a><span class="co">        Get the result of our base function for prediction (i.e. x^t w)</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a><span class="co">            X (array): An N x d matrix of observations.</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a><span class="co">            w (array): A (d+1) x 1 vector of weights.</span></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a><span class="co">            pred (array): A length N vector of f(X).</span></span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a><span class="co">        '''</span></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>        X <span class="op">=</span> np.pad(X, ((<span class="dv">0</span>,<span class="dv">0</span>), (<span class="dv">0</span>,<span class="dv">1</span>)), constant_values<span class="op">=</span><span class="fl">1.</span>, mode<span class="op">=</span><span class="st">'constant'</span>)</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.dot(X, w)</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>        <span class="co">'''</span></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a><span class="co">        Predict labels given a set of inputs.</span></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a><span class="co">            X (array): An N x d matrix of observations.</span></span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a><span class="co">            pred (array): An N x 1 column vector of predictions in {0, 1}</span></span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a><span class="co">        '''</span></span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (<span class="va">self</span>.prediction_function(X, <span class="va">self</span>.weights) <span class="op">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_probability(<span class="va">self</span>, X):</span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>        <span class="co">'''</span></span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a><span class="co">        Predict the probability of each class given a set of inputs</span></span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a><span class="co">            X (array): An N x d matrix of observations.</span></span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a><span class="co">            probs (array): An N x 1 column vector of predicted class probabilities</span></span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a><span class="co">        '''</span></span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> sigmoid(<span class="va">self</span>.prediction_function(X, <span class="va">self</span>.weights))</span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> accuracy(<span class="va">self</span>, X, y):</span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a>        <span class="co">'''</span></span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a><span class="co">        Compute the accuracy of the model's predictions on a dataset</span></span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a><span class="co">            X (array): An N x d matrix of observations.</span></span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a><span class="co">            y (array): A length N vector of labels.</span></span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a><span class="co">            acc (float): The accuracy of the classifier</span></span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true" tabindex="-1"></a><span class="co">        '''</span></span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y.reshape((<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (<span class="va">self</span>.predict(X) <span class="op">==</span> y).mean()</span>
<span id="cb16-61"><a href="#cb16-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-62"><a href="#cb16-62" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> nll(<span class="va">self</span>, X, y, w<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb16-63"><a href="#cb16-63" aria-hidden="true" tabindex="-1"></a>        <span class="co">'''</span></span>
<span id="cb16-64"><a href="#cb16-64" aria-hidden="true" tabindex="-1"></a><span class="co">        Compute the negative log-likelihood loss.</span></span>
<span id="cb16-65"><a href="#cb16-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-66"><a href="#cb16-66" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb16-67"><a href="#cb16-67" aria-hidden="true" tabindex="-1"></a><span class="co">            X (array): An N x d matrix of observations.</span></span>
<span id="cb16-68"><a href="#cb16-68" aria-hidden="true" tabindex="-1"></a><span class="co">            y (array): A length N vector of labels.</span></span>
<span id="cb16-69"><a href="#cb16-69" aria-hidden="true" tabindex="-1"></a><span class="co">            w (array, optional): A (d+1) x 1 matrix of weights.</span></span>
<span id="cb16-70"><a href="#cb16-70" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb16-71"><a href="#cb16-71" aria-hidden="true" tabindex="-1"></a><span class="co">            nll (float): The NLL loss</span></span>
<span id="cb16-72"><a href="#cb16-72" aria-hidden="true" tabindex="-1"></a><span class="co">        '''</span></span>
<span id="cb16-73"><a href="#cb16-73" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> w <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb16-74"><a href="#cb16-74" aria-hidden="true" tabindex="-1"></a>            w <span class="op">=</span> <span class="va">self</span>.weights</span>
<span id="cb16-75"><a href="#cb16-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-76"><a href="#cb16-76" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y.reshape((<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb16-77"><a href="#cb16-77" aria-hidden="true" tabindex="-1"></a>        xw <span class="op">=</span> <span class="va">self</span>.prediction_function(X, w)</span>
<span id="cb16-78"><a href="#cb16-78" aria-hidden="true" tabindex="-1"></a>        py <span class="op">=</span> sigmoid((<span class="dv">2</span> <span class="op">*</span> y <span class="op">-</span> <span class="dv">1</span>) <span class="op">*</span> xw)</span>
<span id="cb16-79"><a href="#cb16-79" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="op">-</span>(np.log(py)).<span class="bu">sum</span>()</span>
<span id="cb16-80"><a href="#cb16-80" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-81"><a href="#cb16-81" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> nll_gradient(<span class="va">self</span>, X, y):</span>
<span id="cb16-82"><a href="#cb16-82" aria-hidden="true" tabindex="-1"></a>        <span class="co">'''</span></span>
<span id="cb16-83"><a href="#cb16-83" aria-hidden="true" tabindex="-1"></a><span class="co">        Compute the gradient of the negative log-likelihood loss.</span></span>
<span id="cb16-84"><a href="#cb16-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-85"><a href="#cb16-85" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb16-86"><a href="#cb16-86" aria-hidden="true" tabindex="-1"></a><span class="co">            X (array): An N x d matrix of observations.</span></span>
<span id="cb16-87"><a href="#cb16-87" aria-hidden="true" tabindex="-1"></a><span class="co">            y (array): A length N vector of labels.</span></span>
<span id="cb16-88"><a href="#cb16-88" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb16-89"><a href="#cb16-89" aria-hidden="true" tabindex="-1"></a><span class="co">            grad (array): A length (d + 1) vector with the gradient</span></span>
<span id="cb16-90"><a href="#cb16-90" aria-hidden="true" tabindex="-1"></a><span class="co">        '''</span></span>
<span id="cb16-91"><a href="#cb16-91" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y.reshape((<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb16-92"><a href="#cb16-92" aria-hidden="true" tabindex="-1"></a>        xw <span class="op">=</span> <span class="va">self</span>.prediction_function(X, <span class="va">self</span>.weights)</span>
<span id="cb16-93"><a href="#cb16-93" aria-hidden="true" tabindex="-1"></a>        py <span class="op">=</span> sigmoid((<span class="dv">2</span> <span class="op">*</span> y <span class="op">-</span> <span class="dv">1</span>) <span class="op">*</span> xw)</span>
<span id="cb16-94"><a href="#cb16-94" aria-hidden="true" tabindex="-1"></a>        grad <span class="op">=</span> ((<span class="dv">1</span> <span class="op">-</span> py) <span class="op">*</span> (<span class="dv">2</span> <span class="op">*</span> y <span class="op">-</span> <span class="dv">1</span>)).reshape((<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)) <span class="op">*</span> np.pad(X, [(<span class="dv">0</span>,<span class="dv">0</span>), (<span class="dv">0</span>,<span class="dv">1</span>)], constant_values<span class="op">=</span><span class="fl">1.</span>)</span>
<span id="cb16-95"><a href="#cb16-95" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="op">-</span>np.<span class="bu">sum</span>(grad, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb16-96"><a href="#cb16-96" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-97"><a href="#cb16-97" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> nll_and_grad_no_autodiff(<span class="va">self</span>, X, y):</span>
<span id="cb16-98"><a href="#cb16-98" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute nll_and_grad without automatic diferentiation</span></span>
<span id="cb16-99"><a href="#cb16-99" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.nll(X, y), <span class="va">self</span>.nll_gradient(X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="q3" class="level4">
<h4 class="anchored" data-anchor-id="q3">Q3</h4>
<p>Write the method <code>nll_and_grad</code> for the LogisticRegression class using the <code>grad</code> function from Autograd. Verify that it gives a similar answer to <code>nll_and_grad_no_autodiff</code>.</p>
<p><em><strong>Hint:</strong> Note that the <code>nll</code> function can optionally take in the parameters. You can use this functionality and the <code>argnum</code> argument of <code>grad</code> in your answer. You can assume that <code>self</code> refers to the model object, so you can access the weights via <code>self.weights</code></em></p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> nll_and_grad(<span class="va">self</span>, X, y):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="va">self</span>.nll(X, y, <span class="va">self</span>.weights)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    grads <span class="op">=</span> grad(<span class="va">self</span>.nll, argnum<span class="op">=</span><span class="dv">2</span>)(X, y, <span class="va">self</span>.weights)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss, grads</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>LogisticRegression.nll_and_grad <span class="op">=</span> nll_and_grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This implementation quite inefficient (we’ll fix this in the future!), so we’ll test our model on a very small dataset.</p>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(<span class="dv">100</span>, noise<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(<span class="dv">2</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>gradient_descent(model, X, y, lr<span class="op">=</span><span class="fl">3e-2</span>, steps<span class="op">=</span><span class="dv">250</span>)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Model accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> model.accuracy(X, y))</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loss 25.74, accuracy: 0.88: 100%|██████████| 250/250 [00:00&lt;00:00, 1484.06it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Model accuracy: 0.880</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-13-output-3.png" class="img-fluid"></p>
</div>
</div>
<p>Now let’s extend our model to be a neural network! We’ll create a neural network class that extends our logistic regression class. First we’ll setup the needed weight matrices.</p>
</section>
<section id="q4" class="level4">
<h4 class="anchored" data-anchor-id="q4">Q4</h4>
<p>Fill in the Neural Network <code>__init__</code> method below. The method should take in the input data dimension and a list of integers specifying the size of each hidden layer (the number of neurons in each layer). The function should create a list of numpy arrays of the appropriate shapes for the weight matrices.</p>
<p>For example if <code>dims</code> is <code>2</code> and <code>hidden_sizes</code> is <code>[4, 4]</code>, then <code>self.weights</code> should have 3 entries of shapes <code>[(4x2), (4x4), (1x4)]</code>. This network is shown below.</p>
<div scale="0.5">
<svg xmlns="http://www.w3.org/2000/svg" style="cursor: move;" viewbox="100 100 1660 899" width="600" height="400">
<g transform="translate(-1450.305465592915,-694.5417988897334) scale(2.441893025338307)"><path class="link" style="stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;" marker-end="" d="M666.6666666666666,469.5, 846.6666666666666,429.5"></path><path class="link" style="stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;" marker-end="" d="M666.6666666666666,469.5, 846.6666666666666,469.5"></path><path class="link" style="stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;" marker-end="" d="M666.6666666666666,469.5, 846.6666666666666,509.5"></path><path class="link" style="stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;" marker-end="" d="M666.6666666666666,469.5, 846.6666666666666,549.5"></path><path class="link" style="stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;" marker-end="" d="M666.6666666666666,509.5, 846.6666666666666,429.5"></path><path class="link" style="stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;" marker-end="" d="M666.6666666666666,509.5, 846.6666666666666,469.5"></path><path class="link" style="stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;" marker-end="" d="M666.6666666666666,509.5, 846.6666666666666,509.5"></path><path class="link" style="stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;" marker-end="" d="M666.6666666666666,509.5, 846.6666666666666,549.5"></path><path class="link" style="stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;" marker-end="" d="M846.6666666666666,429.5, 1026.6666666666667,429.5"></path><path class="link" style="stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;" marker-end="" d="M846.6666666666666,429.5, 1026.6666666666667,469.5"></path><path class="link" style="stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;" marker-end="" d="M846.6666666666666,429.5, 1026.6666666666667,509.5"></path><path class="link" style="stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;" marker-end="" d="M846.6666666666666,469.5, 1026.6666666666667,429.5"></path><path class="link" style="stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;" marker-end="" d="M846.6666666666666,469.5, 1026.6666666666667,469.5"></path><path class="link" style="stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;" marker-end="" d="M846.6666666666666,469.5, 1026.6666666666667,509.5"></path><path class="link" style="stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;" marker-end="" d="M846.6666666666666,509.5, 1026.6666666666667,429.5"></path><path class="link" style="stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;" marker-end="" d="M846.6666666666666,509.5, 1026.6666666666667,469.5"></path><path class="link" style="stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;" marker-end="" d="M846.6666666666666,509.5, 1026.6666666666667,509.5"></path><path class="link" style="stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;" marker-end="" d="M846.6666666666666,549.5, 1026.6666666666667,429.5"></path><path class="link" style="stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;" marker-end="" d="M846.6666666666666,549.5, 1026.6666666666667,469.5"></path><path class="link" style="stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;" marker-end="" d="M846.6666666666666,549.5, 1026.6666666666667,509.5"></path><path class="link" style="stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;" marker-end="" d="M1026.6666666666667,429.5, 1206.6666666666667,489.5"></path><path class="link" style="stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;" marker-end="" d="M1026.6666666666667,469.5, 1206.6666666666667,489.5"></path><path class="link" style="stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;" marker-end="" d="M1026.6666666666667,509.5, 1206.6666666666667,489.5"></path><path class="link" style="stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;" marker-end="" d="M846.6666666666666,429.5, 1026.6666666666667,549.5"></path><path class="link" style="stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;" marker-end="" d="M846.6666666666666,469.5, 1026.6666666666667,549.5"></path><path class="link" style="stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;" marker-end="" d="M846.6666666666666,509.5, 1026.6666666666667,549.5"></path><path class="link" style="stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;" marker-end="" d="M846.6666666666666,549.5, 1026.6666666666667,549.5"></path><path class="link" style="stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80); fill: none;" marker-end="" d="M1026.6666666666667,549.5, 1206.6666666666667,489.5"></path><circle r="10" class="node" id="0_0" style="fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);" cx="666.6666666666666" cy="469.5"></circle><circle r="10" class="node" id="0_1" style="fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);" cx="666.6666666666666" cy="509.5"></circle><circle r="10" class="node" id="1_0" style="fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);" cx="846.6666666666666" cy="429.5"></circle><circle r="10" class="node" id="1_1" style="fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);" cx="846.6666666666666" cy="469.5"></circle><circle r="10" class="node" id="1_2" style="fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);" cx="846.6666666666666" cy="509.5"></circle><circle r="10" class="node" id="1_3" style="fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);" cx="846.6666666666666" cy="549.5"></circle><circle r="10" class="node" id="2_0" style="fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);" cx="1026.6666666666667" cy="429.5"></circle><circle r="10" class="node" id="2_1" style="fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);" cx="1026.6666666666667" cy="469.5"></circle><circle r="10" class="node" id="2_2" style="fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);" cx="1026.6666666666667" cy="509.5"></circle><text class="text" dy=".35em" style="font-size: 12px;" x="631.6666666666666" y="589.5">Input Layer ∈ ℝ²</text><text class="text" dy=".35em" style="font-size: 12px;" x="811.6666666666666" y="589.5">Hidden Layer ∈ ℝ⁴</text><text class="text" dy=".35em" style="font-size: 12px;" x="991.6666666666667" y="589.5">Hidden Layer ∈ ℝ⁴</text><circle r="10" class="node" id="2_3" style="fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);" cx="1026.6666666666667" cy="549.5"></circle><circle r="10" class="node" id="3_0" style="fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);" cx="1206.6666666666667" cy="489.5"></circle><text class="text" dy=".35em" style="font-size: 12px;" x="1171.6666666666667" y="589.5">Output Layer ∈ ℝ¹</text></g><defs><marker id="arrow" viewbox="0 -5 10 10" markerwidth="7" markerheight="7" orient="auto" refx="40"><path d="M0,-5L10,0L0,5" style="stroke: rgb(80, 80, 80); fill: none;"></path></marker></defs>
</svg>
<p>If you find it easier you could also define the weights in terms of <span class="math inline">\(W^T\)</span> instead, in which case the shapes would be: <code>[(2x4), (4x4), (4x1)]</code>. You could also consider how to add a bias term at each layer as in logistic regression (but this isn’t nessecary for full credit).</p>
<p>The values in each array should be drawn from a normal distribution with standard deviation 1. You can create such a matrix in numpy using:</p>
<pre><code>np.random.normal(scale=1., size=shape)</code></pre>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NeuralNetwork(LogisticRegression):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dims, hidden_sizes<span class="op">=</span>[]):</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> [<span class="fl">1.</span> <span class="op">*</span> np.random.normal(scale<span class="op">=</span><span class="fl">1.</span>, size<span class="op">=</span>(i <span class="op">+</span> <span class="dv">1</span>, o)) <span class="cf">for</span> (i, o) <span class="kw">in</span> <span class="bu">zip</span>([dims] <span class="op">+</span> hidden_sizes, hidden_sizes <span class="op">+</span> [<span class="dv">1</span>])]</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>test_nn_constructor(NeuralNetwork)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Passed!</code></pre>
</div>
</div>
<p>Recall that for logistic regression the prediction function (before threholding or sigmoid) was <span class="math inline">\(\mathbf{X}\mathbf{w}\)</span>. We now want to implement the prediction function for our neural network class. This function should perform the appropriate feature transforms and multiply by the regression weights. For a neural network with a single hidden layer this will look like:</p>
<p><span class="math inline">\(f(\mathbf{X}) = \sigma(\mathbf{X}\mathbf{W}_1^T)\mathbf{w}_0\)</span></p>
<p>Use the <strong>sigmoid</strong> activation function for this problem.</p>
<p>For multiple layers we can also think of this a a <strong>chain</strong> of feature transforms: <span class="math display">\[\Phi_1 = \sigma(\mathbf{X}\mathbf{W}_1^T)\]</span> <span class="math display">\[\Phi_2 = \sigma(\Phi_1 \mathbf{W}_2^T)\]</span> <span class="math display">\[...\]</span> <span class="math display">\[\Phi_l = \sigma(\Phi_{l-1} \mathbf{W}_l^T)\]</span> <span class="math display">\[f(\mathbf{X}) = \Phi_l\mathbf{w}_0\]</span> Where <span class="math inline">\(\Phi_i\)</span> is just the variable that represents the neurons at layer <span class="math inline">\(i\)</span> (the result of the first <span class="math inline">\(i\)</span> transforms applied to <span class="math inline">\(\mathbf{X}\)</span>).</p>
</div></section>
<section id="q5" class="level4">
<h4 class="anchored" data-anchor-id="q5">Q5</h4>
<p>Implement the prediction function as described above. Note that the prediction function should use the weights passed into the <code>w</code> argument rather than <code>self.weights</code>, this will make it easier to implement the next question.</p>
<p><em><strong>Hint:</strong> Note that this function should <strong>not</strong> apply a final sigmoid or thresholding, instead it should be the equivalent of <code>linear_function</code> from the previous homework</em></p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prediction_function(<span class="va">self</span>, X, w):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Get the result of our base function for prediction (i.e. x^t w)</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="co">        X (array): An N x d matrix of observations.</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="co">        w (list of arrays): A list of weight matrices</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="co">        pred (array): An N x 1 matrix of f(X).</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> wi <span class="kw">in</span> w[:<span class="op">-</span><span class="dv">1</span>]:</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>        X <span class="op">=</span> sigmoid(LogisticRegression.prediction_function(<span class="va">self</span>, X, wi))</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>    pred <span class="op">=</span> LogisticRegression.prediction_function(<span class="va">self</span>, X, w[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pred.reshape((<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>NeuralNetwork.prediction_function <span class="op">=</span> prediction_function</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>test_nn_prediction_function(NeuralNetwork)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Passed!</code></pre>
</div>
</div>
</section>
<section id="q6" class="level4">
<h4 class="anchored" data-anchor-id="q6">Q6</h4>
<p>Implement an <code>nll_and_grad</code> method for the <code>NeuralNetwork</code> class using Autograd to compute the gradient with respect to each weight matrix.</p>
<p><em><strong>Hint:</strong> If you use <code>np.pad</code> anywhere in your implementation, Autograd may complain if you don’t include the keyword argument <code>mode='constant'</code></em></p>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> nll_and_grad(<span class="va">self</span>, X, y):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Get the negative log-likelihood loss and its gradient</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="co">        X (array): An N x d matrix of observations.</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="co">        y (array): A length N vector of labels</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="co">        nll (float): The negative log-likelihood</span></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="co">        grads (list of arrays): A list of the gradient of the nll with respect</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a><span class="co">                                to each value in self.weights.</span></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="va">self</span>.nll(X, y, <span class="va">self</span>.weights)</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>    grads <span class="op">=</span> grad(<span class="va">self</span>.nll, argnum<span class="op">=</span><span class="dv">2</span>)(X, y, <span class="va">self</span>.weights)</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss, grads</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>NeuralNetwork.nll_and_grad <span class="op">=</span> nll_and_grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We now have everything in place to train a neural network from scratch! Let’s try it on our tiny dataset. Feel free to change the inputs.</p>
<p><em><strong>Hint</strong>: If this give very poor results and/or runs very slowly, make sure to carefully check the shape of each operation in your code to make sure it matches your expectation.</em></p>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(<span class="dv">100</span>, noise<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> NeuralNetwork(<span class="dv">2</span>, [<span class="dv">5</span>, <span class="dv">5</span>])</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>gradient_descent(model, X, y, lr<span class="op">=</span><span class="fl">3e-2</span>, steps<span class="op">=</span><span class="dv">250</span>)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Model accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> model.accuracy(X, y))</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loss 28.37, accuracy: 0.86: 100%|██████████| 250/250 [00:00&lt;00:00, 956.82it/s] </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Model accuracy: 0.860</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="solutions_files/figure-html/cell-17-output-3.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="part-3-forward-mode-automatic-differentiation" class="level2">
<h2 class="anchored" data-anchor-id="part-3-forward-mode-automatic-differentiation">Part 3: Forward-mode automatic differentiation</h2>
<p>We’ll start by developing an automatic differentiation class that uses <em>forward-mode automatic differentiation</em>.</p>
<p>Recall that for this version of automatic differentiaion each operation needs to keep track of the derivative of it’s value with respect <em>each</em> original input. Since for every operation we need to store these extra pieces of data and functions for computing both the operation and its derivative, it makes sense to define a <em>class</em> to represent the result of an operation.</p>
<p>For example, if we want to make a class that represents the operation <code>c=a+b</code> our class needs several properties: - <code>value</code>: The value of the operation (<code>c</code>) - <code>forward_grads</code>: A dictionary that contains the derivatives with respect to each original input (e.g.&nbsp;(<span class="math inline">\(\frac{dc}{da}\)</span> and <span class="math inline">\(\frac{dc}{db}\)</span>)). - <code>func</code>: A function that computes the operation (<code>a+b</code>) - <code>grads</code>: A function that <em>computes</em> the derivatives of the operation (<span class="math inline">\(\frac{dc}{da}\)</span> and <span class="math inline">\(\frac{dc}{db}\)</span>)</p>
<p>For this homework, we’ve provided the outline of such a class, called <code>ForwardValue</code>. This will be the base class for all of our possible operations and represents declaring a variable with a value (<code>a = 5</code>). This is useful because it lets us define values that we might want to find derivatives with respect to.</p>
<p>Let’s see how this will work in practice. If we want to take derivatives we will first define the inputs using <code>ForwardValue</code>.</p>
<pre class="{python}"><code>a = ForwardValue(5)
b = ForwardValue(2)</code></pre>
<p>Then we can perform whatever operations we want on these inputs:</p>
<pre class="{python}"><code>c = a + b
L = log(c)</code></pre>
<p>Each of these operations will produce a new <code>ForwardValue</code> object representing the result of that operation.</p>
<p>As each result should maintain the derivatives with respect to each <em>original</em> inputs, we can access the final derivatives we’re interested (<span class="math inline">\(\frac{dL}{da}\)</span> and <span class="math inline">\(\frac{dL}{db}\)</span>) in from <code>L</code>.</p>
<pre class="{python}"><code>dL_da = L.forward_grads[a]
dL_db = L.forward_grads[b]</code></pre>
<p>We’ll also be able to compute operations with non-AutogradValue numbers, but obviously won’t be able to compute derivaitives with respect to these values.</p>
<pre class="{python}"><code>s = 4
L = s * a
dL_da = L.forward_grads[a] # Will work because a is an ForwardValue
dL_ds = L.forward_grads[s] # Will give an error because s is not an ForwardValue</code></pre>
<p>Now that we’ve seen what our final product will look like, let’s define our <code>ForwardValue</code> class.</p>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ForwardValue:</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Base class for automatic differentiation operations. Represents variable delcaration.</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Subclasses will overwrite func and grads to define new operations.</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Properties:</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="co">        parent_values    (list): A list of raw values of each input (as floats)</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="co">        value   (float): The value of the result of this operation</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="co">        forward_grads (dict): A dictionary mapping inputs to gradients</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, <span class="op">*</span>args):</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.parent_values <span class="op">=</span> [arg.value <span class="cf">if</span> <span class="bu">isinstance</span>(arg, ForwardValue) <span class="cf">else</span> arg <span class="cf">for</span> arg <span class="kw">in</span> args]</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.value <span class="op">=</span> <span class="va">self</span>.forward_pass(args)</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.forward_grads.keys()) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.forward_grads <span class="op">=</span> {<span class="va">self</span>: <span class="dv">1</span>}</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> func(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>        <span class="co">'''</span></span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a><span class="co">        Compute the value of the operation given the inputs.</span></span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a><span class="co">        For declaring a variable, this is just the identity function (return the input).</span></span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a><span class="co">            input (float): The input to the operation</span></span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a><span class="co">            value (float): The result of the operation</span></span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a><span class="co">        '''</span></span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">input</span></span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> grads(<span class="va">self</span>, <span class="op">*</span>args):</span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>        <span class="co">'''</span></span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a><span class="co">        Compute the derivative of the operation with respect to each input.</span></span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a><span class="co">        In the base case the derivative of the identity function is just 1. (da/da = 1).</span></span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a><span class="co">            input (float): The input to the operation</span></span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a><span class="co">            grads (tuple): The derivative of the operation with respect to each input</span></span>
<span id="cb34-41"><a href="#cb34-41" aria-hidden="true" tabindex="-1"></a><span class="co">                            Here there is only a single input, so we return a length-1 tuple.</span></span>
<span id="cb34-42"><a href="#cb34-42" aria-hidden="true" tabindex="-1"></a><span class="co">        '''</span></span>
<span id="cb34-43"><a href="#cb34-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (<span class="dv">1</span>,)</span>
<span id="cb34-44"><a href="#cb34-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-45"><a href="#cb34-45" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward_pass(<span class="va">self</span>, args):</span>
<span id="cb34-46"><a href="#cb34-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calls func to compute the value of this operation </span></span>
<span id="cb34-47"><a href="#cb34-47" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.forward_grads <span class="op">=</span> {}</span>
<span id="cb34-48"><a href="#cb34-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.func(<span class="op">*</span><span class="va">self</span>.parent_values)</span>
<span id="cb34-49"><a href="#cb34-49" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-50"><a href="#cb34-50" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__repr__</span>(<span class="va">self</span>):</span>
<span id="cb34-51"><a href="#cb34-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Python magic function for string representation.</span></span>
<span id="cb34-52"><a href="#cb34-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">str</span>(<span class="va">self</span>.value)</span>
<span id="cb34-53"><a href="#cb34-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-54"><a href="#cb34-54" aria-hidden="true" tabindex="-1"></a> </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that in the base case, when we declare a variable the derivative with respect to itself is just 1 (<span class="math inline">\(\frac{da}{da}=1\)</span>)</p>
<pre class="{python}"><code>da_da = a.forward_grads[a] # Will be 1</code></pre>
<p>Now that we’ve defined the framework for an operation that can be used in automatic differentiation, we need to define some actual useful operations by subclassing <code>ForwardValue</code></p>
<section id="q7" class="level4">
<h4 class="anchored" data-anchor-id="q7">Q7</h4>
<p>Fill out the <code>func</code> and <code>grads</code> methods of each subclass below. Recall that <code>func</code> should always return the result of the operation and <code>grads</code> should always return a <code>tuple</code> of the derivative with respect to each input.</p>
<p><em><strong>Hint:</strong> Look at the <code>_add</code> and <code>_neg</code> examples as a template!</em></p>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> _add(ForwardValue):</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Addition operator (a + b)</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> func(<span class="va">self</span>, a, b):</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> a <span class="op">+</span> b</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> grads(<span class="va">self</span>, a, b):</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">1.</span>, <span class="fl">1.</span></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> _neg(ForwardValue):</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Negation operator (-a)</span></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> func(<span class="va">self</span>, a):</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="op">-</span>a</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> grads(<span class="va">self</span>, a):</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (<span class="op">-</span><span class="fl">1.</span>,)</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> _sub(ForwardValue):</span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Subtraction operator (a - b)</span></span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> func(<span class="va">self</span>, a, b):</span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> a <span class="op">-</span> b</span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> grads(<span class="va">self</span>, a, b):</span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">1.</span>, <span class="op">-</span><span class="fl">1.</span></span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> _mul(ForwardValue):</span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Multiplication operator (a * b)</span></span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> func(<span class="va">self</span>, a, b):</span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> a <span class="op">*</span> b</span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> grads(<span class="va">self</span>, a, b):</span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> b, a</span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-33"><a href="#cb36-33" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> _div(ForwardValue):</span>
<span id="cb36-34"><a href="#cb36-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Division operator (a / b)</span></span>
<span id="cb36-35"><a href="#cb36-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> func(<span class="va">self</span>, a, b):</span>
<span id="cb36-36"><a href="#cb36-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> a <span class="op">/</span> b</span>
<span id="cb36-37"><a href="#cb36-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-38"><a href="#cb36-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> grads(<span class="va">self</span>, a, b):</span>
<span id="cb36-39"><a href="#cb36-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> b, <span class="op">-</span>a <span class="op">/</span> (b <span class="op">*</span> b)</span>
<span id="cb36-40"><a href="#cb36-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-41"><a href="#cb36-41" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> _exp(ForwardValue):</span>
<span id="cb36-42"><a href="#cb36-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Exponent operator (e^a, or exp(a))</span></span>
<span id="cb36-43"><a href="#cb36-43" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> func(<span class="va">self</span>, a):</span>
<span id="cb36-44"><a href="#cb36-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> math.exp(a)</span>
<span id="cb36-45"><a href="#cb36-45" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-46"><a href="#cb36-46" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> grads(<span class="va">self</span>, a):</span>
<span id="cb36-47"><a href="#cb36-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (math.exp(a),)</span>
<span id="cb36-48"><a href="#cb36-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-49"><a href="#cb36-49" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> _log(ForwardValue):</span>
<span id="cb36-50"><a href="#cb36-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (Natural) log operator (log(a))</span></span>
<span id="cb36-51"><a href="#cb36-51" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> func(<span class="va">self</span>, a):</span>
<span id="cb36-52"><a href="#cb36-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> math.log(a)</span>
<span id="cb36-53"><a href="#cb36-53" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-54"><a href="#cb36-54" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> grads(<span class="va">self</span>, a):</span>
<span id="cb36-55"><a href="#cb36-55" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (<span class="dv">1</span> <span class="op">/</span> a,)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Below, we’ll define our basic functions and operators in terms of the operator classes we just wrote.</p>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> exp(a):</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> _exp(a) <span class="cf">if</span> <span class="bu">isinstance</span>(a, ForwardValue) <span class="cf">else</span> math.exp(a)</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log(a):</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> _log(a) <span class="cf">if</span> <span class="bu">isinstance</span>(a, ForwardValue) <span class="cf">else</span> math.log(a)</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: Remember that above we defined a class for each type of operation</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="co"># so in this code we are overriding the basic operators for AutogradValue</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a><span class="co"># such that they construct a new object of the class corresponding to the</span></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a><span class="co"># given operation and return it. </span></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a><span class="co"># (You don't need to everything that's happening here to do the HW)</span></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>ForwardValue.exp <span class="op">=</span> <span class="kw">lambda</span> a: _exp(a)</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>ForwardValue.log <span class="op">=</span> <span class="kw">lambda</span> a: _log(a)</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>ForwardValue.<span class="fu">__add__</span> <span class="op">=</span> <span class="kw">lambda</span> a, b: _add(a, b)</span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a>ForwardValue.<span class="fu">__radd__</span> <span class="op">=</span> <span class="kw">lambda</span> a, b: _add(b, a)</span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a>ForwardValue.<span class="fu">__sub__</span> <span class="op">=</span> <span class="kw">lambda</span> a, b: _sub(a, b)</span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a>ForwardValue.<span class="fu">__rsub__</span> <span class="op">=</span> <span class="kw">lambda</span> a, b: _sub(b, a)</span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a>ForwardValue.<span class="fu">__neg__</span> <span class="op">=</span> <span class="kw">lambda</span> a: _neg(a)</span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a>ForwardValue.<span class="fu">__mul__</span> <span class="op">=</span> <span class="kw">lambda</span> a, b: _mul(a, b)</span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a>ForwardValue.<span class="fu">__rmul__</span> <span class="op">=</span> <span class="kw">lambda</span> a, b: _mul(b, a)</span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a>ForwardValue.<span class="fu">__truediv__</span> <span class="op">=</span> <span class="kw">lambda</span> a, b: _div(a, b)</span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a>ForwardValue.<span class="fu">__rtruediv__</span> <span class="op">=</span> <span class="kw">lambda</span> a, b: _div(b, a)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We should now be able to use our <code>AutogradValue</code> objects as if they are numbers!</p>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> ForwardValue(<span class="dv">5</span>)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> ForwardValue(<span class="dv">2</span>)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>((a <span class="op">+</span> <span class="dv">5</span>) <span class="op">*</span> b)</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(log(b))</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>test_operators(ForwardValue)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>20
0.6931471805599453
Passed!</code></pre>
</div>
</div>
<p>We see now that our <code>forward_pass</code> method needs to update <code>forward_grads</code> (e.g.&nbsp;to compute <span class="math inline">\(\frac{dg}{da}\)</span> and <span class="math inline">\(\frac{dg}{db}\)</span>) using the <code>forward_grads</code> values of its parents (e.g.&nbsp;<span class="math inline">\(\frac{dc}{da}\)</span> and <span class="math inline">\(\frac{dc}{db}\)</span>)</p>
</section>
<section id="q8" class="level4">
<h4 class="anchored" data-anchor-id="q8">Q8</h4>
<p>Update the <code>forward_pass</code> method below for forward-mode automatic differentiation. This method should update the <code>foward_grads</code> property of the operation such that: - <code>foward_grads</code> has an entry for every input that appears in <code>foward_grads</code> of <em>any</em> parent operation. - If an input appears in more than 1 parent, make sure to <em>add</em> the gradients appropritately (if <code>g</code> has parents <code>b</code> and <code>c</code> then <span class="math inline">\(\frac{dg}{da} = \frac{dg}{db}\frac{db}{da} + \frac{dg}{dc}\frac{dc}{da}\)</span> ) - Parents that are not <code>AutogradValue</code> objects are ignored</p>
<p>If our <code>forward_pass</code> method is working correctly, we should have the following behaivior:</p>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define our inputs as ForwardValue objects</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> ForwardValue(<span class="dv">5</span>)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> ForwardValue(<span class="dv">2</span>)</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform operations</span></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> a <span class="op">*</span> b</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> <span class="dv">3</span> <span class="op">*</span> c <span class="op">+</span> a</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a><span class="co"># We should have the following in the forward_grads property of c and d (note that the keys are ForwardValue objects!)</span></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>c.forward_grads <span class="op">=</span> {a: <span class="dv">2</span>, b: <span class="dv">5</span>}  <span class="co"># dc/da and dc/db</span></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>g.forward_grads <span class="op">=</span> {a: <span class="dv">3</span> <span class="op">*</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>, b: <span class="dv">3</span> <span class="op">*</span> <span class="dv">5</span>} <span class="co"># dg/da = dg/dc dc/da + dg/da, dg/db = dg/dc dc/db</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Implement the method below</p>
<div class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward_pass(<span class="va">self</span>, args):</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.forward_grads <span class="op">=</span> {} </span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>    grads <span class="op">=</span> <span class="va">None</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ind, node <span class="kw">in</span> <span class="bu">enumerate</span>(args):</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">hasattr</span>(node, <span class="st">'forward_grads'</span>):</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> grads <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>                grads <span class="op">=</span> <span class="va">self</span>.grads(<span class="op">*</span><span class="va">self</span>.parent_values)</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.forward_grads <span class="op">=</span> {}</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> key, value <span class="kw">in</span> node.forward_grads.items():</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> key <span class="kw">not</span> <span class="kw">in</span> <span class="va">self</span>.forward_grads:</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.forward_grads[key] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.forward_grads[key] <span class="op">+=</span> value <span class="op">*</span> grads[ind]</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Make sure to still return the operation's value</span></span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">self</span>.func(<span class="op">*</span><span class="va">self</span>.parent_values)</span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Overwrite the AutogradValue method so that operators still work</span></span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a>ForwardValue.forward_pass <span class="op">=</span> forward_pass</span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a>test_forward_mode(ForwardValue)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Passed!</code></pre>
</div>
</div>
<p>We can now take derivates of functions!</p>
<div class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> ForwardValue(<span class="dv">5</span>)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> ForwardValue(<span class="dv">2</span>)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> <span class="op">-</span>log(<span class="dv">5</span> <span class="op">*</span>b <span class="op">+</span> a)</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>dL_da <span class="op">=</span> L.forward_grads[a]</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>dL_db <span class="op">=</span> L.forward_grads[b]</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'dL/da = </span><span class="sc">%.3f</span><span class="st">, dL/db = </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> (dL_da, dL_db))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>dL/da = -0.067, dL/db = -0.333</code></pre>
</div>
</div>
<p>We could also implement our own very simple version of Autograd’s <code>grad</code>.</p>
<div class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> grad(f):</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> ad_function(x, <span class="op">*</span>args):</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> ForwardValue(x)</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> f(x, <span class="op">*</span>args)</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output.forward_grads[x]</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ad_function</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function</span></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x):</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x <span class="op">*</span> x</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Use 'grad' to compute the derivative function</span></span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>f_prime <span class="op">=</span> grad(f)</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify that we get the correct answer</span></span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="fl">5.</span></span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'x:</span><span class="ch">\t</span><span class="st">'</span>, x)</span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'f(x):</span><span class="ch">\t</span><span class="st">'</span>, f(x))</span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"f'(x):</span><span class="ch">\t</span><span class="st">"</span>, f_prime(x))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>x:   5.0
f(x):    25.0
f'(x):   10.0</code></pre>
</div>
</div>
<p>In the next homework, we’ll combine our forward-mode AD implementation with our neural network class. Then we’ll look at how to do automatic-differentiation more efficiently with reverse-mode AD.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>