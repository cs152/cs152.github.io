<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.319">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>CS 152: Neural Networks - Lecture 6: Deep neural networks and backpropagation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script type="module" src="../site_libs/quarto-ojs/quarto-ojs-runtime.js"></script>
<link href="../site_libs/quarto-ojs/quarto-ojs.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">CS 152: Neural Networks</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../calendar/calendar.html" rel="" target="">
 <span class="menu-text">Calendar</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/orgs/CS152-Neural-Networks-Fall-2023/repositories" rel="" target="">
 <span class="menu-text">Homeworks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://harveymuddcollege.instructure.com/courses/615/" rel="" target="">
 <span class="menu-text">Canvas</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/orgs/CS152-Neural-Networks-Fall-2023/" rel="" target="">
 <span class="menu-text">Github</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#neural-networks" id="toc-neural-networks" class="nav-link active" data-scroll-target="#neural-networks">Neural networks</a>
  <ul class="collapse">
  <li><a href="#neural-networks-with-matrices" id="toc-neural-networks-with-matrices" class="nav-link" data-scroll-target="#neural-networks-with-matrices">Neural networks with matrices</a></li>
  <li><a href="#benefits-of-neural-networks" id="toc-benefits-of-neural-networks" class="nav-link" data-scroll-target="#benefits-of-neural-networks">Benefits of neural networks</a></li>
  <li><a href="#deep-neural-networks" id="toc-deep-neural-networks" class="nav-link" data-scroll-target="#deep-neural-networks">Deep Neural Networks</a></li>
  <li><a href="#optimizing-neural-networks" id="toc-optimizing-neural-networks" class="nav-link" data-scroll-target="#optimizing-neural-networks">Optimizing neural networks</a></li>
  </ul></li>
  <li><a href="#automatic-differentiation" id="toc-automatic-differentiation" class="nav-link" data-scroll-target="#automatic-differentiation">Automatic Differentiation</a>
  <ul class="collapse">
  <li><a href="#motivation" id="toc-motivation" class="nav-link" data-scroll-target="#motivation">Motivation</a></li>
  <li><a href="#the-chain-rule-revisited" id="toc-the-chain-rule-revisited" class="nav-link" data-scroll-target="#the-chain-rule-revisited">The chain rule revisited</a></li>
  <li><a href="#composing-many-operations" id="toc-composing-many-operations" class="nav-link" data-scroll-target="#composing-many-operations">Composing many operations</a></li>
  <li><a href="#forward-and-reverse-mode-automatic-differentiation" id="toc-forward-and-reverse-mode-automatic-differentiation" class="nav-link" data-scroll-target="#forward-and-reverse-mode-automatic-differentiation">Forward and reverse mode automatic differentiation</a></li>
  <li><a href="#automatic-differentiation-with-multiple-inputs" id="toc-automatic-differentiation-with-multiple-inputs" class="nav-link" data-scroll-target="#automatic-differentiation-with-multiple-inputs">Automatic differentiation with multiple inputs</a></li>
  <li><a href="#reusing-values" id="toc-reusing-values" class="nav-link" data-scroll-target="#reusing-values">Reusing values</a></li>
  <li><a href="#partial-and-total-derivatives" id="toc-partial-and-total-derivatives" class="nav-link" data-scroll-target="#partial-and-total-derivatives">Partial and total derivatives</a></li>
  <li><a href="#implementing-reverse-mode-automatic-differentiation" id="toc-implementing-reverse-mode-automatic-differentiation" class="nav-link" data-scroll-target="#implementing-reverse-mode-automatic-differentiation">Implementing reverse-mode automatic differentiation</a></li>
  <li><a href="#computational-graphs-of-vectors" id="toc-computational-graphs-of-vectors" class="nav-link" data-scroll-target="#computational-graphs-of-vectors">Computational graphs of vectors</a></li>
  <li><a href="#automatic-differentiation-for-linear-regression" id="toc-automatic-differentiation-for-linear-regression" class="nav-link" data-scroll-target="#automatic-differentiation-for-linear-regression">Automatic differentiation for linear regression</a></li>
  <li><a href="#vector-valued-functions" id="toc-vector-valued-functions" class="nav-link" data-scroll-target="#vector-valued-functions">Vector-valued functions</a></li>
  <li><a href="#jacobians" id="toc-jacobians" class="nav-link" data-scroll-target="#jacobians">Jacobians</a></li>
  <li><a href="#vector-jacobian-products" id="toc-vector-jacobian-products" class="nav-link" data-scroll-target="#vector-jacobian-products">Vector-Jacobian products</a></li>
  <li><a href="#vjps-for-element-wise-operations" id="toc-vjps-for-element-wise-operations" class="nav-link" data-scroll-target="#vjps-for-element-wise-operations">VJPs for element-wise operations</a></li>
  <li><a href="#vjps-for-matrices" id="toc-vjps-for-matrices" class="nav-link" data-scroll-target="#vjps-for-matrices">VJPs for matrices</a></li>
  <li><a href="#forward-mode-ad-with-vectors" id="toc-forward-mode-ad-with-vectors" class="nav-link" data-scroll-target="#forward-mode-ad-with-vectors">Forward mode AD with vectors</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Lecture 6: Deep neural networks and backpropagation</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="cell">
<div class="sourceCode cell-code hidden" id="cb1" data-startfrom="1" data-source-offset="-0"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>Plot <span class="op">=</span> <span class="im">import</span>(<span class="st">"https://esm.sh/@observablehq/plot"</span>) </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>d3 <span class="op">=</span> <span class="pp">require</span>(<span class="st">"d3@7"</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>topojson <span class="op">=</span> <span class="pp">require</span>(<span class="st">"topojson"</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>MathJax <span class="op">=</span> <span class="pp">require</span>(<span class="st">"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-svg.min.js"</span>)<span class="op">.</span><span class="fu">catch</span>(() <span class="kw">=&gt;</span> <span class="bu">window</span><span class="op">.</span><span class="at">MathJax</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>tf <span class="op">=</span> <span class="pp">require</span>(<span class="st">"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js"</span>)<span class="op">.</span><span class="fu">catch</span>(() <span class="kw">=&gt;</span> <span class="bu">window</span><span class="op">.</span><span class="at">tf</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>THREE <span class="op">=</span> {</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  <span class="kw">const</span> THREE <span class="op">=</span> <span class="bu">window</span><span class="op">.</span><span class="at">THREE</span> <span class="op">=</span> <span class="cf">await</span> <span class="pp">require</span>(<span class="st">"three@0.130.0/build/three.min.js"</span>)<span class="op">;</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">await</span> <span class="pp">require</span>(<span class="st">"three@0.130.0/examples/js/controls/OrbitControls.js"</span>)<span class="op">.</span><span class="fu">catch</span>(() <span class="kw">=&gt;</span> {})<span class="op">;</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">await</span> <span class="pp">require</span>(<span class="st">"three@0.130.0/examples/js/loaders/SVGLoader.js"</span>)<span class="op">.</span><span class="fu">catch</span>(() <span class="kw">=&gt;</span> {})<span class="op">;</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> THREE<span class="op">;</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">sample</span>(f<span class="op">,</span> start<span class="op">,</span> end<span class="op">,</span> nsamples<span class="op">=</span><span class="dv">100</span>) {</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> arr <span class="op">=</span> [<span class="op">...</span><span class="bu">Array</span>(nsamples)<span class="op">.</span><span class="fu">keys</span>()]</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> dist <span class="op">=</span> end <span class="op">-</span> start</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>  <span class="kw">function</span> <span class="fu">arrmap</span>(ind) {</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">const</span> x <span class="op">=</span> (ind <span class="op">*</span> dist) <span class="op">/</span> nsamples <span class="op">+</span> start<span class="op">;</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [x<span class="op">,</span> <span class="fu">f</span>(x)]<span class="op">;</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> arr<span class="op">.</span><span class="fu">map</span>(arrmap)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">sigmoid</span>(x){</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> <span class="bu">Math</span><span class="op">.</span><span class="fu">exp</span>(<span class="op">-</span>x))<span class="op">;</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">sum</span>(x) {</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> s <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (<span class="kw">let</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> x<span class="op">.</span><span class="at">length</span><span class="op">;</span> i<span class="op">++</span> ) {</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    s <span class="op">+=</span> x[i]<span class="op">;</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> s<span class="op">;</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">mean</span>(x) {</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> s <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (<span class="kw">let</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> x<span class="op">.</span><span class="at">length</span><span class="op">;</span> i<span class="op">++</span> ) {</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    s <span class="op">+=</span> x[i]<span class="op">;</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> s <span class="op">/</span> x<span class="op">.</span><span class="at">length</span><span class="op">;</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">cross_ent</span>(x<span class="op">,</span> y) {</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> y <span class="op">?</span> <span class="op">-</span><span class="bu">Math</span><span class="op">.</span><span class="fu">log</span>(<span class="fu">sigmoid</span>(x)) <span class="op">:</span> <span class="op">-</span><span class="bu">Math</span><span class="op">.</span><span class="fu">log</span>(<span class="fu">sigmoid</span>(<span class="op">-</span>x))<span class="op">;</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">se</span>(x<span class="op">,</span> y) {</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> (x <span class="op">-</span> y) <span class="op">*</span> (x <span class="op">-</span> y)<span class="op">;</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">shuffle</span>(array) {</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> currentIndex <span class="op">=</span> array<span class="op">.</span><span class="at">length</span><span class="op">,</span>  randomIndex<span class="op">;</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>  <span class="co">// While there remain elements to shuffle.</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>  <span class="cf">while</span> (currentIndex <span class="op">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Pick a remaining element.</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>    randomIndex <span class="op">=</span> <span class="bu">Math</span><span class="op">.</span><span class="fu">floor</span>(<span class="bu">Math</span><span class="op">.</span><span class="fu">random</span>() <span class="op">*</span> currentIndex)<span class="op">;</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>    currentIndex<span class="op">--;</span></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>    <span class="co">// And swap it with the current element.</span></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>    [array[currentIndex]<span class="op">,</span> array[randomIndex]] <span class="op">=</span> [</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>      array[randomIndex]<span class="op">,</span> array[currentIndex]]<span class="op">;</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> array<span class="op">;</span></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">acc</span>(x<span class="op">,</span> y) {</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="bu">Number</span>(y <span class="op">==</span> (x  <span class="op">&gt;</span> <span class="dv">0</span>))<span class="op">;</span></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">grid_func</span>(f<span class="op">,</span> width<span class="op">,</span> height<span class="op">,</span> x1<span class="op">,</span> y1<span class="op">,</span> x2<span class="op">,</span> y2) {</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> values <span class="op">=</span> <span class="kw">new</span> <span class="bu">Array</span>(width <span class="op">*</span> height)<span class="op">;</span></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>  <span class="kw">const</span> xstride <span class="op">=</span> (x2 <span class="op">-</span> x1) <span class="op">/</span> width<span class="op">;</span></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>  <span class="kw">const</span> ystride <span class="op">=</span> (y2 <span class="op">-</span> y1) <span class="op">/</span> height<span class="op">;</span></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> y <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> x <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> ind <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (<span class="kw">let</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> height<span class="op">;</span> i<span class="op">++</span> ) {</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (<span class="kw">let</span> j <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> j <span class="op">&lt;</span> width<span class="op">;</span> j<span class="op">++,</span> ind<span class="op">++</span>) {</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>      x <span class="op">=</span> x1 <span class="op">+</span> j <span class="op">*</span> xstride<span class="op">;</span></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>      y <span class="op">=</span> y1 <span class="op">+</span> i <span class="op">*</span> ystride<span class="op">;</span></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>      values[ind] <span class="op">=</span> <span class="fu">f</span>(x<span class="op">,</span> y)<span class="op">;</span></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> {<span class="dt">width</span><span class="op">:</span> width<span class="op">,</span> <span class="dt">height</span><span class="op">:</span> height<span class="op">,</span> <span class="dt">x1</span><span class="op">:</span> x1<span class="op">,</span> <span class="dt">y1</span><span class="op">:</span> y1<span class="op">,</span> <span class="dt">x2</span><span class="op">:</span> x2<span class="op">,</span> <span class="dt">y2</span><span class="op">:</span> y2<span class="op">,</span> <span class="dt">values</span><span class="op">:</span> values}<span class="op">;</span></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">get_accessors</span>(keys<span class="op">,</span> byindex<span class="op">=</span><span class="kw">false</span>) {</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> isString <span class="op">=</span> value <span class="kw">=&gt;</span> <span class="kw">typeof</span> value <span class="op">===</span> <span class="st">'string'</span><span class="op">;</span></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> index <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> indexmap <span class="op">=</span> {}<span class="op">;</span></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> accessors <span class="op">=</span> []<span class="op">;</span></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (<span class="kw">let</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> keys<span class="op">.</span><span class="at">length</span><span class="op">;</span> i<span class="op">++</span>){</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> k <span class="op">=</span> keys[i]<span class="op">;</span></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="bu">Array</span><span class="op">.</span><span class="fu">isArray</span>(k)) {</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>      <span class="kw">let</span> access <span class="op">=</span> <span class="fu">isString</span>(k[<span class="dv">0</span>]) <span class="op">?</span> (x <span class="kw">=&gt;</span> x[k[<span class="dv">0</span>]]) <span class="op">:</span> k[<span class="dv">0</span>]<span class="op">;</span></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> (byindex) {</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (<span class="fu">isString</span>(k[<span class="dv">0</span>]) <span class="op">&amp;&amp;</span> <span class="op">!</span>(k[<span class="dv">0</span>] <span class="kw">in</span> indexmap)) {</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>          indexmap[k[<span class="dv">0</span>]] <span class="op">=</span> index<span class="op">;</span></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>          index<span class="op">++;</span></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>        <span class="kw">let</span> accessindex <span class="op">=</span> indexmap[k[<span class="dv">0</span>]]<span class="op">;</span></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>        access <span class="op">=</span> x <span class="kw">=&gt;</span> x[accessindex]<span class="op">;</span></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>        <span class="kw">let</span> <span class="bu">process</span> <span class="op">=</span> k[<span class="dv">1</span>]<span class="op">;</span></span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>        <span class="kw">let</span> final_access <span class="op">=</span> x <span class="kw">=&gt;</span> <span class="bu">process</span>(<span class="fu">access</span>(x))<span class="op">;</span></span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>        accessors<span class="op">.</span><span class="fu">push</span>(final_access)<span class="op">;</span></span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>      }</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>      <span class="cf">else</span> {</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>        <span class="kw">let</span> <span class="bu">process</span> <span class="op">=</span> k[<span class="dv">1</span>]<span class="op">;</span></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>        <span class="kw">let</span> final_access <span class="op">=</span> x <span class="kw">=&gt;</span> <span class="bu">process</span>(<span class="fu">access</span>(x))<span class="op">;</span></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>        accessors<span class="op">.</span><span class="fu">push</span>(final_access)<span class="op">;</span></span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>      }</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span> {</span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>      <span class="kw">let</span> access <span class="op">=</span> <span class="fu">isString</span>(k) <span class="op">?</span> (x <span class="kw">=&gt;</span> x[k]) <span class="op">:</span> k<span class="op">;</span></span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> (byindex) { </span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (<span class="fu">isString</span>(k) <span class="op">&amp;&amp;</span> <span class="op">!</span>(k <span class="kw">in</span> indexmap)) {</span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a>          indexmap[k] <span class="op">=</span> index<span class="op">;</span></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>          index<span class="op">++;</span></span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>        <span class="kw">let</span> accessindex <span class="op">=</span> indexmap[k]<span class="op">;</span></span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>        access <span class="op">=</span> x <span class="kw">=&gt;</span> x[accessindex]<span class="op">;</span></span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>      }</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>      accessors<span class="op">.</span><span class="fu">push</span>(access)<span class="op">;</span> </span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> accessors<span class="op">;</span></span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">predict</span>(obs<span class="op">,</span> weights<span class="op">,</span> keys<span class="op">=</span>[<span class="st">"0"</span><span class="op">,</span> <span class="st">"1"</span><span class="op">,</span> <span class="st">"2"</span><span class="op">,</span> <span class="st">"3"</span>]<span class="op">,</span> byindex<span class="op">=</span><span class="kw">false</span>) {</span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> isString <span class="op">=</span> value <span class="kw">=&gt;</span> <span class="kw">typeof</span> value <span class="op">===</span> <span class="st">'string'</span><span class="op">;</span></span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> accessors <span class="op">=</span> <span class="fu">get_accessors</span>(keys<span class="op">,</span> byindex)<span class="op">;</span></span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> output <span class="op">=</span> weights[<span class="dv">0</span>]<span class="op">;</span></span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> wi <span class="op">=</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (<span class="kw">let</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> (i <span class="op">&lt;</span> keys<span class="op">.</span><span class="at">length</span>) <span class="op">&amp;&amp;</span> (wi <span class="op">&lt;</span> weights<span class="op">.</span><span class="at">length</span>)<span class="op">;</span> i<span class="op">++,</span> wi<span class="op">++</span>){</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>    output <span class="op">+=</span> weights[wi] <span class="op">*</span> accessors[i](obs)<span class="op">;</span></span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> output<span class="op">;</span></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">mean_loss</span>(f<span class="op">,</span> data<span class="op">,</span> weights<span class="op">,</span> keys<span class="op">,</span> label<span class="op">,</span> l2<span class="op">=</span><span class="dv">0</span>) {</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> reg <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (l2 <span class="op">&gt;</span> <span class="dv">0</span>){</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (<span class="kw">let</span> i <span class="op">=</span> <span class="dv">1</span><span class="op">;</span> i <span class="op">&lt;</span> weights<span class="op">.</span><span class="at">length</span><span class="op">;</span> i<span class="op">++</span>) {</span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>      reg <span class="op">+=</span> weights[i] <span class="op">*</span> weights[i]<span class="op">;</span></span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a>  <span class="kw">const</span> isString <span class="op">=</span> value <span class="kw">=&gt;</span> <span class="kw">typeof</span> value <span class="op">===</span> <span class="st">'string'</span><span class="op">;</span></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a>  <span class="kw">const</span> get_label <span class="op">=</span> <span class="fu">isString</span>(label) <span class="op">?</span> (x <span class="kw">=&gt;</span> x[label]) <span class="op">:</span> label<span class="op">;</span></span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="fu">mean</span>(data<span class="op">.</span><span class="fu">map</span>(x <span class="kw">=&gt;</span> <span class="fu">f</span>(<span class="fu">predict</span>(x<span class="op">,</span> weights<span class="op">,</span> keys)<span class="op">,</span> <span class="fu">get_label</span>(x)))) <span class="op">+</span> l2 <span class="op">*</span> reg<span class="op">;</span></span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">get_domains</span>(data<span class="op">,</span> accessors<span class="op">,</span> margin<span class="op">=</span><span class="fl">0.1</span>) {</span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> domains <span class="op">=</span> []<span class="op">;</span></span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (<span class="kw">let</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> accessors<span class="op">.</span><span class="at">length</span><span class="op">;</span> i<span class="op">++</span>){</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> xdomain <span class="op">=</span> d3<span class="op">.</span><span class="fu">extent</span>(data<span class="op">,</span> accessors[i])<span class="op">;</span></span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> xdsize <span class="op">=</span> (xdomain[<span class="dv">1</span>] <span class="op">-</span> xdomain[<span class="dv">0</span>])<span class="op">;</span></span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> xmin <span class="op">=</span> xdomain[<span class="dv">0</span>] <span class="op">-</span> xdsize <span class="op">*</span> margin<span class="op">;</span></span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> xmax <span class="op">=</span> xdomain[<span class="dv">1</span>] <span class="op">+</span> xdsize <span class="op">*</span> margin<span class="op">;</span></span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>    domains<span class="op">.</span><span class="fu">push</span>([xmin<span class="op">,</span> xmax])<span class="op">;</span></span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> domains<span class="op">;</span></span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">logisticPlot2d</span>(data<span class="op">,</span> weights<span class="op">,</span> keys<span class="op">,</span> label<span class="op">,</span> interval<span class="op">=</span><span class="fl">0.05</span>) {</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a>  <span class="kw">const</span> accuracy <span class="op">=</span> <span class="fu">mean_loss</span>(acc<span class="op">,</span> data<span class="op">,</span> weights<span class="op">,</span> keys<span class="op">,</span> label)<span class="op">;</span></span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> isString <span class="op">=</span> value <span class="kw">=&gt;</span> <span class="kw">typeof</span> value <span class="op">===</span> <span class="st">'string'</span><span class="op">;</span></span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> accessors <span class="op">=</span> <span class="fu">get_accessors</span>(keys)<span class="op">;</span></span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> index_accessors <span class="op">=</span> <span class="fu">get_accessors</span>(keys<span class="op">,</span> <span class="kw">true</span>)<span class="op">;</span></span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> domains <span class="op">=</span> <span class="fu">get_domains</span>(data<span class="op">,</span> accessors)<span class="op">;</span></span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a>  <span class="kw">const</span> get_label <span class="op">=</span> <span class="fu">isString</span>(label) <span class="op">?</span> (x <span class="kw">=&gt;</span> x[label]) <span class="op">:</span> label<span class="op">;</span></span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> Plot<span class="op">.</span><span class="fu">plot</span>({</span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a>    <span class="dt">x</span><span class="op">:</span> {<span class="dt">tickSpacing</span><span class="op">:</span> <span class="dv">80</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"x"</span>}<span class="op">,</span></span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a>    <span class="dt">y</span><span class="op">:</span> {<span class="dt">tickSpacing</span><span class="op">:</span> <span class="dv">80</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"y"</span>}<span class="op">,</span></span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>    <span class="dt">title</span><span class="op">:</span> <span class="st">"Accuracy: "</span> <span class="op">+</span> accuracy<span class="op">.</span><span class="fu">toFixed</span>(<span class="dv">3</span>)<span class="op">,</span></span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a>    <span class="dt">color</span><span class="op">:</span> {<span class="dt">type</span><span class="op">:</span> <span class="st">"linear"</span><span class="op">,</span> <span class="dt">legend</span><span class="op">:</span> <span class="kw">true</span><span class="op">,</span> <span class="dt">scheme</span><span class="op">:</span> <span class="st">"BuRd"</span><span class="op">,</span> <span class="dt">domain</span><span class="op">:</span> [<span class="op">-</span><span class="fl">0.5</span><span class="op">,</span> <span class="fl">1.5</span>]}<span class="op">,</span></span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>    <span class="dt">marks</span><span class="op">:</span> [</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a>      Plot<span class="op">.</span><span class="fu">contour</span>({</span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a>        <span class="dt">fill</span><span class="op">:</span> (x<span class="op">,</span> y) <span class="kw">=&gt;</span> <span class="fu">sigmoid</span>(<span class="fu">predict</span>([x<span class="op">,</span> y]<span class="op">,</span> weights<span class="op">,</span> index_accessors))<span class="op">,</span></span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a>        <span class="dt">x1</span><span class="op">:</span> domains[<span class="dv">0</span>][<span class="dv">0</span>]<span class="op">,</span> <span class="dt">y1</span><span class="op">:</span> domains[<span class="dv">1</span>][<span class="dv">0</span>]<span class="op">,</span> <span class="dt">x2</span><span class="op">:</span> domains[<span class="dv">0</span>][<span class="dv">1</span>]<span class="op">,</span> <span class="dt">y2</span><span class="op">:</span> domains[<span class="dv">1</span>][<span class="dv">1</span>]<span class="op">,</span> <span class="dt">interval</span><span class="op">:</span> interval<span class="op">,</span></span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>      })<span class="op">,</span></span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>      Plot<span class="op">.</span><span class="fu">dot</span>(data<span class="op">,</span> {<span class="dt">x</span><span class="op">:</span> accessors[<span class="dv">0</span>]<span class="op">,</span> <span class="dt">y</span><span class="op">:</span> accessors[<span class="dv">1</span>]<span class="op">,</span> <span class="dt">stroke</span><span class="op">:</span> x<span class="kw">=&gt;</span> (<span class="fu">get_label</span>(x) <span class="op">?</span> <span class="fl">1.35</span> <span class="op">:</span> <span class="op">-</span><span class="fl">0.35</span>)})</span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a>  })<span class="op">;</span></span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">logisticLossPlot2d</span>(data<span class="op">,</span> weights<span class="op">,</span> keys<span class="op">,</span> label) {</span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a>  <span class="kw">const</span> loss <span class="op">=</span> <span class="fu">mean_loss</span>(cross_ent<span class="op">,</span> data<span class="op">,</span> weights<span class="op">,</span> keys<span class="op">,</span> label)<span class="op">;</span></span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> isString <span class="op">=</span> value <span class="kw">=&gt;</span> <span class="kw">typeof</span> value <span class="op">===</span> <span class="st">'string'</span><span class="op">;</span></span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> accessors <span class="op">=</span> <span class="fu">get_accessors</span>(keys)<span class="op">;</span></span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> index_accessors <span class="op">=</span> <span class="fu">get_accessors</span>(keys<span class="op">,</span> <span class="kw">true</span>)<span class="op">;</span></span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> domains <span class="op">=</span> <span class="fu">get_domains</span>(data<span class="op">,</span> accessors)<span class="op">;</span></span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a>  <span class="kw">const</span> get_label <span class="op">=</span> <span class="fu">isString</span>(label) <span class="op">?</span> (x <span class="kw">=&gt;</span> x[label]) <span class="op">:</span> label<span class="op">;</span></span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> Plot<span class="op">.</span><span class="fu">plot</span>({</span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a>    <span class="dt">x</span><span class="op">:</span> {<span class="dt">tickSpacing</span><span class="op">:</span> <span class="dv">80</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"x"</span>}<span class="op">,</span></span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a>    <span class="dt">y</span><span class="op">:</span> {<span class="dt">tickSpacing</span><span class="op">:</span> <span class="dv">80</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"y"</span>}<span class="op">,</span></span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a>    <span class="dt">title</span><span class="op">:</span> <span class="st">"Loss: "</span> <span class="op">+</span> loss<span class="op">.</span><span class="fu">toFixed</span>(<span class="dv">3</span>)<span class="op">,</span></span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a>    <span class="dt">color</span><span class="op">:</span> {<span class="dt">type</span><span class="op">:</span> <span class="st">"linear"</span><span class="op">,</span> <span class="dt">legend</span><span class="op">:</span> <span class="kw">true</span><span class="op">,</span> <span class="dt">scheme</span><span class="op">:</span> <span class="st">"BuRd"</span><span class="op">,</span> <span class="dt">domain</span><span class="op">:</span> [<span class="dv">0</span><span class="op">,</span> <span class="dv">5</span>]}<span class="op">,</span></span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a>    <span class="dt">marks</span><span class="op">:</span> [</span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a>      Plot<span class="op">.</span><span class="fu">contour</span>({</span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a>        <span class="dt">value</span><span class="op">:</span> (x<span class="op">,</span> y) <span class="kw">=&gt;</span> <span class="fu">predict</span>([x<span class="op">,</span> y]<span class="op">,</span> weights<span class="op">,</span> index_accessors)<span class="op">,</span></span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a>        <span class="dt">fillOpacity</span><span class="op">:</span> <span class="fl">0.2</span><span class="op">,</span></span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a>        <span class="dt">stroke</span><span class="op">:</span> <span class="st">"black"</span><span class="op">,</span> <span class="dt">x1</span><span class="op">:</span> domains[<span class="dv">0</span>][<span class="dv">0</span>]<span class="op">,</span> <span class="dt">y1</span><span class="op">:</span> domains[<span class="dv">1</span>][<span class="dv">0</span>]<span class="op">,</span> <span class="dt">x2</span><span class="op">:</span> domains[<span class="dv">0</span>][<span class="dv">1</span>]<span class="op">,</span> <span class="dt">y2</span><span class="op">:</span> domains[<span class="dv">1</span>][<span class="dv">1</span>]<span class="op">,</span></span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a>        <span class="dt">thresholds</span><span class="op">:</span> [<span class="op">-</span><span class="fl">1e6</span><span class="op">,</span>  <span class="dv">0</span><span class="op">,</span> <span class="fl">0.00001</span>]</span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a>      })<span class="op">,</span></span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a>      Plot<span class="op">.</span><span class="fu">dot</span>(data<span class="op">,</span> {<span class="dt">x</span><span class="op">:</span> accessors[<span class="dv">0</span>]<span class="op">,</span> <span class="dt">y</span><span class="op">:</span> accessors[<span class="dv">1</span>]<span class="op">,</span> <span class="dt">stroke</span><span class="op">:</span> x<span class="kw">=&gt;</span> <span class="fu">cross_ent</span>(<span class="fu">predict</span>(x<span class="op">,</span> weights<span class="op">,</span> keys)<span class="op">,</span> <span class="fu">get_label</span>(x))<span class="op">,</span> </span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a>                      <span class="dt">strokeOpacity</span><span class="op">:</span> <span class="fl">0.5</span> })</span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a>  })<span class="op">;</span></span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">lossPlot2d</span>(f<span class="op">,</span> data<span class="op">,</span> keys<span class="op">,</span> label<span class="op">,</span> l2<span class="op">=</span><span class="dv">0</span><span class="op">,</span> res<span class="op">=</span><span class="dv">100</span><span class="op">,</span> x1<span class="op">=-</span><span class="dv">40</span><span class="op">,</span> y1<span class="op">=-</span><span class="fl">0.015</span><span class="op">,</span> x2<span class="op">=</span><span class="dv">40</span><span class="op">,</span>  y2<span class="op">=</span><span class="fl">0.015</span><span class="op">,</span> vmax<span class="op">=</span><span class="dv">50</span><span class="op">,</span> nlines<span class="op">=</span><span class="dv">25</span><span class="op">,</span> ctype<span class="op">=</span><span class="st">"sqrt"</span><span class="op">,</span> scale<span class="op">=</span>(x <span class="kw">=&gt;</span> x)) {</span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> grid <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a>  <span class="kw">function</span> <span class="fu">lossFunc</span>(w<span class="op">,</span> b) {</span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fu">scale</span>(<span class="fu">mean_loss</span>(f<span class="op">,</span> data<span class="op">,</span> [w<span class="op">,</span> b]<span class="op">,</span> keys<span class="op">,</span> label<span class="op">,</span> l2))<span class="op">;</span></span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>  grid <span class="op">=</span> <span class="fu">grid_func</span>(lossFunc<span class="op">,</span></span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a>                 res<span class="op">,</span> res<span class="op">,</span> x1<span class="op">,</span> y1<span class="op">,</span> x2<span class="op">,</span> y2</span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a>                )<span class="op">;</span></span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a>  <span class="kw">function</span> <span class="fu">plot2d</span>(weights) {</span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> w <span class="op">=</span> weights<span class="op">;</span></span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="op">!</span>(<span class="bu">Array</span><span class="op">.</span><span class="fu">isArray</span>(w[<span class="dv">0</span>]))){</span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a>      w <span class="op">=</span> [w]<span class="op">;</span></span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a>    <span class="kw">var</span> arrows <span class="op">=</span> w<span class="op">.</span><span class="fu">slice</span>(<span class="dv">0</span><span class="op">,</span> w<span class="op">.</span><span class="at">length</span> <span class="op">-</span> <span class="dv">1</span>)<span class="op">.</span><span class="fu">map</span>(<span class="kw">function</span>(e<span class="op">,</span> i) {</span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a>      <span class="cf">return</span> e<span class="op">.</span><span class="fu">concat</span>(w[i<span class="op">+</span><span class="dv">1</span>])<span class="op">;</span></span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a>    })<span class="op">;</span></span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> interval<span class="op">=</span> vmax <span class="op">/</span> nlines<span class="op">;</span> </span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> thresholds <span class="op">=</span> []<span class="op">;</span></span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (<span class="kw">let</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> nlines<span class="op">;</span> i<span class="op">++</span>) {</span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a>      thresholds<span class="op">.</span><span class="fu">push</span>(i <span class="op">*</span> interval)<span class="op">;</span></span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> loss <span class="op">=</span> <span class="fu">mean_loss</span>(f<span class="op">,</span> data<span class="op">,</span> w[w<span class="op">.</span><span class="at">length</span> <span class="op">-</span> <span class="dv">1</span>]<span class="op">,</span> keys<span class="op">,</span> label<span class="op">,</span> l2)</span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Plot<span class="op">.</span><span class="fu">plot</span>({</span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a>      <span class="dt">title</span><span class="op">:</span> <span class="st">"Loss: "</span> <span class="op">+</span> loss<span class="op">.</span><span class="fu">toFixed</span>(<span class="dv">3</span>)<span class="op">,</span></span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a>      <span class="dt">color</span><span class="op">:</span> {<span class="dt">type</span><span class="op">:</span> <span class="st">"linear"</span><span class="op">,</span> <span class="dt">legend</span><span class="op">:</span> <span class="kw">true</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Loss"</span><span class="op">,</span> <span class="dt">scheme</span><span class="op">:</span> <span class="st">"BuRd"</span><span class="op">,</span> <span class="dt">domain</span><span class="op">:</span> [<span class="dv">0</span><span class="op">,</span> vmax]<span class="op">,</span> <span class="dt">type</span><span class="op">:</span> ctype}<span class="op">,</span></span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a>      <span class="dt">marks</span><span class="op">:</span> [</span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a>        Plot<span class="op">.</span><span class="fu">contour</span>(grid<span class="op">.</span><span class="at">values</span><span class="op">,</span> {<span class="dt">width</span><span class="op">:</span> grid<span class="op">.</span><span class="at">width</span><span class="op">,</span> <span class="dt">height</span><span class="op">:</span> grid<span class="op">.</span><span class="at">height</span><span class="op">,</span> <span class="dt">x1</span><span class="op">:</span> grid<span class="op">.</span><span class="at">x1</span><span class="op">,</span> <span class="dt">x2</span><span class="op">:</span>grid<span class="op">.</span><span class="at">x2</span><span class="op">,</span> <span class="dt">y1</span><span class="op">:</span> grid<span class="op">.</span><span class="at">y1</span><span class="op">,</span> <span class="dt">y2</span><span class="op">:</span> grid<span class="op">.</span><span class="at">y2</span><span class="op">,</span></span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a>          <span class="dt">stroke</span><span class="op">:</span> Plot<span class="op">.</span><span class="at">identity</span><span class="op">,</span> <span class="dt">thresholds</span><span class="op">:</span> thresholds})<span class="op">,</span></span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a>        Plot<span class="op">.</span><span class="fu">dot</span>(w)<span class="op">,</span></span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a>        Plot<span class="op">.</span><span class="fu">arrow</span>(arrows<span class="op">,</span> {<span class="dt">x1</span><span class="op">:</span> <span class="st">"0"</span><span class="op">,</span> <span class="dt">y1</span><span class="op">:</span> <span class="st">"1"</span><span class="op">,</span> <span class="dt">x2</span><span class="op">:</span> <span class="st">"2"</span><span class="op">,</span> <span class="dt">y2</span><span class="op">:</span> <span class="st">"3"</span><span class="op">,</span> <span class="dt">stroke</span><span class="op">:</span> <span class="st">"black"</span>})</span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a>      ]</span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> plot2d<span class="op">;</span></span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">regressionPlot</span>(data<span class="op">,</span> weights<span class="op">,</span> keys<span class="op">,</span> label<span class="op">,</span> l2<span class="op">,</span> f<span class="op">=</span>se<span class="op">,</span> stroke<span class="op">=</span><span class="st">""</span>) {</span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> loss <span class="op">=</span> <span class="fu">mean_loss</span>(f<span class="op">,</span> data<span class="op">,</span> weights<span class="op">,</span> keys<span class="op">,</span> label<span class="op">,</span> l2)<span class="op">;</span></span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> isString <span class="op">=</span> value <span class="kw">=&gt;</span> <span class="kw">typeof</span> value <span class="op">===</span> <span class="st">'string'</span><span class="op">;</span></span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> accessors <span class="op">=</span> <span class="fu">get_accessors</span>(keys)<span class="op">;</span></span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> index_accessors <span class="op">=</span> <span class="fu">get_accessors</span>(keys<span class="op">,</span> <span class="kw">true</span>)<span class="op">;</span></span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> domains <span class="op">=</span> <span class="fu">get_domains</span>(data<span class="op">,</span> <span class="fu">get_accessors</span>([label]<span class="op">.</span><span class="fu">concat</span>(keys)))<span class="op">;</span></span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a>  <span class="kw">const</span> get_label <span class="op">=</span> <span class="fu">isString</span>(label) <span class="op">?</span> (x <span class="kw">=&gt;</span> x[label]) <span class="op">:</span> label<span class="op">;</span></span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> stroke_shade <span class="op">=</span> stroke<span class="op">;</span></span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (stroke <span class="op">==</span> <span class="st">""</span>) {</span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a>    stroke_shade <span class="op">=</span> (x <span class="kw">=&gt;</span> <span class="fu">f</span>(<span class="fu">predict</span>(x<span class="op">,</span> weights<span class="op">,</span> keys)<span class="op">,</span> <span class="fu">get_label</span>(x)))</span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> Plot<span class="op">.</span><span class="fu">plot</span>({</span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a>    <span class="dt">y</span><span class="op">:</span> {<span class="dt">domain</span><span class="op">:</span> domains[<span class="dv">0</span>]}<span class="op">,</span></span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a>    <span class="dt">title</span><span class="op">:</span> <span class="st">"Loss: "</span> <span class="op">+</span> loss<span class="op">.</span><span class="fu">toFixed</span>(<span class="dv">3</span>)<span class="op">,</span></span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a>    <span class="dt">color</span><span class="op">:</span> {<span class="dt">type</span><span class="op">:</span> <span class="st">"linear"</span><span class="op">,</span> <span class="dt">legend</span><span class="op">:</span> <span class="kw">true</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Loss"</span><span class="op">,</span> <span class="dt">scheme</span><span class="op">:</span> <span class="st">"BuRd"</span><span class="op">,</span> <span class="dt">domain</span><span class="op">:</span> [<span class="dv">0</span><span class="op">,</span> <span class="dv">100</span>]}<span class="op">,</span></span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a>    <span class="dt">marks</span><span class="op">:</span> [</span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a>      Plot<span class="op">.</span><span class="fu">line</span>(<span class="fu">sample</span>((x) <span class="kw">=&gt;</span> <span class="fu">predict</span>([x]<span class="op">,</span> weights<span class="op">,</span> index_accessors)<span class="op">,</span> domains[<span class="dv">1</span>][<span class="dv">0</span>]<span class="op">,</span> domains[<span class="dv">1</span>][<span class="dv">1</span>])<span class="op">,</span> {<span class="dt">stroke</span><span class="op">:</span> <span class="st">'black'</span>})<span class="op">,</span></span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a>      Plot<span class="op">.</span><span class="fu">dot</span>(data<span class="op">,</span> {<span class="dt">x</span><span class="op">:</span> accessors[<span class="dv">0</span>]<span class="op">,</span> <span class="dt">y</span><span class="op">:</span> get_label<span class="op">,</span> <span class="dt">stroke</span><span class="op">:</span> stroke_shade })</span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">errorPlot</span>(data<span class="op">,</span> weights<span class="op">,</span> keys<span class="op">,</span> label<span class="op">,</span> f<span class="op">,</span> options<span class="op">=</span>{}) {</span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a>  <span class="kw">const</span> isString <span class="op">=</span> value <span class="kw">=&gt;</span> <span class="kw">typeof</span> value <span class="op">===</span> <span class="st">'string'</span><span class="op">;</span></span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a>  <span class="kw">const</span> get_label <span class="op">=</span> <span class="fu">isString</span>(label) <span class="op">?</span> (x <span class="kw">=&gt;</span> x[label]) <span class="op">:</span> label<span class="op">;</span></span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> errors <span class="op">=</span> data<span class="op">.</span><span class="fu">map</span>(x <span class="kw">=&gt;</span> [<span class="fu">predict</span>(x<span class="op">,</span> weights<span class="op">,</span> keys) <span class="op">-</span> <span class="fu">get_label</span>(x)<span class="op">,</span> <span class="fu">f</span>(<span class="fu">predict</span>(x<span class="op">,</span> weights<span class="op">,</span> keys)<span class="op">,</span> <span class="fu">get_label</span>(x))])<span class="op">;</span></span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> sigma <span class="op">=</span> (options[<span class="st">'sigma'</span>] <span class="op">||</span> <span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> plots <span class="op">=</span> []<span class="op">;</span></span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a>  <span class="kw">const</span> xdomain <span class="op">=</span> (options[<span class="st">'xdomain'</span>] <span class="op">||</span> [<span class="op">-</span><span class="dv">30</span><span class="op">,</span> <span class="dv">30</span>])<span class="op">;</span></span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a>  <span class="kw">const</span> ydomain <span class="op">=</span> (options[<span class="st">'ydomain'</span>] <span class="op">||</span> [<span class="dv">0</span><span class="op">,</span> <span class="fl">0.1</span>])<span class="op">;</span></span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (options[<span class="st">'plotnormal'</span>]){</span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> pdf <span class="op">=</span> x <span class="kw">=&gt;</span> <span class="bu">Math</span><span class="op">.</span><span class="fu">exp</span>(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> x <span class="op">*</span> x <span class="op">/</span> sigma) <span class="op">*</span> ydomain[<span class="dv">1</span>]<span class="op">;</span></span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> normal <span class="op">=</span> Plot<span class="op">.</span><span class="fu">line</span>(<span class="fu">sample</span>(pdf<span class="op">,</span> xdomain[<span class="dv">0</span>]<span class="op">,</span> xdomain[<span class="dv">1</span>])<span class="op">,</span> {<span class="dt">stroke</span><span class="op">:</span> <span class="st">'crimson'</span>})<span class="op">;</span>    </span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a>    plots<span class="op">.</span><span class="fu">push</span>(normal)<span class="op">;</span></span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (options[<span class="st">'plotlaplace'</span>]){</span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> pdf <span class="op">=</span> x <span class="kw">=&gt;</span> <span class="bu">Math</span><span class="op">.</span><span class="fu">exp</span>(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> <span class="bu">Math</span><span class="op">.</span><span class="fu">abs</span>(x) <span class="op">/</span> sigma) <span class="op">*</span> ydomain[<span class="dv">1</span>]<span class="op">;</span></span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> normal <span class="op">=</span> Plot<span class="op">.</span><span class="fu">line</span>(<span class="fu">sample</span>(pdf<span class="op">,</span> xdomain[<span class="dv">0</span>]<span class="op">,</span> xdomain[<span class="dv">1</span>])<span class="op">,</span> {<span class="dt">stroke</span><span class="op">:</span> <span class="st">'green'</span>})<span class="op">;</span>    </span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a>    plots<span class="op">.</span><span class="fu">push</span>(normal)<span class="op">;</span></span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> Plot<span class="op">.</span><span class="fu">plot</span>({</span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a>    <span class="dt">y</span><span class="op">:</span> {<span class="dt">grid</span><span class="op">:</span> <span class="kw">true</span><span class="op">,</span> <span class="dt">domain</span><span class="op">:</span> ydomain}<span class="op">,</span></span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a>    <span class="dt">x</span><span class="op">:</span> {<span class="dt">domain</span><span class="op">:</span> xdomain}<span class="op">,</span></span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a>    <span class="dt">color</span><span class="op">:</span> {<span class="dt">type</span><span class="op">:</span> <span class="st">"linear"</span><span class="op">,</span> <span class="dt">legend</span><span class="op">:</span> <span class="kw">true</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Loss"</span><span class="op">,</span> <span class="dt">scheme</span><span class="op">:</span> <span class="st">"BuRd"</span><span class="op">,</span> <span class="dt">domain</span><span class="op">:</span> [<span class="dv">0</span><span class="op">,</span> <span class="dv">100</span>]}<span class="op">,</span></span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a>    <span class="dt">marks</span><span class="op">:</span> [</span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a>      <span class="co">//Plot.rectY(errors, Plot.binX({y: "count", fill: x =&gt; mean(x.map(v =&gt; v[1]))}, {x: "0"})),</span></span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a>      Plot<span class="op">.</span><span class="fu">rectY</span>(errors<span class="op">,</span> Plot<span class="op">.</span><span class="fu">binX</span>({<span class="dt">y</span><span class="op">:</span> <span class="st">"proportion"</span>}<span class="op">,</span> {<span class="dt">x</span><span class="op">:</span> <span class="st">"0"</span><span class="op">,</span> <span class="dt">fill</span><span class="op">:</span> <span class="st">'steelblue'</span><span class="op">,</span> <span class="dt">interval</span><span class="op">:</span> <span class="dv">1</span>}))<span class="op">,</span></span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a>      Plot<span class="op">.</span><span class="fu">ruleY</span>([<span class="dv">0</span>])</span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a>    ]<span class="op">.</span><span class="fu">concat</span>(plots)</span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-329"><a href="#cb1-329" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">nnPlot</span>(data<span class="op">,</span> weights<span class="op">,</span> keys<span class="op">,</span> label<span class="op">,</span> l2<span class="op">,</span> f<span class="op">=</span>se<span class="op">,</span> stroke<span class="op">=</span><span class="st">""</span><span class="op">,</span> options<span class="op">=</span>[]) {</span>
<span id="cb1-330"><a href="#cb1-330" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> loss <span class="op">=</span> <span class="fu">mean_loss</span>(f<span class="op">,</span> data<span class="op">,</span> weights<span class="op">,</span> keys<span class="op">,</span> label<span class="op">,</span> l2)<span class="op">;</span></span>
<span id="cb1-331"><a href="#cb1-331" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> isString <span class="op">=</span> value <span class="kw">=&gt;</span> <span class="kw">typeof</span> value <span class="op">===</span> <span class="st">'string'</span><span class="op">;</span></span>
<span id="cb1-332"><a href="#cb1-332" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-333"><a href="#cb1-333" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> accessors <span class="op">=</span> <span class="fu">get_accessors</span>(keys)<span class="op">;</span></span>
<span id="cb1-334"><a href="#cb1-334" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> index_accessors <span class="op">=</span> <span class="fu">get_accessors</span>(keys<span class="op">,</span> <span class="kw">true</span>)<span class="op">;</span></span>
<span id="cb1-335"><a href="#cb1-335" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> domains <span class="op">=</span> <span class="fu">get_domains</span>(data<span class="op">,</span> <span class="fu">get_accessors</span>([label]<span class="op">.</span><span class="fu">concat</span>(keys)))<span class="op">;</span></span>
<span id="cb1-336"><a href="#cb1-336" aria-hidden="true" tabindex="-1"></a>  <span class="kw">const</span> get_label <span class="op">=</span> <span class="fu">isString</span>(label) <span class="op">?</span> (x <span class="kw">=&gt;</span> x[label]) <span class="op">:</span> label<span class="op">;</span></span>
<span id="cb1-337"><a href="#cb1-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-338"><a href="#cb1-338" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> stroke_shade <span class="op">=</span> stroke<span class="op">;</span></span>
<span id="cb1-339"><a href="#cb1-339" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (stroke <span class="op">==</span> <span class="st">""</span>) {</span>
<span id="cb1-340"><a href="#cb1-340" aria-hidden="true" tabindex="-1"></a>    stroke_shade <span class="op">=</span> (x <span class="kw">=&gt;</span> <span class="fu">f</span>(<span class="fu">predict</span>(x<span class="op">,</span> weights<span class="op">,</span> keys)<span class="op">,</span> <span class="fu">get_label</span>(x)))</span>
<span id="cb1-341"><a href="#cb1-341" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-342"><a href="#cb1-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-343"><a href="#cb1-343" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> a <span class="op">=</span> []</span>
<span id="cb1-344"><a href="#cb1-344" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (options<span class="op">.</span><span class="fu">indexOf</span>(<span class="st">"Show feature transforms"</span>) <span class="op">&gt;=</span> <span class="dv">0</span>){</span>
<span id="cb1-345"><a href="#cb1-345" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> [Plot<span class="op">.</span><span class="fu">line</span>(<span class="fu">sample</span>((x) <span class="kw">=&gt;</span>  keys[<span class="dv">1</span>][<span class="dv">1</span>](x)<span class="op">,</span> domains[<span class="dv">1</span>][<span class="dv">0</span>]<span class="op">,</span> domains[<span class="dv">1</span>][<span class="dv">1</span>])<span class="op">,</span> {<span class="dt">stroke</span><span class="op">:</span> <span class="st">'red'</span>})<span class="op">,</span></span>
<span id="cb1-346"><a href="#cb1-346" aria-hidden="true" tabindex="-1"></a>      Plot<span class="op">.</span><span class="fu">line</span>(<span class="fu">sample</span>((x) <span class="kw">=&gt;</span> keys[<span class="dv">2</span>][<span class="dv">1</span>](x)<span class="op">,</span> domains[<span class="dv">1</span>][<span class="dv">0</span>]<span class="op">,</span> domains[<span class="dv">1</span>][<span class="dv">1</span>])<span class="op">,</span> {<span class="dt">stroke</span><span class="op">:</span> <span class="st">'blue'</span>})]</span>
<span id="cb1-347"><a href="#cb1-347" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-348"><a href="#cb1-348" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-349"><a href="#cb1-349" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> Plot<span class="op">.</span><span class="fu">plot</span>({</span>
<span id="cb1-350"><a href="#cb1-350" aria-hidden="true" tabindex="-1"></a>    <span class="dt">y</span><span class="op">:</span> {<span class="dt">domain</span><span class="op">:</span> domains[<span class="dv">0</span>]}<span class="op">,</span></span>
<span id="cb1-351"><a href="#cb1-351" aria-hidden="true" tabindex="-1"></a>    <span class="dt">title</span><span class="op">:</span> <span class="st">"Loss: "</span> <span class="op">+</span> loss<span class="op">.</span><span class="fu">toFixed</span>(<span class="dv">3</span>)<span class="op">,</span></span>
<span id="cb1-352"><a href="#cb1-352" aria-hidden="true" tabindex="-1"></a>    <span class="dt">color</span><span class="op">:</span> {<span class="dt">type</span><span class="op">:</span> <span class="st">"linear"</span><span class="op">,</span> <span class="dt">legend</span><span class="op">:</span> <span class="kw">true</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Loss"</span><span class="op">,</span> <span class="dt">scheme</span><span class="op">:</span> <span class="st">"BuRd"</span><span class="op">,</span> <span class="dt">domain</span><span class="op">:</span> [<span class="dv">0</span><span class="op">,</span> <span class="dv">100</span>]}<span class="op">,</span></span>
<span id="cb1-353"><a href="#cb1-353" aria-hidden="true" tabindex="-1"></a>    <span class="dt">marks</span><span class="op">:</span> [</span>
<span id="cb1-354"><a href="#cb1-354" aria-hidden="true" tabindex="-1"></a>      Plot<span class="op">.</span><span class="fu">line</span>(<span class="fu">sample</span>((x) <span class="kw">=&gt;</span> <span class="fu">predict</span>([x]<span class="op">,</span> weights<span class="op">,</span> index_accessors)<span class="op">,</span> domains[<span class="dv">1</span>][<span class="dv">0</span>]<span class="op">,</span> domains[<span class="dv">1</span>][<span class="dv">1</span>])<span class="op">,</span> {<span class="dt">stroke</span><span class="op">:</span> <span class="st">'black'</span>})<span class="op">,</span></span>
<span id="cb1-355"><a href="#cb1-355" aria-hidden="true" tabindex="-1"></a>      Plot<span class="op">.</span><span class="fu">dot</span>(data<span class="op">,</span> {<span class="dt">x</span><span class="op">:</span> accessors[<span class="dv">0</span>]<span class="op">,</span> <span class="dt">y</span><span class="op">:</span> get_label<span class="op">,</span> <span class="dt">stroke</span><span class="op">:</span> stroke_shade })</span>
<span id="cb1-356"><a href="#cb1-356" aria-hidden="true" tabindex="-1"></a>    ]<span class="op">.</span><span class="fu">concat</span>(a)</span>
<span id="cb1-357"><a href="#cb1-357" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb1-358"><a href="#cb1-358" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-2" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-3" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-4" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-5" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-6" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-7" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-8" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-9" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-10" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-11" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-12" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-13" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-14" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-15" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-16" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-17" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-18" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-19" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-20" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-21" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-22" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-23" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-24" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-25" data-nodetype="declaration">

</div>
</div>
</div>
</div>
<div class="cell" data-execution_count="2">
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Manim Community <span style="color: #008000; text-decoration-color: #008000">v0.18.0</span>

</pre>
</div>
</div>
<section id="neural-networks" class="level1">
<h1>Neural networks</h1>
<section id="neural-networks-with-matrices" class="level2">
<h2 class="anchored" data-anchor-id="neural-networks-with-matrices">Neural networks with matrices</h2>
<p>Lets return to our simple neural network example, where we have 2 inputs and 3 neurons (transforms): <span class="math display">\[\mathbf{x} = \begin{bmatrix} x_1\\ x_2 \end{bmatrix}, \quad \mathbf{w}_0 = \begin{bmatrix} w_{01} \\ w_{02} \end{bmatrix}\]</span> <span class="math display">\[
f(\mathbf{x})=\phi(\mathbf{x})^T \mathbf{w}_0,\quad \phi(\mathbf{x}) = \begin{bmatrix}  \sigma(\mathbf{x}^T \mathbf{w}_1) \\ \sigma(\mathbf{x}^T \mathbf{w}_2) \\ \sigma(\mathbf{x}^T \mathbf{w}_3) \end{bmatrix} =
\begin{bmatrix}  \sigma(x_1 w_{11} + x_2 w_{12}) \\ \sigma(x_1 w_{21} + x_2 w_{22}) \\ \sigma(x_1 w_{31} + x_2 w_{32}) \end{bmatrix}
\]</span></p>
<p>Again, we can represent this pictorially again as a node-link diagram:</p>
<div class="cell" data-execution_count="3">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-4-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Lets look at a more compact way to write this, using a weight <em>matrix</em> for the neural network layer. Lets look at the transform before we apply the sigmoid function:</p>
<p><span class="math display">\[
\begin{bmatrix}  \mathbf{x}^T \mathbf{w}_1 \\ \mathbf{x}^T \mathbf{w}_2 \\ \mathbf{x}^T \mathbf{w}_3 \end{bmatrix} =
\begin{bmatrix}  x_1 w_{11} + x_2 w_{12} \\ x_1 w_{21} + x_2 w_{22} \\ x_1 w_{31} + x_2 w_{32} \end{bmatrix} = \begin{bmatrix} w_{11} &amp; w_{12} \\w_{21} &amp; w_{22} \\ w_{31} &amp; w_{32} \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}
\]</span></p>
<p>If we define a matrix <span class="math inline">\(\mathbf{W}\)</span> for all of the weights as:</p>
<p><span class="math display">\[
\mathbf{W} = \begin{bmatrix} w_{11} &amp; w_{12} \\w_{21} &amp; w_{22} \\ w_{31} &amp; w_{32} \end{bmatrix}
\]</span></p>
<p>we get:</p>
<p><span class="math display">\[
\begin{bmatrix}  \mathbf{x}^T \mathbf{w}_1 \\ \mathbf{x}^T \mathbf{w}_2 \\ \mathbf{x}^T \mathbf{w}_3 \end{bmatrix} = \mathbf{W}\mathbf{x} = (\mathbf{x}^T\mathbf{W}^T)^T
\]</span><br>
If we let <span class="math inline">\(h\)</span> be the number of neuron (or hidden layer units) then this is a <span class="math inline">\(h \times d\)</span> matrix. Therefore, we can write our transform as:</p>
<p><span class="math display">\[
\phi(\mathbf{x}) = \sigma(\mathbf{x}^T\mathbf{W}^T)^T, \quad f(\mathbf{x}) = \sigma(\mathbf{x}^T\mathbf{W}^T) \mathbf{w}_0
\]</span></p>
<p>Recall that if we have multiple observations, as in a dataset, we define them together as an <span class="math inline">\(N \times d\)</span> matrix <span class="math inline">\(\mathbf{X}\)</span> such that each <em>row</em> is an observation:</p>
<p><span class="math display">\[
\mathbf{X} = \begin{bmatrix} \mathbf{x}_1^T \\ \mathbf{x}_2^T \\ \mathbf{x}_3^T  \\ \vdots  \end{bmatrix}
\]</span></p>
<p>Therefore, we can transform all of these observations at once by multiplying this matrix by <span class="math inline">\(\mathbf{W}^T\)</span>.</p>
<p><span class="math display">\[
\phi(\mathbf{X}) = \sigma(\mathbf{X}\mathbf{W}^T)^T = \begin{bmatrix} \sigma(\mathbf{x}_1^T\mathbf{w}_1) &amp; \sigma(\mathbf{x}_1^T\mathbf{w}_2) &amp; \dots  &amp; \sigma(\mathbf{x}_1^T\mathbf{w}_h \\
\sigma(\mathbf{x}_2^T\mathbf{w}_1) &amp; \sigma(\mathbf{x}_2^T\mathbf{w}_2) &amp; \dots  &amp; \sigma(\mathbf{x}_2^T\mathbf{w}_h) \\
\vdots &amp; \vdots &amp; \ddots  &amp; \vdots \\
\sigma(\mathbf{x}_N^T\mathbf{w}_1) &amp; \sigma(\mathbf{x}_N^T\mathbf{w}_2) &amp; \dots  &amp; \sigma(\mathbf{x}_N^T\mathbf{w}_h)
\end{bmatrix}
\]</span></p>
<p>We see that this is an <span class="math inline">\(N \times h\)</span> matrix where each row is a transformed observation! We can then write our full prediction function as</p>
<p><span class="math display">\[
\quad f(\mathbf{x}) = \sigma(\mathbf{X}\mathbf{W}^T) \mathbf{w}_0
\]</span></p>
<p>To summarize:</p>
<ul>
<li><p><span class="math inline">\(\mathbf{X}: \quad N \times d\)</span> matrix of observations</p></li>
<li><p><span class="math inline">\(\mathbf{W}: \quad h \times d\)</span> matrix of network weights</p></li>
<li><p><span class="math inline">\(\mathbf{w}_0: \quad h\ (\times 1)\)</span> vector of linear regression weights</p></li>
</ul>
<p>If we check that our dimensions work for matrix multiplication we see that we get the <span class="math inline">\(N\times 1\)</span> vector of predictions we are looking for!</p>
<p><span class="math display">\[
(N \times d) (h \times d)^T (h \times 1) \rightarrow (N \times d) (d \times h) (h \times 1) \rightarrow (N \times h) (h \times 1)
\]</span></p>
<p><span class="math display">\[
\longrightarrow (N \times1)
\]</span></p>
</section>
<section id="benefits-of-neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="benefits-of-neural-networks">Benefits of neural networks</h2>
<p>Weve seen that the neural network transform is still fairly restrictive, with a limited number of neurons we cant fit any arbitrary function. In fact, if we choose our feature transforms wisely we can do better than than a neural network.</p>
<p>For example, consider the simple 3-neuron network above. We can see that if we try to fit a circular dataset with it, it performs worse than an explicit transform with <span class="math inline">\(x_1^2\)</span> and <span class="math inline">\(x_2^2\)</span>.</p>
<ul>
<li><p><a href="https://playground.tensorflow.org/#activation=sigmoid&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=3&amp;seed=0.46216&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false&amp;showTestData_hide=true&amp;stepButton_hide=true&amp;activation_hide=true&amp;problem_hide=true&amp;noise_hide=true&amp;batchSize_hide=true&amp;dataset_hide=true&amp;regularization_hide=true&amp;resetButton_hide=true&amp;learningRate_hide=true&amp;regularizationRate_hide=true&amp;percTrainData_hide=true&amp;numHiddenLayers_hide=true">Circle dataset with neural network</a></p></li>
<li><p><a href="https://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=&amp;seed=0.10871&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=true&amp;ySquared=true&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false&amp;showTestData_hide=true&amp;stepButton_hide=true&amp;activation_hide=true&amp;problem_hide=true&amp;noise_hide=true&amp;resetButton_hide=true&amp;regularization_hide=true&amp;dataset_hide=true&amp;batchSize_hide=true&amp;learningRate_hide=true&amp;percTrainData_hide=true&amp;regularizationRate_hide=true&amp;numHiddenLayers_hide=true">Circle dataset with</a> <span class="math inline">\(x_1^2\)</span> and <span class="math inline">\(x_2^2\)</span>:</p></li>
</ul>
<p>Similarly, for a cross dataset, we can do better with the feature transform that includes <span class="math inline">\(x_1x_2\)</span> as a feature:</p>
<ul>
<li><p><a href="https://playground.tensorflow.org/#activation=sigmoid&amp;batchSize=10&amp;dataset=xor&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=3&amp;seed=0.46216&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false&amp;showTestData_hide=true&amp;stepButton_hide=true&amp;activation_hide=true&amp;problem_hide=true&amp;noise_hide=true&amp;batchSize_hide=true&amp;dataset_hide=true&amp;regularization_hide=true&amp;resetButton_hide=true&amp;learningRate_hide=true&amp;regularizationRate_hide=true&amp;percTrainData_hide=true&amp;numHiddenLayers_hide=true">Cross dataset with neural network</a></p></li>
<li><p><a href="https://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=xor&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=&amp;seed=0.26985&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=true&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false&amp;showTestData_hide=true&amp;stepButton_hide=true&amp;activation_hide=true&amp;problem_hide=true&amp;noise_hide=true&amp;resetButton_hide=true&amp;regularization_hide=true&amp;dataset_hide=true&amp;batchSize_hide=true&amp;learningRate_hide=true&amp;regularizationRate_hide=true&amp;percTrainData_hide=true&amp;numHiddenLayers_hide=true">Cross dataset with</a> <span class="math inline">\(x_1x_2\)</span></p></li>
</ul>
<p>However, if we choose the <em>wrong</em> feature transform for a given dataset, we do far worse.</p>
<ul>
<li><p><a href="https://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=&amp;seed=0.10871&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=true&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false&amp;showTestData_hide=true&amp;stepButton_hide=true&amp;activation_hide=true&amp;problem_hide=true&amp;noise_hide=true&amp;resetButton_hide=true&amp;regularization_hide=true&amp;dataset_hide=true&amp;batchSize_hide=true&amp;learningRate_hide=true&amp;percTrainData_hide=true&amp;regularizationRate_hide=true&amp;numHiddenLayers_hide=true">Circle dataset with</a> <span class="math inline">\(x_1 x_2\)</span></p></li>
<li><p><a href="https://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=xor&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=&amp;seed=0.06128&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=true&amp;ySquared=true&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false&amp;showTestData_hide=true&amp;stepButton_hide=true&amp;activation_hide=true&amp;problem_hide=true&amp;noise_hide=true&amp;resetButton_hide=true&amp;regularization_hide=true&amp;dataset_hide=true&amp;batchSize_hide=true&amp;learningRate_hide=true&amp;regularizationRate_hide=true&amp;percTrainData_hide=true&amp;numHiddenLayers_hide=true">Cross dataset with</a> <span class="math inline">\(x_1^2\)</span> and <span class="math inline">\(x_2^2\)</span></p></li>
</ul>
<p>We see that the real power of the neural network here is the ability to adapt the transform to the given dataset, without needing to carefully choose the correct transform!</p>
</section>
<section id="deep-neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="deep-neural-networks">Deep Neural Networks</h2>
<p>What weve seen so far is a neural network with a <em>single</em> hidden layer, meaning that we create a feature transform for our data and then simply use that to make our prediction. We see that each individual feature transform is a bit limited, being just a logistic regression function.</p>
<p><span class="math display">\[\phi(\mathbf{x})_i = \sigma(\mathbf{x}^T \mathbf{w}_i)\]</span><br>
No matter what we set <span class="math inline">\(\mathbf{w}_i\)</span> this transform would not be able to replicate a transform like <span class="math inline">\(\phi(\mathbf{x})_i = x_i^2\)</span>. However, weve already seen a way to make logistic regression more expressive: <strong>neural networks</strong>!</p>
<p>The idea behind a <em>deep</em> or <em>multi-layer</em> neural network is that we can apply this idea of neural network feature transforms recursively:</p>
<p><span class="math display">\[\phi(\mathbf{x})_i = \sigma(\sigma(\mathbf{x}^T\mathbf{W}^T) \mathbf{w}_i)\]</span></p>
<p>Here weve transformed our input before computing our feature transform. In terms of a dataset we can write the full prediction function for this <em>2-layer</em> network as:</p>
<p><span class="math display">\[
f(\mathbf{X}) = \sigma(\sigma(\mathbf{X}\mathbf{W}_1^T)\mathbf{W}_2^T)\mathbf{w}_0
\]</span></p>
<p>Weve now defined a set of weight parameters for each of our 2 <em>hidden layers</em> <span class="math inline">\(\mathbf{W}_1\)</span> and <span class="math inline">\(\mathbf{W}_2\)</span>. Its a little easier to see whats happening here if we look a our diagram for this case:</p>
<div class="cell" data-execution_count="4">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-5-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>We can see that stacking these transforms allows us to fit even more complicated functions <a href="https://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=spiral&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,4,4,4&amp;seed=0.88060&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false">here</a>. Note that we are still not limited to doing this twice! We can fit many layers of transforms:</p>
<p><img src="nn(5).svg" class="img-fluid"></p>
<p>Later on in the semester well talk in more depth about the effect of the number of layers and the number of neurons per layer!</p>
</section>
<section id="optimizing-neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="optimizing-neural-networks">Optimizing neural networks</h2>
<p>We can still define a <strong>loss function</strong> for a neural network in the same way we did with our simpler linear models. The only difference is that now we have more parameters to choose:</p>
<p><span class="math display">\[
\mathbf{Loss}(\mathbf{w}_0,\mathbf{W}_1,...)
\]</span></p>
<p>Lets look at the logistic regression negative log-likelihood loss for the simple neural network we saw above (for simplicity well just call the network weights <span class="math inline">\(\mathbf{W}\)</span>). The probability of class 1 is estimated as:</p>
<p><span class="math display">\[
p(y=1\mid \mathbf{x}, \mathbf{w}_0,\mathbf{W})=\sigma(\phi(\mathbf{x})^T \mathbf{w}_0) = \sigma(\sigma(\mathbf{x}^T \mathbf{W}^T) \mathbf{w}_0),\quad \phi(\mathbf{x}) = \begin{bmatrix}  \sigma(\mathbf{x}^T \mathbf{W}_{1}) \\ \sigma(\mathbf{x}^T \mathbf{W}_{2}) \\ \sigma(\mathbf{x}^T \mathbf{W}_{3}) \end{bmatrix}
\]</span> <span class="math display">\[ = \sigma\big(w_{01} \cdot\sigma(x_1 W_{11} + x_2 W_{12}) + w_{02} \cdot\sigma(x_1 W_{21} + x_2 W_{22})+ w_{03} \cdot\sigma(x_1 W_{31} + x_2 W_{32}) \big)\]</span></p>
<p>Therefore the negative log-likelihood is:</p>
<p><span class="math display">\[
\mathbf{NLL}(\mathbf{w}_0,\mathbf{W}, \mathbf{X}, \mathbf{y}) = -\sum_{i=1}^N \bigg[ y_i\log p(y=1\mid \mathbf{x}, \mathbf{w}_0,\mathbf{W}) + (1-y_i)\log p(y=0\mid \mathbf{x}, \mathbf{w}_0,\mathbf{W}) \bigg]
\]</span></p>
<p><span class="math display">\[
= -\sum_{i=1}^N \log \sigma\big((2y_i-1) \phi(\mathbf{x}_i)^T \mathbf{w}\big)
\]</span></p>
<p>We see that we can write out a full expression for this loss in term of all the inputs and weights. We can even define the gradient of this loss with respect to all the weights:</p>
<p><span class="math display">\[
\nabla_{\mathbf{w}_0} \mathbf{NLL}(\mathbf{w}_0,\mathbf{W}, \mathbf{X}, \mathbf{y}) = \begin{bmatrix} \frac{\partial \mathbf{NLL}}{\partial w_{01}} \\ \frac{\partial \mathbf{NLL}}{\partial w_{02}} \\ \frac{\partial \mathbf{NLL}}{\partial w_{03}} \\ \vdots\end{bmatrix}, \quad \nabla_{\mathbf{W}}\mathbf{NLL}(\mathbf{w}_0,\mathbf{W}, \mathbf{X}, \mathbf{y}) =
\begin{bmatrix} \frac{\partial \mathbf{NLL}}{\partial W_{11}} &amp;  \frac{\partial \mathbf{NLL}}{\partial W_{12}} &amp; \dots &amp; \frac{\partial \mathbf{NLL}}{\partial W_{1d}}  \\
\frac{\partial \mathbf{NLL}}{\partial W_{21}} &amp;  \frac{\partial \mathbf{NLL}}{\partial W_{22}} &amp; \dots &amp; \frac{\partial \mathbf{NLL}}{\partial W_{2d}} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial \mathbf{NLL}}{\partial W_{h1}} &amp;  \frac{\partial \mathbf{NLL}}{\partial W_{h2}} &amp; \dots &amp; \frac{\partial \mathbf{NLL}}{\partial W_{hd}}
\end{bmatrix}
\]</span></p>
<p>Note that as <span class="math inline">\(\mathbf{W}\)</span> is a matrix, the gradient with respect to <span class="math inline">\(\mathbf{W}\)</span> is also a matrix! Our gradient descent algorithm can proceed in the same way it did for our linear models, but here we now need to update both sets of parameters:</p>
<p><span class="math display">\[
\mathbf{w}_0^{(k+1)} \longleftarrow \mathbf{w}_0^{(k)} -\alpha \nabla_{\mathbf{w}_0} \mathbf{NLL}(\mathbf{w}_0,\mathbf{W}, \mathbf{X}, \mathbf{y}), \quad \mathbf{W}^{(k+1)} \longleftarrow \mathbf{W}^{(k)} -\alpha \nabla_{\mathbf{W}} \mathbf{NLL}(\mathbf{w}_0,\mathbf{W}, \mathbf{X}, \mathbf{y})
\]</span></p>
<p>The important question now becomes: <em>how do we compute these gradients?</em></p>
</section>
</section>
<section id="automatic-differentiation" class="level1">
<h1>Automatic Differentiation</h1>
<p>In this section well derive <em>algorithms</em> for computing the derivative of <em>any</em> function.</p>
<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>We saw above that the NLL for logistic regression with a neural network is:</p>
<p><span class="math display">\[
\mathbf{NLL}(\mathbf{w}_0,\mathbf{W}, \mathbf{X}, \mathbf{y}) = -\sum_{i=1}^N \log \sigma\big((2y_i-1) \phi(\mathbf{x}_i)^T \mathbf{w}\big)
\]</span></p>
<p>If we write this out in terms of the individual values we get:</p>
<p><span class="math display">\[
= -\sum_{i=1}^N \log \sigma\big((2y_i-1)\sigma\big(w_{01} \cdot\sigma(x_1 W_{11} + x_2 W_{12}) + w_{02} \cdot\sigma(x_1 W_{21} + x_2 W_{22})+ w_{03} \cdot\sigma(x_1 W_{31} + x_2 W_{32}) \big)\big)
\]</span></p>
<p>We could use the same approach as usual to find the derivative of this loss with respect to each individual weight parameter, but it would be very tedious and this is only a <em>single-layer network</em>! Things would only get more complicated with more layers. Furthermore if we changed some aspect of the network, like the activation function, wed have to do it all over again.</p>
<p>Ideally wed like a programmatic way to compute derivatives. Knowing that we compute derivatives using a fixed set of known rules, this should be possible!</p>
</section>
<section id="the-chain-rule-revisited" class="level2">
<h2 class="anchored" data-anchor-id="the-chain-rule-revisited">The chain rule revisited</h2>
<p>While we often think about the chain rule in terms of functions:</p>
<p><span class="math display">\[
\frac{d}{dx}f(g(x)) = f'(g(x))g'(x)
\]</span></p>
<p>Its often easier to view it imperatively, in terms of individual values. For example we might say:</p>
<p><span class="math display">\[
b = g(x)
\]</span></p>
<p><span class="math display">\[
a = f(b)
\]</span></p>
<p>In this case we can write the chain rule as:</p>
<p><span class="math display">\[
\frac{da}{dx} = \frac{da}{db}\frac{db}{dx}
\]</span></p>
<p>This corresponds with how we might think about this in code. For example we might have the code:</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> x <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> log(b)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In this case we have:</p>
<p><span class="math display">\[
a = \log(b), \quad b = x^2
\]</span></p>
<p>We can compute the derivative of <span class="math inline">\(a\)</span> with respect to <span class="math inline">\(x\)</span> using the chain rule as:</p>
<p><span class="math display">\[
\frac{da}{db} = \frac{1}{b}, \quad \frac{db}{dx} = 2x
\]</span></p>
<p><span class="math display">\[
\frac{da}{dx} = \bigg(\frac{1}{b}\bigg)(2x) = \frac{2x}{x^2} = \frac{2}{x}
\]</span></p>
</section>
<section id="composing-many-operations" class="level2">
<h2 class="anchored" data-anchor-id="composing-many-operations">Composing many operations</h2>
<p>For more complex functions, we might be composing many more operations, but we can break down derivative computations in the same way. For example, if we want the derivative with respect to <span class="math inline">\(x\)</span> of some simple loss:</p>
<p><span class="math display">\[
L=-\log \sigma\big(w x^2\big)
\]</span></p>
<p>We can break this down into each individual operation that we apply:</p>
<p><span class="math display">\[
a = x^2
\]</span></p>
<p><span class="math display">\[
b=wa
\]</span></p>
<p><span class="math display">\[
c=\sigma(b)
\]</span></p>
<p><span class="math display">\[
g= \log c
\]</span></p>
<p><span class="math display">\[
L=-g
\]</span><br>
The chain rule tells us that:</p>
<p><span class="math display">\[
\frac{dL}{dx} = \frac{dL}{dg}\frac{dg}{dc}\frac{dc}{db}\frac{db}{da}\frac{da}{dx}
\]</span><br>
Since each step is a single operation with a known derivative, we can easily compute every term above! Thus, we begin to see a recipe for computing derivatives programatically. Every time we perform some operation, we will also compute the derivative with respect to the input (we cant just compute the derivatives because each derivative needs the preceding value, e.g.&nbsp;<span class="math inline">\(\frac{dg}{dc}=\frac{1}{c}\)</span>, so we need to first compute <span class="math inline">\(c\)</span>).</p>
<p>We can visually look at the chain of computation that were performing as a diagram that shows each step and the result.</p>
<div class="cell" data-execution_count="6">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-7-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>We call this structure the <strong>computational graph</strong>.</p>
</section>
<section id="forward-and-reverse-mode-automatic-differentiation" class="level2">
<h2 class="anchored" data-anchor-id="forward-and-reverse-mode-automatic-differentiation">Forward and reverse mode automatic differentiation</h2>
<p>We are not actually interested in all of the intermediate derivatives ( <span class="math inline">\(\frac{db}{da}, \frac{dc}{db}\)</span> etc.), so it doesnt make much sense to compute all of them and then multiply them together. Instead, wed rather just incrementally compute the value were interested in <span class="math inline">\(\frac{dL}{dx}\)</span>, as we go.</p>
<p>There are 2 ways we could consider doing this. One way is to always keep track of the derivative of the current value with respect to <span class="math inline">\(x\)</span>. So in the diagram above, each time we perform a new operation we will also compute the derivative of the operation and then update our knowledge of the derivative with respect to <span class="math inline">\(x\)</span>. For example for the operation going from <span class="math inline">\(b\)</span> to <span class="math inline">\(c\)</span>:</p>
<p><span class="math display">\[
c \leftarrow \sigma(b), \quad \frac{dc}{dx} \leftarrow \frac{dc}{db}\cdot\frac{db}{dx}
\]</span></p>
<p>We call this approach <strong>forward-mode automatic differentiation</strong>.</p>
<p>The alternative approach is to work backwards, first compute <span class="math inline">\(L\)</span> and <span class="math inline">\(\frac{dL}{dg}\)</span> and then go backwards through the chain updating the derivative of the final output with respect to each input for the <span class="math inline">\(b\)</span> to <span class="math inline">\(c\)</span> operation this looks like:</p>
<p><span class="math display">\[
c \leftarrow \sigma(b), \quad \frac{dL}{db} \leftarrow \frac{dc}{db}\cdot\frac{dL}{dc}
\]</span></p>
<p>This means we need to do our computation in 2 passes. First we need to go through the chain of operations to compute <span class="math inline">\(L\)</span>, then we need to go backwards through the chain to compute <span class="math inline">\(\frac{dL}{dx}\)</span>. Note that computing each intermediate derivative requires the a corresponding intermediate value (e.g.&nbsp;<span class="math inline">\(\frac{dc}{db}\)</span> requires <span class="math inline">\(b\)</span> to compute). So we need to store all the intermediate values as we go. The approach is called <strong>reverse-mode automatic differentiation</strong> or more commonly: <strong>backpropagation</strong>. We can summarize both approaches below:</p>
<div class="cell" data-execution_count="7">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-8-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="automatic-differentiation-with-multiple-inputs" class="level2">
<h2 class="anchored" data-anchor-id="automatic-differentiation-with-multiple-inputs">Automatic differentiation with multiple inputs</h2>
<p>You might wonder why wed ever use reverse-mode when it seems to require much more complication in keeping track of all the intermediate values. To see why it is useful, letss consider the common case where we would like to take derivatives with respect to multiple inputs at the same time. For example we might have an expression like:</p>
<p><span class="math display">\[
-\log \sigma (w_1 x_1+w_2x_2 +w_3x_3)
\]</span></p>
<p>In this case we want to find the gradient:</p>
<p><span class="math display">\[
\frac{dL}{d\mathbf{x}} = \begin{bmatrix}\frac{dL}{dx_1} \\ \frac{dL}{dx_2} \\ \frac{dL}{dx_3} \end{bmatrix}
\]</span></p>
<p>We see that in forward mode, we now need to keep a vector of gradients at many steps if we want to compute the derivative with respect to every input!</p>
<div class="cell" data-execution_count="8">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-9-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>In reverse mode, however we only ever need to keep the derivative of the <em>loss</em> with respect to the current value. If we assume that the loss is always a single value, this is much more efficient!</p>
<div class="cell" data-execution_count="9">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-10-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="reusing-values" class="level2">
<h2 class="anchored" data-anchor-id="reusing-values">Reusing values</h2>
<p>One thing we need to consider is the fact that values can be used in multiple different operations. For example, consider the code below.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loss(x):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> x <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> <span class="dv">5</span> <span class="op">*</span> a</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    c <span class="op">=</span> log(a)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    g <span class="op">=</span> b <span class="op">*</span> c</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    L <span class="op">=</span> <span class="op">-</span>g</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> L</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This corresponds to the following sequence of operations:</p>
<p><span class="math display">\[
a = x^2
\]</span></p>
<p><span class="math display">\[
b=5a
\]</span></p>
<p><span class="math display">\[
c=\log a
\]</span></p>
<p><span class="math display">\[
g = bc
\]</span></p>
<p><span class="math display">\[
L=-b
\]</span></p>
<p>We see that both <span class="math inline">\(b\)</span> <em>and</em> <span class="math inline">\(c\)</span> depend on <span class="math inline">\(a\)</span>. Leading to the following computational graph:</p>
<div class="cell" data-execution_count="11">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-12-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>In forward mode this means that we compute 2 different values for <span class="math inline">\(\frac{dg}{dx}\)</span>, one from <span class="math inline">\(b\)</span> <span class="math inline">\((\frac{dg}{db}\cdot\frac{db}{dx})\)</span> and one from <span class="math inline">\(c\)</span> <span class="math inline">\((\frac{dg}{dc}\cdot\frac{dc}{dx})\)</span>.</p>
<div class="cell" data-execution_count="12">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-13-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>In reverse mode this means that we compute 2 different values for <span class="math inline">\(\frac{dL}{da}\)</span>, one from <span class="math inline">\(b\)</span> <span class="math inline">\((\frac{dL}{db}\cdot\frac{db}{da})\)</span> and one from <span class="math inline">\(c\)</span> <span class="math inline">\((\frac{dL}{dc}\cdot\frac{dc}{da})\)</span>.</p>
<div class="cell" data-execution_count="13">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-14-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The resolution in both cases is simple! Just add the two terms. So in forward mode:</p>
<p><span class="math display">\[\frac{dg}{dx} = \frac{dg}{db}\cdot \frac{db}{dx} +\frac{dg}{dc}\cdot \frac{dc}{dx}\]</span></p>
<p>In reverse mode:</p>
<p><span class="math display">\[\frac{dL}{da} = \frac{dL}{db}\cdot \frac{db}{da} + \frac{dL}{dc}\cdot \frac{dc}{da} \]</span></p>
<p>The forward case for this example is just an application of the product rule:</p>
<p><span class="math display">\[
g =bc
\]</span></p>
<p><span class="math display">\[
\frac{dg}{dx} =\frac{dg}{db}\cdot \frac{db}{dx} +\frac{dg}{dc}\cdot \frac{dc}{dx} = c\cdot \frac{db}{dx} +b \cdot \frac{dc}{dx}
\]</span></p>
<p>For reverse mode we need to expand an rearrange a bit:</p>
<p><span class="math display">\[
\frac{dL}{da} =\frac{dL}{dg}\cdot \frac{dg}{da}
\]</span></p>
<p><span class="math display">\[
\frac{dg}{da} =\frac{dg}{db}\cdot \frac{db}{da} +\frac{dg}{dc}\cdot \frac{dc}{da}
\]</span></p>
<p><span class="math display">\[
\frac{dL}{da} =\frac{dL}{dg}\bigg( \frac{dg}{db}\cdot \frac{db}{da} +\frac{dg}{dc}\cdot \frac{dc}{da} \bigg)
\]</span></p>
<p><span class="math display">\[
=\frac{dL}{dg} \frac{dg}{db} \frac{db}{da} + \frac{dL}{dg}\frac{dg}{dc} \frac{dc}{da}
\]</span></p>
<p><span class="math display">\[
=\frac{dL}{db}\cdot \frac{db}{da} + \frac{dL}{dc}\cdot \frac{dc}{da}
\]</span></p>
<p>This also works for addition:</p>
<p><span class="math display">\[
g =b + c
\]</span></p>
<p><span class="math display">\[
\frac{dg}{dx} =\frac{dg}{db}\cdot \frac{db}{dx} +\frac{dg}{dc}\cdot \frac{dc}{dx}
\]</span></p>
<p><span class="math display">\[
\frac{dg}{db}=1,\ \frac{dg}{dc}=1
\]</span></p>
<p><span class="math display">\[
\frac{dg}{dx} =\frac{db}{dx} + \frac{dc}{dx}
\]</span></p>
<p>And in general any binary operation! (Division, powers etc.).</p>
</section>
<section id="partial-and-total-derivatives" class="level2">
<h2 class="anchored" data-anchor-id="partial-and-total-derivatives">Partial and total derivatives</h2>
<p>So far weve been a bit sloppy in our discussion of derivatives. To see why, lets consider one more case:</p>
<p><span class="math display">\[
a = x^2
\]</span></p>
<p><span class="math display">\[
b=5a
\]</span></p>
<p><span class="math display">\[
c = a b
\]</span></p>
<p><span class="math display">\[
L=-c
\]</span></p>
<div class="cell" data-execution_count="14">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-15-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Saying that <span class="math inline">\(\frac{dc}{da}=b\)</span> isnt quite correct, because <span class="math inline">\(b\)</span> <em>also</em> depends on <span class="math inline">\(a\)</span>, really <span class="math inline">\(\frac{dc}{da} =\frac{dc}{da}+\frac{dc}{db}\frac{db}{da}\)</span>. We already account for this in our automatic differentiation though, so we want a way to talk about the derivative of an operation with respect to its inputs <em>ignoring how the inputs may depend on each other</em>.</p>
<p>This is where the notion of a <strong>partial derivative</strong> comes in, the <em>partial</em> <em>derivative</em> of function with respect to an input is the derivative ignoring any dependencies between inputs. Weve already seen how we denote this:</p>
<p><span class="math display">\[
\frac{\partial c}{\partial a} = b =5a
\]</span></p>
<p>The <strong>total derivative</strong> is the derivative where we do account for this. In our example:</p>
<p><span class="math display">\[
\frac{dc}{da} =\frac{\partial c}{\partial a}+\frac{\partial c}{\partial b}\frac{\partial b}{\partial a} = 5a + 5a = 10a
\]</span><br>
In our earlier examples, we typically had partial derivatives equal to total derivatives, so the distinction wasnt really important. This example shows why it is.</p>
<p>Lets see our earlier example, but this time well make the distinction between partial and total derivatives explicit</p>
<div class="cell" data-execution_count="15">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-16-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="16">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-17-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>This is also why we specify gradients in terms of partial derivatives! If were taking the gradient of a function with respect to multiple inputs, we dont know where these inputs come from. They might depend on each other! By specifying gradients at partial derivatives, we make it clear that were not accounting for that.</p>
</section>
<section id="implementing-reverse-mode-automatic-differentiation" class="level2">
<h2 class="anchored" data-anchor-id="implementing-reverse-mode-automatic-differentiation">Implementing reverse-mode automatic differentiation</h2>
<p>Well start by developing an automatic differentiation class that uses <em>reverse-mode automatic differentiation</em>, as this is what will be most useful for neural networks.</p>
<p>Recall that for reverse-mode AD to work, everytime we perform an operation on one or more numbers we need to store the result of that operation as well as the <em>parent values</em> (the inputs to the operation). We also need to be able to compute the derivative of that operation. Since for every operation we need to store several pieces of data and several functions, it makes sense to define a <em>class</em> to represent the result of an operation.</p>
<p>For example, if we want to make a class that represents the operation <code>c=a+b</code> our class needs several properties:</p>
<ul>
<li><code>value</code>: The value of the operation (<code>c</code>)</li>
<li><code>parents</code>: The parent operations (<code>a</code> and <code>b</code>)</li>
<li><code>grad</code>: The derivative of the final loss with respect to <code>c</code> (<span class="math inline">\(\frac{dL}{dc}\)</span>)</li>
<li><code>func</code>: A function that computes the operation (<code>a+b</code>)</li>
<li><code>grads</code>: A function that computes the derivatives of the operation (<span class="math inline">\(\frac{dc}{da}\)</span> and <span class="math inline">\(\frac{dc}{db}\)</span>)</li>
</ul>
<p>For this example, well call our class <code>AutogradValue</code>. This will be the base class for all of our possible operations and represents declaring a variable with a value (<code>a = 5</code>). This is useful because it lets us define values that we might want to find derivatives with respect to.</p>
<p>Lets see how this will work in practice. If we want to take derivatives we will first define the inputs using <code>AutogradValue</code>.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> AutogradValue(<span class="dv">5</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> AutogradValue(<span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Then we can perform whatever operations we want on these inputs:</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> a <span class="op">+</span> b</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> log(c)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Each of these operations will produce a new <code>AutogradValue</code> object representing the result of that operation.</p>
<p>Finally we can run the backward pass by running a method <code>backward()</code> (that we will write) on the outout <code>L</code>. This will compute the gradients of <code>L</code> with respect to each input that we defined (<span class="math inline">\(\frac{dL}{da}\)</span> and <span class="math inline">\(\frac{dL}{db}\)</span>). Rather than returning these derivatives, the <code>backward()</code> method will <em>update</em> the <code>grad</code> property of <code>a</code> and <code>b</code>, making it easy to access the correct derivative.</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>L.backward()</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>dL_da <span class="op">=</span> a.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Well also be able to compute operations with non-AutogradValue numbers, but obviously wont be able to compute derivaitives with respect to these values.</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> s <span class="op">*</span> a</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>dL_da <span class="op">=</span> a.grad <span class="co"># Will work because a is an AutogradValue</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>dL_ds <span class="op">=</span> s.grad <span class="co"># Will give an error because s is not an AutogradValue</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Lets look at one possible implementation for <code>AutogradValue</code>:</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AutogradValue:</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Base class for automatic differentiation operations. </span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Represents variable delcaration. Subclasses will overwrite </span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co">    func and grads to define new operations.</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Properties:</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co">        parents (list):  A list of the inputs to the operation, </span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co">                         may be AutogradValue or float</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="co">        args    (list):  A list of raw values of each </span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co">                         input (as floats)</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="co">        grad    (float): The derivative of the final loss with </span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="co">                         respect to this value (dL/da)</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="co">        value   (float): The value of the result of this operation</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, <span class="op">*</span>args):</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.parents <span class="op">=</span> <span class="bu">list</span>(args)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.args <span class="op">=</span> [arg.value <span class="cf">if</span> <span class="bu">isinstance</span>(arg, AutogradValue) </span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>                     <span class="cf">else</span> arg </span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>                     <span class="cf">for</span> arg <span class="kw">in</span> <span class="va">self</span>.parents]</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.grad <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.value <span class="op">=</span> <span class="va">self</span>.forward_pass()</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward_pass(<span class="va">self</span>):</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calls func to compute the value of this operation </span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.func(<span class="op">*</span><span class="va">self</span>.args)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For convenience, in this implementation weve also defined a property <code>args</code> which simply stores the <code>value</code> of each parent. Note that we can also allow parents to be either be <code>AutogradValue</code> or a primitive data type like <code>float</code>. This will allow us to do things like multiply an <code>AutogradValue</code> variable with a <code>float</code>, e.g.&nbsp;<code>a * 5</code>.</p>
<p>The <code>forward_pass</code> function computes the actual value of the node given its parents. This will depend on what kind of operation were doing (addition, subtraction, multiplication, etc.), so well define a <code>func</code> method that we can override that does the actual calculation. In the base case well just directly assign the value:</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AutogradValue:</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> func(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>        <span class="co">'''</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co">        Compute the value of the operation given the inputs.</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co">        For declaring a variable, this is just the identity </span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co">        function (return the input).</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co">            input (float): The input to the operation</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co">            value (float): The result of the operation</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co">        '''</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">input</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In subclasses well override <code>func</code>:</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> _square(AutogradValue):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Square operator (a ** 2)</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> func(<span class="va">self</span>, a):</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> a <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> _mul(AutogradValue):</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Multiply operator (a * b)</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> func(<span class="va">self</span>, a, b):</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> a <span class="op">*</span> b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Lets consider what the computational graph will look like for the following code:</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> AutogradValue(<span class="dv">2</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> x <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> <span class="dv">5</span> <span class="op">*</span> a</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> b <span class="op">*</span> a</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> <span class="op">-</span>c</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><img src="images/paste-1.png" class="img-fluid"></p>
<p>Here the blue nodes represent constants (of type <code>float</code>), while the black nodes are <code>AutogradValue</code> objects. We see that every <code>AutogradValue</code> is populated with the value, parents and args, but that after this first pass <code>grad</code> is still <code>0</code> for each object, so we have not computed <span class="math inline">\(\frac{dL}{dx}\)</span>. To do so, we need to run the backward pass by calling <code>L.backward()</code>.</p>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>L.backward()</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'dL_dx'</span>, x.grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In the backward pass, each node needs to update the <code>grad</code> property of its parents.</p>
<p><img src="images/paste-2.png" class="img-fluid"></p>
<p>So first <span class="math inline">\(L\)</span> needs to update the <code>grad</code> property for <span class="math inline">\(c\)</span>, which represents <span class="math inline">\(\frac{dL}{dc}\)</span>. Then <span class="math inline">\(c\)</span> is able to update the <code>grad</code> property of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> (<span class="math inline">\(\frac{dL}{da}\)</span> and <span class="math inline">\(\frac{dL}{db}\)</span>).</p>
<p><strong>Note that the order we perform these updates matters!</strong> <span class="math inline">\(\frac{dL}{da}\)</span> will not be correct until both <span class="math inline">\(b\)</span> <em>and</em> <span class="math inline">\(c\)</span> have updated <span class="math inline">\(a\)</span>, thus <span class="math inline">\(a\)</span> cannot perform the update to <span class="math inline">\(\frac{dL}{dx}\)</span> until both <span class="math inline">\(b\)</span> and <span class="math inline">\(c\)</span> have updated <span class="math inline">\(a\)</span>.</p>
<p>For each operation, we see that we also need to be able to compute the appropriate local derivatives of the value with respect to each input. For instance <span class="math inline">\(a\)</span> needs to be able to compute <span class="math inline">\(\frac{da}{dx}\)</span> , <span class="math inline">\(c\)</span> needs to be able to compute <span class="math inline">\(\frac{dc}{da}\)</span> and <span class="math inline">\(\frac{dc}{db}\)</span>. We will definite another method <code>grads</code> that can compute these values for a given operation and override it for each subclass. Since <code>grads</code> might need to compute multiple derivatives (as for multiplication or addition) well have it return a <code>tuple</code>.</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AutogradValue:</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> grads(<span class="va">self</span>, <span class="op">*</span>args):</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>        <span class="co">'''</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co">        Compute the derivative of the operation with respect to each input.</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co">        In the base case the derivative of the identity function is just 1. (da/da = 1).</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co">            input (float): The input to the operation</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co">            grads (tuple): The derivative of the operation with respect to each input</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="co">                            Here there is only a single input, so we return a length-1 tuple.</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co">        '''</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (<span class="dv">1</span>,)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> _square(AutogradValue):</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Square operator (a ** 2)</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> grads(<span class="va">self</span>, a):</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (<span class="dv">2</span> <span class="op">*</span> a,)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> _mul(AutogradValue):</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Multiply operator (a * b)</span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> grads(<span class="va">self</span>, a, b):</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (b, a)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>With this in hand we can write a function that performs the backward update for a given operation. Well call this method <code>backward_pass</code>.</p>
</section>
<section id="computational-graphs-of-vectors" class="level2">
<h2 class="anchored" data-anchor-id="computational-graphs-of-vectors">Computational graphs of vectors</h2>
<p>As weve seen, in the context of neural networks we typically perform operations on large collections of values such as vectors. For example, we might perform an element-wise square on a large vector:</p>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.ones((<span class="dv">500</span>,))</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> x <span class="op">**</span> <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In this case we are performing 500 individual square operations, so our computational graph would look like:</p>
<p><img src="images/paste-3.png" class="img-fluid" width="300"></p>
<p>Remember for our automatic differentiation implementation we would need an object for each of these values.</p>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([AutogradValue(<span class="dv">1</span>), AutogradValue(<span class="dv">1</span>), ...])</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> x <span class="op">**</span> <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Each of these object introduces some overhead as each object needs to be constructed individually and needs to store not just the value and gradients, but also the parents. Furthermore our backward pass needs to determine the order of nodes to visit. For a large neural network, having a node for every single value would be computationally very complex.</p>
<p>Since were performing the same operation on each entry of the vector, theres really no need to have a separate node for each entry. Therefore we might instead prefer each node in our computational graph to represent a <strong>vector</strong> or <strong>matrix</strong> and for our operations to correspond to vector or matrix operations:</p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> AutogradValue(np.ones((<span class="dv">500</span>,)))</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> x <span class="op">**</span> <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In this case the computational graph for this operation would be:</p>
<p><img src="images/paste-4.png" class="img-fluid" width="500"></p>
<p>Our derivative calculations can similarly be performed as element-wise operations:</p>
<p><span class="math display">\[
\begin{bmatrix} \frac{da_1}{dx_1} \\ \frac{da_2}{dx_2} \\ \vdots \\ \frac{da_{500}}{dx_{500}} \end{bmatrix} =
\begin{bmatrix} 2x_1 \\ 2x_2 \\ \vdots \\ 2x_{500} \end{bmatrix} = 2\mathbf{x}
\]</span></p>
<p>So in fact, for this case, we dont actually need to change our <code>AutogradValue</code> implementation for square at all!</p>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> _square(AutogradValue):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Square operator (a ** 2)</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> func(<span class="va">self</span>, a):</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> a <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Returns a vector of the element-wise derivatives!</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> grads(<span class="va">self</span>, a):</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (a <span class="op">**</span> <span class="dv">2</span>,)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="automatic-differentiation-for-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="automatic-differentiation-for-linear-regression">Automatic differentiation for linear regression</h2>
<p>Lets take a look at a concrete example how reverse-mode automatic differentiation with vectors would work. Specifically lets look at taking the gradient of the <strong>mean squared error</strong> loss we used for linear regression.</p>
<p><span class="math display">\[
L = \frac{1}{N}\sum_{i=1}^N (y_i - \mathbf{x}_i^T\mathbf{w})^2 = \frac{1}{N}\sum_{i=1}^N (\mathbf{y} - \mathbf{X}\mathbf{w})_i^2
\]</span></p>
<p>Here the right-hand side of the expression corresponds to how we might write this formula using numpy:</p>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>se <span class="op">=</span> (y <span class="op">-</span> np.dot(X, w)) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>mse <span class="op">=</span> np.<span class="bu">sum</span>(se) <span class="op">/</span> X.shape[<span class="dv">0</span>] </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We are interested in the gradient with respect to <span class="math inline">\(\mathbf{w}\)</span>, or <span class="math inline">\(\frac{dL}{d\mathbf{w}}\)</span> for optimizing our model. Lets start by looking at the simplest sequence of operations:</p>
<p><span class="math display">\[
\mathbf{a} = \mathbf{X}\mathbf{w}
\]</span></p>
<p><span class="math display">\[
\mathbf{b} = \mathbf{y} - \mathbf{a}
\]</span></p>
<p><span class="math display">\[
\mathbf{c} = \mathbf{b}^2
\]</span></p>
<p><span class="math display">\[
g = \sum_{i=1}^N c_i
\]</span></p>
<p><span class="math display">\[
L = \frac{1}{N}g
\]</span></p>
<p>We can setup our computational graph and reverse mode algorithm exactly as we did before, only in this case some of the operations will be on vectors! (<span class="math inline">\(g\)</span> and <span class="math inline">\(L\)</span> are still scalars, but <span class="math inline">\(\mathbf{w}\)</span>, <span class="math inline">\(\mathbf{a}\)</span>, <span class="math inline">\(\mathbf{b}\)</span> and <span class="math inline">\(\mathbf{c}\)</span> are vectors)</p>
<div class="cell" data-execution_count="32">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-33-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Lets walk through the computation of <span class="math inline">\(\frac{dL}{d\mathbf{w}}\)</span>.</p>
<p><strong>Computing</strong> <span class="math inline">\(\frac{dL}{dg}\)</span><strong>:</strong> The first step in the backward pass is straightforward:</p>
<p><span class="math display">\[
\frac{dL}{dg} = \frac{1}{N}
\]</span></p>
<p><strong>Computing</strong> <span class="math inline">\(\frac{dL}{d\mathbf{c}}\)</span><strong>:</strong></p>
<p><span class="math display">\[
\frac{dL}{d\mathbf{c}}= \frac{dL}{dg}\frac{dg}{d\mathbf{c}}
\]</span></p>
<p>Since <span class="math inline">\(\mathbf{c}\)</span> is a vector, <span class="math inline">\(\frac{dg}{d\mathbf{c}}\)</span> and <span class="math inline">\(\frac{dL}{d\mathbf{c}}\)</span> must be <em>vectors</em> of the derivative of <span class="math inline">\(L\)</span> or <span class="math inline">\(g\)</span> with respect to each entry of <span class="math inline">\(\mathbf{c}\)</span>. In other words <span class="math inline">\(\frac{dg}{d\mathbf{c}}\)</span> and <span class="math inline">\(\frac{dL}{d\mathbf{c}}\)</span> are <strong>gradients</strong>!</p>
<p><span class="math display">\[
\frac{dL}{d\mathbf{c}}= \begin{bmatrix} \frac{dL}{dc_1} \\ \frac{dL}{dc_2} \\ \vdots \\ \frac{dL}{dc_N}  \end{bmatrix} = \frac{dL}{dg} \begin{bmatrix} \frac{dg}{dc_1} \\ \frac{dg}{dc_2} \\ \vdots \\ \frac{dg}{dc_N}  \end{bmatrix} = \frac{dL}{dg}\frac{dg}{d\mathbf{c}}
\]</span></p>
<p>We know that the derivative of a sum with respect to a single element is <span class="math inline">\(1\)</span>, as in: <span class="math inline">\(\frac{d}{dc_1}(c_1+c_2)=1\)</span>, so it follows that the gradient of our summation is simply a vector of 1s.</p>
<p><span class="math display">\[
\frac{dg}{d\mathbf{c}} =\begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix}, \quad \frac{dL}{d\mathbf{c}} =\begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix}\frac{1}{N} = \begin{bmatrix} \frac{1}{N} \\ \frac{1}{N} \\ \vdots \\ \frac{1}{N} \end{bmatrix}
\]</span><br>
<strong>Computing</strong> <span class="math inline">\(\frac{dL}{d\mathbf{b}}\)</span><strong>:</strong></p>
<p>Since <span class="math inline">\(\mathbf{b}^2\)</span> is an element-wise operation <span class="math inline">\((c_i=b_i^2)\)</span> we know that:</p>
<p><span class="math display">\[
\frac{dL}{db_i}=\frac{dL}{dc_i}\frac{dc_i}{db_i} = \frac{1}{N}(2b_i)
\]</span></p>
<p>Therefore:</p>
<p><span class="math display">\[
\frac{dL}{d\mathbf{b}} = \begin{bmatrix} \frac{dL}{dc_1}\frac{dc_1}{db_1}  \\ \frac{dL}{dc_2}\frac{dc_2}{db_2}  \\ \vdots \\ \frac{dL}{dc_N}\frac{dc_N}{db_N}   \end{bmatrix} = \begin{bmatrix} \frac{2}{N}b_1  \\ \frac{2}{N}b_2  \\ \vdots \\ \frac{2}{N}b_N   \end{bmatrix}
\]</span></p>
<p><strong>Computing</strong> <span class="math inline">\(\frac{dL}{d\mathbf{a}}\)</span><strong>:</strong></p>
<p>Since <span class="math inline">\(\mathbf{y}-\mathbf{a}\)</span> is also an element-wise operation <span class="math inline">\((b_i=y_i-a_i)\)</span> we similarly see that:</p>
<p><span class="math display">\[
\frac{dL}{da_i}=\frac{dL}{db_i}\frac{db_i}{da_i} = \frac{2b_i}{N}(-1)
\]</span></p>
<p><span class="math display">\[
\frac{dL}{d\mathbf{a}} = \begin{bmatrix} \frac{dL}{db_1}\frac{db_1}{da_1}  \\ \frac{dL}{dc_2}\frac{db_2}{da_2}  \\ \vdots \\ \frac{dL}{dc_N}\frac{db_N}{da_N}   \end{bmatrix} = \begin{bmatrix} \frac{-2}{N}b_1  \\ \frac{-2}{N}b_2  \\ \vdots \\ \frac{-2}{N}b_N   \end{bmatrix}
\]</span></p>
<p><strong>Computing</strong> <span class="math inline">\(\frac{dL}{d\mathbf{w}}\)</span><strong>:</strong></p>
<p>This last calculation is less straightforward. <span class="math inline">\(\mathbf{X}\mathbf{w}\)</span> is <strong>not</strong> an element-wise operation, so we cant just apply the chain rule element-wise. We need a new approach! Lets break down the general problem that we see here.</p>
</section>
<section id="vector-valued-functions" class="level2">
<h2 class="anchored" data-anchor-id="vector-valued-functions">Vector-valued functions</h2>
<p>A <strong>vector-valued function</strong> is a function that takes in a vector and <em>returns</em> a vector:</p>
<p><span class="math display">\[
\mathbf{y} = f(\mathbf{x}), \quad f: \mathbb{R}^n\rightarrow\mathbb{R}^m
\]</span></p>
<p>For example the matrix-vector product weve just seen is a simple vector-valued function:</p>
<p><span class="math display">\[
f(\mathbf{w}) =\mathbf{X}\mathbf{w}
\]</span></p>
<p>If <span class="math inline">\(\mathbf{X}\)</span> is an <span class="math inline">\((N\times d)\)</span> matrix and <span class="math inline">\(\mathbf{w}\)</span> is a length- <span class="math inline">\(d\)</span> vector, then <span class="math inline">\(f(\mathbf{w})=\mathbf{X}\mathbf{w}\)</span> is a mapping <span class="math inline">\(\mathbb{R}^d\rightarrow \mathbb{R}^N\)</span></p>
</section>
<section id="jacobians" class="level2">
<h2 class="anchored" data-anchor-id="jacobians">Jacobians</h2>
<p>A the <strong>Jacobian</strong> <span class="math inline">\((\mathbf{J})\)</span> of a vector-valued function is the matrix of partial derivatives of every output with respect to every input. We can think of it as an extension of the gradient for vector-valued functions. If we have a vector-valued function <span class="math inline">\(f\)</span> and <span class="math inline">\(\mathbf{a}\)</span> is the result of applying <span class="math inline">\(f\)</span> to <span class="math inline">\(\mathbf{w}\)</span>:</p>
<p><span class="math display">\[
\mathbf{a}=f(\mathbf{w}), \quad f:\mathbb{R}^d\rightarrow \mathbb{R}^N
\]</span></p>
<p>The corresponding Jacobian is:</p>
<p><span class="math display">\[\frac{d\mathbf{a}}{d\mathbf{w}} = \begin{bmatrix} \frac{\partial a_1}{\partial w_1} &amp; \frac{\partial a_1}{\partial w_2}&amp; \dots&amp; \frac{\partial a_1}{\partial w_d} \\
\frac{\partial a_2}{\partial w_1} &amp; \frac{\partial a_2}{\partial w_2}&amp; \dots&amp; \frac{\partial a_2}{\partial w_d} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial a_N}{\partial w_1} &amp; \frac{\partial a_N}{\partial w_2}&amp; \dots&amp; \frac{\partial a_N}{\partial w_d} \\
\end{bmatrix}\]</span></p>
<p>In general:</p>
<p><span class="math display">\[
\bigg(\frac{d\mathbf{a}}{d\mathbf{w}}\bigg)_{ij} = \frac{\partial a_i}{\partial w_j}
\]</span></p>
<p>Lets consider the Jacobian of a matrix-vector product:</p>
<p><span class="math display">\[
\mathbf{a} = \mathbf{X}\mathbf{w}
\]</span></p>
<p>We can find each entry in the Jacobian by taking the corresponding partial derivative.</p>
<p><span class="math display">\[
a_i =\sum_{j=1}^d X_{ij}w_j, \quad \frac{\partial a_i}{\partial w_j}=X_{ij}
\]</span></p>
<p>In this case we see that the Jacobian is just <span class="math inline">\(\mathbf{X}\)</span>!</p>
</section>
<section id="vector-jacobian-products" class="level2">
<h2 class="anchored" data-anchor-id="vector-jacobian-products">Vector-Jacobian products</h2>
<p>Lets return to our linear regression example:</p>
<div class="cell" data-execution_count="33">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-34-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>We see that <span class="math inline">\(\frac{d\mathbf{a}}{d\mathbf{w}}\)</span> is actually a Jacobian that we now know how to compute, so the remaining question is how to combine it with <span class="math inline">\(\frac{dL}{d\mathbf{a}}\)</span> in order to get the gradient were looking for <span class="math inline">\(\frac{dL}{d\mathbf{w}}\)</span>? The answer turns out to be simple: use the vector-matrix product:</p>
<p><span class="math display">\[
\frac{dL}{d\mathbf{w}} = \frac{dL}{d\mathbf{a}}^T\frac{d\mathbf{a}}{d\mathbf{w}}
\]</span></p>
<p>We might be curious why this is the right thing to do as opposed to a matrix-vector product or some other function. To see why, lets consider what an entry of our final gradient <span class="math inline">\(\frac{dL}{d\mathbf{w}}\)</span> should be. Weve previously seen that when a value is use by more than one child operation, we need to <em>sum</em> the contribution of each child to the total derivative. So in this case, for a given entry of <span class="math inline">\(\mathbf{w}\)</span> we need to sum the gradient contribution from every element of <span class="math inline">\(\mathbf{a}\)</span>:</p>
<p><span class="math display">\[
\frac{dL}{dw_j} = \frac{dL}{da_1} \frac{da_1}{dw_j}+\frac{dL}{da_2} \frac{da_2}{dw_j}+...\frac{dL}{da_N} \frac{da_N}{dw_j} = \sum_{i=1}^N \frac{dL}{da_i} \frac{da_i}{dw_j}
\]</span></p>
<p>Which we can see is equivalent to an entry in the vector matrix product:</p>
<p><span class="math display">\[
\frac{dL}{d\mathbf{w}} = \begin{bmatrix} \frac{dL}{da_1} &amp; \frac{dL}{da_2} &amp; ... &amp; \frac{dL}{da_N} \end{bmatrix}\begin{bmatrix} \frac{\partial a_1}{\partial w_1} &amp; \frac{\partial a_1}{\partial w_2}&amp; \dots&amp; \frac{\partial a_1}{\partial w_d} \\
\frac{\partial a_2}{\partial w_1} &amp; \frac{\partial a_2}{\partial w_2}&amp; \dots&amp; \frac{\partial a_2}{\partial w_d} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial a_N}{\partial w_1} &amp; \frac{\partial a_N}{\partial w_2}&amp; \dots&amp; \frac{\partial a_N}{\partial w_d} \\
\end{bmatrix} = \frac{dL}{d\mathbf{a}}^T\frac{d\mathbf{a}}{d\mathbf{w}}
\]</span></p>
<p>We call this a <strong>vector-Jacobian product</strong> or VJP for short. We see that because its simply derived from our basic gradient rules, its valid for <em>any</em> vector-valued operation, as long as we can compute the Jacobian! We can use this to perform the final step in the backward pass for the MSE.</p>
<p><strong>Computing</strong> <span class="math inline">\(\frac{dL}{d\mathbf{w}}\)</span><strong>:</strong></p>
<p>From our <em>vector-Jacobian product</em> rule we know that:</p>
<p><span class="math display">\[
\frac{dL}{d\mathbf{w}} = \frac{dL}{d\mathbf{a}}^T\frac{d\mathbf{a}}{d\mathbf{w}}= \frac{-2}{N}\mathbf{b}^T\mathbf{X}
\]</span></p>
<p>If we substitute back in <span class="math inline">\(\mathbf{b}=\mathbf{y}-\mathbf{X}\mathbf{w}\)</span> we see that this is equivalent to the gradient we derived in a previous class!</p>
<p><span class="math display">\[ \frac{dL}{d\mathbf{w}} = \frac{-2}{N}(\mathbf{y}-\mathbf{X}\mathbf{w})^T\mathbf{X} \]</span></p>
</section>
<section id="vjps-for-element-wise-operations" class="level2">
<h2 class="anchored" data-anchor-id="vjps-for-element-wise-operations">VJPs for element-wise operations</h2>
<p>Its worth noting that element-wise operations are still vector-valued functions.</p>
<p><span class="math display">\[
\mathbf{c}=\mathbf{b}^2, \quad \mathbb{R}^n\rightarrow\mathbb{R}^n
\]</span></p>
<p>So why didnt we need to do a vector-Jaobian product for that operation? The answer is that we did, just implicitly! If we consider the Jacobian for this operation we see that because each entry of <span class="math inline">\(\mathbf{c}\)</span> only depends on the corresponding entry of <span class="math inline">\(\mathbf{b}\)</span>, the Jacobian for this operation is <span class="math inline">\(0\)</span> everywhere except the main diagonal:</p>
<p><span class="math display">\[
\frac{d\mathbf{c}}{d\mathbf{b}} = \begin{bmatrix} \frac{\partial c_1}{\partial b_1} &amp; 0 &amp; \dots&amp; 0 \\
0 &amp; \frac{\partial c_2}{\partial b_2}&amp; \dots&amp; 0\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0&amp; 0&amp; \dots&amp; \frac{\partial c_N}{\partial b_d} \\
\end{bmatrix}
\]</span></p>
<p>This means that we can write the vector-Jacobian product as we did before:</p>
<p><span class="math display">\[
\frac{dL}{d\mathbf{c}}\frac{d\mathbf{c}}{d\mathbf{b}}=\begin{bmatrix} \frac{dL}{dc_1}\frac{dc_1}{db_1}  \\ \frac{dL}{dc_2}\frac{dc_2}{db_2}  \\ \vdots \\ \frac{dL}{dc_N}\frac{dc_N}{db_N}   \end{bmatrix}
\]</span></p>
<p>This is a big computational savings over explicitly constructing the full Jacobian and performing a vector-matrix multiplication.</p>
</section>
<section id="vjps-for-matrices" class="level2">
<h2 class="anchored" data-anchor-id="vjps-for-matrices">VJPs for matrices</h2>
<p>So far weve seen VJPs with respect to vectors. What if in the formulation above, we were to take the derivative with respect to <span class="math inline">\(\mathbf{X}\)</span> instead of <span class="math inline">\(\mathbf{w}\)</span>?</p>
<p><span class="math display">\[
\mathbf{a}=\mathbf{X}\mathbf{w},\quad \frac{d\mathbf{a}}{d\mathbf{X}}
\]</span></p>
<p>In this case <span class="math inline">\(\mathbf{X}\)</span> is an <span class="math inline">\(N\times d\)</span> matrix, so if we were to consider all the partial derivatives making up the Jacobian <span class="math inline">\(\frac{d\mathbf{a}}{d\mathbf{X}}\)</span> there would be <span class="math inline">\(N\times N\times d\)</span> values, while the gradient <span class="math inline">\(\frac{dL}{d\mathbf{X}}\)</span> is itself an <span class="math inline">\(N\times d\)</span> matrix of the derivative of <span class="math inline">\(L\)</span> with respect to each value of <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>Lets look at how we can formulate the vector-Jacobian product:</p>
<p><span class="math display">\[
\frac{dL}{d\mathbf{X}} = \frac{dL}{d\mathbf{a}}^T\frac{d\mathbf{a}}{d\mathbf{X}}
\]</span></p>
<p>We know that for a given entry of <span class="math inline">\(\frac{dL}{d\mathbf{X}}\)</span>, we can again compute the derivative by summing the contribution from each child, in this case every entry of <span class="math inline">\(\mathbf{a}\)</span></p>
<p><span class="math display">\[
\frac{dL}{dX_{jk}} = \sum_{i=1}^N \frac{\partial L}{\partial a_{i}} \frac{\partial a_i}{\partial X_{jk}}
\]</span></p>
<p>This suggests that we can compute the VJP by <em>flattening</em> the Jacobian from an <span class="math inline">\(N\times N \times d\)</span> structure into a <span class="math inline">\(N \times Nd\)</span> matrix. Therefore performing the vector-matrix product will give us a length <span class="math inline">\(Nd\)</span> vector with the appropriate values for us to reshape into our <span class="math inline">\(N \times d\)</span> Jacobian <span class="math inline">\(\frac{dL}{d\mathbf{X}}\)</span>.</p>
<p>In code this could look like:</p>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>dL_da, da_dx  <span class="co"># The computed gradients/jacobians</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>da_dx <span class="op">=</span> da_dx.reshape((da_dx.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>dL_dx <span class="op">=</span> np.dot(dL_da, da_dx)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>dL_dx <span class="op">=</span> dL_dx.reshape(x.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Lets return to our example:</p>
<p><span class="math display">\[\mathbf{a}=\mathbf{X}\mathbf{w}\]</span></p>
<p>Do we need to instantiate the full Jacobian here? <strong>No!</strong> In our original operation:</p>
<p><span class="math display">\[
a_i = \sum_{k=1}^d X_{ik}w_k
\]</span></p>
<p>Thus <span class="math inline">\(\frac{\partial a_i}{\partial X_{jk}}\)</span> is only nonzero when <span class="math inline">\(j=i\)</span>. Taking the derivative we get:</p>
<p><span class="math display">\[
\frac{\partial a_i}{\partial X_{jk}} = \frac{\partial}{\partial X_{jk}}\sum_{k=1}^d X_{ik}w_k = \mathbb{I}(i=j)w_k
\]</span></p>
<p>We can therefore write the vector-Jacobian product as:</p>
<p><span class="math display">\[
\frac{\partial L}{\partial X_{ik}}=\frac{\partial L}{\partial a_i}\frac{\partial a_i}{\partial X_{ik}} = \frac{\partial L}{\partial a_i}w_k
\]</span></p>
<p>In code we could write this as:</p>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>w_mat <span class="op">=</span> w.reshape((<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="co"># reshape w to 1 x d</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>dL_da_mat <span class="op">=</span> dL_da.reshape((<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)) <span class="co"># reshape dL_da to N x 1</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>dL_dx <span class="op">=</span> w_mat <span class="op">*</span> dL_da_mat <span class="co"># dL_dx becomes N x d, entry ik = dL_da_i * w_k</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="forward-mode-ad-with-vectors" class="level2">
<h2 class="anchored" data-anchor-id="forward-mode-ad-with-vectors">Forward mode AD with vectors</h2>
<p>What about forward mode for our linear regression example?</p>
<p><span class="math display">\[
\mathbf{a} = \mathbf{X}\mathbf{w}
\]</span></p>
<p><span class="math display">\[
\mathbf{b} = \mathbf{y} - \mathbf{a}
\]</span></p>
<p><span class="math display">\[
\mathbf{c} = \mathbf{b}^2
\]</span></p>
<p><span class="math display">\[
g = \sum_{i=1}^N c_i
\]</span></p>
<p><span class="math display">\[
L = \frac{1}{N}g
\]</span></p>
<div class="cell" data-execution_count="36">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-37-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>As weve seen <span class="math inline">\(\frac{d\mathbf{a}}{d\mathbf{w}}\)</span> is an <span class="math inline">\(N\times d\)</span> Jacobian matrix. After computing this, the next step will be to compute:</p>
<p><span class="math display">\[\frac{d\mathbf{b}}{d\mathbf{w}}=\frac{d\mathbf{b}}{d\mathbf{a}}\frac{d\mathbf{a}}{d\mathbf{w}}\]</span></p>
<p>Here we are multiplying the <span class="math inline">\(N\times N\)</span> Jacobian <span class="math inline">\(\frac{d\mathbf{b}}{d\mathbf{a}}\)</span> with the <span class="math inline">\(N\times d\)</span> gradient <span class="math inline">\(\frac{d\mathbf{a}}{d\mathbf{w}}\)</span> to get the <span class="math inline">\(N\times d\)</span> gradient <span class="math inline">\(\frac{d\mathbf{b}}{d\mathbf{w}}\)</span>. We can again verify that this is the correct computation by checking an individual element of <span class="math inline">\(\frac{d\mathbf{b}}{d\mathbf{w}}\)</span>:</p>
<p><span class="math display">\[
\frac{db_i}{dw_j}=\sum_{k=1}^N \frac{db_i}{da_k}\frac{da_k}{dw_j}
\]</span></p>
<p>We call this operation a <strong>Jacobian-vector product</strong> or JVP.</p>


</section>
</section>

</main> <!-- /main -->
<script type="ojs-module-contents">
{"contents":[{"methodName":"interpret","cellName":"ojs-cell-1","inline":false,"source":"Plot = import(\"https://esm.sh/@observablehq/plot\") \nd3 = require(\"d3@7\")\ntopojson = require(\"topojson\")\nMathJax = require(\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-svg.min.js\").catch(() => window.MathJax)\ntf = require(\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js\").catch(() => window.tf)\n\nTHREE = {\n  const THREE = window.THREE = await require(\"three@0.130.0/build/three.min.js\");\n  await require(\"three@0.130.0/examples/js/controls/OrbitControls.js\").catch(() => {});\n  await require(\"three@0.130.0/examples/js/loaders/SVGLoader.js\").catch(() => {});\n  return THREE;\n}\n\nfunction sample(f, start, end, nsamples=100) {\n  let arr = [...Array(nsamples).keys()]\n  let dist = end - start\n  function arrmap(ind) {\n    const x = (ind * dist) / nsamples + start;\n    return [x, f(x)];\n  }\n  return arr.map(arrmap)\n}\n\nfunction sigmoid(x){\n  return 1 / (1 + Math.exp(-x));\n}\n\nfunction sum(x) {\n  let s = 0;\n  for (let i = 0; i < x.length; i++ ) {\n    s += x[i];\n  }\n  return s;\n}\n\nfunction mean(x) {\n  let s = 0;\n  for (let i = 0; i < x.length; i++ ) {\n    s += x[i];\n  }\n  return s / x.length;\n}\n\nfunction cross_ent(x, y) {\n  return y ? -Math.log(sigmoid(x)) : -Math.log(sigmoid(-x));\n}\n\nfunction se(x, y) {\n  return (x - y) * (x - y);\n}\n\nfunction shuffle(array) {\n  let currentIndex = array.length,  randomIndex;\n\n  // While there remain elements to shuffle.\n  while (currentIndex > 0) {\n\n    // Pick a remaining element.\n    randomIndex = Math.floor(Math.random() * currentIndex);\n    currentIndex--;\n\n    // And swap it with the current element.\n    [array[currentIndex], array[randomIndex]] = [\n      array[randomIndex], array[currentIndex]];\n  }\n\n  return array;\n}\n\nfunction acc(x, y) {\n  return Number(y == (x  > 0));\n}\n\nfunction grid_func(f, width, height, x1, y1, x2, y2) {\n  let values = new Array(width * height);\n  const xstride = (x2 - x1) / width;\n  const ystride = (y2 - y1) / height;\n\n  \n  let y = 0;\n  let x = 0;\n  let ind = 0;\n  for (let i = 0; i < height; i++ ) {\n    for (let j = 0; j < width; j++, ind++) {\n      x = x1 + j * xstride;\n      y = y1 + i * ystride;\n      values[ind] = f(x, y);\n    }\n  }\n  return {width: width, height: height, x1: x1, y1: y1, x2: x2, y2: y2, values: values};\n}\n\nfunction get_accessors(keys, byindex=false) {\n  let isString = value => typeof value === 'string';\n  \n  let index = 0;\n  let indexmap = {};\n  let accessors = [];\n  for (let i = 0; i < keys.length; i++){\n    let k = keys[i];\n    if (Array.isArray(k)) {\n      let access = isString(k[0]) ? (x => x[k[0]]) : k[0];\n      \n      if (byindex) {\n        if (isString(k[0]) && !(k[0] in indexmap)) {\n          indexmap[k[0]] = index;\n          index++;\n        }\n        let accessindex = indexmap[k[0]];\n        access = x => x[accessindex];\n        let process = k[1];\n        let final_access = x => process(access(x));\n        accessors.push(final_access);\n      }\n      else {\n        let process = k[1];\n        let final_access = x => process(access(x));\n        accessors.push(final_access);\n      }\n      \n    }\n    else {\n      let access = isString(k) ? (x => x[k]) : k;\n      if (byindex) { \n        if (isString(k) && !(k in indexmap)) {\n          indexmap[k] = index;\n          index++;\n        }\n        let accessindex = indexmap[k];\n        access = x => x[accessindex];\n      }\n      accessors.push(access); \n    }\n  }\n  return accessors;\n}\n\nfunction predict(obs, weights, keys=[\"0\", \"1\", \"2\", \"3\"], byindex=false) {\n  let isString = value => typeof value === 'string';\n  let accessors = get_accessors(keys, byindex);\n  \n  let output = weights[0];\n  let wi = 1;\n  for (let i = 0; (i < keys.length) && (wi < weights.length); i++, wi++){\n    output += weights[wi] * accessors[i](obs);\n  }\n  return output;\n}\n\nfunction mean_loss(f, data, weights, keys, label, l2=0) {\n  let reg = 0;\n  if (l2 > 0){\n    for (let i = 1; i < weights.length; i++) {\n      reg += weights[i] * weights[i];\n    }\n  }\n  \n  const isString = value => typeof value === 'string';\n  const get_label = isString(label) ? (x => x[label]) : label;\n  return mean(data.map(x => f(predict(x, weights, keys), get_label(x)))) + l2 * reg;\n}\n\nfunction get_domains(data, accessors, margin=0.1) {\n  let domains = [];\n  for (let i = 0; i < accessors.length; i++){\n    let xdomain = d3.extent(data, accessors[i]);\n    let xdsize = (xdomain[1] - xdomain[0]);\n    let xmin = xdomain[0] - xdsize * margin;\n    let xmax = xdomain[1] + xdsize * margin;\n    domains.push([xmin, xmax]);\n  }\n  return domains;\n}\n\nfunction logisticPlot2d(data, weights, keys, label, interval=0.05) {\n  const accuracy = mean_loss(acc, data, weights, keys, label);\n  \n  let isString = value => typeof value === 'string';\n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, accessors);\n  const get_label = isString(label) ? (x => x[label]) : label;\n  \n  return Plot.plot({\n    x: {tickSpacing: 80, label: \"x\"},\n    y: {tickSpacing: 80, label: \"y\"},\n    title: \"Accuracy: \" + accuracy.toFixed(3),\n    color: {type: \"linear\", legend: true, scheme: \"BuRd\", domain: [-0.5, 1.5]},\n    marks: [\n      Plot.contour({\n        fill: (x, y) => sigmoid(predict([x, y], weights, index_accessors)),\n        x1: domains[0][0], y1: domains[1][0], x2: domains[0][1], y2: domains[1][1], interval: interval,\n      }),\n      Plot.dot(data, {x: accessors[0], y: accessors[1], stroke: x=> (get_label(x) ? 1.35 : -0.35)})\n    ]\n  });\n}\n\nfunction logisticLossPlot2d(data, weights, keys, label) {\n  const loss = mean_loss(cross_ent, data, weights, keys, label);\n  \n  let isString = value => typeof value === 'string';\n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, accessors);\n  const get_label = isString(label) ? (x => x[label]) : label;\n  \n  return Plot.plot({\n    x: {tickSpacing: 80, label: \"x\"},\n    y: {tickSpacing: 80, label: \"y\"},\n    title: \"Loss: \" + loss.toFixed(3),\n    color: {type: \"linear\", legend: true, scheme: \"BuRd\", domain: [0, 5]},\n    marks: [\n      Plot.contour({\n        value: (x, y) => predict([x, y], weights, index_accessors),\n        fillOpacity: 0.2,\n        stroke: \"black\", x1: domains[0][0], y1: domains[1][0], x2: domains[0][1], y2: domains[1][1],\n        thresholds: [-1e6,  0, 0.00001]\n      }),\n      Plot.dot(data, {x: accessors[0], y: accessors[1], stroke: x=> cross_ent(predict(x, weights, keys), get_label(x)), \n                      strokeOpacity: 0.5 })\n    ]\n  });\n}\n\nfunction lossPlot2d(f, data, keys, label, l2=0, res=100, x1=-40, y1=-0.015, x2=40,  y2=0.015, vmax=50, nlines=25, ctype=\"sqrt\", scale=(x => x)) {\n  let grid = 0;\n  function lossFunc(w, b) {\n    return scale(mean_loss(f, data, [w, b], keys, label, l2));\n  }\n\n  grid = grid_func(lossFunc,\n                 res, res, x1, y1, x2, y2\n                );\n\n  function plot2d(weights) {\n    let w = weights;\n    if (!(Array.isArray(w[0]))){\n      w = [w];\n    }\n\n    var arrows = w.slice(0, w.length - 1).map(function(e, i) {\n      return e.concat(w[i+1]);\n    });\n\n    let interval= vmax / nlines; \n    let thresholds = [];\n    for (let i = 0; i < nlines; i++) {\n      thresholds.push(i * interval);\n    }\n    let loss = mean_loss(f, data, w[w.length - 1], keys, label, l2)\n    return Plot.plot({\n      title: \"Loss: \" + loss.toFixed(3),\n      color: {type: \"linear\", legend: true, label: \"Loss\", scheme: \"BuRd\", domain: [0, vmax], type: ctype},\n      marks: [\n        Plot.contour(grid.values, {width: grid.width, height: grid.height, x1: grid.x1, x2:grid.x2, y1: grid.y1, y2: grid.y2,\n          stroke: Plot.identity, thresholds: thresholds}),\n        Plot.dot(w),\n        Plot.arrow(arrows, {x1: \"0\", y1: \"1\", x2: \"2\", y2: \"3\", stroke: \"black\"})\n      ]\n    })\n  }\n  return plot2d;\n}\n\n\n\nfunction regressionPlot(data, weights, keys, label, l2, f=se, stroke=\"\") {\n  let loss = mean_loss(f, data, weights, keys, label, l2);\n  let isString = value => typeof value === 'string';\n  \n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, get_accessors([label].concat(keys)));\n  const get_label = isString(label) ? (x => x[label]) : label;\n\n  let stroke_shade = stroke;\n  if (stroke == \"\") {\n    stroke_shade = (x => f(predict(x, weights, keys), get_label(x)))\n  }\n  \n  return Plot.plot({\n    y: {domain: domains[0]},\n    title: \"Loss: \" + loss.toFixed(3),\n    color: {type: \"linear\", legend: true, label: \"Loss\", scheme: \"BuRd\", domain: [0, 100]},\n    marks: [\n      Plot.line(sample((x) => predict([x], weights, index_accessors), domains[1][0], domains[1][1]), {stroke: 'black'}),\n      Plot.dot(data, {x: accessors[0], y: get_label, stroke: stroke_shade })\n    ]\n  })\n}\n\nfunction errorPlot(data, weights, keys, label, f, options={}) {\n  const isString = value => typeof value === 'string';\n  const get_label = isString(label) ? (x => x[label]) : label;\n  let errors = data.map(x => [predict(x, weights, keys) - get_label(x), f(predict(x, weights, keys), get_label(x))]);\n\n  \n  let sigma = (options['sigma'] || 1);\n  let plots = [];\n  const xdomain = (options['xdomain'] || [-30, 30]);\n  const ydomain = (options['ydomain'] || [0, 0.1]);\n  \n\n  if (options['plotnormal']){\n    let pdf = x => Math.exp(-0.5 * x * x / sigma) * ydomain[1];\n    let normal = Plot.line(sample(pdf, xdomain[0], xdomain[1]), {stroke: 'crimson'});    \n    plots.push(normal);\n  }\n  if (options['plotlaplace']){\n    let pdf = x => Math.exp(-0.5 * Math.abs(x) / sigma) * ydomain[1];\n    let normal = Plot.line(sample(pdf, xdomain[0], xdomain[1]), {stroke: 'green'});    \n    plots.push(normal);\n  }\n  \n  return Plot.plot({\n    y: {grid: true, domain: ydomain},\n    x: {domain: xdomain},\n    color: {type: \"linear\", legend: true, label: \"Loss\", scheme: \"BuRd\", domain: [0, 100]},\n    marks: [\n      //Plot.rectY(errors, Plot.binX({y: \"count\", fill: x => mean(x.map(v => v[1]))}, {x: \"0\"})),\n      \n      Plot.rectY(errors, Plot.binX({y: \"proportion\"}, {x: \"0\", fill: 'steelblue', interval: 1})),\n      Plot.ruleY([0])\n    ].concat(plots)\n  })\n}\n\nfunction nnPlot(data, weights, keys, label, l2, f=se, stroke=\"\", options=[]) {\n  let loss = mean_loss(f, data, weights, keys, label, l2);\n  let isString = value => typeof value === 'string';\n  \n  let accessors = get_accessors(keys);\n  let index_accessors = get_accessors(keys, true);\n  let domains = get_domains(data, get_accessors([label].concat(keys)));\n  const get_label = isString(label) ? (x => x[label]) : label;\n\n  let stroke_shade = stroke;\n  if (stroke == \"\") {\n    stroke_shade = (x => f(predict(x, weights, keys), get_label(x)))\n  }\n\n  let a = []\n  if (options.indexOf(\"Show feature transforms\") >= 0){\n    a = [Plot.line(sample((x) =>  keys[1][1](x), domains[1][0], domains[1][1]), {stroke: 'red'}),\n      Plot.line(sample((x) => keys[2][1](x), domains[1][0], domains[1][1]), {stroke: 'blue'})]\n  }\n  \n  return Plot.plot({\n    y: {domain: domains[0]},\n    title: \"Loss: \" + loss.toFixed(3),\n    color: {type: \"linear\", legend: true, label: \"Loss\", scheme: \"BuRd\", domain: [0, 100]},\n    marks: [\n      Plot.line(sample((x) => predict([x], weights, index_accessors), domains[1][0], domains[1][1]), {stroke: 'black'}),\n      Plot.dot(data, {x: accessors[0], y: get_label, stroke: stroke_shade })\n    ].concat(a)\n  })\n}\n"}]}
</script>
<script type="module">
if (window.location.protocol === "file:") { alert("The OJS runtime does not work with file:// URLs. Please use a web server to view this document."); }
window._ojs.paths.runtimeToDoc = "../../lecture6-backpropagation";
window._ojs.paths.runtimeToRoot = "../..";
window._ojs.paths.docToRoot = "..";
window._ojs.selfContained = false;
window._ojs.runtime.interpretFromScriptTags();
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>