{"cells":[{"cell_type":"markdown","metadata":{"id":"lRUN_ZZtPMxd"},"source":["# **Homework 3:** Logistic regression and feature transforms"]},{"cell_type":"markdown","metadata":{"id":"L5neuxSzPMxf"},"source":["### Collaborators\n","\n","Please list anyone you discussed or collaborated on this assignment with below."]},{"cell_type":"markdown","metadata":{"id":"M_TUWXgcPMxg"},"source":["LIST COLLABORATORS HERE"]},{"cell_type":"markdown","metadata":{"id":"8pPxN3eyPMxg"},"source":["### Course feedback\n","\n","Please submit this week's course survey here: https://forms.gle/PC3AYmqanDChmqCDA"]},{"cell_type":"markdown","metadata":{"id":"3SRoUqooPMxh"},"source":["# Important notes\n","\n","**This homework requires installing additional python libraries. You can install all the nessecary dependencies by running: `pip install -r requirements.txt` in the homework directory. If using Colab, this step should not be needed.**"]},{"cell_type":"markdown","metadata":{"id":"_4f3AEG_PMxh"},"source":["In this homework, we will use the following convention for dimentionality:\n","\n","$N:\\quad\\text{Number of observations in a dataset, so } \\mathcal{D} = \\{ (\\mathbf{x}_1, y_1),\\ (\\mathbf{x}_2, y_2),\\ ... \\,(\\mathbf{x}_N, y_N) \\}$\n","\n","$d:\\quad\\ \\text{Dimension of input (number of features), so } \\mathbf{x}_i \\in \\mathbb{R}^d$\n","\n","$C: \\quad\\ \\text{Number of classes, so } y_i \\in \\{1,...,C\\}$"]},{"cell_type":"code","source":["# Uncomment and run if using Colab!\n","\n","#import urllib.request\n","#remote_url = 'https://gist.githubusercontent.com/gabehope/2f0c3af9eed3b910037df98e21c6c035/raw/925ff6b8ad5dcc2c17674b6ec609d854a811c453/hw2_code.py'\n","#with urllib.request.urlopen(remote_url) as remote, open('hw2_code.py', 'w') as local:\n","#  [local.write(str(line, encoding='utf-8')) for line in remote]"],"metadata":{"id":"XvCGbbpVPPcQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZwE-WRQbPMxh"},"outputs":[],"source":["# Run me first!\n","import autograd.numpy as np\n","import matplotlib.pyplot as plt\n","from hw2_code import get_dataset, gradient_descent, test_nll, test_nll_grad, test_predict, test_predict_probability, test_softmax, test_split\n"]},{"cell_type":"markdown","metadata":{"id":"eRYzqdHpPMxi"},"source":["# Background"]},{"cell_type":"markdown","metadata":{"id":"JsmqIGwyPMxj"},"source":["In class we derived the logistic regression model for making predictions on binary data. Recall the the prediction function for logistic regression can be written as:\n","\n","$$f(\\mathbf{x}) = \\mathbb{I}(\\mathbf{x}^T\\mathbf{w} \\geq 0)$$\n","\n","The estimated probability of $y=1$ as:\n","$$p(y=1\\mid \\mathbf{x}, \\mathbf{w}) = \\sigma(\\mathbf{x}^T\\mathbf{w})$$\n","\n","Also recall that the negative log-likelihood loss for logistic regression can be written as:\n","\n","$$\\mathbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})=-\\sum_{i=1}^N \\log \\sigma\\big((2 y_i - 1)\\mathbf{x}_i^T\\mathbf{w}\\big)$$\n","\n","and it's gradient with respect to $\\mathbf{w}$ is:\n","$$\\nabla_{\\mathbf{w}}\\mathbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})=-\\sum_{i=1}^N \\big(1 - \\sigma((2 y_i - 1)\\mathbf{x}_i^T\\mathbf{w})\\big) \\big(2 y_i - 1\\big)\\mathbf{x}_i$$\n"]},{"cell_type":"markdown","metadata":{"id":"Ijj6fTOJPMxj"},"source":["Below is an implementation of logistic regression using the functions we derived in class. In this example, we've created a logistic regression class that encapsulates the weights along with all of the functions we need to train and make predictions with the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zFS2_cdtPMxj"},"outputs":[],"source":["def linear_function(X, w):\n","    # Returns a linear function of X (and adds bias)\n","    X = np.pad(X, ((0,0), (0,1)), constant_values=1.)\n","    return np.dot(X, w)\n","\n","def sigmoid(x):\n","    # Computes the sigmoid function\n","    return 1 / (1 + np.exp(-x))\n","\n","class LogisticRegression:\n","    def __init__(self, dims):\n","        '''\n","        Args:\n","            dims (int): d, the dimension of each input\n","        '''\n","        self.weights = np.zeros((dims + 1,))\n","\n","    def predict(self, X):\n","        '''\n","        Predict labels given a set of inputs.\n","\n","        Args:\n","            X (array): An N x d matrix of observations.\n","        Returns:\n","            pred (int array): A length N array of predictions in {0, 1}\n","        '''\n","        return (linear_function(X, self.weights) > 0).astype(int)\n","\n","    def predict_probability(self, X):\n","        '''\n","        Predict the probability of each class given a set of inputs\n","\n","        Args:\n","            X (array): An N x d matrix of observations.\n","        Returns:\n","            probs (array): A length N vector of predicted class probabilities\n","        '''\n","        return sigmoid(linear_function(X, self.weights))\n","\n","    def accuracy(self, X, y):\n","        '''\n","        Compute the accuracy of the model's predictions on a dataset\n","\n","        Args:\n","            X (array): An N x d matrix of observations.\n","            y (array): A length N vector of labels.\n","        Returns:\n","            acc (float): The accuracy of the classifier\n","        '''\n","        return np.mean(self.predict(X) == y)\n","\n","    def nll(self, X, y):\n","        '''\n","        Compute the negative log-likelihood loss.\n","\n","        Args:\n","            X (array): An N x d matrix of observations.\n","            y (int array): A length N vector of labels.\n","        Returns:\n","            nll (float): The NLL loss\n","        '''\n","        xw = linear_function(X, self.weights)\n","        py = sigmoid((2 * y - 1) * xw)\n","        return -np.sum(np.log(py))\n","\n","    def nll_gradient(self, X, y):\n","        '''\n","        Compute the gradient of the negative log-likelihood loss.\n","\n","        Args:\n","            X (array): An N x d matrix of observations.\n","            y (array): A length N vector of labels.\n","        Returns:\n","            grad (array): A length (d + 1) vector with the gradient\n","        '''\n","        xw = linear_function(X, self.weights)\n","        py = sigmoid((2 * y - 1) * xw)\n","        grad = ((1 - py) * (2 * y - 1)).reshape((-1, 1)) * np.pad(X, [(0,0), (0,1)], constant_values=1.)\n","        return -np.sum(grad, axis=0)\n","\n","    def nll_and_grad(self, X, y):\n","        '''\n","        Compute both the NLL and it's gradient\n","\n","        Args:\n","            X (array): An N x d matrix of observations.\n","            y (array): A length N vector of labels.\n","        Returns:\n","            nll (float): The NLL loss\n","            grad (array): A length (d + 1) vector with the gradient\n","        '''\n","        return self.nll(X, y), self.nll_gradient(X, y)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FZdSV5v8PMxk"},"source":["Let's take a look at how to use this class. The provided code includes a function `get_dataset` that downloads and loads one of serval different datasets. For this first example, we will use the humans and horses dataset, a dataset of images of humans and horses. We can load the dataset as follows:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"voE39sMcPMxk"},"outputs":[],"source":["images, labels, label_names = get_dataset('horses_and_humans')"]},{"cell_type":"markdown","metadata":{"id":"aI8PodqiPMxk"},"source":["As we saw in class, before we can use logistic regression on image data, we first need to reshape it from a 3-dimensional array into a 2-dimensional matrix:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQ-O2s2mPMxk"},"outputs":[],"source":["image_shape = images[0].shape                # Keep track of the original image shape\n","X = images.reshape((images.shape[0], -1))    # Reshape into an N x d matrix X\n","y = labels\n","print('Image shape: ', images.shape, ', X shape: ', X.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"uFRiiqXhPMxl"},"source":["We can create a model using the `LogisticRegression` class, specifying the number of features ($d$):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jmom4h5WPMxl"},"outputs":[],"source":["model = LogisticRegression(X.shape[1])"]},{"cell_type":"markdown","metadata":{"id":"RxVn_UwcPMxl"},"source":["We can train the model using the `gradient_descent` function provided in the support code:\n","\n","***Note:** Use this learning rate and number of steps throughout the homework!*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YY32NhJePMxl"},"outputs":[],"source":["losses = gradient_descent(model, X, y, lr=1e-6, steps=2500, image_shape=image_shape, watch=False)\n","\n","# Uncomment to run with a live visualization\n","# losses = gradient_descent(model, X, y, lr=1e-6, steps=500, image_shape=images[0].shape, watch=True)"]},{"cell_type":"markdown","metadata":{"id":"crN5g_wwPMxl"},"source":["We can make predictions using the built-in methods:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aUgyXcE6PMxl"},"outputs":[],"source":["prediction = model.predict(X)\n","probabilities = model.predict_probability(X)\n","\n","# Get the probability of the prediction [p(y=1) if prediction is 1 otherwise p(y=0)]\n","probability_of_prediction = np.where(prediction, probabilities, 1 - probabilities)\n","\n","# Show an image and the corresponding prediction\n","plt.imshow(X[0].reshape(image_shape), cmap='gray')\n","print('Prediction: %s, probability: %.3f' % (label_names[prediction[0]], probability_of_prediction[0]))"]},{"cell_type":"markdown","metadata":{"id":"yTIxfGpoPMxm"},"source":["# Part 1: Logistic regression and feature transforms"]},{"cell_type":"markdown","metadata":{"id":"3dqD_vTrPMxm"},"source":["We'll first evaluate the performance of the logistic regression model above."]},{"cell_type":"markdown","metadata":{"id":"kkxUjf-fPMxm"},"source":["#### **Q1:** Train and test splits"]},{"cell_type":"markdown","metadata":{"id":"RyyQTxecPMxm"},"source":["Write a function to split the provided dataset into a *train set* and a *test set*. The train set should include 70% of the observations and the test set should include the remaining 30%. The data should be randomly shuffled to make sure there is no bias in the ordering."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DwLG6QviPMxm"},"outputs":[],"source":["def split_data(X, y):\n","    ## YOUR CODE HERE\n","\n","    return Xtrain, ytrain, Xtest, ytest\n","\n","# Test the function\n","Xtrain, ytrain, Xtest, ytest = split_data(X, y)\n","test_split(X, y, Xtrain, ytrain, Xtest, ytest)"]},{"cell_type":"markdown","metadata":{"id":"tgpKex11PMxn"},"source":["#### **Q2:** Model evaluation"]},{"cell_type":"markdown","metadata":{"id":"rekDKGcePMxn"},"source":["Using the function you just wrote, train a new logistic regression model on just the training data. Evaluate the **accuracy** and **loss** of the trained model on *both* the **training data** and the **test data**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LhY0IfVsPMxn"},"outputs":[],"source":["## YOUR CODE HERE\n","\n","\n","\n","print('Training accuracy: %.3f, loss: %.3f' % (train_acc, train_loss))\n","print('Test accuracy: %.3f, loss: %.3f' % (test_acc, test_loss))"]},{"cell_type":"markdown","metadata":{"id":"gIn5-QuXPMxn"},"source":["Recall that in class we dicussed *feature transforms* an easy way to get more expressive models, using our linear model tools. Here we'll try applying some basic feature transforms to this problem and see if we can improve the performance."]},{"cell_type":"markdown","metadata":{"id":"xSpwffjxPMxn"},"source":["#### **Q3:** Quadratic feature transforms\n","\n","Create a *transformed* versions of the training and test datasets by adding quadratic features. Only add the unary quadratic terms ($x_i^2$) **not** the cross terms ($x_i x_j$). For a single dimension the transform would look like:\n","$$\\phi(x_i) = \\begin{bmatrix} x_i \\\\ x_i^2 \\end{bmatrix}$$\n","\n","\n","In general, the transform should look like:\n","\n","$$\\textbf{Single observation: }\\phi(\\mathbf{x}) = \\begin{bmatrix}x_1 \\\\ \\vdots \\\\ x_d \\\\ x_1^2 \\\\ \\vdots \\\\ x_d^2 \\end{bmatrix}, \\quad \\textbf{Dataset: } \\phi(\\mathbf{X}) = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1d} & x_{11}^2 & \\dots & x_{1d}^2 \\\\ x_{21} & x_{22} & \\dots & x_{2d} & x_{21}^2 & \\dots & x_{2d}^2 \\\\  \\vdots & \\vdots & & \\vdots & \\vdots & & \\vdots \\\\ x_{N1} & x_{N2} & \\dots & x_{Nd} & x_{N1}^2 & \\dots & x_{Nd}^2 \\\\  \\end{bmatrix} $$\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qExR8k_ePMxn"},"outputs":[],"source":["## YOUR CODE HERE\n","Xtrain_quad =\n","Xtest_quad =\n","\n","assert Xtrain_quad.shape == (Xtrain.shape[0], 2 * Xtrain.shape[1])"]},{"cell_type":"markdown","metadata":{"id":"P9ovm8XMPMxo"},"source":["#### **Q4:** Evaluating quadratic transforms"]},{"cell_type":"markdown","metadata":{"id":"tf61q3EbPMxo"},"source":["Train a **new** logistic regression model and evaluate the training and test accuracy and loss as you did in question 2."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KJQcDX_vPMxo"},"outputs":[],"source":["## YOUR CODE HERE\n","\n","\n","\n","print('Training accuracy: %.3f, loss: %.3f' % (train_acc, train_loss))\n","print('Test accuracy: %.3f, loss: %.3f' % (test_acc, test_loss))"]},{"cell_type":"markdown","metadata":{"id":"1ol5sPGpPMxo"},"source":["#### **Q5:** Evaluating sin transforms\n","\n","Repeat questions 3 & 4, but using a different transform, defined as:\n","\n","$$\\phi(x_i) = \\begin{bmatrix} x_i \\\\ \\sin(10 x_i) \\end{bmatrix}$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g8hnFVFDPMxo"},"outputs":[],"source":["## YOUR CODE HERE\n","Xtrain_sin =\n","Xtest_sin =\n","\n","assert Xtrain_sin.shape == (Xtrain.shape[0], 2 * Xtrain.shape[1])\n","\n","\n","print('Training accuracy: %.3f, loss: %.3f' % (train_acc, train_loss))\n","print('Test accuracy: %.3f, loss: %.3f' % (test_acc, test_loss))"]},{"cell_type":"markdown","metadata":{"id":"0pIerR86PMxp"},"source":["#### **Q6:** Comparing feature transforms\n","\n","Based on the results, would you use any feature transform for this problem? If so, which one?"]},{"cell_type":"markdown","metadata":{"id":"KETu-RrePMxp"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"6Tc14pQSPMxp"},"source":["# Part 2: Multinomial logistic regression"]},{"cell_type":"markdown","metadata":{"id":"uHucRDGxPMxp"},"source":["In this part, we will look at implementing **multinomial logistic regression**. Recall that this model extends logistic regression to the cases where there may be more than 2 possible labels, so $y\\in\\{1,...,C\\}$, where $C$ is the number of *classes* (possible outputs).\n","\n","We saw that rather than having a single weight vector, this model has a weight vector for *each class*, $\\mathbf{w}_1,...,\\mathbf{w}_C$. We can view these together as the *rows* of a weight matrix $\\mathbf{W}$:\n","$$\\mathbf{W} = \\begin{bmatrix} \\mathbf{w}_1^T \\\\ \\mathbf{w}_2^T \\\\ \\vdots \\\\ \\mathbf{w}_C^T \\end{bmatrix}$$\n","\n","We saw that the prediction function for this model was:\n","$$f(\\mathbf{x}) = \\underset{c\\in \\{1,...,C\\}}{\\text{argmax}}\\ \\mathbf{x}^T\\mathbf{w}_c$$\n","\n","The probabilistic model was defined as:\n","$$\n","p(y_i=c \\mid \\mathbf{x}, \\mathbf{W}) = \\text{softmax}(\\mathbf{x}^T\\mathbf{W})_c=\\frac{e^{\\mathbf{x}^T\\mathbf{w}_c}}{\\sum_{j=1}^Ce^{\\mathbf{x}^T\\mathbf{w}_j}}\n","$$\n","\n","The negative log-likelihood loss was defined as:\n","$$\n","\\textbf{NLL}(\\mathbf{W}, \\mathbf{X}, \\mathbf{y})=-\\sum_{i=1}^N \\bigg(\\mathbf{x}_i^T\\mathbf{w}_{y_i}- \\log\\sum_{j=1}^Ce^{\\mathbf{x}_i^T\\mathbf{w}_{j}}\\bigg)\n","$$"]},{"cell_type":"markdown","metadata":{"id":"mOq4reO6PMxp"},"source":["In the next few questions we will create a *modified* version of our logistic regression class that supports multinomal logistic regression. The class definition is below. A few things to note:\n","\n","**1:** We will still assume `y` is an array of `int` in numpy. We can convert an array `y` to an array of `int` with `y = y.astype(int)` and back with `y = y.astype(float)`\n","\n","**2:** Remeber that numpy is **0-indexed**, so our classes will actually be $0$ to $C-1$, ($y\\in \\{0,...,C-1\\}$)\n","\n","**2:** We will assume our weight matrix is a $C \\times d$ matrix as shown below"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ZfB14J_PMxp"},"outputs":[],"source":["class MultinomialLogisticRegression(LogisticRegression):\n","    def __init__(self, classes, dims):\n","        '''\n","        Args:\n","            classes (int): C, the number of possible outputs\n","            dims (int): d, the dimension of each input\n","        '''\n","        self.classes = classes\n","        self.weights = np.zeros((classes, dims + 1,))"]},{"cell_type":"markdown","metadata":{"id":"3H8IEma_PMxp"},"source":["#### **Q7:** Prediction\n","\n","Write a function to make a prediction using the multinomial logistic regression prediction rule above. (assume `self` is the `MultinomialLogisticRegression` object)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3JeFP2RoPMxp"},"outputs":[],"source":["def multiclass_predict(self, X):\n","    '''\n","    Predict labels given a set of inputs.\n","\n","    Args:\n","        X (array): An N x d matrix of observations.\n","    Returns:\n","        pred (int array): A length N array of predictions in {0,...,(C-1)}\n","    '''\n","    W = self.weights\n","\n","    ## YOUR CODE HERE\n","    pred =\n","\n","    return pred\n","\n","## Test the function\n","test_predict(multiclass_predict)\n","\n","## Add it to our class\n","MultinomialLogisticRegression.predict = multiclass_predict"]},{"cell_type":"markdown","metadata":{"id":"EvpeR12OPMxq"},"source":["#### **Q8:** Softmax\n","\n","Implement the softmax function.\n","$$\n","\\text{softmax}(\\mathbf{x})_c = \\frac{e^{x_c}}{\\sum_{j=1}^Ce^{x_j}}, \\quad\n","\\text{softmax}(\\mathbf{x}) = \\begin{bmatrix}\\frac{e^{x_1}}{\\sum_{j=1}^Ce^{x_j}} \\\\ \\frac{e^{x_2}}{\\sum_{j=1}^Ce^{x_j}} \\\\ \\vdots \\\\ \\frac{e^{x_C}}{\\sum_{j=1}^Ce^{x_j}} \\end{bmatrix}\n","$$\n","\n","You function should accept inputs as either a length $C$ vector or as an $N\\times C$ matrix. If the input is a matrix, the softmax function should be applied to each **row** of the matrix."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZXiOe79dPMxq"},"outputs":[],"source":["def softmax(x):\n","    '''\n","    Apply the softmax function to a vector or matrix\n","\n","    Args:\n","        X (array): An N x C matrix of transformed inputs (or a length C vector)\n","    Returns:\n","        probs (array):  An N x C matrix with the softmax function applied to each row\n","    '''\n","    ## YOUR CODE HERE\n","\n","\n","test_softmax(softmax)"]},{"cell_type":"markdown","metadata":{"id":"slBYEj6vPMxq"},"source":["#### **Q9:** Multinomial logistic regression NLL\n","\n","Implement a function to compute the multinomial logistic regression negative log-likelihood.\n","$$\n","\\textbf{NLL}(\\mathbf{W}, \\mathbf{X}, \\mathbf{y})=-\\sum_{i=1}^N \\bigg(\\mathbf{x}_i^T\\mathbf{w}_{y_i}- \\log\\sum_{j=1}^Ce^{\\mathbf{x}_i^T\\mathbf{w}_{j}}\\bigg)\n","$$\n","\n","***Hint:** Recall that $y_i$ is an integer, so $\\mathbf{w}_{j}$ refers to the row of the weight matrix at index $y_i$ (you could access this as `W[y[i]]`). It's possible to answer this question without loops, but you may find it easier to loop over each possible class/observation.*\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I5RKr7wCPMxq"},"outputs":[],"source":["def nll(self, X, y):\n","    '''\n","    Compute the negative log-likelihood loss.\n","\n","    Args:\n","        X (array): An N x d matrix of observations.\n","        y (int array): A length N vector of labels.\n","    Returns:\n","        nll (float): The NLL loss\n","    '''\n","    W = self.weights\n","    ## YOUR CODE HERE\n","\n","    return nll\n","\n","test_nll(nll)\n","MultinomialLogisticRegression.nll = nll"]},{"cell_type":"markdown","metadata":{"id":"8BhLUB3yPMxq"},"source":["#### **Q10:** Gradient of NLL\n","\n","Derive the gradient of the negative log-likelihood with respect to $\\mathbf{w}_c$, the weight vector for a **single class**.\n","\n","***Hint:** Again note that $\\mathbf{w}_{y_i}$ refers to the weight vector corresponding to the **true** class of observation $i$, and so only depends on $\\mathbf{w}_c$ if $y_i=c$. This means that:*\n","$$\\frac{d}{d\\mathbf{w}_c} \\mathbf{x_i}^T \\mathbf{w}_{y_i} = \\begin{cases}\\mathbf{x}_i \\quad \\text{ if } y_i = c \\\\ 0 \\quad \\ \\text{ otherwise}  \\end{cases}$$\n","*We can write this more compactly using an indicator function:*\n","$$\\frac{d}{d\\mathbf{w}_c} \\mathbf{x_i}^T \\mathbf{w}_{y_i} = \\mathbb{I}(y_i=c)\\ \\mathbf{x}_i $$"]},{"cell_type":"markdown","metadata":{"id":"ZdnuyUsCPMxr"},"source":["YOUR ANSWER HERE\n","\n","$$\\nabla_{\\mathbf{w}_c} \\textbf{NLL}(\\mathbf{W}, \\mathbf{X}, \\mathbf{y}) = $$"]},{"cell_type":"markdown","metadata":{"id":"fJKRErNVPMxr"},"source":["#### **Q11:** Implementing the gradient\n","\n","Write a function that computes the gradient of the negative log-likelihood with repect to the weight vector for a given class, using the results of the derivation above."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZgW6PRpjPMxr"},"outputs":[],"source":["def nll_gradient_c(W, X, y, c):\n","    '''\n","    Compute the negative log-likelihood loss.\n","\n","    Args:\n","        W (array): The C x d weight matrix.\n","        X (array): An N x d matrix of observations.\n","        y (int array): A length N vector of labels.\n","        c (int): The class to compute the gradient for\n","    Returns:\n","        grad (array): A length d vector representing the gradient with respect to w_c\n","    '''\n","    ## YOUR CODE HERE\n","\n","    return grad\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YeRT4vOoPMxr"},"source":["#### **Q12:** Implementing the full gradient\n","\n","Using the function you just wrote, write a function to compute the full gradient with respect to the $C \\times d$ weight matrix.\n","***Hint:** The output should be a matrix!*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZD0kzkBBPMxr"},"outputs":[],"source":["def nll_gradient(self, X, y, c):\n","    '''\n","    Compute the negative log-likelihood loss.\n","\n","    Args:\n","        X (array): An N x d matrix of observations.\n","        y (int array): A length N vector of labels.\n","    Returns:\n","        grad (array): A C x d matrix representing the gradient with respect to W\n","    '''\n","    ## YOUR CODE HERE\n","    W = self.weights\n","\n","    return grad\n","\n","test_nll_grad(nll_gradient)\n","MultinomialLogisticRegression.nll_gradient = nll_gradient"]},{"cell_type":"markdown","metadata":{"id":"HVK2vaT7PMxs"},"source":["Note if you are struggling with this problem, you can uncomment the following cell to get a valid gradient function based on your nll function. You can use this for testing to to complete the remaining questions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NDhr7ztfPMxs"},"outputs":[],"source":["'''\n","def autograd_nll_gradient(self, X, y):\n","    import autograd\n","    def autograd_nll(W):\n","        temp_model = MultinomialLogisticRegression(self.classes, X.shape[1])\n","        temp_model.weights = W\n","        return temp_model.nll(X, y)\n","    return autograd.grad(autograd_nll)(self.weights)\n","MultinomialLogisticRegression.nll_gradient = autograd_nll_gradient\n","'''"]},{"cell_type":"markdown","metadata":{"id":"dW7VGfICPMxs"},"source":["Finally, we will test out our multinomial logistic regression classifier on the MNIST dataset (https://en.wikipedia.org/wiki/MNIST_database), one of the most popular datasets in machine learning! We'll start by loading it as before."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BIaQq1JZPMxs"},"outputs":[],"source":["images, labels, label_names = get_dataset('mnist')\n","\n","image_shape = images[0].shape                # Keep track of the original image shape\n","X = images.reshape((images.shape[0], -1))    # Reshape into an N x d matrix X\n","y = labels\n","print('Image shape: ', images.shape, ', X shape: ', X.shape)\n","\n","# Create the initial model\n","model = MultinomialLogisticRegression(classes=len(label_names), dims=X.shape[1])"]},{"cell_type":"markdown","metadata":{"id":"oX3KgGNuPMxs"},"source":["#### **Q13:** Repeat question 2 using the MNIST dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ss8FbM7kPMxs"},"outputs":[],"source":["## YOUR CODE HERE\n","\n","\n","\n","print('Training accuracy: %.3f, loss: %.3f' % (train_acc, train_loss))\n","print('Test accuracy: %.3f, loss: %.3f' % (test_acc, test_loss))"]},{"cell_type":"markdown","metadata":{"id":"N-IuT9vrPMxs"},"source":["#### **Q14:** Visualize the learned weights\n","\n","The Matplotlib function `plt.imshow` (or `ax.imshow` for subplots) will display a matrix as an image as shown earlier in this notebook.\n","\n","Reshape the weight vector for **each of the 10 classes** in your trained model into a $28 \\times 28$ matrix (ignore the last, bias dimension for each). Then plot each weight vector using the `imshow` function.\n","\n","***Hint:** Your weight matrix should be of size $10 \\times 785$*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B6T5RIouPMxs"},"outputs":[],"source":["## YOUR CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"9YW0bl1yPMxt"},"source":["#### **Q15:** Repeat question 3 using the MNIST dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1XtVPwMHPMxt"},"outputs":[],"source":["## YOUR CODE HERE\n","Xtrain_quad =\n","Xtest_quad =\n","\n","assert Xtrain_quad.shape == (Xtrain.shape[0], 2 * Xtrain.shape[1])"]},{"cell_type":"markdown","metadata":{"id":"_tHDNyBSPMxt"},"source":["#### **Q16:** Repeat question 4 using the MNIST dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"__VhO7jMPMxt"},"outputs":[],"source":["## YOUR CODE HERE\n","\n","\n","\n","print('Training accuracy: %.3f, loss: %.3f' % (train_acc, train_loss))\n","print('Test accuracy: %.3f, loss: %.3f' % (test_acc, test_loss))"]},{"cell_type":"markdown","metadata":{"id":"LEVR67hxPMxt"},"source":["#### **Q17:** Repeat question 5 using the MNIST dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M8hcKbU8PMxt"},"outputs":[],"source":["## YOUR CODE HERE\n","Xtrain_sin =\n","Xtest_sin =\n","\n","assert Xtrain_sin.shape == (Xtrain.shape[0], 2 * Xtrain.shape[1])\n","\n","\n","print('Training accuracy: %.3f, loss: %.3f' % (train_acc, train_loss))\n","print('Test accuracy: %.3f, loss: %.3f' % (test_acc, test_loss))"]},{"cell_type":"markdown","metadata":{"id":"uwqQFGO4PMxt"},"source":["#### **Q18:** Final evaluation\n","\n","Based on the results, would you use any feature transform for this problem? If so, which one?"]},{"cell_type":"markdown","metadata":{"id":"OBM8dj4jPMxt"},"source":[]},{"cell_type":"markdown","metadata":{"id":"YE3jCYwXPMxt"},"source":["YOUR ANSWER HERE"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"orig_nbformat":4,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}